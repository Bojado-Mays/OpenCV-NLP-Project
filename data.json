[
 {
  "repo": "Ewenwan/MVision",
  "language": "C++",
  "readme_contents": "# MVision\u3000Machine Vision \u673a\u5668\u89c6\u89c9\n[AI\u7b97\u6cd5\u5de5\u7a0b\u5e08\u624b\u518c \u6570\u5b66\u57fa\u7840 \u7edf\u8ba1\u5b66\u4e60 \u6df1\u5ea6\u5b66\u4e60 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5de5\u5177\u4f7f\u7528](http://www.huaxiaozhuan.com/)\n\n[AI \u5b89\u5168\u6570\u636e\u79d1\u5b66\u548c\u7b97\u6cd5 ](https://github.com/Ewenwan/AI-Security-Learning)\n\n[\u6fb3\u5927\u5229\u4e9a\u673a\u5668\u4eba\u89c6\u89c9\u7814\u7a76\u4e2d\u5fc3](https://www.roboticvision.org/)\n\n[NIPS Neural Information Processing Systems](https://papers.nips.cc/)\n\n[icml Proceedings of Machine Learning Research PMLR](http://proceedings.mlr.press/index.html)\n\n[ICDM IEEE International Conference on Data Mining](http://www.cs.uvm.edu/~icdm/)\n\n[Computer Vision and Pattern Recognition arxiv.org \u6700\u65b0\u63d0\u4ea4\u7684\u8bba\u6587](https://arxiv.org/list/cs.CV/recent)\n\n[papercept \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://controls.papercept.net/conferences/scripts/start.pl)\n\n[easychair \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://easychair.org/my/roles.cgi?welcome=1)\n\n[DBLP \u8ba1\u7b97\u673a\u6838\u5fc3\u6280\u672f\u6587\u732e](https://dblp.uni-trier.de/)\n\n[\u6280\u672f\u5218 \u589e\u5f3a\u73b0\u5b9e\u3001\u56fe\u50cf\u8bc6\u522b\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u4eba](http://liuxiao.org/category/robots/)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0a\uff09](https://cloud.tencent.com/developer/article/1005894)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0b\uff09](https://cloud.tencent.com/developer/article/1005893)\n\n[\u4f18\u79c0\u7684\u535a\u5ba2\u8bba\u6587\u7b14\u8bb0](https://github.com/Ewenwan/antkillerfarm.github.com)\n\n[CSCI 1430: Introduction to Computer Vision \u8ba1\u7b97\u673a\u89c6\u89c9\u8bfe\u7a0b](http://cs.brown.edu/courses/csci1430/#schedule)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u7b97\u6cd5 \u4e66\u7c4d](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf)\n\n[Computer vision:models, learning and inference \u4e66\u7c4d](http://web4.cs.ucl.ac.uk/staff/s.prince/book/book.pdf)\n\n[TLD\uff1atracking-learning-detection \u8ddf\u8e2a\u7b97\u6cd5](https://github.com/Ewenwan/opencv_TLD)\n\n[\u673a\u5668\u4eba\u9009\u4fee\u8bfe](http://www.diag.uniroma1.it/%7Elanari/EIR/)\n\n[Andrew Davison\u7684\u8bfe\u7a0b\uff1a Robotics Lecture Course (course code 333)](http://www.doc.ic.ac.uk/~ajd/Robotics/index.html)\n\n[Simultaneous Localization and Mapping: Part I ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial1.pdf)\n\n[Simultaneous Localization and Mapping: Part II ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial2.pdf)\n\n[\u745e\u58eb\u82cf\u9ece\u4e16\u7406\u5de5\u7684\u5b66\u751f\u7ec3\u4e60](http://www.csc.kth.se/~kootstra/index.php?item=313&menu=300)\n\n[\u4e66\u7c4d Robotics, Vision & Control \u63a8\u8350\uff01\uff01\uff01\uff01](http://petercorke.com/wordpress/)\n\n[Robotics, Vision & Control.PDF \u767e\u5ea6\u7f51\u76d8](https://pan.baidu.com/s/1c1TcEgo)\n\n[Robotics, Vision and Control csdn](https://download.csdn.net/download/u013834525/10169878)\n\n[\u4f18\u8fbe\u5b66\u57ce \u673a\u5668\u4eba\u4eba\u5de5\u667a\u80fd\u8bfe\u7a0b](https://classroom.udacity.com/courses/cs373)\n\n[\u5b66\u4e60\u65e0\u4eba\u9a7e\u9a76\u8f66\uff0c\u4f60\u6240\u5fc5\u987b\u77e5\u9053\u7684](https://zhuanlan.zhihu.com/p/27686577)\n\n[\u5f3a\u5316\u5b66\u4e60\u4ece\u5165\u95e8\u5230\u653e\u5f03\u7684\u8d44\u6599](https://zhuanlan.zhihu.com/p/34918639?utm_source=wechat_session&utm_medium=social&wechatShare=1&from=singlemessage&isappinstalled=0)\n\n[\u53f0\u5927 \u673a\u5668\u5b66\u4e60\u6df1\u5ea6\u5b66\u4e60\u8bfe\u7a0b](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)\n\n[\u65af\u5766\u798fCS231\u8ba1\u7b97\u673a\u89c6\u89c92017](http://www.mooc.ai/course/268/learn?lessonid=1819#lesson/1819)\n\n[\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u793a\u4f8b cs231n\u7b49](https://github.com/autoliuweijie/DeepLearning)\n\n[2018 MIT 6.S094 \u9ebb\u7701\u7406\u5de5\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u4e2d\u6587](http://www.mooc.ai/course/483/notes)\n\n[MIT  \u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u82f1\u6587](https://selfdrivingcars.mit.edu/)\n\n[DeepTraffic](https://selfdrivingcars.mit.edu/deeptraffic/)\n\n[SegFuse: Dynamic Driving Scene Segmentation](https://selfdrivingcars.mit.edu/segfuse/)\n\n[DeepTesla - End-to-End Steering Model](https://selfdrivingcars.mit.edu/deeptesla/)\n\n[\u4e2d\u6587slam\u9996\u9875](http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5)\n\n[ORB-LSD-SVO\u6bd4\u8f83-\u5218\u6d69\u654f_bilibili](https://www.bilibili.com/video/av5934066/)\n\n[LSD\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7136033.html)\n\n[SVO\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7135847.html)\n\n[DSO \u534a\u95f2\u5c45\u58eb \u89e3\u6790](https://zhuanlan.zhihu.com/p/29177540)\n\n[\u8def\u5f84\u89c4\u5212A*\u7b97\u6cd5\u53caSLAM\u81ea\u4e3b\u5730\u56fe\u521b\u5efa\u5bfc\u822a\u7b97\u6cd5](http://www.voidcn.com/article/p-yfjpnwte-tz.html)\n\n[\u51af\u5175\u7684blog slam](http://www.fengbing.net)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801\uff08EKF)](https://github.com/ethz-asl/rovio)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801(\u975e\u7ebf\u6027\u4f18\u5316\uff09](https://github.com/ethz-asl/okvis_ros)\n\n[\u53cc\u76ee\u7acb\u4f53\u5339\u914d](https://wenku.baidu.com/view/08f86102e518964bcf847c6c.html)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e00\u4e9b\u5e93\u6587\u4ef6](https://blog.csdn.net/garfielder007/article/details/50533052)\n\n[\u4eba\u8138\u68c0\u6d4b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/52983093)\n\n[\u884c\u4e3a\u8bc6\u522b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/79504214)\n\n[Free-SpaceEstimation \u65e0\u969c\u788d\u7269\u7a7a\u95f4\u4f30\u8ba1 \u7a20\u5bc6\u5730\u56fe \u6805\u683c\u5730\u56fe \u52a8\u6001\u89c4\u5212 \u9ad8\u5ea6\u5206\u5272 \u8def\u9762\u4fe1\u606f\u63d0\u53d6](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_free_space.pdf)\n\n[2D Object Detection 2d\u76ee\u6807\u68c0\u6d4b RCNN ](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_2D_detection.pdf)\n\n[3D Object Detection 3D\u76ee\u6807\u68c0\u6d4b \u52a8\u673a](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf)\n\n[Semantic Segmentation \u8bed\u4e49\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/07_segmentation.pdf)\n\n[Instance-level Segmentation \u5b9e\u4f8b\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/08_instance.pdf)\n\n[Tracking \u8ddf\u8e2a ]()\n\n[Kalibr calibration toolbox \u6807\u5b9a\u591a\u76ee\u76f8\u673a\u7cfb\u7edf\u3001\u76f8\u673a IMU \u76f8 \u5bf9 \u4f4d \u59ff \u548c \u5377 \u5e18 \u5feb \u95e8 \u76f8 \u673a  ](https://github.com/Ewenwan/kalibr)\n\n[\u970d\u592b\u68ee\u6797(Hough Forest) \u968f\u673a\u68ee\u6797\u548c\u970d\u592b\u6295\u7968\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u7528\u5728\u7269\u4f53\u68c0\u6d4b\uff0c\u8ddf\u8e2a\u548c\u52a8\u4f5c\u8bc6\u522b](https://github.com/Ewenwan/HoughForest)\n\n[\u767e\u5ea6\u81ea\u52a8\u9a7e\u9a76\u5f00\u6e90\u6846\u67b6 apollo](https://github.com/Ewenwan/apollo)\n\n[\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u8bc6\u522b \u6570\u636e\u96c6](https://cg.cs.tsinghua.edu.cn/traffic-sign/)\n\n[Halcon \u4f7f\u7528\u53c2\u8003](https://blog.csdn.net/maweifei/article/details/52613392)\n\n[\u6709\u4ee3\u7801\u7684\u8bba\u6587](https://github.com/Ewenwan/pwc)\n\n[\u56fe\u50cf\u5904\u7406\u57fa\u672c\u7b97\u6cd5\u4ee3\u7801](http://www.cnblogs.com/Imageshop/p/3430742.html)\n\n# \u611f\u8c22\u652f\u6301\n\n![](https://github.com/Ewenwan/EwenWan/blob/master/zf.jpg)\n\n# \u65e0\u4eba\u9a7e\u9a76\u7684\u5404\u4e2a\u65b9\u9762\u77e5\u8bc6\n[\u53c2\u8003](https://blog.csdn.net/qq_40027052/article/details/78485120)\n\n    1. \u611f\u77e5\uff08Perception\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u573a\u666f\u7406\u89e3\u3001\u4ea4\u901a\u72b6\u51b5\u5206\u6790\u3001\u8def\u9762\u68c0\u6d4b\u3001\u7a7a\u95f4\u68c0\u6d4b\u3001\n        \u969c\u788d\u7269\u68c0\u6d4b\u3001\u884c\u4eba\u68c0\u6d4b\u3001\u8def\u6cbf\u68c0\u6d4b\u3001\u8f66\u9053\u68c0\u6d4b\u3002\u8fd8\u6709\u4e00\u4e2a\u6bd4\u8f83\u65b0\u9896\u6709\u8da3\u7684\u662f\u901a\u8fc7\u80ce\u538b\u53bb\u68c0\u6d4b\u9053\u8def\u8d28\u91cf\u3002\n        \u5728\u65e0\u4eba\u9a7e\u9a76\u884c\u4e1a\uff0c\u6709\u4e00\u5957\u901a\u7528\u7684\u6570\u636e\u96c6\u2014\u2014KITTI\u6570\u636e\u96c6\uff0c\u91cc\u9762\u6709\u4e0d\u540c\u7684\u6570\u636e\uff0c\u5305\u62ec\u53cc\u76ee\u89c6\u89c9\u7684\u6570\u636e\u3001\u5b9a\u4f4d\u5bfc\u822a\u7684\u6570\u636e\u7b49\u3002\n        \u7269\u4f53\u68c0\u6d4b\uff08Object Detection\uff09\uff1a\n            \u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u662f\u9488\u5bf9\u56fa\u5b9a\u7269\u4f53\u7684\u68c0\u6d4b\u3002\u4e00\u822c\u7684\u65b9\u6cd5\u662fHOG\uff08 \u65b9\u5411\u68af\u5ea6\u76f4\u65b9\u56fe\uff09\uff0c\u7136\u540e\u518d\u52a0\u4e00\u4e2aSVM\u7684\u5206\u7c7b\u5668\u3002\n            \u800c\u5bf9\u4e8e\u52a8\u6001\u7269\u4f53\u7684\u68c0\u6d4b\uff0c\u4e3b\u8981\u4f7f\u7528\u7684\u662fDPM\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5148\u628a\u624b\u548c\u811a\u8bc6\u522b\u51fa\u6765\uff0c\u518d\u8fdb\u884c\u7ec4\u5408\u3002\n            \u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 RCNN YOLO\n       \u573a\u666f\u5206\u5272\uff08Segmentation\uff09 \u00a0\uff1a\n            \u4eba\u884c\u9053\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u9053\u8def\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u5728\u573a\u666f\u4e2d\u5bf9\u4e0d\u540c\u7684\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u662f\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u95ee\u9898\u3002\n            \u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u91c7\u7528CRF\uff08 \u6761\u4ef6\u968f\u673a\u573a\uff09\uff0c\u57fa\u672c\u539f\u7406\u5728\u4e8e\u56fe\u50cf\u90fd\u662f\u7531\u50cf\u7d20\u70b9\u7ec4\u6210\u7684\uff0c\n            \u82e5\u4e24\u4e2a\u50cf\u7d20\u70b9\u90fd\u6bd4\u8f83\u50cf\u8f66\uff0c\u90a3\u5c31\u628a\u4e8c\u8005\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u5bf9\u8f66\u8f86\u7684\u8bc6\u522b\u3002\n\n            \u8fd0\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5219\u4f7f\u7528\u7684\u662f\u53e6\u4e00\u79cd\u6a21\u578b\uff0c\u88ab\u79f0\u4e3aPSPnet\uff08\u8bed\u4e49\u5206\u5272\uff09\u3002\n            \u8fd9\u662f\u91d1\u5b57\u5854\u578b\u7684\u573a\u666f\u5206\u89e3\u6a21\u578b\uff0c\u5c06\u4e00\u4e2a\u573a\u666f\u4e0d\u65ad\u5730\u538b\u7f29\uff0c\u628a\u7c7b\u4f3c\u7684\u7269\u4f53\u805a\u7c7b\uff0c\u7136\u540e\u518d\u505a\u5224\u65ad\u3002\n       \u53cc\u76ee \u5149\u6d41\uff08Optical Flow\uff09\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff1a\n            \u5149\u6d41\u662f\u9488\u5bf92D\u56fe\u50cf\u6765\u8bf4\u7684\uff0c\u5982\u679c\u8bf4\u4e00\u4e2a\u56fe\u7247\u6d41\u5230\u53e6\u5916\u4e00\u4e2a\u56fe\u7247\uff0c\u90fd\u662f2D\u7684\u7269\u4f53\u79fb\u52a8\uff0c\u90a3\u5c31\u7528\u5149\u6d41\u6765\u505a\u3002\n            \u5982\u679c\u662f3D\u7684\u7269\u4f53\u6d41\u52a8\uff0c\u90a3\u6211\u4eec\u5c31\u7528\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff0c\u573a\u666f\u6d41\u5728\u4f20\u7edf\u7684\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528\u7684\u662fSGBM\uff0c\n            \u5229\u7528\u7684\u662f\u53cc\u76ee\u6210\u50cf\u7684\u6280\u672f\uff0c\u628a\u5de6\u56fe\u548c\u53f3\u56fe\u5408\u8d77\u6765\u63d0\u53d6\u51fa\u7a7a\u95f4\u7684\u70b9\uff0c\u7528\u5149\u6d41\u5728\u4e0a\u9762\u505a\uff0c\u5c31\u80fd\u628a\u573a\u666f\u7684\u6d41\u52a8\u5206\u6790\u51fa\u6765\u3002\n\n            \u5149\u6d41\u4e5f\u53ef\u4ee5\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u6765\u505a\uff0c\u628a\u5de6\u53f3\u4e24\u56fe\u7528\u540c\u6837\u7684\u6a21\u578b\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u7ecf\u8fc7\u8ba1\u7b97\u5c31\u80fd\u5f97\u51fa\u4e00\u4e2a\u6df1\u5ea6\u7684\u4fe1\u606f\u3002\n            \u4f46\u662f\u8fd9\u4e2a\u65b9\u5f0f\u7684\u8ba1\u7b97\u91cf\u975e\u5e38\u5927\u3002\n\n       \u7269\u4f53\u8ffd\u8e2a\uff08Object Tracking\uff09\uff1a \u00a0 \u00a0\n            \u8fd9\u4e5f\u662f\u65e0\u4eba\u9a7e\u9a76\u4e2d\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u6280\u672f\u3002\u5982\u4f55\u9884\u6d4b\u884c\u4eba\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3001\u600e\u4e48\u53bb\u8ddf\u8e2a\u8fd9\u4e2a\u884c\u4eba\uff0c\u4e5f\u6709\u4e00\u7cfb\u5217\u95ee\u9898\u3002\n            \u91cc\u9762\u7528\u5230\u7684\u662f\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e2a\u6280\u672f\u53eb\u505aMDP\uff0c\u8ddf\u8e2a\u4e00\u4e2a\u4eba\uff0c\u968f\u65f6\u8ddf\u8e2a\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u9884\u6d4b\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3002\n            \u4ee5\u4e0a\u5176\u5b9e\u90fd\u662f\u4e00\u4e9b\u4f20\u7edf\u7684\u611f\u77e5\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u5e74\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u5e94\u7528\u4e5f\u975e\u5e38\u5e7f\u6cdb\u3002\n            \n    2. \u8fd0\u52a8\u89c4\u5212\uff08Motion Planning\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u8fd0\u52a8\u89c4\u5212\u3001\u8f68\u8ff9\u89c4\u5212\u3001\u901f\u5ea6\u89c4\u5212\u3001\u8fd0\u52a8\u6a21\u578b\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u901a\u8fc7\u8d5b\u8f66\u6e38\u620f\u53bb\u5b66\u4e60\u57fa\u4e8e\u7f51\u683c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u91cd\u91cf\u7ea7\u8d27\u8f66\u7684\u907f\u969c\u89c4\u5212\uff0c\u666e\u4e16\u7684\u9002\u7528\u4e8e\u65e0\u4eba\u9a7e\u9a76\u7684\u53cc\u8f6e\u6a21\u578b\u7b49\u7b49\u3002\n\n    3. \u9632\u78b0\u649e\uff08CollisionAvoidance\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u8f66\u5185\u7684\u611f\u77e5\u7cfb\u7edf\u4ee5\u53caV2X \u7cfb\u7edf\u53bb\u8f85\u52a9\u9632\u78b0\u649e\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5b9e\u65f6\u5730\u53bb\u8bc4\u4f30\u5f53\u524d\u9a7e\u9a76\u884c\u4e3a\u7684\u5371\u9669\u6027\uff0c\u5982\u4f55\u901a\u8fc7\u5f53\u524d\u9053\u8def\u7684\u62d3\u6251\u53bb\u589e\u5f3a\u81ea\u884c\u8f66\u9a91\u58eb\u7684\u5b89\u5168\u6027\u7b49\u7b49\u3002\n\n    4. \u5730\u56fe\u4e0e\u5b9a\u4f4d\uff08Mapping andLocalization\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7684\u4f20\u611f\u5668\uff0c\u5305\u62ec\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\u3001GNSS\uff0c\u4ee5\u53ca V2X \u53bb\u5efa\u56fe\u4e0e\u5b9a\u4f4d\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u53bb\u5b9a\u4f4d\uff0c\u6bd4\u5982\u5728\u957f\u96a7\u9053\u91cc\u9762\uff0c\u65e2\u6ca1\u6709 GNSS \u4fe1\u53f7\uff0c\u4e5f\u6ca1\u6709\u592a\u597d\u7684\u6fc0\u5149\u6216\u8005\u89c6\u89c9\u7279\u5f81\u7684\u65f6\u5019\u5982\u4f55\u5b9a\u4f4d\u3002\n\n    5. \u5408\u4f5c\u7cfb\u7edf\uff08CooperativeSystems\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u534f\u540c\u591a\u4e2a\u65e0\u4eba\u8f66\u53bb\u5b8c\u6210\u4e00\u4e9b\u4efb\u52a1\uff0c\u6bd4\u5982\u5728\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u51fa\u73b0\u65f6\u5982\u4f55\u8c03\u5ea6\uff0c\n        \u8fd8\u6709\u5c31\u662f\u5f53\u6709\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u505c\u8f66\u573a\u8bd5\u5982\u4f55\u6709\u5e8f\u7684\u505c\u8f66\u3002\n\n    6. \u63a7\u5236\u7b56\u7565\uff08Control Strategy\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5728\u4e0d\u540c\u7684\u7ec6\u5206\u573a\u666f\u4e0b\u7684\u63a7\u5236\u7b56\u7565\uff0c\u6bd4\u5982\u5728\u5341\u5b57\u8def\u53e3\u5982\u4f55\u63a7\u5236\uff0c\u8f6c\u7ebf\u5982\u4f55\u63a7\u5236\uff0c\u5728\u611f\u77e5\u6570\u636e\u4e0d\u53ef\u9760\u65f6\u5982\u4f55\u5c3d\u91cf\u5b89\u5168\u7684\u63a7\u5236\u7b49\u7b49\u3002\n\n    7. \u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff08VehicleDetection and Tracking\uff09\uff1a\n        \u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\uff0c\u4ee5\u53ca\u6beb\u7c73\u6ce2\u96f7\u8fbe\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u5de5\u4f5c\u5305\u62ec\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6df1\u5ea6\u89c6\u89c9\u7684\u7ed3\u5408\u8fdb\u884c\u8f66\u8f86\u8ddf\u8e2a\uff0c\n        \u901a\u8fc7\u5355\u76ee\u89c6\u89c9\u6df1\u5ea6\u5b66\u4e60\u53bb\u5c3d\u91cf\u4f30\u8ba1\u8f66\u4f53\u5927\u5c0f\uff0c\u901a\u8fc7\u4f20\u7edf\u89c6\u89c9\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u53bb\u5224\u65ad\u662f\u5426\u8f66\u4f53\u7b49\u7b49\u3002\n\n    8. \u9759\u6001\u7269\u4f53\u68c0\u6d4b\uff08Static ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u4ee5\u53ca\u6fc0\u5149\u96f7\u8fbe\u53bb\u68c0\u6d4b\u4e00\u4e9b\u9759\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u4ea4\u901a\u706f\u3001\u4ea4\u901a\u6307\u793a\u724c\u3001\u8def\u6cbf\u3001\u8def\u9762\u7b49\u7b49\uff0c\u6bcf\u4e2a\u7269\u4f53\u54c1\u7c7b\u7684\u68c0\u6d4b\u90fd\u662f\u4e00\u4e2a\u7ec6\u5206\u65b9\u5411\u3002\n\n    9. \u52a8\u6001\u7269\u4f53\u68c0\u6d4b\uff08Moving ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u4ee5\u53ca\u4f20\u611f\u5668\u878d\u5408\u7684\u65b9\u6cd5\u53bb\u68c0\u6d4b\u4e00\u4e9b\u52a8\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u884c\u4eba\u3001\u8f66\u8f86\u3001\u81ea\u884c\u8f66\u9a91\u58eb\u7b49\u7b49\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u52a8\u6001\u7269\u4f53\u7684\u52a8\u4f5c\u53bb\u9884\u6d4b\u884c\u4e3a\u3002\n\n    10. \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\uff08Road andIntersection Detection\uff09\uff1a\n        \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\u7531\u4e8e\u5176\u7279\u6b8a\u6027\u4ee5\u53ca\u5bf9\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u88ab\u5355\u72ec\u5217\u51fa\u4f5c\u4e3a\u4e00\u4e2a\u7ec6\u5206\u7684\u5c0f\u65b9\u5411\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u4e00\u4e9b\u7ec6\u5206\u573a\u666f\uff0c\u6bd4\u5982\u5efa\u7b51\u5de5\u5730\u7684\u68c0\u6d4b\u3001\u505c\u8f66\u4f4d\u7684\u68c0\u6d4b\u7b49\u7b49\u3002\n\n    11. \u51b3\u7b56\u7cfb\u7edf\uff08Planning andDecision\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u6bcf\u4e2a\u65e0\u4eba\u8f66\u7684\u52a8\u4f5c\u7684\u51b3\u7b56\uff0c\u6bd4\u5982\u52a0\u901f\u3001\u5239\u8f66\u3001\u6362\u7ebf\u3001\u8d85\u8f66\u3001\u8c03\u5934\u7b49\u7b49\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u5728\u9ad8\u901f\u884c\u9a76\u4e2d\u5982\u4f55\u5b89\u5168\u7684\u6362\u7ebf\uff0c\u5728\u901a\u8fc7\u89c6\u89c9\u7406\u89e3\u4e86\u573a\u666f\u540e\u5982\u4f55\u51b3\u7b56\uff0c\u5728\u611f\u77e5\u4fe1\u606f\u7f3a\u5931\u7684\u65f6\u5019\uff08\u6bd4\u5982\u5728\u96a7\u9053\u91cc\u9762\uff09\u5982\u4f55\u51b3\u7b56\u7b49\u7b49\u3002\n\n    12. \u4e3b\u52a8\u4e0e\u88ab\u52a8\u5b89\u5168\uff08Active andPassive Safety\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u4f20\u611f\u5668\u7684\u611f\u77e5\u53bb\u786e\u4fdd\u65e0\u4eba\u9a7e\u9a76\u4ee5\u53ca\u884c\u4eba\u5b89\u5168\uff0c\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u901a\u8fc7\u5bf9 CAN \u603b\u7ebf\u7684\u5f02\u5e38\u68c0\u6d4b\u53bb\u8bc4\u4f30\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5bf9\u505c\u8f66\u573a\u7684\u89c6\u9891\u76d1\u63a7\u53bb\u8bad\u7ec3\u81ea\u52a8\u6cca\u8f66\u6a21\u578b\u7b49\u7b49\u3002\n\n    13. \u65e0\u4eba\u8f66\u4e0e\u4ea4\u901a\u7684\u4ea4\u4e92\uff08AutonomousVehicles: Interaction with Traffic\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u65e0\u4eba\u8f66\u5982\u4f55\u4e0e\u73b0\u6709\u7684\u4ea4\u901a\u751f\u6001\u5171\u5b58\uff0c\u7279\u522b\u662f\u4f20\u7edf\u8f66\u4e0e\u65e0\u4eba\u8f66\u7684\u5171\u5b58\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec V2X \u865a\u62df\u4ea4\u901a\u6807\u5fd7\uff0c\u901a\u8fc7\u89c6\u89c9\u53bb\u8bc4\u4f30\u65c1\u8fb9\u8f66\u9053\u53f8\u673a\u7684\u9a7e\u9a76\u884c\u4e3a\u7b49\u7b49\u3002\n\n    14. \u89c6\u89c9\u5b9a\u4f4d\uff08SLAM and VisualOdometry\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u7528\u89c6\u89c9\u4e0e\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u89c6\u89c9\u7684\u7ebf\u4e0a\u6821\u51c6\uff0c\u4f7f\u7528\u8f66\u9053\u7ebf\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7b49\u7b49\u3002\n\n    15. \u73af\u5883\u5b66\u4e60\u4e0e\u5efa\u56fe\uff08Mapping andLearning the Environment\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u5efa\u7acb\u7cbe\u51c6\u7684\u73af\u5883\u4fe1\u606f\u56fe\u3002\u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u4f7f\u7528\u4f4e\u7a7a\u65e0\u4eba\u673a\u53bb\u521b\u5efa\u7ed9\u65e0\u4eba\u9a7e\u9a76\u4f7f\u7528\u7684\u5730\u56fe\uff0c\n        \u4ee5\u53ca\u901a\u8fc7\u505c\u8f66\u573a\u76d1\u63a7\u6444\u50cf\u5934\u5efa\u7acb\u8f85\u52a9\u81ea\u52a8\u6cca\u8f66\u7684\u5730\u56fe\u7b49\u7b49\u3002\n\n## \u65e0\u4eba\u9a7e\u9a76\u9762\u8bd5\u77e5\u8bc6\u70b9\n[\u53c2\u8003\u535a\u5ba2](https://blog.csdn.net/xiangxianghehe/article/details/82528180)\n```\n1. \u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\n    \u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u533a\u522b\uff0c\u5404\u81ea\u9002\u7528\u4e8e\u4ec0\u4e48\u95ee\u9898\n    CNN\u57fa\u672c\u539f\u7406\uff0cCNN\u7684\u90a3\u4e9b\u90e8\u5206\u662f\u795e\u7ecf\u5143\n    CNN\u53bb\u6389\u6fc0\u6d3b\u51fd\u6570\u4f1a\u600e\u4e48\u6837\n    \u4ecb\u7ecdYOLO/SSD/RCNN/Faster-RCNN/Mask-RCNN\u7b97\u6cd5\n    YOLO v1/v2/v3 \u533a\u522b\u7ec6\u8282\uff0cSSD\u5982\u4f55\u6539\u8fdb\u6709\u601d\u8003\u8fc7\u561b\uff0c\u77e5\u9053DSSD\u548cFSSD\u561b\n    \u662f\u5426\u4e86\u89e3RPN\uff0cRoI pooling,\u548cRoIAlign\n    YOLO/SSD\u91cc\u9762\u6709\u5168\u8fde\u63a5\u5c42\u561b\n    YOLO/SSD\u7b97\u6cd5\u601d\u60f3\u5982\u4f55\u7528\u5230\u4e09\u7ef4\u70b9\u4e91\u76ee\u6807\u68c0\u6d4b\n    \u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5one-stage\u548ctwo-stage\u533a\u522b\u70b9\u5728\u54ea\u91cc\n    two-stage\u7b97\u6cd5\u76f8\u6bd4\u4e8eone-stage\u6709\u4f55\u4f18\u52bf\n    \u5355\u5f20\u56fe\u7247\u7269\u4f53\u8d8a\u591a\u8d8a\u5bc6\u96c6\uff0cYOLO/SSD/Faster-RCNN\u4e2d\u8ba1\u7b97\u91cf\u662f\u5426\u4e5f\u968f\u7740\u589e\u52a0\n    CVPR/ECCV 2018 \u6700\u65b0\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u6709\u4e86\u89e3\u8fc7\u561b\n    \u5982\u4f55\u7406\u89e3\u4e0a\u91c7\u6837\uff0c\u548c\u4e0b\u91c7\u6837\u7684\u533a\u522b\u662f\u4ec0\u4e48\n    \u4e0a\u91c7\u6837(UNSampling)\u4e0e\u4e0a\u6c60\u5316(UnPooling)\u533a\u522b\n    \u5168\u8fde\u63a5\u5c42\u7406\u8bba\u4e0a\u53ef\u4ee5\u66ff\u4ee3\u5377\u79ef\u5c42\u561b\n    \u795e\u7ecf\u7f51\u7edc\u91cc\u9762\u53ef\u4ee5\u7528\u4ec0\u4e48\u65b9\u6cd5\u66ff\u6362\u6389pooling\n    \u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\u7684\u65b9\u5f0f\u6709\u54ea\u4e9b\n    \u4ecb\u7ecd\u4e0b\u4f60\u4e86\u89e3\u7684\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\n    \u7f51\u7edc\u6a21\u578b\u538b\u7f29\u65b9\u9762\u7684\u526a\u679d\uff0c\u91cf\u5316\u548c\u4e8c\u503c\u5316\u7f16\u7801\n    \u57fa\u4e8e\u89c6\u9891\u7684C3D\u4e09\u7ef4\u7f51\u7edc\u6a21\u578b\u6709\u542c\u8bf4\u8fc7\u561b\n    2.5D\u5377\u79ef\u5462\n    \u4ec0\u4e48\u662f\u7a7a\u6d1e\u5377\u79ef\uff0c\u4ec0\u4e48\u662f\u53cd\u5377\u79ef\uff0c\u4f5c\u7528\u662f\u4ec0\u4e48\n    \u5982\u4f55\u4e00\u5f20RGB\u56fe\u7247\u751f\u6210\u4e09\u7ef4\u6a21\u578b\n    PNG/JPG\u5b58\u50a8\u56fe\u50cf\u7684\u539f\u7406\n    global average pooling \u548caverage pooling\u533a\u522b\n    FPN\u7684\u539f\u7406\uff0c\u4e3a\u4ec0\u4e48\u4e0d\u540c\u5c3a\u5ea6feature map\u878d\u5408\u4f1a\u6709\u6548\u679c\u63d0\u5347\n    \u65e0\u76d1\u7763/\u534a\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6709\u4e86\u89e3\u8fc7\u561b\n    GAN\u7684\u539f\u7406\n    \u57fa\u4e8eRGB\u56fe\u7684\u6df1\u5ea6\u4fe1\u606f\u4f30\u8ba1\u6709\u4e86\u89e3\u8fc7\u561b\n    MobileNet V1/V2\u533a\u522b\n    ShuffleNet\u548cSqueezeNet\n    \u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u6709\u54ea\u4e9b\n    \u53cc\u7ebf\u6027\u63d2\u503c\uff0c\u91cf\u5316\u5bf9\u9f50\n    Relu\u4e3a\u4ec0\u4e48\u6bd4sigmod\u597d\n    \u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u5e38\u7528\u8bc4\u6d4b\u65b9\u5f0f\n    IOU\u548cmAP\uff0cAUC\u548cROC\u5206\u522b\u662f\u4ec0\u4e48\n    \u4ecb\u7ecd\u4e0b\u5e38\u89c1\u635f\u5931\u51fd\u6570\uff0csoftmax\u4e00\u822c\u548c\u54ea\u4e2a\u6fc0\u6d3b\u51fd\u6570\u4f7f\u7528\n    \u4ecb\u7ecd\u4e0bPointNet/PointNet++/VoxelNet\u4ee5\u53ca\u4ed6\u4eec\u7684\u4f18\u7f3a\u70b9\n    PointCNN\u4ecb\u7ecd\u4e00\u4e0b\n    \u65cb\u8f6c\u77e9\u9635\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u6027\u8d28\uff0cPointNet\u4e2dT-Net\u65cb\u8f6c\u77e9\u9635\u7684\u635f\u5931\u51fd\u6570\u5982\u4f55\u8bbe\u8ba1\n    \u5982\u4f55\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\n    \u4ecb\u7ecd\u4e0b\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u53c2\u6570\u7c7b\u7b97\u6cd5\u548c\u975e\u53c2\u6570\u7c7b\u7b97\u6cd5\n    \u968f\u673a\u68af\u5ea6\u4e0b\u964d\n    \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5982\u4f55\u89e3\u51b3\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\n    L1\u6b63\u5219\u5316\u548cL2\u6b63\u5219\u5316\u533a\u522b\uff0c\u5177\u4f53\u6709\u4f55\u7528\u9014\n    L1\u6b63\u5219\u5316\u76f8\u6bd4\u4e8e L2\u6b63\u5219\u5316\u4e3a\u4f55\u5177\u6709\u7a00\u758f\u89e3\n    \n2. C++\u5f00\u53d1\u76f8\u5173\n    c++\u5e38\u89c1\u5bb9\u5668\uff0cvector\u5bb9\u5668capacity\u548csize\u533a\u522b\uff0c\u5982\u4f55\u52a8\u6001\u589e\u957f\n    vector\u904d\u5386\u6709\u54ea\u51e0\u79cd\u65b9\u5f0f\uff08\u5c3d\u53ef\u80fd\u591a\uff09\n    cv:Mat \u6709\u51e0\u79cd\u8bbf\u95ee\u65b9\u5f0f\n    map\u5bb9\u5668\u589e\u5220\u6539\u67e5\uff0c\u548cunorder_map\u533a\u522b\uff0cmap\u5e95\u5c42\u5982\u4f55\u5b9e\u73b0\n    c++\u667a\u80fd\u6307\u9488\n    c++14/17\u65b0\u7279\u6027\n    c++\u548cc\u8bed\u8a00\u533a\u522b\n    c++\u5982\u4f55\u5b9e\u73b0\u591a\u6001\uff0c\u6709\u51e0\u79cd\u65b9\u5f0f\uff0c\u52a8\u6001\u591a\u6001\u548c\u9759\u6001\u591a\u6001\u533a\u522b\n    \u6a21\u677f\u4e86\u89e3\u561b\n    c++\u7ee7\u627f\u591a\u6001\n    c++\u6df1\u62f7\u8d1d\u4e0e\u6d45\u62f7\u8d1d\n    \u62f7\u8d1d\u6784\u9020\u51fd\u6570\u548c\u59d4\u6258\u6784\u9020\u51fd\u6570\n    c++\u9762\u5411\u5bf9\u8c61\n    \u53f3\u503c\u5f15\u7528\uff0cmove\u8bed\u4e49\uff0c\u5b8c\u7f8e\u8f6c\u53d1\n    emplace_back\u548cpush_back\u533a\u522b\n    Eigen\u5e93\u4e86\u89e3\u561b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2ac++\u7684\u5355\u4f8b\u6a21\u5f0f\n    \u5185\u8054\u51fd\u6570\u548c\u5b8f\u7684\u533a\u522b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2a\u53ea\u5728\u5806\u6216\u8005\u6808\u4e0a\u521d\u59cb\u5316\u7684\u7c7b\n    \u5982\u4f55\u67e5\u627e\u5bb9\u5668\u5185\u6240\u6709\u7b26\u5408\u6761\u4ef6\u7684\u5143\u7d20\n    \n3. Python\u5f00\u53d1\u76f8\u5173\n    list tuple\u533a\u522b\n    \u751f\u6210\u5668\u548c\u8fed\u4ee3\u5668\n    Python\u7c7b\u7684\u5b9a\u4e49\u548c\u5b9e\u4f8b\u5316\u65b9\u6cd5\n    \n4. \u6570\u636e\u7ed3\u6784\u76f8\u5173\n    \u7ea2\u9ed1\u6811\u7ed3\u6784\uff0c\u67e5\u627e\u65f6\u95f4\u590d\u6742\u5ea6\n    \u5806\u6392\u5e8f\u7684\u65f6\u95f4\u590d\u6742\u5ea6\n    Top K\u6392\u5e8f\n    \u5982\u4f55\u7528O(1)\u590d\u6742\u5ea6\u67e5\u627e\u5230stack\u91cc\u9762\u7684\u6700\u5c0f\u503c\n    \u516b\u7687\u540e\n    C++\u81ea\u5df1\u5b9e\u73b0\u4e00\u4e2a\u961f\u5217\n    \u6570\u7ec4\u548c\u94fe\u8868\u7684\u533a\u522b\n    \u4ec0\u4e48\u662fkd-tree\uff0c\u5982\u4f55\u5b9e\u73b0\n    \u9752\u86d9\u8df3\u53f0\u9636\u7684\u9012\u5f52\u548c\u975e\u9012\u5f52\u5b9e\u73b0\n    \n5. \u64cd\u4f5c\u7cfb\u7edf\u76f8\u5173\n    \u5982\u4f55\u8c03\u8bd5\u6808\u6ea2\u51fa\n    \u8ba1\u7b97\u673a\u5185\u5b58\u5806\u548c\u6808\u7684\u533a\u522b\n    \u7ebf\u7a0b\u540c\u6b65\u7684\u65b9\u5f0f\uff0c\u4e92\u65a5\u9501\u548c\u4fe1\u53f7\u91cf\u7684\u5bf9\u6bd4\n    \u8fdb\u7a0b\u548c\u7ebf\u7a0b\u7684\u533a\u522b\n    \u56fe\u7247\u5b58\u50a8\u539f\u7406\u4ecb\u7ecd\u4e00\u4e0b\n    \n6. \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u5173\n    Tensorflow\u7ed3\u6784\u6846\u67b6\uff0c\u5982\u4f55\u7528Tensorflow\u5b9e\u73b0\u4e00\u4e2a\u53cd\u5411\u6c42\u68af\u5ea6\n    Tensorflow\u5982\u4f55\u5408\u5e76\u4e24\u4e2aTensor\n    caffe\u548cPytorch\u4e86\u89e3\u561b\n    caffe\u548cTensorflow\u533a\u522b\u5728\u4ec0\u4e48\u5730\u65b9\n    Tensorflow serving\u548cTensorRT\u6709\u4e86\u89e3\u8fc7\u561b\n    caffe\u7ed3\u6784\u6846\u67b6\n    \n7. \u89c6\u89c9SLAM\u76f8\u5173\n    SLAM\u4e3b\u8981\u5206\u4e3a\u54ea\u51e0\u4e2a\u6a21\u5757\n    ORB-SLAM2\u7684\u4f18\u7f3a\u70b9\u5206\u6790\uff0c\u5982\u4f55\u6539\u8fdb\n    ORB\u548cFAST\u5bf9\u6bd4\n    BA\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\n    ORB-SLAM2\u7684\u4e09\u4e2a\u7ebf\u7a0b\u662f\u4ec0\u4e48\n    ORB-SLAM2\u7684\u5b9a\u4f4d\u5982\u4f55\u5b9e\u73b0\n    \u5982\u4f55\u7406\u89e3ORB-SLAM2\u7684\u56fe\u4f18\u5316\n    \u7ed3\u6784\u5149\u3001TOF\u3001\u53cc\u76ee\u89c6\u89c9\u539f\u7406\n    \u76f4\u63a5\u6cd5\u3001\u534a\u76f4\u63a5\u6cd5\u3001\u7279\u5f81\u70b9\u6cd5\u533a\u522b\u4e0e\u8054\u7cfb\n    Apollo\u7684\u611f\u77e5\u6a21\u5757\u539f\u7406\n    Apollo\u76842D\u548c3D\u8ddf\u8e2a\n    \u5982\u4f55\u6c42\u89e3\u65cb\u8f6c\u77e9\u9635\n    \u5982\u679c\u53ea\u670932\u7ebf\u96f7\u8fbe\uff0c\u4e2a\u6570\u4e0d\u9650\uff0c\u80fd\u5b9e\u73b0360\u5ea6\u89c6\u89d2\u8986\u76d6\u5417\uff0c\u5982\u4f55\u5b9e\u73b0\uff0c64\u7ebf\u5462\uff1f\n```\n\n##  \u516c\u53f8\n[\u89c6\u89c9\u9886\u57df\u7684\u90e8\u5206\u56fd\u5185\u516c\u53f8](http://www.ipcv.org/cvcom/)\n###  \u521d\u521b\u516c\u53f8\uff1a\n[\u56fe\u666e\u79d1\u6280](http://www.tuputech.com/)---[Face++](http://www.faceplusplus.com.cn/)---[Linkface](http://www.linkface.cn/index.html)---[Minieye](http://www.minieye.cc/cn/)---[\u77e5\u56feCogtu](http://www.cogtu.com/?lang=zh)---[\u5546\u6c64\u79d1\u6280Sensetime](http://www.sensetime.com/cn)---[\u4eae\u98ce\u53f0Hiscene](http://www.hiscene.com/)---[\u638c\u8d62\u79d1\u6280](http://www.zhangying.mobi/index.html)---[\u683c\u7075\u6df1\u77b3DeepPG](http://www.deepglint.com/)---[\u51cc\u611f\u79d1\u6280usens](http://www.lagou.com/gongsi/j114187.html)---[\u56fe\u68eeTuSimple](http://www.tusimple.com/)---[\u4e2d\u79d1\u89c6\u62d3Seetatech(\u5c71\u4e16\u5149)](http://www.seetatech.com/)---[\u7b2c\u56db\u8303\u5f0f](https://www.4paradigm.com/product/prophet)\n\n### \u4e0a\u5e02\u516c\u53f8\uff1a\n[\u767e\u5ea6DL\u5b9e\u9a8c\u5ba4](http://idl.baidu.com/)---[\u817e\u8baf\u4f18\u56fe](http://youtu.qq.com/)---[\u963f\u91cc\u9ad8\u5fb7](http://www.newsmth.net/nForum/#!article/Career_Upgrade/429476)---[\u66b4\u98ce\u9b54\u955c](http://www.newsmth.net/nForum/#!article/Career_PHD/225254)---[\u641c\u72d7](http://www.newsmth.net/nForum/#!article/Career_PHD/224449)---[\u4e50\u89c6tv](http://www.newsmth.net/nForum/#!article/Career_PHD/222651)---[\u5947\u864e360](http://www.newsmth.net/nForum/#!article/Career_PHD/222379)---[\u4eac\u4e1c\u5b9e\u9a8c\u5ba4](http://www.newsmth.net/nForum/#!article/Career_PHD/223133/a>)---[\u963f\u91cc\u5df4\u5df4](http://www.newsmth.net/nForum/#!article/Career_PHD/222007)---[\u8054\u60f3\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/220225)---[\u534e\u4e3a\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/225976)\n\n### \u77e5\u540d\u5916\u4f01\uff1a\n[\u4f73\u80fd\u4fe1\u606f](http://www.newsmth.net/nForum/#!article/Career_PHD/222548)---[\u7d22\u5c3c\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/223437)---[\u5bcc\u58eb\u901a\u7814\u53d1\u4e2d\u5fc3](http://www.newsmth.net/nForum/#!article/Career_PHD/220654)---[\u5fae\u8f6f\u7814\u7a76\u9662](https://careers.microsoft.com/?rg=cn)---[\u82f1\u7279\u5c14\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/221175)---[\u4e09\u661f\u7814\u7a76\u9662](http://www.yingjiesheng.com/job-001-742-124.html)\n\n\n\n## 0 \u8ba1\u7b97\u6444\u5f71\u3000\u6444\u5f71\u51e0\u4f55\n[\u8ba1\u7b97\u6444\u5f71\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/cp-lecture/)\n[\u76f8\u673a\u5185\u90e8\u56fe\u50cf\u5904\u7406\u6d41\u7a0b](http://www.comp.nus.edu.sg/~brown/ICIP2013_Brown.html)\n[pdf](http://www.comp.nus.edu.sg/~brown/ICIP2013_Tutorial_Brown.pdf)\n\n    \u76f8\u673a = \u5149\u6d4b\u91cf\u88c5\u7f6e(Camera = light-measuring device)\n        \u7167\u660e\u5149\u6e90(Illumination source)\uff08\u8f90\u5c04(radiance)\uff09 --> \n        \u573a\u666f\u5143\u7d20(Scene Element)   --->\n        \u6210\u50cf\u7cfb\u7edf(Imaging System)  --->\n        \u5185\u90e8\u56fe\u50cf\u5e73\u9762(Internal Image Plane) --->\n        \u8f93\u51fa\uff08\u6570\u5b57\uff09\u56fe\u50cf(Output (digital) image) \n    \u56fe\u50cf = \u8f90\u5c04\u80fd\u91cf\u6d4b\u91cf(Image = radiant-energy measurement)  \n    \n    \n    \u73b0\u4ee3\u6444\u5f71\u6d41\u6c34\u7ebf\u3000Modern photography pipeline \n    \u573a\u666f\u8f90\u5c04\u3000--->\u3000\u76f8\u673a\u524d\u7aef(\u955c\u5934\u8fc7\u6ee4\u5668 \u955c\u5934Lens \u5feb\u95e8Shutter \u5b54\u5f84)\u3000--->\u3000\n    \u76f8\u673a\u5185\u90e8(ccd\u54cd\u5e94response\uff08RAW\uff09 CCD\u63d2\u503cDemosaicing \uff08\u539f\uff09)\u3000--->\u3000\n    \u76f8\u673a\u540e\u7aef\u5904\u7406(\u76f4\u65b9\u56fe\u5747\u8861Hist equalization\u3001\u7a7a\u95f4\u626d\u66f2Spatial warping)--->\u3000\u8f93\u51fa\n    \n\n\n    \u900f\u8fc7\u68f1\u955c\u7684\u767d\u5149 \u3000\u201cWhite light\u201d through a prism  ------> \u6298\u5c04\u5149(Refracted light)----> \u5149\u8c31 Spectral \u3000\n    \u6211\u4eec\u7684\u773c\u775b\u6709\u4e09\u4e2a\u53d7\u4f53\uff08\u9525\u7ec6\u80de\uff09\uff0c\u5b83\u4eec\u5bf9\u53ef\u89c1\u5149\u4f5c\u51fa\u53cd\u5e94\u5e76\u4ea7\u751f\u989c\u8272\u611f\u3002\n    \n[CSC320S: Introduction to Visual Computing \u89c6\u89c9\u8ba1\u7b97\u5bfc\u8bba ](http://www.cs.toronto.edu/~kyros/courses/320/)\n\n[Facebook surround 360 \u300a\u5168\u666f\u56fe\u62fc\u63a5\u300b](https://github.com/facebook/Surround360)\n\n        \u8f93\u5165\uff1a17\u5f20raw\u56fe\u50cf\uff0c\u5305\u62ec14\u5f20side images\u30012\u5f20top images\u30011\u5f20bottom image\n        \u8f93\u51fa\uff1a3D\u7acb\u4f53360\u5ea6\u5168\u666f\u56fe\u50cf  \n[\u535a\u5ba2\u7b14\u8bb0](https://blog.csdn.net/electech6/article/details/53618965)   \n        \n\n[\u6df1\u5ea6\u6444\u5f71\u98ce\u683c\u8f6c\u6362 Deep Photo Style Transfer](https://github.com/luanfujun/deep-photo-styletransfer)\n\n### \u56fe\u50cf\u5f62\u53d8 Image warping\n[\u53c2\u8003](http://www.ipcv.org/image-warping/)\n### \u8272\u5f69\u589e\u5f3a/\u8f6c\u6362\u3000Color transfer\n[\u53c2\u8003](http://www.ipcv.org/colortransfer/)\n### \u56fe\u50cf\u4fee\u8865 Image repair\n[\u53c2\u8003](http://www.ipcv.org/imagerepair/)\n### \u56fe\u50cf\u53bb\u566a Image denoise\n[\u53c2\u8003](http://www.ipcv.org/imagedenoise/)\n### \u56fe\u50cf\u53bb\u6a21\u7cca Image deblur \n[\u53c2\u8003](http://www.ipcv.org/imagedeblur/)\n###  \u56fe\u50cf\u6ee4\u6ce2 Image filter\n[\u53c2\u8003](http://www.ipcv.org/imagefilter/)\n\n###  \u8d85\u5206\u8fa8\u7387 Super-resolution\n[\u53c2\u8003](http://www.ipcv.org/code-superresolution/)  \n\n\n## 1\u3000\u4e09\u7ef4\u91cd\u5efa 3D Modeling\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/3dd/)\n### \u76f8\u673a\u77eb\u6b63\u3000Camera calibration\n[\u53c2\u8003](http://www.ipcv.org/poseestimation/)\n### \u975e\u521a\u4f53\u91cd\u5efa\u3000Non-rigid modeling\u3000\n[\u53c2\u8003](http://www.ipcv.org/nonnigitreco/)\n### \u4e09\u7ef4\u91cd\u6784 3D modeling\n[\u53c2\u8003](http://www.ipcv.org/3dmodeling/)\n[\u89c6\u89c9SLAM](http://www.ipcv.org/on-visual-slam/)\n\n[Self-augmented Convolutional Neural Networks](https://github.com/msraig/self-augmented-net)\n\n[\u8fd0\u52a8\u4f30\u8ba1 motion estimation](http://www.ipcv.org/on-motion-estimation/)\n\n[\u9762\u90e8\u53d8\u5f62\u3000face morphing\u3000](http://www.ipcv.org/about-face-morphing/)\n\n[\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u7684\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/people-3d-modeling/)\n\n\n## 2  \u5339\u914d/\u8ddf\u8e2a Matching & Tracking\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/tracking/)\n\n### 2.a \u7279\u5f81\u63d0\u53d6 Feature extraction\n[\u53c2\u8003](http://www.ipcv.org/featureextraction/)\n\n### 2.b \u7279\u5f81\u5339\u914d Feature matching\n[\u53c2\u8003](http://www.ipcv.org/code-featmatching/)\n\n### 2.c \u65f6\u7a7a\u5339\u914d Space-time matching\n[\u53c2\u8003](http://www.ipcv.org/code-spacetimematching/)\n\n### 2.d \u533a\u57df\u5339\u914d Region matching\n[\u53c2\u8003](http://www.ipcv.org/code-regionmatching/)\n\n### 2.e \u8f6e\u5ed3\u5339\u914d Contour matching\n[\u53c2\u8003](http://www.ipcv.org/code-coutourmatching/)\n\n### 2.f \u7acb\u4f53\u5339\u914d Stereo matching \n[\u53c2\u8003](http://www.ipcv.org/code-stereomatching/)\n\n[\u53cc\u76ee\u89c6\u89c9\u81ea\u52a8\u9a7e \u573a\u666f\u7269\u4f53\u8ddf\u8e2apaper](http://www.cvlibs.net/publications/Menze2015CVPR.pdf)\n\n[kitti\u53cc\u76ee\u6570\u636e\u96c6\u89e3\u51b3\u65b9\u6848](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo)\n\n[\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5c062D\u7535\u5f71\u8f6c\u6362\u4e3a3D\u7535\u5f71\u7684\u8f6c\u6362](https://github.com/piiswrong/deep3d)\n\n[\u795e\u7ecf\u7f51\u7edc\u3000\u53cc\u76ee\u5339\u914d](https://github.com/jzbontar/mc-cnn)\n\n\n[\u4e2d\u5c71\u5927\u5b66\u5f20\u5f1b\u535a\u58eb](http://chizhang.me/)\n\n    MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation\n    1\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\uff08Stereo Matching\uff09\u95ee\u9898\u7684\u5b9a\u4e49\u53ca\u5176\u4e0e\u4eba\u773c\u611f\u77e5\u6df1\u5ea6\u7684\u5173\u7cfb\uff1b\n    2\u3001\u5bf9Matching Cost Volume\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u5206\u6790\uff0c\u4ee5\u671f\u671b\u8fbe\u5230\u542c\u8005\u5bf9\u5176\u7684\u76f4\u89c2\u4e14\u672c\u8d28\u7684\u7406\u89e3\uff1b\n    3\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\u95ee\u9898\u4e2d\u7684\u56db\u4e2a\u7ecf\u5178\u65b9\u6cd5\uff08\n        Graph Cut\uff0cAdaptive Support Weight Aggregation, \n        Semi-Global Matching, \n        \u4ee5\u53ca PatchMatch Stereo\uff09\uff1b\n    4\u3001\u8ba8\u8bba\u4e86MeshStereo\u7684\u8bd5\u56fe\u7edf\u4e00disparity\u6c42\u89e3\u4ee5\u53ca\u7f51\u683c\u751f\u6210\u4e24\u4e2a\u6b65\u9aa4\u7684motivation\uff0c\n        \u4ee5\u53caformulate\u8fd9\u6837\u4e00\u4e2aunified model\u4f1a\u9047\u5230\u7684\u56f0\u96be\uff1b\n    5\u3001\u8ba8\u8bba\u4e86MeshStereo\u5f15\u5165splitting probability\u7684\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u4f18\u5316\u8fc7\u7a0b\u3002\n    \n    Webinar\u6700\u540e\u5c55\u793a\u4e86MeshStereo\u5728\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca\u65b0\u89c6\u89d2\u6e32\u67d3\u4e24\u4e2a\u4efb\u52a1\u4e2d\u7684\u7ed3\u679c\u3002\n\n\n[Stereo Matching Using Tree Filtering non-local\u7b97\u6cd5\u5728\u53cc\u76ee\u7acb\u4f53\u5339\u914d\u4e0a\u7684\u5e94\u7528 ](https://blog.csdn.net/wsj998689aa/article/details/45584725)\n\n\n\n\n### 2.g \u6df1\u5ea6\u5339\u914d depth matching \n[\u6df1\u5ea6\u5339\u914d depth matching ](http://www.ipcv.org/on-depth-matching/)\n\n### 2.h \u59ff\u6001\u8ddf\u8e2a Pose tracking \n[\u53c2\u8003](http://www.ipcv.org/code-posetracking/)\n\n### 2.i \u7269\u4f53\u8ddf\u8e2a Object tracking\n[\u53c2\u8003](http://www.ipcv.org/code-objtracking/)\n\n### 2.j \u7fa4\u4f53\u5206\u6790 Crowd analysis\n[\u53c2\u8003](http://www.ipcv.org/code-crowdanalysis/)\n[\u7fa4\u4f53\u8fd0\u52a8\u5ea6\u91cf](https://github.com/metalbubble/collectiveness)\n\n### 2.k \u5149\u6d41\u573a\u8ddf\u8e2a Optical flow\n[\u53c2\u8003](http://www.ipcv.org/code-opticalflow/)\n\n\n## 3 \u8bed\u4e49/\u5b9e\u4f8b\u5206\u5272&\u89e3\u6790\u3000Segmentation & Parsing\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/parsing/)\n### 3.a \u89c6\u9891\u5206\u5272  Video  segmentation\n[\u53c2\u8003](http://www.ipcv.org/video-segmentation/)\n\n### 3.b \u4eba\u4f53\u89e3\u6790  Person parsing\n[\u53c2\u8003](http://www.ipcv.org/code-poseparsing/)\n\n[person parsing](http://www.ipcv.org/about-person-parsing/)\n\n### 3.c \u573a\u666f\u89e3\u6790  Scene  parsing\n[scene parsing](http://www.ipcv.org/about-scene-parsing/)\n[\u53c2\u8003](http://www.ipcv.org/code-sceneparsing/)\n\n### 3.d \u8fb9\u7f18\u68c0\u6d4b  Edge   detection\n[\u53c2\u8003](http://www.ipcv.org/code-edgedetection/)\n[\u8fb9\u7f18\u68c0\u6d4b](http://www.ipcv.org/on-edge-detection/)\n\n\n### 3.e \u56fe\u50cf\u7269\u4f53\u5206\u5272 Image object segmentation \n[\u53c2\u8003](http://www.ipcv.org/code-imobjseg/)\n \n### 3.f \u89c6\u9891\u7269\u4f53\u5206\u5272 Video object segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-viobseg/)\n[object segmentation](http://www.ipcv.org/about-video-object-segmentation/)\n\n\n### 3.g \u4ea4\u4e92\u5f0f\u5206\u5272   Interactive segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-intseg/)\n\n### 3.h \u5171\u5206\u5272      Co-segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-cosegmentation/)\n\n### 3.i \u80cc\u666f\u5dee      Background subtraction \n[\u53c2\u8003](http://www.ipcv.org/code-backsub/)\n\n### 3.j \u56fe\u50cf\u5206\u5272\u65b9\u9762 Image segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-imgseg/)\n \n\n## 4 \u8bc6\u522b/\u68c0\u6d4b\u3000Recognition & Detection\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/detect/)\n###  4.a \u5176\u4ed6\u8bc6\u522b Other recognition\n[\u53c2\u8003](http://www.ipcv.org/otherrecog/)\n\n### 4.b \u56fe\u50cf\u68c0\u7d22 Image retrieval\n[\u53c2\u8003](http://www.ipcv.org/%e5%9b%be%e5%83%8f%e6%a3%80%e7%b4%a2/)\n\n### 4.c \u663e\u8457\u68c0\u6d4b Saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saldetection/)\n\n### 4.d \u901a\u7528\u7269\u4f53\u68c0\u6d4b Object proposal\n[\u53c2\u8003](http://www.ipcv.org/code-objproposal/)\n\n### 4.e \u884c\u4e3a\u8bc6\u522b Action recognition\n[\u53c2\u8003](http://www.ipcv.org/code-actionrecogntion/)\n\n### 4.f \u7269\u4f53\u8bc6\u522b Object recognition\n[\u53c2\u8003](http://www.ipcv.org/code-objrecogntion/)\n\n### 4.g \u884c\u4eba\u68c0\u6d4b Human detection\n[\u53c2\u8003](http://www.ipcv.org/code-humandetection/)\n\n### 4.h \u4eba\u8138\u89e3\u6790 Face Parsing\n[\u53c2\u8003](http://www.ipcv.org/code-facerecog/)\n\n### 4.i \u7eb9\u7406\u5206\u6790 Texture Analysis\n[\u7eb9\u7406\u5206\u6790 Texture Analysis](http://www.ipcv.org/on-texture-analysis/)\n[\u76f8\u5173\u4eba\u7269](http://www.ipcv.org/people-reidentity/)\n\n## 5 \u673a\u5668\u5b66\u4e60 Maching Learning\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/ml/)\n\n### 5.a \u751f\u6210\u5bf9\u6297\u7f51\u7edc GAN Generative Adversarial Networks\n[\u53c2\u8003](http://www.ipcv.org/adversarial-networks/)\n\n### 5.b \u6df1\u5ea6\u5b66\u4e60    Deep learning \n[\u53c2\u8003](http://www.ipcv.org/deeplearning/)\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/lecture-deeplearning/)\n\n[CNN Models \u5377\u79ef\u7f51\u7edc\u6a21\u578b](http://www.ipcv.org/on-object-detection/)\n\n[Deep Learning Libraries\u3000\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u5e93](http://www.ipcv.org/deep_learning_libraries/)\n\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/dl-researcher/)\n\n### 5.c \u80fd\u91cf\u4f18\u5316    Energy optimization \n[\u53c2\u8003](http://www.ipcv.org/energyopt/)\n\n### 5.d \u6a21\u578b\u8bbe\u8ba1    Model design\n[\u53c2\u8003](http://www.ipcv.org/modelbuilding/)\n\n### 5.e \u7a7a\u95f4\u964d\u7ef4    Dimention reduction \n[\u53c2\u8003](http://www.ipcv.org/dimention/)\n\n### 5.f \u805a\u7c7b       Clustering \n[\u53c2\u8003](http://www.ipcv.org/clustering/)\n\n### 5.g \u5206\u7c7b\u5668     Classifier\n[\u53c2\u8003](http://www.ipcv.org/classifier/)\n\n## 6 \u5f00\u6e90\u5e93\u3000Open library\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/lib/)\n###\n###\n\n\n## 7 \u6570\u636e\u96c6\u3000Public dataset\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/dataset/)\n### 7.a \u5176\u4ed6\u65b9\u9762 Other datasets\n[\u53c2\u8003](http://www.ipcv.org/otherdb/)\n[\u4eba\u8138\u68c0\u6d4bFace tracking and recognition database ](http://seqam.rutgers.edu/site/index.php?option=com_content&view=article&id=65&Itemid=76)\n\n[\u4eba\u8138\u68c0\u6d4bCaltech 10,000 Web Faces](http://vision.caltech.edu/archive.html)\n\n[\u4eba\u8138\u68c0\u6d4bHelen dataset](http://www.ifp.illinois.edu/~vuongle2/helen/)\n\n[\u6df1\u5ea6\u56fe RGB-D dataset](http://mobilerobotics.cs.washington.edu/projects/kdes/)\n\n[\u89c6\u9891\u5206\u5272 2010 ECCV Efficient Hierarchical Graph Based Video Segmentation](http://www.cc.gatech.edu/cpl/projects/videosegmentation/)\n\n[\u624b\u52bf\u8ddf\u8e2a Hand dataset](https://engineering.purdue.edu/RVL/Database.html)\n\n[\u624b\u52bf\u8ddf\u8e2a2](http://www.robots.ox.ac.uk/~vgg/research/hands/index.html)\n\n[\u8f66\u8f86\u68c0\u6d4b 2002 ECCV Learning a sparse representation for object detection](http://cogcomp.cs.illinois.edu/Data/Car/)\n\n### 7.b \u4eba\u4f53\u68c0\u6d4b dataset on human annotation\n[\u53c2\u8003](http://www.ipcv.org/humandetection/)\n[Caltech Pedestrian Detection Benchmark](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n\n[Ethz](http://www.vision.ee.ethz.ch/~aess/dataset/)\n[bleibe](http://www.vision.ee.ethz.ch/~bleibe/data/datasets.html)\n\n[RGB-D People Dataset](http://www.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)\n\n[TUD Campus](http://www.d2.mpi-inf.mpg.de/tud-brussels)\n[382](https://www.d2.mpi-inf.mpg.de/node/382)\n\n[PSU HUB Dataset](http://www.cse.psu.edu/~rcollins/software.html)\n\n[Pedestrian parsing](http://vision.ics.uci.edu/datasets/)\n\n[Human Eva](http://vision.cs.brown.edu/humaneva/)\n\n\n### 7.c \u7269\u4f53\u8bc6\u522b dataset on object recognition\n\n[\u53c2\u8003](http://www.ipcv.org/objectrecognition/)\n\n[ e-Lab Video Data Set](https://engineering.purdue.edu/elab/eVDS/)\n\n[Image Net](http://www.image-net.org/)\n\n[Places2 Database](http://places2.csail.mit.edu)\n\n[Microsoft CoCo: Common Objects in Context](http://mscoco.org/)\n\n[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n[MIT\u2019s Place2]( http://places2.csail.mit.edu/)\n\n### 7.d \u663e\u8457\u68c0\u6d4b\u65b9\u9762 dataset on saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saliencydetection/)\n\n[\u89c6\u89c9\u663e\u8457\u6027\u68c0\u6d4b\u6280\u672f\u53d1\u5c55\u60c5\u51b5](http://blog.csdn.net/anshan1984/article/details/8657176)\n\n[2012 ECCV Salient Objects Dataset (SOD)](http://elderlab.yorku.ca/SOD/)\n\n[2012 ECCV Neil D. B. Bruce Eye Tracking Data](http://cs.umanitoba.ca/~bruce/datacode.html)\n\n[2012 ECCV DOVES:A database of visual eye movements](http://live.ece.utexas.edu/research/doves/)\n\n[2012 ECCV MSRA:Salient Object Database](http://research.microsoft.com/en-us/um/people/jiansun/SalientObject/salient_object.htm)\n\n[2012 ECCV NUS: Predicting Saliency Beyond Pixels](http://www.ece.nus.edu.sg/stfpage/eleqiz/predicting.html)\n\n[2012 ECCV saliency benchmark](http://people.csail.mit.edu/tjudd/SaliencyBenchmark/index.html)\n\n[2010 ECCV The DUT-OMRON Image Dataset](http://ice.dlut.edu.cn/lu/DUT-OMRON/Homepage.htm)\n\n[2010 ECCV An eye fixation database for saliency detection in images](http://mmas.comp.nus.edu.sg/NUSEF.html)\n\n### 7.e \u884c\u4e3a\u8bc6\u522b dataset on action recognition\n[\u53c2\u8003](http://www.ipcv.org/actionrecognition/)\n\n[UCF](http://www.cs.ucf.edu/~liujg/YouTube_Action_dataset.html)\n[ChaoticInvariants](http://www.cs.ucf.edu/~sali/Projects/ChaoticInvariants/index.html)\n[datasetsActions](http://vision.eecs.ucf.edu/datasetsActions.html)\n\n[Hollywood Human Actions dataset](http://www.di.ens.fr/~laptev/download.html)\n[data](http://lear.inrialpes.fr/data)\n\n[Weizmann: Actionsas Space-Time Shapes](http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html)\n\n[KTH](http://www.nada.kth.se/cvap/actions/)\n\n[UMD](http://www.umiacs.umd.edu/~zhuolin/Keckgesturedataset.html)\n\n[HMDB: A Large Video Database for Human Motion Recognition](http://serre-lab.clps.brown.edu/resources/HMDB/related_data/)\n\n[Collective Activity Dataset](http://www.eecs.umich.edu/vision/activity-dataset.html)\n\n[MSR Action Recognition Datasets and Codes](http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/default.htm)\n\n[Visual Event Recognition in Videos](http://vc.sce.ntu.edu.sg/index_files/VisualEventRecognition/VisualEventRecognition.html)\n\n\n\n### 7.f \u7269\u4f53\u5206\u5272 dataset on object segmentation\n[\u53c2\u8003](http://www.ipcv.org/objectsegmentation/)\n[microsoft MSRC-V2](http://research.microsoft.com/en-us/projects/objectclassrecognition/)\n\n[2010 CVPR iCoseg: Interactive cosegmentation by touch](http://chenlab.ece.cornell.edu/projects/touch-coseg/)\n\n[2010 CVPR Caltech-UCSD Birds 200](http://www.vision.caltech.edu/visipedia/CUB-200.html)\n\n[2010 CVPR Flower Datasets](http://www.robots.ox.ac.uk/~vgg/data/flowers/)\n\n[2009 ICCV An efficient algorithm for co-segmentation](http://www.biostat.wisc.edu/~vsingh/)\n\n[2008 CVPR Unsupervised Learning of Probabilistic Object Models (POMs) for Object Classification, Segmentation and Recognition](http://people.csail.mit.edu/leozhu/)\n\n[2008 CVPR Caltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)\n\n[2004 ECCV The Weizmann Horse Database](http://www.msri.org/people/members/eranb/)\n\n\n### 7.g \u573a\u666f\u89e3\u6790 dataset on scene parsing\n[\u53c2\u8003](http://www.ipcv.org/sceneparsing/)\n\n[ImageNet](http://www.image-net.org/)\n\n[ADE 20k](http://sceneparsing.csail.mit.edu/)\n\n[Cityscapes](https://www.cityscapes-dataset.com/)\n\n[COCO](http://cocodataset.org/#home)\n\n[Lab, Koch](http://www.mis.tu-darmstadt.de/tudds)\n\n[uiuc, D hoiem](http://www.cs.illinois.edu/homes/dhoiem/)\n\n[mit, cbcl](http://cbcl.mit.edu/software-datasets/streetscenes/)\n\n[mit LabelMeVideo](http://labelme.csail.mit.edu/LabelMeVideo/)\n\n[2013 BMVC Hierarchical Scene Annotation](http://www.vision.caltech.edu/~mmaire/)\n\n[2010 ECCV SuperParsing: Scalable Nonparametric Image Parsing with Superpixels](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/)\n\n[2009 CVPR Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment](ttp://people.csail.mit.edu/celiu/CVPR2009/)\n\n[2009 Scene Understanding Datasets](http://dags.stanford.edu/projects/scenedataset.html)\n\n[2008 IJCV 6D-Vision](http://www.6d-vision.com/scene-labeling)\n\n[2008 IJCV The Daimler Urban Segmentation Dataset](http://www.6d-vision.com/scene-labeling)\n\n[2008 ECCV Motion-based Segmentation and Recognition Dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec))/CamVid/\n\n[2008 York Urban Dataset](http://www.elderlab.yorku.ca/YorkUrbanDB/)\n\n[2008 IJCV LabelMe](http://labelme.csail.mit.edu/LabelMeToolbox/index.html)\n\n\n## 8 \u4f1a\u8bae\u3000\u671f\u520a\u3000\n### CVPR Computer vision  and  Pattern Reconition \u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6a21\u5f0f\u8bc6\u522b\n### ECCV European Conference on Computer Vision   \u6b27\u6d32\u8ba1\u7b97\u673a\u89c6\u89c9\u56fd\u9645\u4f1a\u8bae \n### ICCV IEEE International Conference on Computer Vision  \u56fd\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u5927\u4f1a\n### \u5176\u4ed6\n[\u5176\u4ed6](http://www.ipcv.org/otherpaper/)\n###\n###\n###\n###\n\n\n## AR&VR\n[\u53c2\u8003](http://www.ipcv.org/category/top-dir/arvr/)\n\n\n## \n"
 },
 {
  "repo": "opencv/opencv_contrib",
  "language": "C++",
  "readme_contents": "## Repository for OpenCV's extra modules\n\nThis repository is intended for the development of so-called \"extra\" modules,\ncontributed functionality. New modules quite often do not have stable API,\nand they are not well-tested. Thus, they shouldn't be released as a part of\nofficial OpenCV distribution, since the library maintains binary compatibility,\nand tries to provide decent performance and stability.\n\nSo, all the new modules should be developed separately, and published in the\n`opencv_contrib` repository at first. Later, when the module matures and gains\npopularity, it is moved to the central OpenCV repository, and the development team\nprovides production-quality support for this module.\n\n### How to build OpenCV with extra modules\n\nYou can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV.\n\nHere is the CMake command for you:\n\n```\n$ cd <opencv_build_directory>\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory>\n$ make -j5\n```\n\nAs the result, OpenCV will be built in the `<opencv_build_directory>` with all\nmodules from `opencv_contrib` repository. If you don't want all of the modules,\nuse CMake's `BUILD_opencv_*` options. Like in this example:\n\n```\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory>\n```\n\nIf you also want to build the samples from the \"samples\" folder of each module, also include the \"-DBUILD_EXAMPLES=ON\" option.\n\nIf you prefer using the gui version of cmake (cmake-gui), then, you can add `opencv_contrib` modules within `opencv` core by doing the following:\n\n1. Start cmake-gui.\n\n2. Select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface).\n\n3. Press the `configure` button. You will see all the opencv build parameters in the central interface.\n\n4. Browse the parameters and look for the form called `OPENCV_EXTRA_MODULES_PATH` (use the search form to focus rapidly on it).\n\n5. Complete this `OPENCV_EXTRA_MODULES_PATH` by the proper pathname to the `<opencv_contrib>/modules` value using its browse button.\n\n6. Press the `configure` button followed by the `generate` button (the first time, you will be asked which makefile style to use).\n\n7. Build the `opencv` core with the method you chose (make and make install if you chose Unix makefile at step 6).\n\n8. To run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, \"-lopencv_aruco\" flag will be added.\n\n### Update the repository documentation\n\nIn order to keep a clean overview containing all contributed modules, the following files need to be created/adapted:\n\n1. Update the README.md file under the modules folder. Here, you add your model with a single line description.\n\n2. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also.\n"
 },
 {
  "repo": "PySimpleGUI/PySimpleGUI",
  "language": "Python",
  "readme_contents": "\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Logo%20with%20text%20for%20GitHub%20Top.png\" alt=\"\u4eba\u9593\u306e\u305f\u3081\u306ePythonGUI \">\n  <h2 align=\"center\">\u4eba\u9593\u306e\u305f\u3081\u306ePython\u306eGUI</h2>\n</p>\n\ntkinter\u3001Qt\u3001WxPython\u3001\u304a\u3088\u3073Remi(\u30d6\u30e9\u30a6\u30b6\u30d9\u30fc\u30b9)\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u3001\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\u306f\u521d\u5fc3\u8005\u304c\u7406\u89e3\u3059\u308bPython\u30b3\u30a2\u30c7\u30fc\u30bf\u578b (\u30ea\u30b9\u30c8\u3068\u8f9e\u66f8) \u3092\u4f7f\u7528\u3057\u3066\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u6e21\u3059\u30e2\u30c7\u30eb\u306b\u30a4\u30d9\u30f3\u30c8\u51e6\u7406\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3055\u3089\u306b\u5358\u7d14\u5316\u304c\u884c\u308f\u308c\u307e\u3059\u3002 \n\n\u30b3\u30fc\u30c9\u306f\u3088\u308a\u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b\u306e\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u6301\u3064*\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093*\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u7406\u89e3\u3057\u3084\u3059\u3044\u3082\u306e\u3067\u3059\u304c\u3001\u5fc5\u305a\u3057\u3082*\u5358\u7d14*\u306a\u554f\u984c\u3060\u3051\u306b\u5236\u9650\u3055\u308c\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u305f\u3060\u3057\u3001\u4e00\u90e8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fPySimpleGUI\u306b\u306f\u9069\u3057\u3066\u3044\u307e\u305b\u3093\u3002 \u5b9a\u7fa9\u4e0a\u3001PySimpleGUI \u306f\u57fa\u76e4\u3068\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304cPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u3066\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u9069\u3057\u3066\u3044\u306a\u3044\u304b\u3092\u6b63\u78ba\u306b\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3067\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8a73\u7d30\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\u3002\u30a8\u30af\u30bb\u30eb\u3092\u8a73\u7d30\u306b\u8907\u88fd\u3059\u308b\u3053\u3068\u306fPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u306a\u3044\u3082\u306e\u306e\u4f8b\u3067\u3059\u3002\n\n<hr>\n\n# \u7d71\u8a08 :chart_with_upwards_trend:\n\n## PyPI \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n<p align=\"center\">\ntkinter <img src=\"http://pepy.tech/badge/pysimplegui?color=blue&style=for-the-badge\" width=\"100px\"  align=\"center\">\ntkinter 2.7 <img src=\"https://pepy.tech/badge/pysimplegui27?color=blue&style=for-the-badge\"  align=\"center\"><br>\nQt <img src=\"https://pepy.tech/badge/pysimpleguiqt?color=blue&style=for-the-badge\"  align=\"center\">\nWxPython<img src=\"https://pepy.tech/badge/pysimpleguiwx?color=blue&style=for-the-badge\"  align=\"center\">\nWeb (Remi) <img src=\"https://pepy.tech/badge/pysimpleguiweb?color=blue&style=for-the-badge\"  align=\"center\">\n</p>\n\n\n## GitHub\n\n<p align=\"center\">\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\" width=\"180px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-closed-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\"  width=\"200px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/commit-activity/m/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"  width=\"260px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/last-commit/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"width=\"200px\"></a>\n<a href=\"\"><img src=\"http://ForTheBadge.com/images/badges/makes-people-smile.svg\" alt=\"img\"width=\"190px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/stars/PySimpleGUI/PySimpleGUI.svg?style=social&label=Star&maxAge=2592000\" alt=\"img\"width=\"140x\"></a>\n</p>\n\n\n<p align=\"center\">\n  <img src=\"https://github-readme-stats.vercel.app/api/?username=PySimpleGUI&bg_color=3e7bac&title_color=ffdd55&icon_color=ffdd55&text_color=ffdd55&show_icons=true&count_private=true\">\n</p>\n\n## \u6700\u65b0\u306e PyPI \u30d0\u30fc\u30b8\u30e7\u30f3\n\n\n<p align=\"center\">\ntkinter\n<a href=\"pypi tkinter\"><img src=\"https://img.shields.io/pypi/v/pysimplegui.svg?style=for-the-badge&color=red\" alt=\"img\" align=\"center\" width=\"150px\"></a>\nQt\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWeb\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWxPython\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\n</p>\n\n<hr>\n\n# PySimpleGUI\u3068\u306f\u4f55\u3067\u3059\u304b:question:\n\nPySimpleGUI\u306f\u3042\u3089\u3086\u308b\u30ec\u30d9\u30eb\u306ePython\u30d7\u30ed\u30b0\u30e9\u30de\u304cGUI\u3092\u4f5c\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308bPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080 \u300c\u30ec\u30a4\u30a2\u30a6\u30c8\u300d\u3092\u4f7f\u7528\u3057\u3066 GUI \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u6307\u5b9a\u3057\u307e\u3059 (PySimpleGUI \u3067\u306f\u300c\u30a8\u30ec\u30e1\u30f3\u30c8\u300d\u3068\u547c\u3073\u307e\u3059)\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b4\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u8868\u793a\u3084\u64cd\u4f5c\u3059\u308b\u306e\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u30b5\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001tkinter\u3001Qt\u3001WxPython\u3001WxPython\u307e\u305f\u306fRemi\u304c\u542b\u307e\u308c\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u300c\u30e9\u30c3\u30d1\u30fc\u300d\u3068\u3044\u3046\u7528\u8a9e\u304c\u4f7f\u308f\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\n\nPySimpleGUI\u306f\u300c\u30dc\u30a4\u30e9\u30fc\u30d7\u30ec\u30fc\u30c8\u30b3\u30fc\u30c9\u300d\u306e\u591a\u304f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u57fa\u3068\u306a\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082\u5358\u7d14\u3067\u77ed\u304b\u3044\u30b3\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002\n\u3055\u3089\u306b\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306f\u3001\u671b\u3093\u3060\u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u3067\u304d\u308b\u3060\u3051\u5c11\u306a\u304f\u3059\u308b\u3088\u3046\u306b\u5358\u7d14\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u3082\u3088\u308a\u307e\u3059\u304c\u3001PySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u76f4\u63a5\u4f7f\u7528\u3057\u3066\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u3088\u308a\u3082\u3001\u30b3\u30fc\u30c9\u306e\u91cf\u306f1/2\u304b\u30891/10\u7a0b\u5ea6\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u76ee\u6a19\u306f\u4f7f\u7528\u3057\u3066\u3044\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u4e0a\u306e\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3084\u30b3\u30fc\u30c9\u3092\u30ab\u30d7\u30bb\u30eb\u5316/\u975e\u8868\u793a\u306b\u3059\u308b\u3053\u3068\u3067\u3059\u304c\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3084\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u30a2\u30af\u30bb\u30b9\u3067\u304d\u307e\u3059\u3002\n\u8a2d\u5b9a\u3084\u6a5f\u80fd\u304c\u307e\u3060\u516c\u958b\u3055\u308c\u3066\u304a\u3089\u305a\u3001PySimpleGUI API\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u906e\u65ad\u3055\u308c\u3066\u307e\u305b\u3093\u3002PySimpleGUI\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u81ea\u4f53\u3092\u76f4\u63a5\u5909\u66f4\u305b\u305a\u306b\u6a5f\u80fd\u3092\u62e1\u5f35\u3067\u304d\u307e\u3059\u3002\n## \u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7\u300d\u3092\u57cb\u3081\u308b\n\nPython \u306f\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u591a\u304f\u306e\u4eba\u3005\u3092\u62db\u3044\u3066\u3044\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6570\u3068\u6271\u3046\u9818\u57df\u306e\u7bc4\u56f2\u306f\u6c17\u304c\u9060\u304f\u306a\u308a\u307e\u3059 \u3057\u304b\u3057\u591a\u304f\u306e\u5834\u5408\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3068\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u4e00\u63e1\u308a\u306e\u4eba\u3005\u4ee5\u5916\u306e\u624b\u306e\u5c4a\u304b\u306a\u3044\u3068\u3053\u308d\u306b\u3042\u308a\u307e\u3059\u3002Python \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u5927\u534a\u306f\"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\"\u30d9\u30fc\u30b9\u3067\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u7cfb\u306e\u4eba\u306f\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3092\u4ecb\u3057\u3066\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u3053\u3068\u306b\u6163\u308c\u3066\u3044\u3066\u3001\u3053\u306e\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u307b\u3068\u3093\u3069\u306e\u300c\u666e\u901a\u306e\u4eba\u300d\u306f\u554f\u984c\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30b8\u30bf\u30eb\u30fb\u30c7\u30a3\u30d0\u30a4\u30c9\u3001\u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7 \u300d\u304c\u751f\u307f\u51fa\u3055\u308c\u307e\u3059\u3002\n\u30d7\u30ed\u30b0\u30e9\u30e0\u306bGUI\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u591a\u304f\u306e\u4eba\u306b\u77e5\u3063\u3066\u3082\u3089\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u89aa\u3057\u307f\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002GUI\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u6163\u308c\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u3067\u3042\u3063\u3066\u3082\u3001\u3044\u304f\u3064\u304b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u64cd\u4f5c\u3092\u7c21\u5358\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u305d\u3057\u3066\u6700\u5f8c\u306bGUI\u3092\u5fc5\u8981\u3068\u3059\u308b\u554f\u984c\u3082\u3042\u308a\u307e\u3059\u3002   \n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/GUI%20Gap%202020.png\" width=\"600px\">\n</p>\n\n\n<hr>\n\n# \u79c1\u306b\u3064\u3044\u3066 :wave:\n\u3053\u3093\u306b\u3061\u306f\uff01 \u79c1\u306f\u30de\u30a4\u30af\u3067\u3059\u3002 GitHub\u306ePySimpleGUI\u3067\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066PySimpleGUI\u3092\u7d99\u7d9a\u7684\u306b\u524d\u9032\u3055\u305b\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u663c\u3068\u591c\u3068\u9031\u672b\u3082\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068PySimpleGUI\u30e6\u30fc\u30b6\u30fc\u306b\u6367\u3052\u3066\u304d\u307e\u3057\u305f\u3002\u79c1\u305f\u3061\u306e\u6210\u529f\u306f\u6700\u7d42\u7684\u306b\u5171\u6709\u3055\u308c\u307e\u3059\u3002 \u3042\u306a\u305f\u304c\u6210\u529f\u3057\u305f\u3068\u304d\u306b\u79c1\u306f\u6210\u529f\u3057\u3066\u3044\u307e\u3059\u3002\n\nPython\u3067\u306f\u76f8\u5bfe\u7684\u306a\u65b0\u4eba\u3067\u3059\u304c\u300170\u5e74\u4ee3\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u3002 \u79c1\u306e\u30ad\u30e3\u30ea\u30a2\u306e\u5927\u534a\u306f\u30b7\u30ea\u30b3\u30f3\u30d0\u30ec\u30fc\u3067\u306e\u88fd\u54c1\u958b\u767a\u306b\u8cbb\u3084\u3055\u308c\u307e\u3057\u305f\u3002PySimpleGUI\u306b\u306f\u81ea\u5206\u304c\u958b\u767a\u3057\u305f\u4f01\u696d\u88fd\u54c1\u3068\u540c\u3058\u3088\u3046\u306a\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30ea\u30ba\u30e0\u3068\u732e\u8eab\u3092\u3082\u305f\u3089\u3057\u307e\u3059\u3002\u4eca\u3001\u3042\u306a\u305f\u306f\u79c1\u306e\u9867\u5ba2\u3067\u3059\u3002\n\n\n## \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u6a19 :goal_net:\n\nPySimpleGUI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u91cd\u8981\u306a\u76ee\u6a19\u306f\u4ee5\u4e0b\u306e2\u3064\u3067\u3059\u3002\n\n* \u697d\u3057\u3080\u3053\u3068\n* \u3042\u306a\u305f\u306e\u6210\u529f\n\n\u771f\u9762\u76ee\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30b4\u30fc\u30eb\u3068\u3057\u3066**\u697d\u3057\u3080**\u3068\u3044\u3046\u306e\u306f\u5909\u306b\u805e\u3053\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u306f\u771f\u9762\u76ee\u306a\u76ee\u6a19\u3067\u3059\u3002\u79c1\u306f\u3053\u308c\u3089\u306eGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u304f\u3053\u3068\u306f\u3068\u3066\u3082\u697d\u3057\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u306e\u7406\u7531\u306e1\u3064\u306f\u3001\u5b8c\u5168\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u306b\u304b\u304b\u308b\u6642\u9593\u304c\u3044\u304b\u306b\u77ed\u3044\u304b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3082\u3057\u79c1\u9054\u304c\u30d7\u30ed\u30bb\u30b9\u3092\u697d\u3057\u3093\u3067\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u8ab0\u304b\u304c\u3042\u304d\u3089\u3081\u3066\u3044\u307e\u3059\u3002\n\n\u81a8\u5927\u306a\u91cf\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3001\u30af\u30c3\u30af\u30d6\u30c3\u30af\u3001\u3059\u3050\u306b\u4f7f\u3048\u308b100\u7a2e\u985e\u4ee5\u4e0a\u306e\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3001\u8a73\u7d30\u306a\u30b3\u30fc\u30eb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3001YouTube\u306e\u30d3\u30c7\u30aa\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u306eTrinket\u306e\u30c7\u30e2\u306a\u3069\u3001\u3059\u3079\u3066\u304c\u697d\u3057\u3044\u4f53\u9a13\u3092\u751f\u307f\u51fa\u3059\u305f\u3081\u306b\u4f5c\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n**\u3042\u306a\u305f\u306e\u6210\u529f**\u306f\u5171\u901a\u306e\u76ee\u6a19\u3067\u3059\u3002 PySimpleGUI \u306f\u958b\u767a\u8005\u5411\u3051\u306b\u69cb\u7bc9\u3055\u308c\u307e\u3057\u305f\u3002\u3042\u306a\u305f\u306f\u79c1\u306e\u4ef2\u9593\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u3068PySimpleGUI\u306e\u5171\u540c\u4f5c\u696d\u306e\u7d50\u679c\u3092\u898b\u308b\u306e\u306f\u4e88\u60f3\u5916\u306e\u5831\u916c\u3067\u3057\u305f\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3084\u305d\u306e\u4ed6\u306e\u8cc7\u6599\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u69cb\u7bc9\u306b\u5f79\u7acb\u3066\u3066\u304f\u3060\u3055\u3044\u3002\u30c8\u30e9\u30d6\u30eb\u306b\u906d\u9047\u3057\u305f\u5834\u5408\u306f\u3001[PySimpleGUI GitHub \u306e\u554f\u984c](http://Issues.PySimpleGUI.org)\u3067Issue \u3092\u958b\u3044\u3066\u30d8\u30eb\u30d7\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u30b5\u30dd\u30fc\u30c8\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002\n\n<hr>\n\n# \u6559\u80b2\u30ea\u30bd\u30fc\u30b9 :books:\n\nwww.PySimpleGUI.org \u306f\u899a\u3048\u3084\u3059\u304f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5834\u6240\u3067\u3059\u3002\u4e0a\u90e8\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u7570\u306a\u308b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8868\u3059\u30bf\u30d6\u304c\u3042\u308a\u307e\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u300cRead The Docs\u300d\u306b\u8a18\u8f09\u3055\u308c\u3066\u304a\u308a\u3001\u5404\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u76ee\u6b21\u304c\u3042\u308a\u691c\u7d22\u304c\u7c21\u5358\u3067\u3059\u3002\n\n\u6570\u767e\u30da\u30fc\u30b8\u306e\u6587\u66f8\u5316\u3055\u308c\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u6570\u767e\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u3042\u308a\u3001\u3042\u306a\u305f\u304c\u975e\u5e38\u306b\u901f\u304f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\u5358\u4e00\u306e GUI \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5b66\u3076\u306e\u306b\u6570\u65e5\u307e\u305f\u306f\u6570\u9031\u9593\u6295\u8cc7\u3059\u308b\u3088\u308a\u3082\u3001PySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u5348\u5f8c\u4e00\u56de\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b8c\u6210\u3055\u305b\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\n## \u4f8b 1 - \u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u30bf\u30a4\u30d7\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c1\u56de\u8868\u793a\u3055\u308c\u3066\u53ce\u96c6\u3055\u308c\u305f\u5024\u304c\u9589\u3058\u3089\u308c\u308b\u305f\u3081\u3001\u300c\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u300d\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u547c\u3070\u308c\u307e\u3059\u3002 \u30ef\u30fc\u30c9\u30d7\u30ed\u30bb\u30c3\u30b5\u306e\u3088\u3046\u306b\u9577\u3044\u9593\u958b\u3044\u305f\u307e\u307e\u306b\u306a\u3063\u3066\u3044\u307e\u305b\u3093\u3002\n### \u5358\u7d14\u306aPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u89e3\u5256\u5b66\n\nPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f5\u3064\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\n\n\n\n```python\nimport PySimpleGUI as sg                                 # \u30d1\u30fc\u30c8 1 - \u30a4\u30f3\u30dd\u30fc\u30c8\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [  [sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],     # \u30d1\u30fc\u30c8 2 - \u30ec\u30a4\u30a2\u30a6\u30c8\n            [sg.Input()],\n            [sg.Button('\u306f\u3044')] ]\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb', layout)      # \u30d1\u30fc\u30c8 3- \u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\n                                                \n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nevent, values = window.read()                   # \u30d1\u30fc\u30c8 4- \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u307e\u305f\u306f Window.read \u547c\u3073\u51fa\u3057\n\n# \u53ce\u96c6\u3055\u308c\u305f\u60c5\u5831\u3067\u4f55\u304b\u3092\u3059\u308b\nprint('\u30cf\u30ed\u30fc ', values[0], \"! PySimpleGUI\u3092\u8a66\u3057\u3066\u304f\u308c\u3066\u3042\u308a\u304c\u3068\u3046\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()                                  #\u30d1\u30fc\u30c8 5 - \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\n```\n\n\u30b3\u30fc\u30c9\u306f\u3001\u4ee5\u4e0b\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank1.jpg\">\n</p>\n\n\n<hr>\n\n## \u4f8b 2 - \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\u304b\u3001\u307e\u305f\u306f [\u7d42\u4e86] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u307e\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u753b\u9762\u4e0a\u306b\u6b8b\u308a\u307e\u3059\u3002 \u5148\u307b\u3069\u898b\u305f\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e3b\u306a\u9055\u3044\u306f\u3001\u300c\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u300d\u306e\u8ffd\u52a0\u3067\u3059\u3002\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u304b\u3089\u30a4\u30d9\u30f3\u30c8\u3068\u5165\u529b\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002 \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4e2d\u5fc3\u306f\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],\n          [sg.Input(key='-\u5165\u529b-')],\n          [sg.Text(size=(55,1), key='-\u51fa\u529b-')],\n          [sg.Button('\u306f\u3044'), sg.Button('\u7d42\u4e86')]]\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb',\u30ec\u30a4\u30a2\u30a6\u30c8)\n\n# \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nwhile True:\n    event, values = window.read()\n# \u30e6\u30fc\u30b6\u30fc\u304c\u7d42\u4e86\u3057\u305f\u3044\u306e\u304b\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n\n    # Output a message to the window\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()\n```\n\n\u4ee5\u4e0b\u306f\u306f\u3001\u4f8b2\u304c\u4f5c\u6210\u3059\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank.jpg\">\n</p>\n\n\n\n\u5165\u529b\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u5024\u3092\u5165\u529b\u3057\u3066 [OK] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u5f8c\u306e\u8868\u793a\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorld1.jpg\">\n</p>\n\n\n\u3053\u306e\u4f8b\u3068\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u9055\u3044\u306b\u6c17\u3065\u304f\u3067\u3057\u3087\u3046\u3002 \u7279\u306b2\u3064\u306e\u5909\u66f4\u304c\u91cd\u8981\u3067\u3059\u3002 1\u3064\u306f`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u3068`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e1\u3064\u306b`key`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u300ckey\u300d\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u540d\u524d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 \u307e\u305f\u306f\u3001Python\u306e\u8a00\u8449\u3067\u306f\u3001\u8f9e\u66f8\u30ad\u30fc\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 `Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ad\u30fc\u306f\u3001\u30b3\u30fc\u30c9\u306e\u5f8c\u534a\u3067\u8f9e\u66f8\u30ad\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\n\n\n\u3082\u30461\u3064\u306e\u9055\u3044\u306f\u3001\u3053\u306e `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u8ffd\u52a0\u3067\u3059:\n```python\n          [sg.Text(size=(40,1), key='-OUTPUT-')],\n```\n\n\u3059\u3067\u306b\u30ab\u30d0\u30fc\u3057\u3066\u3044\u308b\u300c\u30ad\u30fc\u300d\u3068\u3044\u30462\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002 `Size`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u6587\u5b57\u6570\u306e\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5e4540\u6587\u5b57\u3001\u9ad8\u30551\u6587\u5b57\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u306e\u6587\u5b57\u5217\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u7a7a\u767d\u306b\u306a\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4f5c\u6210\u3055\u308c\u305f\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306f\u7a7a\u767d\u884c\u304c\u7c21\u5358\u306b\u898b\u308c\u307e\u3059\u3002\n\n\u307e\u305f \u3001[\u7d42\u4e86]\u30dc\u30bf\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306f\u3001\u304a\u306a\u3058\u307f\u306e`window.read()`\u547c\u3073\u51fa\u3057\u3057\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u8aad\u307f\u8fbc\u3093\u3060\u5f8c\u306b\u7d9a\u304f\u306e\u306f\u3001\u3053\u306eif\u6587\u3067\u3059\u3002\n```python\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n```\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c \u300cX\uff08\u9589\u3058\u308b\uff09\u300d \u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u305f\u304b\u3001\u307e\u305f\u306f\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306e\u3044\u305a\u308c\u304b\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u30b3\u30fc\u30c9\u306f\u30a4\u30d9\u30f3\u30c8 \u30eb\u30fc\u30d7\u304b\u3089\u629c\u3051\u51fa\u3057\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305a\u3001\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u304c\u30af\u30ea\u30c3\u30af\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u52d5\u4f5c\u304c\u7d99\u7d9a\u3055\u308c\u307e\u3059\u3002 \u8d77\u3053\u308a\u3046\u308b\u552f\u4e00\u306e\u4e8b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u300cOK\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u3053\u3068\u3067\u3059\u3002 \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306e\u6700\u5f8c\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\n\n\n\n```python\n    window['-OUTPUT-'].update('\u30cf\u30ed\u30fc  ' + values['-INPUT-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n```\n\n\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u3001`-OUTPUT-`\u30ad\u30fc \u3092\u6301\u3064`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u6587\u5b57\u5217\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002`window['-OUTPUT-']`\u306f`-OUTPUT-`\u30ad\u30fc\u3092\u6301\u3064\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u691c\u7d22\u3057\u307e\u3059\u3002 \u30ad\u30fc\u306f\u3001\u7a7a\u767d\u306e`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5c5e\u3057\u307e\u3059\u3002 \u30a8\u30ec\u30e1\u30f3\u30c8\u304c\u691c\u7d22\u304b\u3089\u8fd4\u3055\u308c\u308b\u3068\u3001\u305d\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update`\u30e1\u30bd\u30c3\u30c9\u304c\u547c\u3073\u51fa\u3055\u308c\u307e\u3059\u3002 \u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306f`update`\u30e1\u30bd\u30c3\u30c9\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u5024\u3084\u69cb\u6210\u3092\u5909\u66f4\u3057\u305f\u308a\u3059\u308b\u306e\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u30c6\u30ad\u30b9\u30c8\u3092\u9ec4\u8272\u306b\u3057\u305f\u3044\u5834\u5408\u306f\u3001`update`\u30e1\u30bd\u30c3\u30c9\u306b`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u51e6\u7406\u3057\u307e\u3059\u3002\n```python\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\", text_color='yellow')\n```\n\n`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u305f\u5f8c\u3001\u3053\u308c\u304c\u65b0\u3057\u3044\u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorldYellow.jpg\">\n</p>\n\n\n\u5404\u30a8\u30ec\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306f[call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](http://calls.PySimpleGUI.org)\u3068docstrings \u3068\u4e21\u65b9\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306b\u306f\u3001\u5229\u7528\u53ef\u80fd\u306a\u3059\u3079\u3066\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u8c4a\u5bcc\u306a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002 `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update'`\u30e1\u30bd\u30c3\u30c9\u3092\u691c\u7d22\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9a\u7fa9\u304c\u898b\u3064\u304b\u308a\u307e\u3059:\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/TextUpdate.jpg\">\n</p>\n\n\n\u3054\u89a7\u306e\u3088\u3046\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u3082\u306e\u306f\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306fPySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u7c21\u5358\u306b\u3059\u308b\u8cb4\u91cd\u306a\u30ea\u30bd\u30fc\u30b9\u3067\u3059\u3002\n\n<hr>\n\n##\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u9762\u767d\u3044\u3067\u3059 LOL! :laughing:\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u300c\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u300d(LOL)\u3067\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u300c\u884c\u300d\u306b\u5206\u5272\u3055\u308c\u307e\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5404\u884c\u306f\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u30ea\u30b9\u30c8\u306b\u306a\u308a\u307e\u3059\u3002 \u3059\u3079\u3066\u306e\u30ea\u30b9\u30c8\u3092\u9023\u7d50\u3059\u308b\u3068\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002...\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\u884c\u306e\u5b9a\u7fa9\u65b9\u6cd5\u3092\u7c21\u5358\u306b\u78ba\u8a8d\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5404\u884c\u306b\u8ffd\u52a0\u306e 'Text' \u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u305f\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u3001\u4ee5\u524d\u3068\u540c\u3058\u3067\u3059:\n\n```python\nlayout = [  [sg.Text('\u30e9\u30a4\u30f3 1'), sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\")],\n            [sg.Text('\u30e9\u30a4\u30f3 2'), sg.Input()],\n            [sg.Text('\u30e9\u30a4\u30f3 3'), sg.Button('\u306f\u3044')] ]\n```\n\n\u3053\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5404\u884c\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u306e\u884c\u306b\u8868\u793a\u3055\u308c\u308b\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/rows.jpg\">\n</p>\n\n\n\n\u30ea\u30b9\u30c8\u3092\u4f7f\u7528\u3057\u3066GUI\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u4ed6\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066GUI\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u884c\u3046\u65b9\u6cd5\u306b\u304f\u3089\u3079\u3066\u3044\u304f\u3064\u304b\u5927\u304d\u306a\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001Python \u306e\u30ea\u30b9\u30c8\u5185\u5305\u8868\u8a18\u3092\u5229\u7528\u3057\u3066\u30011 \u884c\u306e\u30b3\u30fc\u30c9\u3067\u30dc\u30bf\u30f3\u306e\u30b0\u30ea\u30c3\u30c9\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\n\n\u6b21\u306e3\u884c\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\nlayout = [[sg.Button(f'{row}, {col}') for col in range(4)] for row in range(4)]\n\nevent, values = sg.Window('List Comprehensions', layout).read(close=True)\n```\n\n\u30dc\u30bf\u30f3\u306e4 x 4\u30b0\u30ea\u30c3\u30c9\u3092\u6301\u3064\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/4x4grid.jpg\">\n</p>\n\n\u300c\u697d\u3057\u3080\u300d\u304c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u306e\uff11\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Python\u306e\u5f37\u529b\u306a\u57fa\u672c\u6a5f\u80fd\u3092GUI\u306e\u554f\u984c\u306b\u76f4\u63a5\u9069\u7528\u3059\u308b\u306e\u306f\u697d\u3057\u3044\u3067\u3059\u3002GUI\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u306e\u30da\u30fc\u30b8\u306e\u4ee3\u308f\u308a\u306b\u3001\u6570\u884c (\u307e\u305f\u306f\u591a\u304f\u306e\u5834\u54081\u884c) \u306e\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n## \u30b3\u30fc\u30c9\u306e\u6298\u308a\u305f\u305f\u307f\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30b3\u30fc\u30c9\u30921\u884c\u306e\u30b3\u30fc\u30c9\u306b\u51dd\u7e2e\u3067\u304d\u307e\u3059\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5b9a\u7fa9\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u3001\u8868\u793a\u3001\u304a\u3088\u3073\u30c7\u30fc\u30bf\u53ce\u96c6\u306f\u3059\u3079\u3066\u3001\u6b21\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u307e\u3059\u3002\n```python\nevent, values = sg.Window('Window Title', [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],[sg.Input()],[sg.Button('\u306f\u3044')]]).read(close=True)\n```\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourName.jpg\">\n</p>\n\n\n\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u8868\u793a\u3055\u308c\u3001PySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u793a\u3059\u4f8b\u3068\u540c\u3058\u5024\u304c\u8fd4\u3055\u308c\u307e\u3059\u3002 \u975e\u5e38\u306b\u5c11\u306a\u3044\u91cf\u3067\u591a\u304f\u306e\u3053\u3068\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001Python\u30b3\u30fc\u30c9\u306b\u3059\u3070\u3084\u304f\u7c21\u5358\u306bGUI\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u9078\u629e\u3092\u5f97\u305f\u3044\u5834\u5408\u306f\u30011\u30da\u30fc\u30b8\u306e\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u77ed\u7e2e\u30a8\u30a4\u30ea\u30a2\u30b9\u3092\u4f7f\u7528\u3057\u3066\u3088\u308a\u5c11\u306a\u3044\u6587\u5b57\u6570\u3067\u30b3\u30fc\u30c9\u306e\u30b9\u30da\u30fc\u30b9\u3092\u3055\u3089\u306b\u77ed\u304f\u3067\u304d\u307e\u3059\u3002  \u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u306f\u3001\u4f7f\u7528\u3067\u304d\u308b\u77ed\u3044\u540d\u524d\u304c\uff11\u3064\u4ee5\u4e0a\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5358\u306b`T`\u3068\u3057\u3066\u66f8\u3051\u307e\u3059\u3002`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f `I`\u3001`Button`\u306f`B`\u3068\u66f8\u3051\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306b\u306b\u306a\u308a\u307e\u3059:\n\n```python\nevent, values = sg.Window('Window Title', [[sg.T(\"\u3042\u306a\u305f\u306e\u540d\u524d\u306f\u4f55\u3067\u3059\u304b?\")],[sg.I()],[sg.B('\u306f\u3044')]]).read(close=True)\n```\n\n\n### \u30b3\u30fc\u30c9\u306e\u79fb\u690d\u6027\n\nPySimpleGUI\u306f\u73fe\u5728\u30014\u3064\u306ePython\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002 \u4f7f\u7528\u3059\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001import\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u6307\u5b9a\u3057\u307e\u3059\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3092\u5909\u66f4\u3059\u308b\u3068\u3001\u57fa\u672c\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3088\u3063\u3066\u306f\u3001\u5225\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306fimport \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u4ee5\u5916\u306e\u5909\u66f4\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002 \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u3001\u30a4\u30f3\u30dd\u30fc\u30c8\u3092`PySimpleGUI`\u304b\u3089`PySimpleGUIQt`\u3001`PySimpleGUIWx`\u3001`PySimpleGUIWeb`\u3001`PySimpleGUIWeb`\u306b\u5909\u66f4\u3059\u308b\u3068\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\n\n| \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 | \u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6 |\n|--|--|\n| PySimpleGUI |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-tkinter.jpg) |\n| PySimpleGUIQt |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Qt.jpg) |\n| PySimpleGUIWx |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-WxPython.jpg) |\n| PySimpleGUIWeb |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Remi.jpg) |\n\n\n\nGUI\u306e\u30b3\u30fc\u30c9\u3092\u3042\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u5225\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u79fb\u690d\u3059\u308b (\u4f8b\u3048\u3070\u3001\u30b3\u30fc\u30c9\u3092tkinter\u304b\u3089Qt\u306b\u79fb\u52d5\u3059\u308b) \u306b\u306f\u3001\u901a\u5e38\u306f\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u63db\u3048\u304c\u5fc5\u8981\u3067\u3059\u3002  PySimpleGUI \u306f\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u306e\u7c21\u5358\u306a\u79fb\u52d5\u3092\u53ef\u80fd\u306b\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3044\u304f\u3064\u304b\u306e\u5909\u66f4\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u76ee\u7684\u306f\u6700\u5c0f\u9650\u306e\u5909\u66f4\u3067\u79fb\u690d\u6027\u306e\u9ad8\u3044\u30b3\u30fc\u30c9\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \n\n\u30b7\u30b9\u30c6\u30e0 \u30c8\u30ec\u30a4 \u30a2\u30a4\u30b3\u30f3\u306a\u3069\u306e\u4e00\u90e8\u306e\u6a5f\u80fd\u306f\u3001\u3059\u3079\u3066\u306e\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u306a\u3044\u3067\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u6a5f\u80fd\u306fQt\u304a\u3088\u3073WxPython\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306ftkinter\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u306f\u3001PySimpleGUIWeb\u30dd\u30fc\u30c8\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u307e\u305b\u3093\u3002\n\n##  \u30e9\u30f3\u30bf\u30a4\u30e0\u74b0\u5883\n\n|\u74b0\u5883 |\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b |\n|--|--|\n|\u30d1\u30a4\u30bd\u30f3| Python  3.4+ |\n|\u30aa\u30da\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0 \u30b7\u30b9\u30c6\u30e0 |\u30a6\u30a3\u30f3\u30c9\u30a6\u30ba, Linux, \u30de\u30c3\u30af |\n|\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2 |\u30c7\u30b9\u30af\u30c8\u30c3\u30d7 PC, \u30ce\u30fc\u30c8\u30d1\u30bd\u30b3\u30f3, \u30e9\u30ba\u30d9\u30ea\u30fc\u30d1\u30a4, PyDroid3 \u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30a2\u30f3\u30c9\u30ed\u30a4\u30c9\u30c7\u30d0\u30a4\u30b9 |\n|\u30aa\u30f3\u30e9\u30a4\u30f3 |repli.it\u3001Trinket.com (\u3069\u3061\u3089\u3082\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067tkinter\u3092\u5b9f\u884c\u3059\u308b) |\n|GUI \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af |tkinter, pyside2, WxPython, Remi |\n\n\n## \u7d71\u5408\n200 \u4ee5\u4e0a\u306e\u300c\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u300d\u306e\u4e2d\u306b\u306f\u3001\u591a\u304f\u306e\u4eba\u6c17\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u3092GUI\u306b\u7d71\u5408\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u304c\u898b\u3064\u304b\u308a\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306bMatplotlib\u306e\u63cf\u753b\u3092\u57cb\u3081\u8fbc\u307f\u305f\u3044\u3067\u3059\u304b?  \u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3001 \u30c7\u30e2\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5373\u5ea7\u306b\u3042\u306a\u305f\u306e\u5922\u306eMatplotlib\u306e\u63cf\u753b\u3092\u3042\u306a\u305f\u306eGUI\u306b\u7d44\u307f\u8fbc\u3081\u307e\u3059\u3002  \n\n\u3053\u308c\u3089\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3084\u305d\u306e\u4ed6\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3001\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30c7\u30e2\u30ec\u30dd\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001GUI\u306b\u5165\u308c\u308b\u6e96\u5099\u304c\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\n|\u30d1\u30c3\u30b1\u30fc\u30b8 |\u8aac\u660e |\n|--|--|\n Matplotlib |\u30b0\u30e9\u30d5\u3084\u30d7\u30ed\u30c3\u30c8\u306e\u591a\u304f\u306e\u7a2e\u985e |\n OpenCV |\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3 (AI\u3067\u3088\u304f\u4f7f\u7528) |\n VLC |\u30d3\u30c7\u30aa\u518d\u751f |\n pymunk |\u7269\u7406\u30a8\u30f3\u30b8\u30f3|\n psutil |\u30b7\u30b9\u30c6\u30e0\u74b0\u5883\u306e\u7d71\u8a08 |\n prawn |Reddit  API |\njson |PySimpleGUI \u306f\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u8a2d\u5b9a\u300d\u3092\u683c\u7d0d\u3059\u308b\u7279\u5225\u306aAPI\u3092\u30e9\u30c3\u30d7\u3057\u307e\u3059\u3002 |\n weather |\u304a\u5929\u6c17\u30a2\u30d7\u30ea\u3092\u4f5c\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u5929\u6c17API\u3068\u7d71\u5408 |\n mido |MIDI \u518d\u751f |\n beautiful soup |\u30a6\u30a7\u30d6\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0 (GitHub issue\u30a6\u30a9\u30c3\u30c1\u30e3\u30fc\u3067\u306e\u4f8b) |\n\n<hr>\n\n# \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb :floppy_disk:\n\n\nPySimpleGUI\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4e00\u822c\u7684\u306b2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\n\n1. PyPI\u304b\u3089pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\n2. PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u914d\u7f6e\u3057\u307e\u3059\n\n\n### Pip\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n\u73fe\u5728\u63d0\u6848\u3055\u308c\u3066\u3044\u308b`pip`\u30b3\u30de\u30f3\u30c9\u3092\u547c\u3073\u51fa\u3059\u65b9\u6cd5\u306f\u3001Python\u3092\u4f7f\u3063\u3066\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u4ee5\u524d\u306f\u3001`pip`\u307e\u305f\u306f`pip3`\u30b3\u30de\u30f3\u30c9\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3/\u30b7\u30a7\u30eb\u4e0a\u3067\n\u76f4\u63a5\u5b9f\u884c\u3055\u308c\u307e\u3057\u305f\u3002 \u63d0\u6848\u3055\u308c\u305f\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\nWindows \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python -m pip install PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python3 -m pip install PySimpleGUI`\n\n`pip`\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3059\u308b\u306b\u306f\u3001\u5358\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf`--upgrade --no-cache-dir`\u3092\u6307\u5b9a\u3059\u308b\u3060\u3051\u3067\u3059\u3002\n\nWindows \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python3 -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\n\n### \u5358\u4e00\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nPySimpleGUI\u306fRaspberry Pi \u306e\u3088\u3046\u306a\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306b\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u30b7\u30b9\u30c6\u30e0\u306b\u3082\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5358\u4e00\u306e .py \u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u306b\u7f6e\u304f\u3060\u3051\u3067\u3059\u3002Python \u306f\u30a4\u30f3\u30dd\u30fc\u30c8\u6642\u306b\u30ed\u30fc\u30ab\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n.py\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f\u3001PyPI\u304b\u3089\u5165\u624b\u3059\u308b\u304b\u3001\u6700\u65b0\u306e\u672a\u30ea\u30ea\u30fc\u30b9\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u306fGitHub\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\nPyPI\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306b\u306f\u3001wheel\u307e\u305f\u306f .gz \u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u89e3\u51cd\u3057\u307e\u3059\u3002 .whl\u30d5\u30a1\u30a4\u30eb\u3092.zip\u306b\u30ea\u30cd\u30fc\u30e0\u3059\u308b\u3068\u3001\u901a\u5e38\u306ezip\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u3058\u3088\u3046\u306b\u958b\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d5\u30a9\u30eb\u30c0\u306e\u4e2d\u306bPySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5b8c\u4e86\u3067\u3059\u3002\n\ntkinter \u30d0\u30fc\u30b8\u30e7\u30f3\u306e PySimpleGUI \u306e PyPI \u30ea\u30f3\u30af\u3067\u3059\nhttps://pypi.org/project/PySimpleGUI/#files\n\nGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u3001\u3053\u3061\u3089\u3067\u78ba\u8a8d\u3067\u304d\u307e\u3059\nhttps://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/PySimpleGUI.py\n\n\n\u300c\u305d\u3046\u3060\u3051\u3069\u3001\u5de8\u5927\u306a\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u3064\u3060\u3051\u6301\u3064\u306e\u306f\u306a\u3093\u3066\u3072\u3069\u3044\u8003\u3048\u3067\u3059\u300d\u3068\u4eca\u3001\u8003\u3048\u3066\u3044\u308b\u4eba\u3082\u3044\u308b\u3067\u3057\u3087\u3046\u3002 \u3053\u308c\u306f*\u6642\u306b\u306f*\u3072\u3069\u3044\u8003\u3048\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u4eca\u56de\u306f\u3001\u30e1\u30ea\u30c3\u30c8\u306f\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u5927\u5e45\u306b\u4e0a\u56de\u308a\u307e\u3057\u305f\u3002 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6982\u5ff5\u306e\u591a\u304f\u306f\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u307e\u305f\u306f\u4e3b\u89b3\u7684\u306a\u3082\u306e\u3067\u3059\u3002 \u4e00\u90e8\u306e\u4eba\u304c\u671b\u3080\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u3001\u3059\u3079\u3066\u304c\u767d\u9ed2\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u591a\u304f\u306e\u5834\u5408\u3001\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u306f\u300c\u6b21\u7b2c\u300d\u3067\u3059\u3002\n\n\n\n## \u30ae\u30e3\u30e9\u30ea\u30fc :art:\n\n\u30e6\u30fc\u30b6\u30fc\u304c\u6295\u7a3f\u3057\u305fGUI\u3068GitHub\u306b\u3042\u308bGUI\u306e\u3088\u308a\u6b63\u5f0f\u306a\u30ae\u30e3\u30e9\u30ea\u30fc\u306e\u4f5c\u6210\u306f\u9032\u884c\u4e2d\u3067\u3059\u304c\u3001readme\u3092\u4f5c\u6210\u6642\u70b9\u3067\u306f\u307e\u3060\u5b8c\u6210\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u73fe\u5728\u307e\u3068\u307e\u3063\u3066\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u898b\u308c\u308b\u5834\u6240\u306f2\u304b\u6240\u3042\u308a\u307e\u3059\u3002\u9858\u308f\u304f\u3070\u4eba\u3005\u304c\u4f5c\u3063\u3066\u3044\u308b\u7d20\u6674\u3089\u3057\u3044\u4f5c\u54c1\u3092\u6b63\u5f53\u5316\u3059\u308b\u305f\u3081\u306eWiki\u3084\u305d\u306e\u4ed6\u306e\u4ed5\u7d44\u307f\u304c\u3059\u3050\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002\n\n### \u30e6\u30fc\u30b6\u30fc\u304c\u63d0\u51fa\u3057\u305f\u30ae\u30e3\u30e9\u30ea\u30fc\n\n1\u3064\u76ee\u306f\u3001GitHub\u306b\u3042\u308b[\u30e6\u30fc\u30b6\u30fc\u304c\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u63d0\u51fa\u3057\u305fissue](https://github.com/PySimpleGUI/PySimpleGUI/issues/10)\u3067\u3059\u3002 \u3053\u308c\u306f\u3001\u4eba\u3005\u304c\u4f5c\u3063\u305f\u3082\u306e\u3092\u62ab\u9732\u3059\u308b\u305f\u3081\u306e\u975e\u516c\u5f0f\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u7406\u60f3\u7684\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u30b9\u30bf\u30fc\u30c8\u3067\u3057\u305f\u3002\n\n### \u5927\u91cf\u306b\u30b9\u30af\u30e9\u30c3\u30d7\u3055\u308c\u305fGitHub\u306e\u753b\u50cf\n\n2\u3064\u76ee\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u4f1d\u3048\u3089\u308c\u3066\u3044\u308bGitHub\u306e1,000\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u96c6\u3081\u305f[3,000\u4ee5\u4e0a\u306e\u753b\u50cf\u306e\u5927\u898f\u6a21\u306a\u30ae\u30e3\u30e9\u30ea\u30fc ](https://www.dropbox.com/sh/g67ms0darox0i2p/AAAMrkIM6C64nwHLDkboCWnaa?dl=0)\u3067\u3059\u3002 \u624b\u4f5c\u696d\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u3066\u304a\u308a\u521d\u671f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u305f\u53e4\u3044\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u3053\u306b\u3042\u306a\u305f\u306e\u60f3\u50cf\u529b\u3092\u5f15\u304d\u8d77\u3053\u3059\u4f55\u304b\u304c\u898b\u3064\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n<hr>\n\n# PySimpleGUI \u306e\u7528\u9014\u3067\u3059 :hammer:\n\n\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001PySimpleGUI\u306e\u7528\u9014\u306e\u4e00\u90e8\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 GitHub \u3060\u3051\u3067\u30821,000 \u4ee5\u4e0a\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067PySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u672c\u5f53\u306b\u3053\u308c\u3060\u3051\u306e\u591a\u304f\u306e\u4eba\u3005\u306e\u53ef\u80fd\u6027\u304c\u5e83\u304c\u3063\u305f\u3053\u3068\u306f\u306f\u672c\u5f53\u306b\u9a5a\u304f\u3079\u304d\u3053\u3068\u3067\u3059\u3002 \u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u4ee5\u524d\u306bPython\u3067GUI\u3092\u4f5c\u6210\u3057\u3088\u3046\u3068\u3057\u3066\u5931\u6557\u3057\u3057\u305f\u3068\u8a71\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5f7c\u3089\u304cPySimpleGUI\u3092\u8a66\u3057\u3066\u307f\u305f\u3068\u304d\u306b\u6700\u7d42\u7684\u306b\u81ea\u5206\u306e\u5922\u3092\u9054\u6210\u3057\u305f\u3068\u8a71\u3092\u3057\u307e\u3057\u305f\u3002\n\n## \u6700\u521d\u306eGUI\n\n\u3082\u3061\u308d\u3093\u3001PySimpleGUI\u306e\u6700\u3082\u512a\u308c\u305f\u4f7f\u3044\u65b9\u306e\u4e00\u3064\u306fPython\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u3060\u3051\u306e\u5c0f\u3055\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u3089\u958b\u59cb\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u305f\u3081\u306b\u306f\u3001`popup`\u3068\u547c\u3070\u308c\u308b\u300c\u30cf\u30a4\u30ec\u30d9\u30eb\u95a2\u6570\u300d\u306e1\u3064\u30921\u56de\u547c\u3073\u51fa\u3059\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002 \u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u306b\u306f\u3042\u3089\u3086\u308b\u7a2e\u985e\u304c\u3042\u308a\u3001\u4e00\u90e8\u306f\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\n\n`popup`\u81ea\u4f53\u3067\u60c5\u5831\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002print\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6e21\u305b\u307e\u3059\u3002\u60c5\u5831\u3092\u53d6\u5f97\u3057\u305f\u3044\u5834\u5408\u306f\u3001`popup_get_filename`\u306e\u3088\u3046\u306b`popup_get_\u3067`\u59cb\u307e\u308b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3057\u307e\u3059\u3002\n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e1\u884c\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u300c\u666e\u901a\u306e\u4eba\u300d\u304c\u5feb\u9069\u306b\u4f7f\u7528\u3067\u304d\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5909\u8eab\u3057\u307e\u3059\u3002\n\n\n```python\nimport PySimpleGUI as sg\n\nfilename = sg.popup_get_file('\u51e6\u7406\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044')\nsg.popup('\u5165\u529b\u3057\u305f', filename)\n```\n\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u30012\u3064\u306e\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u307e\u3059\u3002 1\u3064\u306f\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u3067\u3001\u5165\u529b\u30dc\u30c3\u30af\u30b9\u306e\u95b2\u89a7\u3084\u30da\u30fc\u30b9\u30c8\u304c\u3067\u304d\u307e\u3059\u3002  \n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\u3082\u3046\u4e00\u65b9\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u53ce\u96c6\u3055\u308c\u305f\u5185\u5bb9\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"  alt=\"img\" width=\"175px\"></a>\n\n</p>\n\n\n<br>\n\n## Rainmeter\u98a8\u30b9\u30bf\u30a4\u30eb\u30a6\u30a3\u30f3\u30c9\u30a6\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u898b\u6804\u3048\u306e\u826f\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u4f5c\u6210\u3067\u304d\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u7d30\u90e8\u306b\u6ce8\u610f\u3059\u308b\u3053\u3068\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9b45\u529b\u7684\u306b\u898b\u305b\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u3053\u3068\u3092\u304a\u3053\u306a\u3048\u307e\u3059\u3002 PySimpleGUI\u306f\u3001\u8272\u3084\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u306a\u3069\u306e\u6a5f\u80fd\u3092\u3088\u308a\u7c21\u5358\u306b\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002 \u305d\u306e\u7d50\u679c\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u3092Windows\u3067\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u3053\u306e\u4f8b\u3067\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30bf\u30a4\u30c8\u30eb \u30d0\u30fc\u304c\u524a\u9664\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u7d50\u679c\u3068\u3057\u3066\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u306eRainmeter\u3088\u3046\u306b\u898b\u3048\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002\n\n<br><br>\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u900f\u660e\u5ea6\u3082\u7c21\u5358\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002 \u540c\u3058Rainmeter\u30b9\u30bf\u30a4\u30eb\u306e\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u4ed6\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u534a\u900f\u660e\u306a\u306e\u3067\u3001\u8584\u6697\u304f\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\n\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u3068\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u534a\u900f\u660e\u5316\u306e\u4e21\u65b9\u306e\u52b9\u679c\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306fPySimpleGUI\u304c\u3044\u304b\u306b\u6a5f\u80fd\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u304b\u3092\u793a\u3059\u4f8b\u3067\u3059\u3002 \u307e\u305f\u3001PySimpleGUI \u306e\u30b3\u30fc\u30c9\u306fGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u3067\u79fb\u690d\u53ef\u80fd\u306a\u306e\u3067\u3001Qt\u306e\u3088\u3046\u306a\u4ed6\u306e\u30dd\u30fc\u30c8\u3067\u3082\u540c\u3058\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002\n\n\n\u4f8b\uff11\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u306e\u547c\u3073\u51fa\u3057\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u5909\u66f4\u3059\u308b\u3068\u540c\u69d8\u306e\u534a\u900f\u660e\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\n```python\nwindow = sg.Window('My window', layout, no_titlebar=True, alpha_channel=0.5)\n```\n\n## \u30b2\u30fc\u30e0\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306e SDK \u3068\u3057\u3066\u306f\u7279\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u3053\u306e\u30c1\u30a7\u30b9\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30c1\u30a7\u30b9\u3092\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u30c1\u30a7\u30b9AI\u300cStockfish\u300d\u3092\u7d71\u5408\u3057\u307e\u3059\u3002\n<br><br><br><br><br><br><br><br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u306e\u3044\u304f\u3064\u304b\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\n\n<br><br><br><br>\n<br><br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br><br><br><br><br><br><br><br><br>\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br>\n\nPySimpleGUI\u306e`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u3068\u753b\u50cf\u306e\u64cd\u4f5c\u304c\u7c21\u5358\u306a\u306e\u3067\u3001\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306eSDK\u3068\u3057\u3066\u66f8\u304b\u308c\u305f\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002<br><br>\n<br><br>\n<br><br><br>\n\n\n## \u30e1\u30c7\u30a3\u30a2\u306e\u30ad\u30e3\u30d7\u30c1\u30e3\u3068\u518d\u751f\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"  alt=\"img\" align=\"right\" width=\"400px\"></a>\n\n\nWEB\u30ab\u30e1\u30e9\u304b\u3089\u30d3\u30c7\u30aa\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3057\u3066GUI\u3067\u8868\u793a\u3059\u308b\u306e\u306b\u306f\u3001PySimpleGUI\u306e\u30b3\u30fc\u30c9\u3067\u306f4\u884c\u3067\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u5370\u8c61\u7684\u306a\u306e\u306f\u3053\u3089\u306e4\u884c\u306e\u30b3\u30fc\u30c9\u304c tkinter\u3001Qt\u3001\u304a\u3088\u3073 Web \u30dd\u30fc\u30c8\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002  tkinter\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u8868\u793a\u3059\u308b\u306e\u3068\u540c\u3058\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30d6\u30e9\u30a6\u30b6\u3067Web\u30ab\u30e1\u30e9\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u304c\u8868\u793a\u3067\u304d\u307e\u3059\u3002\n\n\u307e\u305f\u3001VLC\u30d7\u30ec\u30fc\u30e4\u30fc\u3092\u4f7f\u3063\u3066\u3001\u30aa\u30fc\u30c7\u30a3\u30aa\u3084\u30d3\u30c7\u30aa\u306a\u3069\u306e\u30e1\u30c7\u30a3\u30a2\u518d\u751f\u3082\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30e2\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5b9f\u969b\u306e\u4f5c\u696d\u4f8b\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u306ereadme\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u306f\u5168\u3066\u3001\u3042\u306a\u305f\u81ea\u8eab\u306e\u5275\u4f5c\u306e\u51fa\u767a\u70b9\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u307e\u3059\u3002\n<br><br><br><br><br>\n<br><br><br><br><br>\n<br><br>\n## \u4eba\u5de5\u77e5\u80fd\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\nAI\u3068Python\u306f\u9577\u3044\u9593\u3001\u3053\u306e2\u3064\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u305f\u3068\u304d\u306e\u30b9\u30fc\u30d1\u30fc\u30d1\u30ef\u30fc\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u591a\u304f\u306e\u5834\u5408\u3001\u30e6\u30fc\u30b6\u30fc\u304cGUI\u3092\u4f7f\u7528\u3057\u3066\u3053\u308c\u3089\u306eAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8eab\u8fd1\u306b\u64cd\u4f5c\u3059\u308b\u65b9\u6cd5\u304c\u6b20\u3051\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u308c\u3089\u306eYOLO\u306e\u30c7\u30e2\u306f\u3001GUI\u304cAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u306e\u5bfe\u8a71\u306b\u304a\u3044\u3066\u3044\u304b\u306b\u5927\u304d\u306a\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u304b\u306e\u7d20\u6674\u3089\u3057\u3044\u4f8b\u3067\u3059\u3002 \u3053\u308c\u3089\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e0b\u90e8\u306b\u3042\u308b2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306f\u3001YOLO\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u4f7f\u7528\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066YOLO\u30c7\u30e2\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5834\u5408\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u8d77\u52d5\u3059\u308b\u3068\u304d\u306b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8a2d\u5b9a\u3057\u3001\u305d\u306e\u5b9f\u884c\u65b9\u6cd5\u3092\u78ba\u8a8d\u3057\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u505c\u6b62\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5909\u66f4\u3057\u3001\u6700\u5f8c\u306b\u65b0\u3057\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n<br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u3053\u308c\u3089\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3001GUI\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u884c\u3067\u304d\u308b\u64cd\u4f5c\u3068\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\u3002 GUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u3001\u3059\u3050\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\n\n<br><br><br><br><br>\n<br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u516c\u958b\u3055\u308c\u3066\u3044\u308bAI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u52d5\u304b\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u975e\u5e38\u306b\u591a\u304f\u5b58\u5728\u3057\u307e\u3059\u3002 \u3053\u308c\u81ea\u4f53\u306f\u5927\u304d\u306a\u30cf\u30fc\u30c9\u30eb\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5165\u529b/\u8cbc\u308a\u4ed8\u3051\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u3066\u3001\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30d3\u30e5\u30fc\u30a2\u3067\u958b\u304f\u306b\u306f\u5341\u5206\u300c\u9762\u5012\u304f\u3055\u3044\u300d\u3067\u3059\u3002\n\n\nGUI \u306b\u306f\u3001**\u30e6\u30fc\u30b6\u30fc\u30a8\u30af\u30b9\u30da\u30ea\u30a8\u30f3\u30b9\u3092\u5909\u66f4\u3059\u308b**\u3092\u300cGUI\u30ae\u30e3\u30c3\u30d7\u300d\u306b\u5909\u5316\u3055\u305b\u308b\u529b\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306f\u753b\u50cf\u304c\u683c\u7d0d\u3055\u308c\u3066\u305f\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3057\u3066\u3001\u753b\u50cf\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3068\u7d50\u679c\u8868\u793a\u306e\u4e21\u65b9\u3092\u884c\u3048\u307e\u3059\u3002  \n\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0/\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u81ea\u7531\u306b\u5229\u7528\u53ef\u80fd\u3067\u3001\u4f7f\u7528\u53ef\u80fd\u3067\u3057\u305f\u3002 \u4e0d\u8db3\u3057\u3066\u3044\u305f\u306e\u306fGUI\u304c\u3082\u305f\u3089\u3059\u4f7f\u3044\u3084\u3059\u3055\u3067\u3059\u3002\n\n\n<hr>\n\n## \u30b0\u30e9\u30d5\u5316\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\nGUI\u3067\u306e\u30c7\u30fc\u30bf\u306e\u8868\u793a\u3068\u64cd\u4f5c\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\u3044\u304f\u3064\u304b\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002\n\u7d44\u307f\u8fbc\u307f\u306e\u63cf\u753b/\u30b0\u30e9\u30d5\u4f5c\u6210\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30ab\u30b9\u30bf\u30e0\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u3053\u306eCPU\u4f7f\u7528\u7387\u30e2\u30cb\u30bf\u306f`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nMatplotlib\u306fPython\u30e6\u30fc\u30b6\u30fc\u306b\u4eba\u6c17\u304c\u3042\u308a\u307e\u3059\u3002 PySimpleGUI\u306f\u3001Matplotlib\u306e\u30b0\u30e9\u30d5\u3092GUI\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u57cb\u3081\u8fbc\u3081\u307e\u3059\u3002 Matplotlib\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u6a5f\u80fd\u3092\u4fdd\u6301\u3057\u305f\u3044\u5834\u5408\u306f\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u57cb\u3081\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nPySimpleGUI\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u306e\u4eba\u304cMatplotlib\u3067\u4f5c\u6210\u3059\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u3088\u308a\u3082\u4e00\u6bb5\u4e0a\u306e\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<hr>\n\n## \u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u524d\u8ff0\u306e\u300cGUI \u30ae\u30e3\u30c3\u30d7\u300d\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002 GUI\u3092\u8ffd\u52a0\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u7528\u610f\u3059\u308b\u5fc5\u8981\u3082\u3042\u308a\u307e\u305b\u3093\u3002 \u300c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u300dGUI \u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u6e21\u3059\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308bGUI\u3067\u3059\u3002\n\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9GUI \u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3068\u3063\u3066\u30e6\u30fc\u30b6\u30fc\u304c\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30fb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u4f7f\u3044\u5fc3\u5730\u3088\u304f\u611f\u3058\u306a\u304b\u3063\u305f\u305f\u3081\u306b\u3001\u4ee5\u524d\u306f\u4f7f\u3044\u305f\u304c\u3089\u306a\u304b\u3063\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u914d\u5e03\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u3089\u306eGUI\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u305f\u3081\u306e\u552f\u4e00\u306e\u9078\u629e\u80a2\u3067\u3059\u3002\n\u3053\u306e\u4f8b\u306f\u3001\u300cJump Cutter\u300d\u3068\u3044\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u3067\u3059\u3002 \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306fGUI\u3092\u3068\u304a\u3057\u3066\u53ce\u96c6\u3055\u308c\u3066\u3001\u305d\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304c\u69cb\u7bc9\u3055\u308c\u3066\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u304cGUI\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3055\u308c\u3066\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001\u5b9f\u884c\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u304c\u9ec4\u8272\u3067\u8868\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n<br><br>\n<hr>\n\n## Raspberry Pi\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nPySimpleGUI\u306fPython 3.4\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u305f\u3081\u3001Raspberry Pi\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u30bf\u30c3\u30c1\u30b9\u30af\u30ea\u30fc\u30f3\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u7279\u306b\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u3002 \u30e2\u30cb\u30bf\u30fc\u304c\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001PySimpleGUIWeb\u3092\u4f7f\u7528\u3057\u3066Pi\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br><br>\n<hr>\n\n\n## \u9ad8\u5ea6\u306a\u6a5f\u80fd\u3078\u306e\u7c21\u5358\u306a\u30a2\u30af\u30bb\u30b9\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u57fa\u790e\u3068\u306a\u308b GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u591a\u304f\u306b\u975e\u5e38\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u305f\u3081\u3001GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u76f4\u63a5\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u307e\u3059\u3002\n\n\u305f\u3068\u3048\u3070\u3001tkinter\u3084\u305d\u306e\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3057\u3066\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u8272\u3084\u5916\u898b\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u304c\u3001PySimpleGUI \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30ab\u30b9\u30bf\u30e0\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u3092\u6301\u3063\u3066\u3044\u308b\u304b\u306e\u3088\u3046\u306b\u8868\u793a\u3055\u308c\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n<br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u4fe1\u3058\u3089\u308c\u306a\u3044\u3053\u3068\u306b\u3001\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u30b9\u30af\u30ea\u30fc\u30f3\u30bb\u30fc\u30d0\u30fc\u306e\u3088\u3046\u306b\u898b\u3048\u308b\u3082\u306e\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306btkinter\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306ftkinter \u306f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u80cc\u666f\u3092\u5b8c\u5168\u306b\u53d6\u308a\u9664\u3051\u307e\u3059\u3002 \u7e70\u308a\u8fd4\u3057\u307e\u3059\u304cPySimpleGUI\u306f\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3092\u7c21\u5358\u306b\u3057\u307e\u3059\u3002 \u900f\u660e\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001`Window`\u3092\u4f5c\u6210\u3059\u308b\u547c\u3073\u51fa\u3057\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u30921\u3064\u8ffd\u52a0\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 1\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u66f4\u3060\u3051\u3067\u3001\u6b21\u306e\u52b9\u679c\u3092\u6301\u3064\u5358\u7d14\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u4e0a\u306e\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u30d5\u30eb\u30b9\u30af\u30ea\u30fc\u30f3\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002\n<hr>\n\n# \u30c6\u30fc\u30de\n\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30ec\u30fc\u306eGUI\u306b\u3046\u3093\u3056\u308a\u3057\u307e\u3057\u305f\u304b?  PySimpleGUI \u306f`theme`\u95a2\u6570\u306e\u547c\u3073\u51fa\u3057\u3092\u884c\u3046\u3053\u3060\u3051\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u898b\u305f\u76ee\u3092\u7d20\u6575\u306b\u3057\u307e\u3059\u3002 150\u7a2e\u985e\u4ee5\u4e0a\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u9078\u629e\u3067\u304d\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ThemePreview.jpg\"  alt=\"img\" width=\"900px\"></a>\n</p>\n\n\n\u307b\u3068\u3093\u3069\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u306f\u3001\u4f5c\u6210\u3059\u308b\u3059\u3079\u3066\u306e\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002  PySimpleGUI\u306f\u3001\u3053\u306e\u96d1\u7528\u3092\u4ee3\u308f\u308a\u306b\u884c\u3044\u81ea\u52d5\u7684\u306b\u9078\u629e\u3057\u305f\u30c6\u30fc\u30de\u306b\u5408\u308f\u305b\u3066\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8272\u4ed8\u3051\u3057\u307e\u3059\u3002\n\n\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u524d\u306b\u30c6\u30fc\u30de\u540d\u3092\u6307\u5b9a\u3057\u3066`theme`\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u307e\u3059\u3002\u8aad\u307f\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c6\u30fc\u30de\u3092\u300cdark grey 9\u300d\u306b\u8a2d\u5b9a\u3059\u308b\u306b\u306f\n```python\nimport PySimpleGUI as sg\n\nsg.theme('dark grey 9')\n```\n\n\u3053\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5916\u89b3\u3092\u5b8c\u5168\u306b\u5909\u66f4\u3057\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/DarkGreyJapanese.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\n\u30c6\u30fc\u30de\u306f\u3001\u80cc\u666f\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u5165\u529b\u80cc\u666f\u3001\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3001\u304a\u3088\u3073\u30dc\u30bf\u30f3\u306e\u8272\u3092\u5909\u66f4\u3057\u307e\u3057\u305f\u3002 \u3053\u306e\u3088\u3046\u306a\u914d\u8272\u3092\u5909\u66f4\u3059\u308b\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u306f\u3001\u5404\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u500b\u5225\u306b\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u30b3\u30fc\u30c9\u3092\u4f55\u5ea6\u3082\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n<hr>\n\n# \u30b5\u30dd\u30fc\u30c8:muscle:\n\n\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3](http://www.PySimpleGUI.org)\u3068[\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0](http://Demos.PySimpleGUI.org)\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u3067\u3059\u3002\u3082\u3057\u307e\u3060\u8cea\u554f\u304c\u3042\u3063\u305f\u308a\u3001\u30d8\u30eb\u30d7\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f...\u554f\u984c\u3042\u308a\u307e\u305b\u3093...\u30d8\u30eb\u30d7\u306f\u7121\u6599\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3067[Issue\u3092\u63d0\u51fa](http://Issues.PySimpleGUI.org)\u3059\u308b\u3060\u3051\u3067\u3001\u52a9\u3051\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\u307b\u3068\u3093\u3069\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4f1a\u793e\u306f\u3001\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306b\u4ed8\u968f\u3059\u308b\u30d5\u30a9\u30fc\u30e0\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u306f\u60aa\u3044\u53d6\u5f15\u3067\u306f\u3042\u308a\u307e\u305b\u3093.\u30d5\u30a9\u30fc\u30e0\u306b\u5fc5\u8981\u4e8b\u9805\u8a18\u5165\u3059\u308c\u3070\u3001\u7121\u6599\u3067\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u60c5\u5831\u306f\u52b9\u7387\u7684\u306b\u56de\u7b54\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\nPySimpleGUI\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u756a\u53f7\u3084\u57fa\u306b\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3069\u306e\u60c5\u5831\u3092\u8981\u6c42\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u554f\u984c\u306e\u89e3\u6c7a\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u30ea\u30b9\u30c8\u3082\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\n***\u30d5\u30a9\u30fc\u30e0\u306b\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044 \u3002*** \u3000\u3042\u306a\u305f\u306b\u306f\u7121\u610f\u5473\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u307b\u3093\u306e\u4e00\u77ac\u3067\u3059\u304c\u82e6\u75db\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u8a18\u5165\u306f\u3042\u306a\u305f\u304c\u3088\u308a\u65e9\u304f\u89e3\u6c7a\u7b56\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u3082\u3057\u3042\u306a\u305f\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u30fc\u306a\u56de\u7b54\u3068\u89e3\u6c7a\u3092\u5f97\u308b\u305f\u3081\u306b\u5f79\u7acb\u3064\u5fc5\u8981\u306a\u60c5\u5831\u3067\u306a\u3051\u308c\u3070\u3001\u8a18\u5165\u306f\u5fc5\u8981\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u79c1\u306f\u3042\u306a\u305f\u3092\u52a9\u3051\u308b\u305f\u3081\u306b\u79c1\u3092\u52a9\u3051\u308b\u300d\u3002\n\n\n# \u30b5\u30dd\u30fc\u30c8\t<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"  alt=\"img\"  width=\"90px\"></a>\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8ca1\u653f\u7684\u652f\u63f4\u306f\u975e\u5e38\u306b\u9ad8\u304f\u8a55\u4fa1\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u6b63\u76f4\u306b\u8a00\u3046\u3068\u3001\u7d4c\u6e08\u7684\u306a\u63f4\u52a9\u304c\u5fc5\u8981\u3067\u3059\u3002 \u30e9\u30a4\u30c8\u3092\u3064\u3051\u7d9a\u3051\u308b\u3060\u3051\u3067\u9ad8\u4fa1\u3067\u3059\u3002 \u30c9\u30e1\u30a4\u30f3\u540d\u767b\u9332\u3001\u30c8\u30ea\u30f3\u30b1\u30c3\u30c8\u3001\u30b3\u30f3\u30b5\u30eb\u30c6\u30a3\u30f3\u30b0\u30d8\u30eb\u30d7\u306a\u3069\u306e\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u9577\u3044\u30ea\u30b9\u30c8\u306f\u3001\u3059\u3050\u306b\u304b\u306a\u308a\u306e\u7e70\u308a\u8fd4\u3057\u30b3\u30b9\u30c8\u306b\u52a0\u7b97\u3055\u308c\u307e\u3059\u3002\n\nPySimpleGUI \u306f\u4f5c\u6210\u3059\u308b\u306e\u306b\u5b89\u4fa1\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u611b\u306e\u52b4\u50cd\u306f\u3001\u975e\u5e38\u306b\u9762\u5012\u3067\u3057\u305f\u3002\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3068\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306b\u5230\u9054\u3059\u308b\u306b\u306f\u3001\u90317\u65e5\u306e\u4f5c\u696d\u306b2\u5e74\u4ee5\u4e0a\u5fc5\u8981\u3067\u3059\u3002\n\nPySimpleGUI\u306b\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u3001\u305d\u306e\u307e\u307e\u6b8b\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\u7d20\u6674\u3089\u3057\u3044\u3053\u3068\u3067\u3059\u3002 \u304a\u5ba2\u69d8\u307e\u305f\u306f\u304a\u5ba2\u69d8\u306e\u4f1a\u793e (\u7279\u306b\u4f01\u696d\u3067PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u5834\u5408) \u304c\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7d4c\u6e08\u7684\u306b\u5229\u76ca\u3092\u5f97\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5bff\u547d\u3092\u5ef6\u9577\u3059\u308b\u6a5f\u80fd\u3092\u6301\u3061\u307e\u3059\u3002\n\n###\u3000Buy Me a Coffee\n\n\u300cBuy Me a Coffee\u300d\u306f\u3001\u958b\u767a\u8005\u3092\u516c\u7684\u306b\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002 \u7d20\u65e9\u304f\u3001\u7c21\u5358\u306b\u3001\u8ca2\u732e\u306f\u8a18\u9332\u3055\u308c\u308b\u306e\u3067\u3001\u3042\u306a\u305f\u304cPySimpleGUI \u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308b\u3053\u3068\u3092\u4ed6\u306e\u4eba\u306b\u898b\u305b\u3089\u308c\u307e\u3059\u3002\u5bc4\u4ed8\u3092\u975e\u516c\u958b\u306b\u3082\u3067\u304d\u307e\u3059\u3002\n\n<a href=\"https://www.buymeacoffee.com/PySimpleGUI\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" width=\"217px\" ></a>\n\n\n\n### GitHub\u30b9\u30dd\u30f3\u30b5\u30fc\n\n<a href=\"https://github.com/sponsors/PySimpleGUI\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=%3Curl%3E&color=f88379\"></a>\n\n[GitHub\u5b9a\u671f\u7684\u306a\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7](https://github.com/sponsors\u30fc/PySimpleGUI)\u306f\u3001\u7d99\u7d9a\u7684\u306b\u3055\u307e\u3056\u307e\u306a\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u30b9\u30dd\u30f3\u30b5\u30fc\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u591a\u304f\u306e\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u4f01\u696d\u30ec\u30d9\u30eb\u306e\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u91d1\u92ad\u7684\u306b\u8ca2\u732e\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3001\u975e\u5e38\u306b\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u958b\u767a\u8005\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u7d4c\u6e08\u7684\u306b\u56f0\u96e3\u3067\u3059\u3002YouTube\u52d5\u753b\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306f\u3001\u52d5\u753b\u4f5c\u6210\u3067\u751f\u8a08\u3092\u7acb\u3066\u3066\u3044\u307e\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u306f\u307e\u3060\u305d\u308c\u307b\u3069\u7c21\u5358\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n# \u8ca2\u732e:construction_worker:\n\nPySimpleGUI \u306f\u73fe\u5728\u3001\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u81ea\u4f53\u306f\u88fd\u54c1\u306e\u3088\u3046\u306b\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u306f\u53d7\u3051\u4ed8\u3051\u3089\u308c\u307e\u305b\u3093\u3002\n\n\u8ca2\u732e\u3059\u308b\u6700\u3082\u826f\u3044\u65b9\u6cd5\u306e1\u3064\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u3068\u516c\u958b\u3067\u3059\u3002 \u30e6\u30fc\u30b6\u30fc\u306f\u3001\u4ed6\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u69cb\u7bc9\u3059\u308b\u3082\u306e\u3092\u898b\u3066\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3044\u307e\u3059\u3002 GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u30b3\u30fc\u30c9\u3092\u6295\u7a3f\u3057\u3066\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u306eReadme\u30d5\u30a1\u30a4\u30eb\u306b\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\u3002  \n\n\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a5f\u80fd\u304c\u3042\u3063\u305f\u308a\u3001\u6a5f\u80fd\u5f37\u5316\u3092\u63d0\u6848\u3057\u305f\u3044\u5834\u5408\u306f\u3001[issue\u3092\u958b\u3044\u3066\u304f\u3060\u3055\u3044](https://github.com/PySimpleGUI/PySimpleGUI/issues/new?assignees=&labels=&template=issue-form---must-fill-in-this-form-with-every-new-issue-submitted.md&title=%5B+Enhancement%2FBug%2FQuestion%5D+My+problem+is.)) \u3002\n\n\n# \u7279\u5225\u306a\u611f\u8b1d :pray:\n\n\nPySimpleGUI\u306e\u3053\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306ereadme\u306f[@M4cs](https://github.com/M4cs)\u306e\u52a9\u3051\u306a\u3057\u3067\u306f\u5b9f\u73fe\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5f7c\u306f\u7d20\u6674\u3089\u3057\u3044\u958b\u767a\u8005\u3067\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u7acb\u3061\u4e0a\u3052\u304b\u3089\u305a\u3063\u3068PySimpleGUI\u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3059\u3002 [@Israel Dryer](https://github.com/israel-dryer)\u3082\u307e\u305f\u9577\u671f\u7684\u306a\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308a\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u6a5f\u80fd\u306e\u9650\u754c\u3092\u62bc\u3057\u5e83\u3052\u305f\u3044\u304f\u3064\u304b\u306ePySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u3044\u3066\u3044\u307e\u3059\u3002 \u30dc\u30fc\u30c9\u306e\u753b\u50cf\u3092\u4f7f\u7528\u3057\u305f\u30e6\u30cb\u30fc\u30af\u306a\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30fc\u306fIsrael\u306b\u304c\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 [@jason990420](https://github.com/jason990420)\u306f\u4e0a\u306e\u5199\u771f\u306e\u3088\u3046\u306a\u3001PySimpleGUI \u3092\u4f7f\u3063\u305f\u6700\u521d\u306e\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u3068\u3001PySimpleGUI \u3067\u4f5c\u3089\u308c\u305f\u6700\u521d\u306e\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30b2\u30fc\u30e0\u3092\u516c\u958b\u3057\u3066\u3001\u591a\u304f\u306e\u4eba\u3092\u9a5a\u304b\u305b\u307e\u3057\u305f\u3002\n\u65e5\u672c\u8a9e\u7248\u306e readme \u306f[@okajun35](https://github.com/okajun35) \u3055\u3093\u306e\u5354\u529b\u3067\u5927\u5e45\u306b\u6539\u5584\u3055\u308c\u307e\u3057\u305f\u3002\n\nPySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b 1,200 \u4ee5\u4e0a\u306e GitHub \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3082\u300c\u3042\u308a\u304c\u3068\u3046\u300d\u306e\u8a00\u8449\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30a8\u30f3\u30b8\u30f3\u3092\u52d5\u304b\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u3042\u306a\u305f\u306e\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u304a\u304b\u3052\u3067\u3059\u3002\n\n\u4e00\u6669\u4e2dTwitter \u306b\u6295\u7a3f\u3057\u3066\u304f\u308c\u308b\u6d77\u5916\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001PySimpleGUI\u306e\u4e00\u65e5\u306e\u4f5c\u696d\u3092\u59cb\u3081\u308b\u304d\u3063\u304b\u3051\u3068\u306a\u308a\u307e\u3059\u3002\u5f7c\u3089\u306f\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u6e90\u3067\u3042\u308a\u3001\u958b\u767a\u30a8\u30f3\u30b8\u30f3\u3092\u59cb\u52d5\u3055\u305b\u3001\u6bce\u65e5\u7a3c\u50cd\u3055\u305b\u308b\u6e96\u5099\u3092\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\u611f\u8b1d\u306e\u610f\u3092\u8fbc\u3081\u3066\u3001\u3053\u306ereadme\u30d5\u30a1\u30a4\u30eb\u3092(\u65e5\u672c\u8a9e)[https://github.com/PySimpleGUI/PySimpleGUI/blob/master/readme.ja.md]\u306b\u7ffb\u8a33\u3057\u307e\u3057\u305f\u3002\n\n\u7686\u3055\u3093\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u671b\u3080\u6700\u9ad8\u306e\u30e6\u30fc\u30b6\u30fc\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3067\u3059\u3002\n\n\n\n&copy; Copyright 2020 PySimpleGUI.org \n"
 },
 {
  "repo": "opencv/opencv",
  "language": "C++",
  "readme_contents": "## OpenCV: Open Source Computer Vision Library\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/master/>\n* Q&A forum: <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib> \n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n"
 },
 {
  "repo": "CMU-Perceptual-Computing-Lab/openpose",
  "language": "C++",
  "readme_contents": "<div align=\"center\">\n    <img src=\".github/Logo_main_black.png\", width=\"300\">\n</div>\n\n-----------------\n\n|                  |`Default Config`  |`CUDA (+Python)`  |`CPU (+Python)`   |`OpenCL (+Python)`| `Debug`          | `Unity`          |\n| :---:            | :---:            | :---:            | :---:            | :---:            | :---:            | :---:            |\n| **`Linux`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/1)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/2)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/3)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/4)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/5)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/6)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`MacOS`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/8)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/9)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/10)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/11)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`Windows`** | [![Status](https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true)](https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master) | | | | |\n<!--\nNote: Currently using [travis-matrix-badges](https://github.com/bjfish/travis-matrix-badges) vs. traditional [![Build Status](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose.svg?branch=master)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose)\n-->\n\n[**OpenPose**](https://github.com/CMU-Perceptual-Computing-Lab/openpose) represents the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**.\n\nIt is **authored by [Gines Hidalgo](https://www.gineshidalgo.com), [Zhe Cao](https://people.eecs.berkeley.edu/~zhecao), [Tomas Simon](http://www.cs.cmu.edu/~tsimon), [Shih-En Wei](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [Hanbyul Joo](https://jhugestar.github.io), and [Yaser Sheikh](http://www.cs.cmu.edu/~yaser)**. Currently, it is being **maintained by [Gines Hidalgo](https://www.gineshidalgo.com) and [Yaadhav Raaj](https://www.raaj.tech)**. In addition, OpenPose would not be possible without the [**CMU Panoptic Studio dataset**](http://domedb.perception.cs.cmu.edu). We would also like to thank all the people who helped OpenPose in any way. The main contributors are listed in [doc/contributors.md](doc/contributors.md).\n\n<!-- The [original CVPR 2017 repo](https://github.com/ZheC/Multi-Person-Pose-Estimation) includes Matlab and Python versions, as well as the training code. The body pose estimation work is based on [the original ECCV 2016 demo](https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose). -->\n\n\n<p align=\"center\">\n    <img src=\"doc/media/pose_face_hands.gif\", width=\"480\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left) and <a href=\"https://jhugestar.github.io\" target=\"_blank\">Hanbyul Joo</a> (right) in front of the <a href=\"http://domedb.perception.cs.cmu.edu\" target=\"_blank\">CMU Panoptic Studio</a></sup>\n</p>\n\n## Features\n- **Functionality**:\n    - **2D real-time multi-person keypoint detection**:\n        - 15 or 18 or **25-keypoint body/foot keypoint estimation**. **Running time invariant to number of detected people**.\n        - **6-keypoint foot keypoint estimation**. Integrated together with the 25-keypoint body/foot keypoint detector.\n        - **2x21-keypoint hand keypoint estimation**. Currently, **running time depends** on **number of detected people**.\n        - **70-keypoint face keypoint estimation**. Currently, **running time depends** on **number of detected people**.\n    - **3D real-time single-person keypoint detection**:\n        - 3-D triangulation from multiple single views.\n        - Synchronization of Flir cameras handled.\n        - Compatible with Flir/Point Grey cameras, but provided C++ demos to add your custom input.\n    - **Calibration toolbox**:\n        - Easy estimation of distortion, intrinsic, and extrinsic camera parameters.\n    - **Single-person tracking** for further speed up or visual smoothing.\n- **Input**: Image, video, webcam, Flir/Point Grey and IP camera. Included C++ demos to add your custom input.\n- **Output**: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), and/or keypoints as array class.\n- **OS**: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\n- **Training and datasets**:\n    - [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train).\n    - [**Foot dataset website**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/).\n- **Others**:\n    - Available: command-line demo, C++ wrapper, and C++ API.\n    - [**Python API**](doc/python_module.md).\n    - [**Unity Plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin).\n    - CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only (no GPU) versions.\n\nFor further details, check [all released features](doc/released_features.md) and [release notes](doc/release_notes.md).\n\n\n\n## Related Work\n- Since Sep 2019: [**Training code**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)!\n- Since Jan 2019: [**Unity plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin)!\n- Since Dec 2018: [**Foot dataset**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset) and [**new paper released**](https://arxiv.org/abs/1812.08008)!\n\n\n\n## Results\n### Body and Foot Estimation\n<p align=\"center\">\n    <img src=\"doc/media/dance_foot.gif\", width=\"360\">\n    <br>\n    <sup>Testing the <a href=\"https://www.youtube.com/watch?v=2DiQUX11YaY\" target=\"_blank\"><i>Crazy Uptown Funk flashmob in Sydney</i></a> video sequence with OpenPose</sup>\n</p>\n\n### 3-D Reconstruction Module (Body, Foot, Face, and Hands)\n<p align=\"center\">\n    <img src=\"doc/media/openpose3d.gif\", width=\"360\">\n    <br>\n    <sup>Testing the 3D Reconstruction Module of OpenPose</sup>\n</p>\n\n### Body, Foot, Face, and Hands Estimation\n<p align=\"center\">\n    <img src=\"doc/media/pose_face.gif\", width=\"360\">\n    <img src=\"doc/media/pose_hands.gif\", width=\"360\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left image) and <a href=\"http://www.cs.cmu.edu/~tsimon\" target=\"_blank\">Tomas Simon</a> (right image) testing OpenPose</sup>\n</p>\n\n### Unity Plugin\n<p align=\"center\">\n    <img src=\"doc/media/unity_main.png\", width=\"240\">\n    <img src=\"doc/media/unity_body_foot.png\", width=\"240\">\n    <img src=\"doc/media/unity_hand_face.png\", width=\"240\">\n    <br>\n    <sup><a href=\"http://tianyizhao.com\" target=\"_blank\">Tianyi Zhao</a> and <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> testing their <a href=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin\" target=\"_blank\">OpenPose Unity Plugin</a></sup>\n</p>\n\n### Runtime Analysis\nInference time comparison between the 3 available pose estimation libraries: OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN:\n<p align=\"center\">\n    <img src=\"doc/media/openpose_vs_competition.png\", width=\"360\">\n</p>\nThis analysis was performed using the same images for each algorithm and a batch size of 1. Each analysis was repeated 1000 times and then averaged. This was all performed on a system with a Nvidia 1080 Ti and CUDA 8. Megvii (Face++) and MSRA GitHub repositories were excluded because they only provide pose estimation results given a cropped person. However, they suffer the same problem than Alpha-Pose and Mask R-CNN, their runtimes grow linearly with the number of people.\n\n\n\n## Contents\n1. [Features](#features)\n2. [Related Work](#related-work)\n3. [Results](#results)\n4. [Installation, Reinstallation and Uninstallation](#installation-reinstallation-and-uninstallation)\n5. [Quick Start](#quick-start)\n6. [Output](#output)\n7. [Speeding Up OpenPose and Benchmark](#speeding-up-openpose-and-benchmark)\n8. [Training Code and Foot Dataset](#training-code-and-foot-dataset)\n9. [Send Us Failure Cases and Feedback!](#send-us-failure-cases-and-feedback)\n10. [Citation](#citation)\n11. [License](#license)\n\n\n\n## Installation, Reinstallation and Uninstallation\n**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.\n\nOtherwise, check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.\n\n\n\n## Quick Start\nMost users do not need the OpenPose C++/Python API, but can simply use the OpenPose Demo:\n\n- **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g., run OpenPose in a video with:\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi\n```\n\n- **OpenPose C++ API**: If you want to read a specific input, and/or add your custom post-processing function, and/or implement your own display/saving, check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/). You can easily **create your custom code** on [examples/user_code/](examples/user_code/) and CMake will automatically compile it together with the whole OpenPose project. See [examples/user_code/README.md](examples/user_code/README.md) for more details.\n\n- **OpenPose Python API**: Analogously to the C++ API, find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).\n\n- **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/calibration/README.md](doc/calibration/README.md).\n\n- **Standalone face or hand detector**:\n    - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces), check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).\n    - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors, rather than using the body detector. E.g., useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).\n\n\n\n## Output\nOutput (format, keypoint index ordering, etc.) in [doc/output.md](doc/output.md).\n\n\n\n## Speeding Up OpenPose and Benchmark\nCheck the OpenPose Benchmark as well as some hints to speed up and/or reduce the memory requirements for OpenPose on [doc/speed_up_openpose.md](doc/speed_up_openpose.md).\n\n\n\n## Training Code and Foot Dataset\nFor training OpenPose, check [github.com/CMU-Perceptual-Computing-Lab/openpose_train](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train).\n\nFor the foot dataset, check the [foot dataset website](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/) and new [OpenPose paper](https://arxiv.org/abs/1812.08008) for more information.\n\n\n\n## Send Us Failure Cases and Feedback!\nOur library is open source for research purposes, and we want to continuously improve it! So please, let us know if...\n\n1. ... you find videos or images where OpenPose does not seems to work well. Feel free to send them to openposecmu@gmail.com (email only for failure cases!), we will use them to improve the quality of the algorithm!\n2. ... you find any bug (in functionality or speed).\n3. ... you added some functionality to some class or some new Worker<T> subclass which we might potentially incorporate.\n4. ... you know how to speed up or improve any part of the library.\n5. ... you have a request about possible functionality.\n6. ... etc.\n\nJust comment on GitHub or make a pull request and we will answer as soon as possible! Send us an email if you use the library to make a cool demo or YouTube video!\n\n\n\n## Citation\nPlease cite these papers in your publications if it helps your research. Most of OpenPose is based on `[8765346]`. In addition, the hand and face keypoint detectors are a combination of `[8765346]` and `[Simon et al. 2017]` (the face detector was trained using the same procedure than the hand detector).\n\n    @article{8765346,\n      author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2019}\n    }\n\n    @inproceedings{simon2017hand,\n      author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\n      year = {2017}\n    }\n\n    @inproceedings{cao2017realtime,\n      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2017}\n    }\n\n    @inproceedings{wei2016cpm,\n      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Convolutional pose machines},\n      year = {2016}\n    }\n\nLinks to the papers:\n\n- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\n    - [IEEE TPAMI](https://ieeexplore.ieee.org/document/8765346)\n    - [ArXiv](https://arxiv.org/abs/1812.08008)\n- [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809)\n- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)\n- [Convolutional Pose Machines](https://arxiv.org/abs/1602.00134)\n\n\n\n## License\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the [license](LICENSE) for further details. Interested in a commercial license? Check this [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740). For commercial queries, use the `Contact` section from the [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740) and also send a copy of that message to [Yaser Sheikh](mailto:yaser@cs.cmu.edu).\n"
 },
 {
  "repo": "oarriaga/face_classification",
  "language": "Python",
  "readme_contents": "# This repository is deprecated for at TF-2.0 rewrite visit:\n# https://github.com/oarriaga/paz\n------------------------------------------------\n# Face classification and detection.\nReal-time face detection and emotion/gender classification using fer2013/IMDB datasets with a keras CNN model and openCV.\n* IMDB gender classification test accuracy: 96%.\n* fer2013 emotion classification test accuracy: 66%.\n\nFor more information please consult the [publication](https://github.com/oarriaga/face_classification/blob/master/report.pdf)\n\n# Emotion/gender examples:\n\n![alt tag](images/demo_results.png)\n\nGuided back-prop\n![alt tag](images/gradcam_results.png)\n\nReal-time demo:\n<div align='center'>\n  <img src='images/color_demo.gif' width='400px'>\n</div>\n\n[B-IT-BOTS](https://mas-group.inf.h-brs.de/?page_id=622) robotics team :)\n![alt tag](images/robocup_team.png)\n\n## Instructions\n\n### Run real-time emotion demo:\n> python3 video_emotion_color_demo.py\n\n### Run real-time guided back-prop demo:\n> python3 image_gradcam_demo.py\n\n### Make inference on single images:\n> python3 image_emotion_gender_demo.py <image_path>\n\ne.g.\n\n> python3 image_emotion_gender_demo.py ../images/test_image.jpg\n\n### Running with Docker\n\nWith a few steps one can get its own face classification and detection running. Follow the commands below:\n\n* ```docker pull ekholabs/face-classifier```\n* ```docker run -d -p 8084:8084 --name=face-classifier ekholabs/face-classifier```\n* ```curl -v -F image=@[path_to_image]  http://localhost:8084/classifyImage > image.png```\n\n### To train previous/new models for emotion classification:\n\n\n* Download the fer2013.tar.gz file from [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xzf fer2013.tar\n\n* Run the train_emotion_classification.py file\n> python3 train_emotion_classifier.py\n\n### To train previous/new models for gender classification:\n\n* Download the imdb_crop.tar file from [here](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) (It's the 7GB button with the tittle Download faces only).\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xfv imdb_crop.tar\n\n* Run the train_gender_classification.py file\n> python3 train_gender_classifier.py\n\n"
 },
 {
  "repo": "vipstone/faceai",
  "language": "Python",
  "readme_contents": "[English Doc](README_en.md)\n# \u529f\u80fd #\n\n1. \u4eba\u8138\u68c0\u6d4b\u3001\u8bc6\u522b\uff08\u56fe\u7247\u3001\u89c6\u9891\uff09\n2. \u8f6e\u5ed3\u6807\u8bc6\n3. \u5934\u50cf\u5408\u6210\uff08\u7ed9\u4eba\u6234\u5e3d\u5b50\uff09\n4. \u6570\u5b57\u5316\u5986\uff08\u753b\u53e3\u7ea2\u3001\u7709\u6bdb\u3001\u773c\u775b\u7b49\uff09\n5. \u6027\u522b\u8bc6\u522b\n6. \u8868\u60c5\u8bc6\u522b\uff08\u751f\u6c14\u3001\u538c\u6076\u3001\u6050\u60e7\u3001\u5f00\u5fc3\u3001\u96be\u8fc7\u3001\u60ca\u559c\u3001\u5e73\u9759\u7b49\u4e03\u79cd\u60c5\u7eea\uff09\n7. \u89c6\u9891\u5bf9\u8c61\u63d0\u53d6\n8. \u56fe\u7247\u4fee\u590d\uff08\u53ef\u7528\u4e8e\u6c34\u5370\u53bb\u9664\uff09\n9. \u56fe\u7247\u81ea\u52a8\u4e0a\u8272\n10. \u773c\u52a8\u8ffd\u8e2a\uff08\u5f85\u5b8c\u5584\uff09\n11. \u6362\u8138\uff08\u5f85\u5b8c\u5584\uff09\n\n**\u67e5\u770b\u529f\u80fd\u9884\u89c8\u2193\u2193\u2193**\n\n# \u5f00\u53d1\u73af\u5883 #\n\n- Windows 10\uff08x64\uff09\n- Python 3.6.4\n- OpenCV 3.4.1\n- Dlib 19.8.1\n- face_recognition 1.2.2\n- keras 2.1.6\n- tensorflow 1.8.0\n- Tesseract OCR 4.0.0-beta.1\n\n\n# \u6559\u7a0b #\n\n[OpenCV\u73af\u5883\u642d\u5efa](doc/settingup.md)\n\n[Tesseract OCR\u6587\u5b57\u8bc6\u522b](doc/tesseractOCR.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/detectionOpenCV.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/detectionDlib.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/videoOpenCV.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/videoDlib.md)\n\n[\u8138\u90e8\u8f6e\u5ed3\u7ed8\u5236](doc/faceRecognitionOutline.md)\n\n[\u6570\u5b57\u5316\u5986](doc/faceRecognitionMakeup.md)\n\n[\u89c6\u9891\u4eba\u8138\u8bc6\u522b](doc/faceRecognition.md)\n\n[\u5934\u50cf\u7279\u6548\u5408\u6210](doc/compose.md)\n\n[\u6027\u522b\u8bc6\u522b](doc/gender.md)\n\n[\u8868\u60c5\u8bc6\u522b](doc/emotion.md)\n\n[\u89c6\u9891\u5bf9\u8c61\u63d0\u53d6](https://github.com/vipstone/faceai/blob/master/doc/hsv-opencv.md)\n\n[\u56fe\u7247\u4fee\u590d](https://github.com/vipstone/faceai/blob/master/doc/inpaint.md)\n\n\n# \u5176\u4ed6\u6559\u7a0b #\n\n[Ubuntu apt-get\u548cpip\u6e90\u66f4\u6362](doc/ubuntuChange.md)\n\n[pip/pip3\u66f4\u6362\u56fd\u5185\u6e90\u2014\u2014Windows\u7248](doc/pipChange.md)\n\n[OpenCV\u6dfb\u52a0\u4e2d\u6587](doc/chinese.md)\n\n[\u4f7f\u7528\u9f20\u6807\u7ed8\u56fe\u2014\u2014OpenCV](https://github.com/vipstone/faceai/blob/master/doc/opencv/mouse.md)\n\n\n# \u529f\u80fd\u9884\u89c8 #\n\n**\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/face_recognition-outline.png\" width = \"250\" height = \"300\" alt=\"\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3\" />\n\n----------\n\n**\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/dlib68.png\" width = \"230\" height = \"300\" alt=\"\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6\" />\n\n----------\n\n**\u5934\u50cf\u7279\u6548\u5408\u6210**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/compose.png\" width = \"200\" height = \"300\" alt=\"\u5934\u50cf\u7279\u6548\u5408\u6210\"  />\n\n----------\n\n**\u6027\u522b\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/gender.png\" width = \"430\" height = \"220\" alt=\"\u6027\u522b\u8bc6\u522b\"  />\n\n----------\n\n**\u8868\u60c5\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/emotion.png\" width = \"250\" height = \"300\" alt=\"\u8868\u60c5\u8bc6\u522b\"  />\n\n----------\n\n**\u6570\u5b57\u5316\u5986**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognitionMakeup-1.png\" width = \"450\" height = \"300\" alt=\"\u89c6\u9891\u4eba\u8138\u8bc6\u522b\"  />\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u68c0\u6d4b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/video-jiance.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognition.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](http://icdn.apigo.cn/opencv-hsv.gif)\n\n----------\n\n**\u56fe\u7247\u4fee\u590d**\n\n![](http://icdn.apigo.cn/inpaint.png?2)\n\n----------\n\n**\u56fe\u7247\u81ea\u52a8\u4e0a\u8272**\n\n![](http://icdn.apigo.cn/colorize-faceai.png)\n\n----------\n\n# \u6280\u672f\u65b9\u6848 #\n\n\u6280\u672f\u5b9e\u73b0\u65b9\u6848\u4ecb\u7ecd\n\n\t\u4eba\u8138\u8bc6\u522b\uff1aOpenCV / Dlib\n\t\n\t\u4eba\u8138\u68c0\u6d4b\uff1aface_recognition\n\t\n\t\u6027\u522b\u8bc6\u522b\uff1akeras + tensorflow\n\t\n\t\u6587\u5b57\u8bc6\u522b\uff1aTesseract OCR\n\n\n### TODO ###\n\n\u6362\u8138\u2014\u2014\u5f85\u5b8c\u5584\n\n\u773c\u775b\u79fb\u52a8\u65b9\u5411\u68c0\u6d4b\u2014\u2014\u5f85\u5b8c\u5584\n\nDlib\u6027\u80fd\u4f18\u5316\u65b9\u6848\n\nDlib\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\nTesseract\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\n# \u8d21\u732e\u8005\u540d\u5355\uff08\u7279\u522b\u611f\u8c22\uff09\t\n\n[archersmind](https://github.com/archersmind)\t\n\t\n[rishab-sharma](https://github.com/rishab-sharma)\n\n# \u5fae\u4fe1\u6253\u8d4f\n\n![\u5fae\u4fe1\u6253\u8d4f](http://icdn.apigo.cn/myinfo/wchat-pay.png)"
 },
 {
  "repo": "spmallick/learnopencv",
  "language": "Jupyter Notebook",
  "readme_contents": "# LearnOpenCV\nThis repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog [LearnOpenCV.com](https://www.LearnOpenCV.com).\n\nWant to become an expert in AI? [AI Courses by OpenCV](https://opencv.org/courses/) is a great place to start.\n\n<a href=\"https://opencv.org/courses/\">\n<p align=\"center\">\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/04/AI-Courses-By-OpenCV-Github.png\">\n</p>\n</a>\n\n## List of Blog Posts\n\n| Blog Post | |\n| ------------- |:-------------|\n|[Image Classification with OpenCV for Android](https://www.learnopencv.com/image-classification-with-opencv-for-android/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android) |\n|[Image Classification with OpenCV Java](https://www.learnopencv.com/image-classification-with-opencv-java)|[Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java) |\n|[PyTorch to Tensorflow Model Conversion](https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion) |\n|[Snake Game with OpenCV Python](https://www.learnopencv.com/snake-game-with-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SnakeGame) |\n|[Stanford MRNet Challenge: Classifying Knee MRIs](https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/)|[Code](https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model) |\n|[Experiment Logging with TensorBoard and wandb](https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging) |\n|[Understanding Lens Distortion](https://www.learnopencv.com/understanding-lens-distortion/)|[Code](https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion) |\n|[Image Matting with state-of-the-art Method \u201cF, B, Alpha Matting\u201d](https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FBAMatting) |\n|[Bag Of Tricks For Image Classification - Let's check if it is working or not](https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification) |\n|[Getting Started with OpenCV CUDA Module](https://www.learnopencv.com/getting-started-opencv-cuda-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module) |\n|[Training a Custom Object Detector with DLIB & Making Gesture Controlled Applications](https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib) |\n|[How To Run Inference Using TensorRT C++ API](https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP) |\n|[Using Facial Landmarks for Overlaying Faces with Medical Masks](https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay) |\n|[Tensorboard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning) |\n|[Otsu's Thresholding with OpenCV](https://www.learnopencv.com/otsu-thresholding-with-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/otsu-method) |\n|[PyTorch-to-CoreML-model-conversion](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion) |\n|[Playing Rock, Paper, Scissors with AI](https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI) |\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[CNN Fully Convolutional Image Classification with TensorFlow](https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow) | [Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification) |\n|[How to convert a model from PyTorch to TensorRT and speed up inference](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT) |\n|[Efficient image loading](https://www.learnopencv.com/efficient-image-loading/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading) |\n|[Graph Convolutional Networks: Model Relations In Data](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data)|\n|[Getting Started with Federated Learning with PyTorch and PySyft](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro)|\n|[Creating a Virtual Pen & Eraser](http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser) |\n|[Getting Started with PyTorch Lightning](https://www.learnopencv.com/getting-started-with-pytorch-lightning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning)|\n|[Multi-Label Image Classification with PyTorch: Image Tagging](https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging)|\n|[Funny Mirrors Using OpenCV](https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/)|[code](https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors)|\n|[t-SNE for ResNet feature visualization](https://www.learnopencv.com/t-sne-for-resnet-feature-visualization/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TSNE)|\n|[Multi-Label Image Classification with Pytorch](https://www.learnopencv.com/multi-label-image-classification-with-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification)|\n|[CNN Receptive Field Computation Using Backprop](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop)|\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[Augmented Reality using AruCo Markers in OpenCV(C++ and Python)](https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/) |[Code](https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers)|\n|[Fully Convolutional Image Classification on Arbitrary Sized Image](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification)|\n|[Camera Calibration using OpenCV](https://www.learnopencv.com/camera-calibration-using-opencv/) |[Code](https://github.com/spmallick/learnopencv/tree/master/CameraCalibration)|\n|[Geometry of Image Formation](https://www.learnopencv.com/geometry-of-image-formation/) ||\n|[Ensuring Training Reproducibility in Pytorch](https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch) ||\n|[Gaze Tracking](https://www.learnopencv.com/gaze-tracking/) ||\n|[Simple Background Estimation in Videos Using OpenCV](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation)|\n|[Applications of Foreground-Background separation with Semantic Segmentation](https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg) |\n|[EfficientNet: Theory + Code](https://www.learnopencv.com/efficientnet-theory-code) | [Code](https://github.com/spmallick/learnopencv/tree/master/EfficientNet) |\n|[PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch](https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/) | [Code](./PyTorch-Mask-RCNN) |\n|[PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN) |\n|[PyTorch for Beginners: Semantic Segmentation using torchvision](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision) |\n|[PyTorch for Beginners: Comparison of pre-trained models for Image Classification](https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb) |\n|[PyTorch for Beginners: Basics](https://www.learnopencv.com/pytorch-for-beginners-basics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb) |\n|[PyTorch Model Inference using ONNX and Caffe2](https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2) |\n|[Image Classification Using Transfer Learning in PyTorch](https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch) |\n|[Hangman: Creating games in OpenCV](https://www.learnopencv.com/hangman-creating-games-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hangman) |\n|[Image Inpainting with OpenCV (C++/Python)](https://www.learnopencv.com/image-inpainting-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting) |\n|[Hough Transform with OpenCV (C++/Python)](https://www.learnopencv.com/hough-transform-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hough-Transform) |\n|[Xeus-Cling: Run C++ code in Jupyter Notebook](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/) | [Code](https://github.com/spmallick/learnopencv/tree/master/XeusCling) |\n|[Gender & Age Classification using OpenCV Deep Learning ( C++/Python )](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AgeGender) |\n|[Invisibility Cloak using Color Detection and Segmentation with OpenCV](https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak) |\n|[Fast Image Downloader for Open Images V4 (Python)](https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/) | [Code](https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages) |\n|[Deep Learning based Text Detection Using OpenCV (C++/Python)](https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST) |\n|[Video Stabilization Using Point Feature Matching in OpenCV](https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoStabilization) |\n|[Training YOLOv3 : Deep Learning based Custom Object Detector](https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector\t) |\n|[Using OpenVINO with OpenCV](https://www.learnopencv.com/using-openvino-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV) |\n|[Duplicate Search on Quora Dataset](https://www.learnopencv.com/duplicate-search-on-quora-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search) |\n|[Shape Matching using Hu Moments (C++/Python)](https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HuMoments) |\n|[Install OpenCV 4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh) |\n|[Install OpenCV 4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh) |\n|[Install OpenCV 4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh) |\n|[Install OpenCV 3.4.4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh) |\n|[Install OpenCV 3.4.4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh) |\n|[OpenCV QR Code Scanner (C++ and Python)](https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV) |\n|[Install OpenCV 3.4.4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3) |\n|[Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh) |\n|[Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh) |\n|[Universal Sentence Encoder](https://www.learnopencv.com/universal-sentence-encoder) | [Code](https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder) |\n|[Install OpenCV 4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh) |\n|[Install OpenCV 4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4) |\n|[Hand Keypoint Detection using Deep Learning and OpenCV](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HandPose)|\n|[Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN) |\n|[Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh) |\n|[Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh) |\n|[Multi-Person Pose Estimation in OpenCV using OpenPose](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person) |\n|[Heatmap for Logo Detection using OpenCV (Python)](https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/heatmap)|\n|[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO)|\n|[Convex Hull using OpenCV in Python and C++](https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ConvexHull)|\n|[MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)](https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker) |\n|[Convolutional Neural Network based Image Colorization using OpenCV](https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colorization)|\n|[SVM using scikit-learn](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[GOTURN: Deep Learning based Object Tracking](https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/) | [Code](https://github.com/spmallick/learnopencv/tree/master/GOTURN)|\n|[Find the Center of a Blob (Centroid) using OpenCV (C++/Python)](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CenterofBlob)|\n|[Support Vector Machines (SVM)](https://www.learnopencv.com/support-vector-machines-svm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[Batch Normalization in Deep Networks](https://www.learnopencv.com/batch-normalization-in-deep-networks/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BatchNormalization)|\n|[Deep Learning based Character Classification using Synthetic Dataset](https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CharClassification)|\n|[Image Quality Assessment : BRISQUE](https://www.learnopencv.com/image-quality-assessment-brisque/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageMetrics)|\n|[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)|\n|[Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/OCR)|\n|[Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose)|\n|[Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)](https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/)| |\n|[How to convert your OpenCV C++ code into a Python module](https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/pymodule)|\n|[CV4Faces : Best Project Award 2018](https://www.learnopencv.com/cv4faces-best-project-award-2018/)| |\n|[Facemark : Facial Landmark Detection using OpenCV](https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection)|\n|[Image Alignment (Feature Based) using OpenCV (C++/Python)](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased)|\n|[Barcode and QR code Scanner using ZBar and OpenCV](https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner)|\n|[Keras Tutorial : Fine-tuning using pre-trained models](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning)|\n|[OpenCV Transparent API](https://www.learnopencv.com/opencv-transparent-api/)| |\n|[Face Reconstruction using EigenFaces (C++/Python)](https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces) |\n|[Eigenface using OpenCV (C++/Python)](https://www.learnopencv.com/eigenface-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/EigenFace)|\n|[Principal Component Analysis](https://www.learnopencv.com/principal-component-analysis/)| |\n|[Keras Tutorial : Transfer Learning using pre-trained models](https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning) |\n|[Keras Tutorial : Using pre-trained Imagenet models](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models) |\n|[Technical Aspects of a Digital SLR](https://www.learnopencv.com/technical-aspects-of-a-digital-slr/) | |\n|[Using Harry Potter interactive wand with OpenCV to create magic](https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/)| |\n|[Install OpenCV 3 and Dlib on Windows ( Python only )](https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/)| |\n|[Image Classification using Convolutional Neural Networks in Keras](https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR)|\n|[Understanding Autoencoders using Tensorflow (Python)](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder)|\n|[Best Project Award : Computer Vision for Faces](https://www.learnopencv.com/best-project-award-computer-vision-for-faces/) | |\n|[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)      | |\n|[Image Classification using Feedforward Neural Network in Keras](https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasMLP-MNIST)|\n|[Exposure Fusion using OpenCV (C++/Python)](https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/ExposureFusion)|\n|[Understanding Feedforward Neural Networks](https://www.learnopencv.com/understanding-feedforward-neural-networks/)      | |\n|[High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)](http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python)      | [Code](https://github.com/spmallick/learnopencv/tree/master/hdr)|\n|[Deep learning using Keras \u2013 The Basics](http://www.learnopencv.com/deep-learning-using-keras-the-basics)      | [Code](https://github.com/spmallick/learnopencv/tree/master/keras-linear-regression)|\n|[Selective Search for Object Detection (C++ / Python)](http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch) |\n|[Installing Deep Learning Frameworks on Ubuntu with CUDA support](http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/) | |\n|[Parallel Pixel Access in OpenCV using forEach](http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/) | [Code](https://github.com/spmallick/learnopencv/tree/master/forEach) |\n|[cvui: A GUI lib built on top of OpenCV drawing primitives](http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/) | [Code](https://github.com/spmallick/learnopencv/tree/master/UI-cvui) |\n|[Install Dlib on Windows](http://www.learnopencv.com/install-dlib-on-windows/) | |\n|[Install Dlib on Ubuntu](http://www.learnopencv.com/install-dlib-on-ubuntu/) | |\n|[Install OpenCV3 on Ubuntu](http://www.learnopencv.com/install-opencv3-on-ubuntu/) | |\n|[Read, Write and Display a video using OpenCV ( C++/ Python )](http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay) |\n|[Install Dlib on MacOS](http://www.learnopencv.com/install-dlib-on-macos/) | |\n|[Install OpenCV 3 on MacOS](http://www.learnopencv.com/install-opencv3-on-macos/) | |\n|[Install OpenCV 3 on Windows](http://www.learnopencv.com/install-opencv3-on-windows/) | |\n|[Get OpenCV Build Information ( getBuildInformation )](http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/) | |\n|[Color spaces in OpenCV (C++ / Python)](http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ColorSpaces)|\n|[Neural Networks : A 30,000 Feet View for Beginners](http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/) | |\n|[Alpha Blending using OpenCV (C++ / Python)](http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AlphaBlending) |\n|[User stories : How readers of this blog are applying their knowledge to build applications](http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/) | |\n|[How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?](http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/) | |\n|[Automatic Red Eye Remover using OpenCV (C++ / Python)](http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover) |\n|[Bias-Variance Tradeoff in Machine Learning](http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/) | |\n|[Embedded Computer Vision: Which device should you choose?](http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/) | |\n|[Object Tracking using OpenCV (C++/Python)](http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/tracking) |\n|[Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/digits-classification) |\n|[Training a better Haar and LBP cascade based Eye Detector using OpenCV](http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/) | |\n|[Deep Learning Book Gift Recipients](http://www.learnopencv.com/deep-learning-book-gift-recipients/) | |\n|[Minified OpenCV Haar and LBP Cascades](http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector)|\n|[Deep Learning Book Gift](http://www.learnopencv.com/deep-learning-book-gift/) | |\n|[Histogram of Oriented Gradients](http://www.learnopencv.com/histogram-of-oriented-gradients/) | |\n|[Image Recognition and Object Detection : Part 1](http://www.learnopencv.com/image-recognition-and-object-detection-part1/) | |\n|[Head Pose Estimation using OpenCV and Dlib](http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HeadPose) |\n|[Live CV : A Computer Vision Coding Application](http://www.learnopencv.com/live-cv/) | |\n|[Approximate Focal Length for Webcams and Cell Phone Cameras](http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) | |\n|[Configuring Qt for OpenCV on OSX](http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qt-test) |\n|[Rotation Matrix To Euler Angles](http://www.learnopencv.com/rotation-matrix-to-euler-angles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles) |\n|[Speeding up Dlib\u2019s Facial Landmark Detector](http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/) | |\n|[Warp one triangle to another using OpenCV ( C++ / Python )](http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/WarpTriangle) |\n|[Average Face : OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/average-face-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceAverage) |\n|[Face Swap using OpenCV ( C++ / Python )](http://www.learnopencv.com/face-swap-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceSwap) |\n|[Face Morph Using OpenCV \u2014 C++ / Python](http://www.learnopencv.com/face-morph-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceMorph) |\n|[Deep Learning Example using NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/) | |\n|[NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/nvidia-digits-3-on-ec2/) | |\n|[Homography Examples using OpenCV ( Python / C ++ )](http://www.learnopencv.com/homography-examples-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Homography) |\n|[Filling holes in an image using OpenCV ( Python / C++ )](http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Holes) |\n|[How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?](http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FPS) |\n|[Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) ](http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Delaunay) |\n|[OpenCV (C++ vs Python) vs MATLAB for Computer Vision](http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/) | |\n|[Facial Landmark Detection](http://www.learnopencv.com/facial-landmark-detection/) | |\n|[Why does OpenCV use BGR color format ?](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/) | |\n|[Computer Vision for Predicting Facial Attractiveness](http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness) |\n|[applyColorMap for pseudocoloring in OpenCV ( C++ / Python )](http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colormap) |\n|[Image Alignment (ECC) in OpenCV ( C++ / Python )](http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment) |\n|[How to find OpenCV version in Python and C++ ?](http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/) | |\n|[Baidu banned from ILSVRC 2015](http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/) | |\n|[OpenCV Transparent API](http://www.learnopencv.com/opencv-transparent-api/) | |\n|[How Computer Vision Solved the Greatest Soccer Mystery of All Time](http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/) | |\n|[Embedded Vision Summit 2015](http://www.learnopencv.com/embedded-vision-summit-2015/) | |\n|[Read an Image in OpenCV ( Python, C++ )](http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/imread) |\n|[Non-Photorealistic Rendering using OpenCV ( Python, C++ )](http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering) |\n|[Seamless Cloning using OpenCV ( Python , C++ )](http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning) |\n|[OpenCV Threshold ( Python , C++ )](http://www.learnopencv.com/opencv-threshold-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Threshold) |\n|[Blob Detection Using OpenCV ( Python, C++ )](http://www.learnopencv.com/blob-detection-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BlobDetector) |\n|[Turn your OpenCV Code into a Web API in under 10 minutes \u2014 Part 1](http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/) | |\n|[How to compile OpenCV sample Code ?](http://www.learnopencv.com/how-to-compile-opencv-sample-Code/) | |\n|[Install OpenCV 3 on Yosemite ( OSX 10.10.x )](http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/) | |\n"
 },
 {
  "repo": "Hironsan/BossSensor",
  "language": "Python",
  "readme_contents": "# BossSensor\nHide your screen when your boss is approaching.\n\n## Demo\nThe boss stands up. He is approaching.\n\n![standup](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/standup.jpg)\n\nWhen he is approaching, the program fetches face images and classifies the image.\n \n![approaching](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/approach.jpg)\n\nIf the image is classified as the Boss, it will monitor changes.\n\n![editor](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/editor.jpg)\n\n## Requirements\n\n* WebCamera\n* Python3.5\n* OSX\n* Anaconda\n* Lots of images of your boss and other person image\n\nPut images into [data/boss](https://github.com/Hironsan/BossSensor/tree/master/data/boss) and [data/other](https://github.com/Hironsan/BossSensor/tree/master/data/other).\n\n## Usage\nFirst, Train boss image.\n\n```\n$ python boss_train.py\n```\n\n\nSecond, start BossSensor. \n\n```\n$ python camera_reader.py\n```\n\n## Install\nInstall OpenCV, PyQt4, Anaconda.\n\n```\nconda create -n venv python=3.5\nsource activate venv\nconda install -c https://conda.anaconda.org/menpo opencv3\nconda install -c conda-forge tensorflow\npip install -r requirements.txt\n```\n\nChange Keras backend from Theano to TensorFlow. \n\n## Licence\n\n[MIT](https://github.com/Hironsan/BossSensor/blob/master/LICENSE)\n\n## Author\n\n[Hironsan](https://github.com/Hironsan)\n"
 },
 {
  "repo": "openframeworks/openFrameworks",
  "language": "C++",
  "readme_contents": "[openFrameworks](http://openframeworks.cc/)\n================\n\nopenFrameworks is a C++ toolkit for creative coding.  If you are new to OF, welcome!\n\n[![Slack Status](https://ofslack.herokuapp.com/badge.svg)](https://ofslack.herokuapp.com)\n\n## Build status\n\n* The **master** branch contains the newest, most recently updated code. This code is packaged and available for download in the \"Nightly Builds\" section of [openframeworks.cc/download](https://openframeworks.cc/download/).\n* The **stable** branch contains the code corresponding to the last stable openFrameworks release. This stable code is packaged and available for download at [openframeworks.cc/download](https://openframeworks.cc/download/).\n\nPlatform                     | Master branch  | Stable branch\n-----------------------------|:---------|:---------\nWindows MSYS2 32bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows MSYS2 64bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | N/A\nWindows Visual Studio 32bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows Visual Studio 64bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/4)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nLinux 64                     | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv6l                 | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv7l                 | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nEmscripten                   | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos                        | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos makefiles              | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\niOS                          | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\ntvos                         | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid Arm7                 | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid X86                  | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\n\n\n## folder structure\n\nThis release of OF comes with several folders:\n\n* addons\n* apps\n* docs\n* examples\n* export (on some systems)\n* libs\n* other\n* scripts\n* project generator\n\n\n`docs` has some documentation around OF usage, per platform things to consider, etc. You should definitely take a look in there; for example, if you are on OSX, read the osx.md.   `apps` and `examples` are where projects go -- `examples` contains a variety of projects that show you how to use OF, and `apps` is where your own projects will go.  `libs` contains the libraries that OF uses, including the openframeworks core itself.  `addons` are for additional functionality that's not part of the core.  `export` is for DLLs and dylibs that need to be put in each compiled project.  The `scripts` folder has the templates and small scripts for automating OF per platform. `project generator` is a GUI based tool for making new projects - this folder is only there in packaged releases.  \n\nOne idea that's important is that OF releases are designed to be self-contained.  You can put them anywhere on your hard drive, but it's not possible to mix different releases of OF together, so please keep each release (0.8.0, 0.8.1) separate.  Projects may generally work from release to release, but this is not guaranteed.  Because OF is self-contained, there's extensive use of local file paths (ie, ../../../) throughout OF.  It's important to be aware of how directories are structured.  A common error is to take a project and move it so that it's a level below or above where it used to be compared to the root of OF.  This means that links such as ../../../libs will break.  \n\n## Get involved\n\nThe openframeworks forum:\n\n[http://forum.openframeworks.cc/](http://forum.openframeworks.cc/)\n\nis a warm and friendly place.  Please ask or answer a question.  The most important part of this project is that it's a community, more than just a tool, so please join us!  Also, this is free software, and we learn so much about what is hard, what doesn't make sense, what is useful, etc. The most basic questions are acceptable here!  Don't worry, just join the conversation.  Learning in OF is social, it's hard to do it alone, but together we can get far!\n\nOur GitHub site is active:\n\n[https://github.com/openframeworks/openFrameworks](https://github.com/openframeworks/openFrameworks)\n\nif you have bugs or feature requests, consider opening an issue.  If you are a developer and want to help, pull requests are warmly welcome.  Please read the contributing guide for guidelines:\n\n[https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md](https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md\n)\n\nWe also have a developer's mailing list, which is useful for discussing issues around the development and future of OF.\n\n## Developers\n\nTo grab a copy of openFrameworks for your platform, check the [download page](http://openframeworks.cc/download) on the main site.  \n\nIf you are working with the Git repository, the `stable` branch of the OF repository corresponds to the most recent release, with a few important differences:  \n\n1. The release includes a simple openFrameworks project generator.\n2. This GitHub repository contains code and libs for all the platforms, but the releases are done on a per-platform basis.\n3. This GitHub repository has no project files for the different examples. They are generated automatically for each release using a tool in `apps/projectGenerator/`.\n4. There are no external dependencies in this repository, you can download them using the download_libs.sh script for each platform in the particular platform folder inside scripts.\n\nIf you want to work with the openFrameworks GitHub repository, you need to download the external dependencies and you should use the project generator to create project files for all the code in `examples/`.  To generate the project files with the project generator enable the 'Advanced Options' in the settings tab, then use 'Update Multiple' to update the projects for the `examples/` folder path in the repo.\n\nTo set up the project generator submodule within the OF repo, use the command `git submodule init` then `git submodule update` whilst inside the openFrameworks repo.\n\nFor more info on working with the Project Generator, for per-platform readmes, and more information, see the [documentation](docs/table_of_contents.md).\n\n## Versioning\n\nopenFrameworks uses [Semantic Versioning](http://semver.org/), although strict adherence will only come into effect at version 1.0.0.\n"
 },
 {
  "repo": "bytedeco/javacv",
  "language": "Java",
  "readme_contents": "JavaCV\r\n======\r\n\r\n[![Gitter](https://badges.gitter.im/bytedeco/javacv.svg)](https://gitter.im/bytedeco/javacv) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform) [![Sonatype Nexus (Snapshots)](https://img.shields.io/nexus/s/https/oss.sonatype.org/org.bytedeco/javacv.svg)](http://bytedeco.org/builds/) [![Build Status](https://travis-ci.org/bytedeco/javacv.svg?branch=master)](https://travis-ci.org/bytedeco/javacv) <sup>Commercial support:</sup> [![xscode](https://img.shields.io/badge/Available%20on-xs%3Acode-blue?style=?style=plastic&logo=appveyor&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////////VXz1bAAAAAJ0Uk5T/wDltzBKAAAAlUlEQVR42uzXSwqAMAwE0Mn9L+3Ggtgkk35QwcnSJo9S+yGwM9DCooCbgn4YrJ4CIPUcQF7/XSBbx2TEz4sAZ2q1RAECBAiYBlCtvwN+KiYAlG7UDGj59MViT9hOwEqAhYCtAsUZvL6I6W8c2wcbd+LIWSCHSTeSAAECngN4xxIDSK9f4B9t377Wd7H5Nt7/Xz8eAgwAvesLRjYYPuUAAAAASUVORK5CYII=)](https://xscode.com/bytedeco/javacv)\r\n\r\n\r\nIntroduction\r\n------------\r\nJavaCV uses wrappers from the [JavaCPP Presets](https://github.com/bytedeco/javacpp-presets) of commonly used libraries by researchers in the field of computer vision ([OpenCV](http://opencv.org/), [FFmpeg](http://ffmpeg.org/), [libdc1394](http://damien.douxchamps.net/ieee1394/libdc1394/), [PGR FlyCapture](https://www.ptgrey.com/flycapture-sdk), [OpenKinect](http://openkinect.org/), [librealsense](https://github.com/IntelRealSense/librealsense), [CL PS3 Eye Driver](https://codelaboratories.com/downloads/), [videoInput](http://muonics.net/school/spring05/videoInput/), [ARToolKitPlus](https://launchpad.net/artoolkitplus), [flandmark](http://cmp.felk.cvut.cz/~uricamic/flandmark/), [Leptonica](http://www.leptonica.org/), and [Tesseract](https://github.com/tesseract-ocr/tesseract)) and provides utility classes to make their functionality easier to use on the Java platform, including Android.\r\n\r\nJavaCV also comes with hardware accelerated full-screen image display (`CanvasFrame` and `GLCanvasFrame`), easy-to-use methods to execute code in parallel on multiple cores (`Parallel`), user-friendly geometric and color calibration of cameras and projectors (`GeometricCalibrator`, `ProCamGeometricCalibrator`, `ProCamColorCalibrator`), detection and matching of feature points (`ObjectFinder`), a set of classes that implement direct image alignment of projector-camera systems (mainly `GNImageAligner`, `ProjectiveTransformer`, `ProjectiveColorTransformer`, `ProCamTransformer`, and `ReflectanceInitializer`), a blob analysis package (`Blobs`), as well as miscellaneous functionality in the `JavaCV` class. Some of these classes also have an OpenCL and OpenGL counterpart, their names ending with `CL` or starting with `GL`, i.e.: `JavaCVCL`, `GLCanvasFrame`, etc.\r\n\r\nTo learn how to use the API, since documentation currently lacks, please refer to the [Sample Usage](#sample-usage) section below as well as the [sample programs](https://github.com/bytedeco/javacv/tree/master/samples/), including two for Android (`FacePreview.java` and `RecordActivity.java`), also found in the `samples` directory. You may also find it useful to refer to the source code of [ProCamCalib](https://github.com/bytedeco/procamcalib) and [ProCamTracker](https://github.com/bytedeco/procamtracker) as well as [examples ported from OpenCV2 Cookbook](https://github.com/bytedeco/javacv-examples/) and the associated [wiki pages](https://github.com/bytedeco/javacv-examples/tree/master/OpenCV_Cookbook).\r\n\r\nPlease keep me informed of any updates or fixes you make to the code so that I may integrate them into the next release. Thank you! And feel free to ask questions on [the mailing list](http://groups.google.com/group/javacv) if you encounter any problems with the software! I am sure it is far from perfect...\r\n\r\n\r\nDownloads\r\n---------\r\nArchives containing JAR files are available as [releases](https://github.com/bytedeco/javacv/releases). The binary archive contains builds for Android, iOS, Linux, Mac OS X, and Windows. The JAR files for specific child modules or platforms can also be obtained individually from the [Maven Central Repository](http://search.maven.org/#search|ga|1|bytedeco).\r\n\r\nTo install manually the JAR files, follow the instructions in the [Manual Installation](#manual-installation) section below.\r\n\r\nWe can also have everything downloaded and installed automatically with:\r\n\r\n * Maven (inside the `pom.xml` file)\r\n```xml\r\n  <dependency>\r\n    <groupId>org.bytedeco</groupId>\r\n    <artifactId>javacv-platform</artifactId>\r\n    <version>1.5.4</version>\r\n  </dependency>\r\n```\r\n\r\n * Gradle (inside the `build.gradle` file)\r\n```groovy\r\n  dependencies {\r\n    implementation group: 'org.bytedeco', name: 'javacv-platform', version: '1.5.4'\r\n  }\r\n```\r\n\r\n * Leiningen (inside the `project.clj` file)\r\n```clojure\r\n  :dependencies [\r\n    [org.bytedeco/javacv-platform \"1.5.4\"]\r\n  ]\r\n```\r\n\r\n * sbt (inside the `build.sbt` file)\r\n```scala\r\n  libraryDependencies += \"org.bytedeco\" % \"javacv-platform\" % \"1.5.4\"\r\n```\r\n\r\nThis downloads binaries for all platforms, but to get binaries for only one platform we can set the `javacpp.platform` system property (via the `-D` command line option) to something like `android-arm`, `linux-x86_64`, `macosx-x86_64`, `windows-x86_64`, etc. Please refer to the [README.md file of the JavaCPP Presets](https://github.com/bytedeco/javacpp-presets#downloads) for details. Another option available to Gradle users is [Gradle JavaCPP](https://github.com/bytedeco/gradle-javacpp), and similarly for Scala users there is [SBT-JavaCV](https://github.com/bytedeco/sbt-javacv).\r\n\r\n\r\nRequired Software\r\n-----------------\r\nTo use JavaCV, you will first need to download and install the following software:\r\n\r\n * An implementation of Java SE 7 or newer:\r\n   * OpenJDK  http://openjdk.java.net/install/  or\r\n   * Oracle JDK  http://www.oracle.com/technetwork/java/javase/downloads/  or\r\n   * IBM JDK  http://www.ibm.com/developerworks/java/jdk/\r\n\r\nFurther, although not always required, some functionality of JavaCV also relies on:\r\n\r\n * CL Eye Platform SDK (Windows only)  http://codelaboratories.com/downloads/\r\n * Android SDK API 21 or newer  http://developer.android.com/sdk/\r\n * JOCL and JOGL from JogAmp  http://jogamp.org/\r\n\r\nFinally, please make sure everything has the same bitness: **32-bit and 64-bit modules do not mix under any circumstances**.\r\n\r\n\r\nManual Installation\r\n-------------------\r\nSimply put all the desired JAR files (`opencv*.jar`, `ffmpeg*.jar`, etc.), in addition to `javacpp.jar` and `javacv.jar`, somewhere in your class path. Here are some more specific instructions for common cases:\r\n\r\nNetBeans (Java SE 7 or newer):\r\n\r\n 1. In the Projects window, right-click the Libraries node of your project, and select \"Add JAR/Folder...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nEclipse (Java SE 7 or newer):\r\n\r\n 1. Navigate to Project > Properties > Java Build Path > Libraries and click \"Add External JARs...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nIntelliJ IDEA (Android 5.0 or newer):\r\n\r\n 1. Follow the instructions on this page: http://developer.android.com/training/basics/firstapp/\r\n 2. Copy all the JAR files into the `app/libs` subdirectory.\r\n 3. Navigate to File > Project Structure > app > Dependencies, click `+`, and select \"2 File dependency\".\r\n 4. Select all the JAR files from the `libs` subdirectory.\r\n\r\nAfter that, the wrapper classes for OpenCV and FFmpeg, for example, can automatically access all of their C/C++ APIs:\r\n\r\n * [OpenCV documentation](http://docs.opencv.org/master/)\r\n * [FFmpeg documentation](http://ffmpeg.org/doxygen/trunk/)\r\n\r\n\r\nSample Usage\r\n------------\r\nThe class definitions are basically ports to Java of the original header files in C/C++, and I deliberately decided to keep as much of the original syntax as possible. For example, here is a method that tries to load an image file, smooth it, and save it back to disk:\r\n\r\n```java\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgcodecs.*;\r\n\r\npublic class Smoother {\r\n    public static void smooth(String filename) {\r\n        Mat image = imread(filename);\r\n        if (image != null) {\r\n            GaussianBlur(image, image, new Size(3, 3), 0);\r\n            imwrite(filename, image);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nJavaCV also comes with helper classes and methods on top of OpenCV and FFmpeg to facilitate their integration to the Java platform. Here is a small demo program demonstrating the most frequently useful parts:\r\n\r\n```java\r\nimport java.io.File;\r\nimport java.net.URL;\r\nimport org.bytedeco.javacv.*;\r\nimport org.bytedeco.javacpp.*;\r\nimport org.bytedeco.javacpp.indexer.*;\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport org.bytedeco.opencv.opencv_calib3d.*;\r\nimport org.bytedeco.opencv.opencv_objdetect.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_calib3d.*;\r\nimport static org.bytedeco.opencv.global.opencv_objdetect.*;\r\n\r\npublic class Demo {\r\n    public static void main(String[] args) throws Exception {\r\n        String classifierName = null;\r\n        if (args.length > 0) {\r\n            classifierName = args[0];\r\n        } else {\r\n            URL url = new URL(\"https://raw.github.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\");\r\n            File file = Loader.cacheResource(url);\r\n            classifierName = file.getAbsolutePath();\r\n        }\r\n\r\n        // We can \"cast\" Pointer objects by instantiating a new object of the desired class.\r\n        CascadeClassifier classifier = new CascadeClassifier(classifierName);\r\n        if (classifier == null) {\r\n            System.err.println(\"Error loading classifier file \\\"\" + classifierName + \"\\\".\");\r\n            System.exit(1);\r\n        }\r\n\r\n        // The available FrameGrabber classes include OpenCVFrameGrabber (opencv_videoio),\r\n        // DC1394FrameGrabber, FlyCapture2FrameGrabber, OpenKinectFrameGrabber, OpenKinect2FrameGrabber,\r\n        // RealSenseFrameGrabber, RealSense2FrameGrabber, PS3EyeFrameGrabber, VideoInputFrameGrabber, and FFmpegFrameGrabber.\r\n        FrameGrabber grabber = FrameGrabber.createDefault(0);\r\n        grabber.start();\r\n\r\n        // CanvasFrame, FrameGrabber, and FrameRecorder use Frame objects to communicate image data.\r\n        // We need a FrameConverter to interface with other APIs (Android, Java 2D, JavaFX, Tesseract, OpenCV, etc).\r\n        OpenCVFrameConverter.ToMat converter = new OpenCVFrameConverter.ToMat();\r\n\r\n        // FAQ about IplImage and Mat objects from OpenCV:\r\n        // - For custom raw processing of data, createBuffer() returns an NIO direct\r\n        //   buffer wrapped around the memory pointed by imageData, and under Android we can\r\n        //   also use that Buffer with Bitmap.copyPixelsFromBuffer() and copyPixelsToBuffer().\r\n        // - To get a BufferedImage from an IplImage, or vice versa, we can chain calls to\r\n        //   Java2DFrameConverter and OpenCVFrameConverter, one after the other.\r\n        // - Java2DFrameConverter also has static copy() methods that we can use to transfer\r\n        //   data more directly between BufferedImage and IplImage or Mat via Frame objects.\r\n        Mat grabbedImage = converter.convert(grabber.grab());\r\n        int height = grabbedImage.rows();\r\n        int width = grabbedImage.cols();\r\n\r\n        // Objects allocated with `new`, clone(), or a create*() factory method are automatically released\r\n        // by the garbage collector, but may still be explicitly released by calling deallocate().\r\n        // You shall NOT call cvReleaseImage(), cvReleaseMemStorage(), etc. on objects allocated this way.\r\n        Mat grayImage = new Mat(height, width, CV_8UC1);\r\n        Mat rotatedImage = grabbedImage.clone();\r\n\r\n        // The OpenCVFrameRecorder class simply uses the VideoWriter of opencv_videoio,\r\n        // but FFmpegFrameRecorder also exists as a more versatile alternative.\r\n        FrameRecorder recorder = FrameRecorder.createDefault(\"output.avi\", width, height);\r\n        recorder.start();\r\n\r\n        // CanvasFrame is a JFrame containing a Canvas component, which is hardware accelerated.\r\n        // It can also switch into full-screen mode when called with a screenNumber.\r\n        // We should also specify the relative monitor/camera response for proper gamma correction.\r\n        CanvasFrame frame = new CanvasFrame(\"Some Title\", CanvasFrame.getDefaultGamma()/grabber.getGamma());\r\n\r\n        // Let's create some random 3D rotation...\r\n        Mat randomR    = new Mat(3, 3, CV_64FC1),\r\n            randomAxis = new Mat(3, 1, CV_64FC1);\r\n        // We can easily and efficiently access the elements of matrices and images\r\n        // through an Indexer object with the set of get() and put() methods.\r\n        DoubleIndexer Ridx = randomR.createIndexer(),\r\n                   axisIdx = randomAxis.createIndexer();\r\n        axisIdx.put(0, (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4);\r\n        Rodrigues(randomAxis, randomR);\r\n        double f = (width + height) / 2.0;  Ridx.put(0, 2, Ridx.get(0, 2) * f);\r\n                                            Ridx.put(1, 2, Ridx.get(1, 2) * f);\r\n        Ridx.put(2, 0, Ridx.get(2, 0) / f); Ridx.put(2, 1, Ridx.get(2, 1) / f);\r\n        System.out.println(Ridx);\r\n\r\n        // We can allocate native arrays using constructors taking an integer as argument.\r\n        Point hatPoints = new Point(3);\r\n\r\n        while (frame.isVisible() && (grabbedImage = converter.convert(grabber.grab())) != null) {\r\n            // Let's try to detect some faces! but we need a grayscale image...\r\n            cvtColor(grabbedImage, grayImage, CV_BGR2GRAY);\r\n            RectVector faces = new RectVector();\r\n            classifier.detectMultiScale(grayImage, faces);\r\n            long total = faces.size();\r\n            for (long i = 0; i < total; i++) {\r\n                Rect r = faces.get(i);\r\n                int x = r.x(), y = r.y(), w = r.width(), h = r.height();\r\n                rectangle(grabbedImage, new Point(x, y), new Point(x + w, y + h), Scalar.RED, 1, CV_AA, 0);\r\n\r\n                // To access or pass as argument the elements of a native array, call position() before.\r\n                hatPoints.position(0).x(x - w / 10     ).y(y - h / 10);\r\n                hatPoints.position(1).x(x + w * 11 / 10).y(y - h / 10);\r\n                hatPoints.position(2).x(x + w / 2      ).y(y - h / 2 );\r\n                fillConvexPoly(grabbedImage, hatPoints.position(0), 3, Scalar.GREEN, CV_AA, 0);\r\n            }\r\n\r\n            // Let's find some contours! but first some thresholding...\r\n            threshold(grayImage, grayImage, 64, 255, CV_THRESH_BINARY);\r\n\r\n            // To check if an output argument is null we may call either isNull() or equals(null).\r\n            MatVector contours = new MatVector();\r\n            findContours(grayImage, contours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);\r\n            long n = contours.size();\r\n            for (long i = 0; i < n; i++) {\r\n                Mat contour = contours.get(i);\r\n                Mat points = new Mat();\r\n                approxPolyDP(contour, points, arcLength(contour, true) * 0.02, true);\r\n                drawContours(grabbedImage, new MatVector(points), -1, Scalar.BLUE);\r\n            }\r\n\r\n            warpPerspective(grabbedImage, rotatedImage, randomR, rotatedImage.size());\r\n\r\n            Frame rotatedFrame = converter.convert(rotatedImage);\r\n            frame.showImage(rotatedFrame);\r\n            recorder.record(rotatedFrame);\r\n        }\r\n        frame.dispose();\r\n        recorder.stop();\r\n        grabber.stop();\r\n    }\r\n}\r\n```\r\n\r\nFurthermore, after creating a `pom.xml` file with the following content:\r\n```xml\r\n<project>\r\n    <modelVersion>4.0.0</modelVersion>\r\n    <groupId>org.bytedeco.javacv</groupId>\r\n    <artifactId>demo</artifactId>\r\n    <version>1.5.4</version>\r\n    <properties>\r\n        <maven.compiler.source>1.7</maven.compiler.source>\r\n        <maven.compiler.target>1.7</maven.compiler.target>\r\n    </properties>\r\n    <dependencies>\r\n        <dependency>\r\n            <groupId>org.bytedeco</groupId>\r\n            <artifactId>javacv-platform</artifactId>\r\n            <version>1.5.4</version>\r\n        </dependency>\r\n    </dependencies>\r\n    <build>\r\n        <sourceDirectory>.</sourceDirectory>\r\n    </build>\r\n</project>\r\n```\r\n\r\nAnd by placing the source code above in `Demo.java`, or similarly for other classes found in the [`samples`](samples), we can use the following command to have everything first installed automatically and then executed by Maven:\r\n```bash\r\n $ mvn compile exec:java -Dexec.mainClass=Demo\r\n```\r\n\r\n**Note**: In case of errors, please make sure that the `artifactId` in the `pom.xml` file reads `javacv-platform`, not `javacv` only, for example. The artifact `javacv-platform` adds all the necessary binary dependencies.\r\n\r\n\r\nBuild Instructions\r\n------------------\r\nIf the binary files available above are not enough for your needs, you might need to rebuild them from the source code. To this end, the project files were created for:\r\n\r\n * Maven 3.x  http://maven.apache.org/download.html\r\n * JavaCPP 1.5.4  https://github.com/bytedeco/javacpp\r\n * JavaCPP Presets 1.5.4  https://github.com/bytedeco/javacpp-presets\r\n\r\nOnce installed, simply call the usual `mvn install` command for JavaCPP, its Presets, and JavaCV. By default, no other dependencies than a C++ compiler for JavaCPP are required. Please refer to the comments inside the `pom.xml` files for further details.\r\n\r\nInstead of building the native libraries manually, we can run `mvn install` for JavaCV only and rely on the snapshot artifacts from the CI builds:\r\n\r\n * http://bytedeco.org/builds/\r\n\r\n\r\n----\r\nProject lead: Samuel Audet [samuel.audet `at` gmail.com](mailto:samuel.audet&nbsp;at&nbsp;gmail.com)  \r\nDeveloper site: https://github.com/bytedeco/javacv  \r\nDiscussion group: http://groups.google.com/group/javacv\r\n"
 },
 {
  "repo": "hamuchiwa/AutoRCCar",
  "language": "Python",
  "readme_contents": "## AutoRCCar\n### Python3 + OpenCV3\n\nSee self-driving in action  \n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=BBwEF6WBUQs\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/BBwEF6WBUQs/0.jpg\" width=\"360\" height=\"240\" border=\"10\" /></a>\n\nThis project builds a self-driving RC car using Raspberry Pi, Arduino and open source software. Raspberry Pi collects inputs from a camera module and an ultrasonic sensor, and sends data to a computer wirelessly. The computer processes input images and sensor data for object detection (stop sign and traffic light) and collision avoidance respectively. A neural network model runs on computer and makes predictions for steering based on input images. Predictions are then sent to the Arduino for RC car control. \n  \n### Setting up environment with Anaconda\n  1. Install [`miniconda(Python3)`](https://conda.io/miniconda.html) on your computer\n  2. Create `auto-rccar` environment with all necessary libraries for this project  \n     ```conda env create -f environment.yml```\n     \n  3. Activate `auto-rccar` environment  \n     ```source activate auto-rccar```\n  \n  &ensp; To exit, simply close the terminal window. More info about managing Anaconda environment, please see [here](https://conda.io/docs/user-guide/tasks/manage-environments.html).\n  \n### About the files\n**test/**  \n  &emsp; &emsp; `rc_control_test.py`: RC car control with keyboard  \n  &emsp; &emsp;  `stream_server_test.py`: video streaming from Pi to computer  \n  &emsp; &emsp;  `ultrasonic_server_test.py`: sensor data streaming from Pi to computer  \n  &emsp; &emsp;  **model_train_test/**  \n      &emsp; &emsp;  &emsp; &emsp; `data_test.npz`: sample data  \n      &emsp; &emsp;  &emsp; &emsp; `train_predict_test.ipynb`: a jupyter notebook that goes through neural network model in OpenCV3  \n  \n**raspberryPi/**    \n  &emsp; &emsp;  `stream_client.py`:        stream video frames in jpeg format to the host computer  \n  &emsp; &emsp;  `ultrasonic_client.py`:    send distance data measured by sensor to the host computer  \n  \n**arduino/**  \n  &emsp; &emsp;  `rc_keyboard_control.ino`: control RC car controller  \n  \n**computer/**    \n  &emsp; &emsp;  **cascade_xml/**  \n      &emsp; &emsp;  &emsp; &emsp;  trained cascade classifiers  \n  &emsp; &emsp;  **chess_board/**   \n      &emsp; &emsp;  &emsp; &emsp;  images for calibration, captured by pi camera  \n      \n  &emsp; &emsp;  `picam_calibration.py`:     pi camera calibration  \n  &emsp; &emsp;  `collect_training_data.py`: collect images in grayscale, data saved as `*.npz`  \n  &emsp; &emsp;  `model.py`:                 neural network model  \n  &emsp; &emsp;  `model_training.py`:        model training and validation  \n  &emsp; &emsp;  `rc_driver_helper.py`:      helper classes/functions for `rc_driver.py`  \n  &emsp; &emsp;  `rc_driver.py`:             receive data from raspberry pi and drive the RC car based on model prediction  \n  &emsp; &emsp;  `rc_driver_nn_only.py`:     simplified `rc_driver.py` without object detection  \n  \n  \n**Traffic_signal**  \n  &emsp; &emsp;  trafic signal sketch contributed by [@geek111](https://github.com/geek1111)\n\n\n### How to drive\n1. **Testing:** Flash `rc_keyboard_control.ino` to Arduino and run `rc_control_test.py` to drive the RC car with keyboard. Run `stream_server_test.py` on computer and then run `stream_client.py` on raspberry pi to test video streaming. Similarly, `ultrasonic_server_test.py` and `ultrasonic_client.py` can be used for sensor data streaming testing.   \n\n2. **Pi Camera calibration (optional):** Take multiple chess board images using pi camera module at various angles and put them into **`chess_board`** folder, run `picam_calibration.py` and returned parameters from the camera matrix will be used in `rc_driver.py`.\n\n3. **Collect training/validation data:** First run `collect_training_data.py` and then run `stream_client.py` on raspberry pi. Press arrow keys to drive the RC car, press `q` to exit. Frames are saved only when there is a key press action. Once exit, data will be saved into newly created **`training_data`** folder.\n\n4. **Neural network training:** Run `model_training.py` to train a neural network model. Please feel free to tune the model architecture/parameters to achieve a better result. After training, model will be saved into newly created **`saved_model`** folder.\n\n5. **Cascade classifiers training (optional):** Trained stop sign and traffic light classifiers are included in the **`cascade_xml`** folder, if you are interested in training your own classifiers, please refer to [OpenCV doc](http://docs.opencv.org/doc/user_guide/ug_traincascade.html) and this great [tutorial](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n6. **Self-driving in action**: First run `rc_driver.py` to start the server on the computer (for simplified no object detection version, run `rc_driver_nn_only.py` instead), and then run `stream_client.py` and `ultrasonic_client.py` on raspberry pi. \n\n[\u4e2d\u6587\u6587\u6863](https://github.com/zhaoying9105/AutoRCCar) (\u611f\u8c22[zhaoying9105](https://github.com/zhaoying9105))\n"
 },
 {
  "repo": "hybridgroup/gocv",
  "language": "Go",
  "readme_contents": "# GoCV\n\n[![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/gocvlogo.jpg)](http://gocv.io/)\n\n[![GoDoc](https://godoc.org/gocv.io/x/gocv?status.svg)](https://godoc.org/github.com/hybridgroup/gocv)\n[![CircleCI Build status](https://circleci.com/gh/hybridgroup/gocv/tree/dev.svg?style=svg)](https://circleci.com/gh/hybridgroup/gocv/tree/dev)\n[![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/9asd5foet54ru69q/branch/dev?svg=true)](https://ci.appveyor.com/project/deadprogram/gocv/branch/dev)\n[![codecov](https://codecov.io/gh/hybridgroup/gocv/branch/dev/graph/badge.svg)](https://codecov.io/gh/hybridgroup/gocv)\n[![Go Report Card](https://goreportcard.com/badge/github.com/hybridgroup/gocv)](https://goreportcard.com/report/github.com/hybridgroup/gocv)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/hybridgroup/gocv/blob/release/LICENSE.txt)\n\nThe GoCV package provides Go language bindings for the [OpenCV 4](http://opencv.org/) computer vision library.\n\nThe GoCV package supports the latest releases of Go and OpenCV (v4.5.0) on Linux, macOS, and Windows. We intend to make the Go language a \"first-class\" client compatible with the latest developments in the OpenCV ecosystem.\n\nGoCV supports [CUDA](https://en.wikipedia.org/wiki/CUDA) for hardware acceleration using Nvidia GPUs. Check out the [CUDA README](./cuda/README.md) for more info on how to use GoCV with OpenCV/CUDA.\n\nGoCV also supports [Intel OpenVINO](https://software.intel.com/en-us/openvino-toolkit). Check out the [OpenVINO README](./openvino/README.md) for more info on how to use GoCV with the Intel OpenVINO toolkit.\n\n## How to use\n\n### Hello, video\n\nThis example opens a video capture device using device \"0\", reads frames, and shows the video in a GUI window:\n\n```go\npackage main\n\nimport (\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n\twebcam, _ := gocv.OpenVideoCapture(0)\n\twindow := gocv.NewWindow(\"Hello\")\n\timg := gocv.NewMat()\n\n\tfor {\n\t\twebcam.Read(&img)\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### Face detect\n\n![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/face-detect.jpg)\n\nThis is a more complete example that opens a video capture device using device \"0\". It also uses the CascadeClassifier class to load an external data file containing the classifier data. The program grabs each frame from the video, then uses the classifier to detect faces. If any faces are found, it draws a green rectangle around each one, then displays the video in an output window:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"image/color\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n    // set to use a video capture device 0\n    deviceID := 0\n\n\t// open webcam\n\twebcam, err := gocv.OpenVideoCapture(deviceID)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tdefer webcam.Close()\n\n\t// open display window\n\twindow := gocv.NewWindow(\"Face Detect\")\n\tdefer window.Close()\n\n\t// prepare image matrix\n\timg := gocv.NewMat()\n\tdefer img.Close()\n\n\t// color for the rect when faces detected\n\tblue := color.RGBA{0, 0, 255, 0}\n\n\t// load classifier to recognize faces\n\tclassifier := gocv.NewCascadeClassifier()\n\tdefer classifier.Close()\n\n\tif !classifier.Load(\"data/haarcascade_frontalface_default.xml\") {\n\t\tfmt.Println(\"Error reading cascade file: data/haarcascade_frontalface_default.xml\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"start reading camera device: %v\\n\", deviceID)\n\tfor {\n\t\tif ok := webcam.Read(&img); !ok {\n\t\t\tfmt.Printf(\"cannot read device %v\\n\", deviceID)\n\t\t\treturn\n\t\t}\n\t\tif img.Empty() {\n\t\t\tcontinue\n\t\t}\n\n\t\t// detect faces\n\t\trects := classifier.DetectMultiScale(img)\n\t\tfmt.Printf(\"found %d faces\\n\", len(rects))\n\n\t\t// draw a rectangle around each face on the original image\n\t\tfor _, r := range rects {\n\t\t\tgocv.Rectangle(&img, r, blue, 3)\n\t\t}\n\n\t\t// show the image in the window, and wait 1 millisecond\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### More examples\n\nThere are examples in the [cmd directory](./cmd) of this repo in the form of various useful command line utilities, such as [capturing an image file](./cmd/saveimage), [streaming mjpeg video](./cmd/mjpeg-streamer), [counting objects that cross a line](./cmd/counter), and [using OpenCV with Tensorflow for object classification](./cmd/tf-classifier).\n\n## How to install\n\nTo install GoCV, run the following command:\n\n```\ngo get -u -d gocv.io/x/gocv\n```\n\nTo run code that uses the GoCV package, you must also install OpenCV 4.5.0 on your system. Here are instructions for Ubuntu, Raspian, macOS, and Windows.\n\n## Ubuntu/Linux\n\n### Installation\n\nYou can use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install\n\nIf you need static opencv libraries\n\n\tmake install BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n#### Install Cuda\n\n\t[cuda directory](./cuda)\n\n#### Install OpenVINO\n\n\t[openvino directory](./openvino)\n\t\n#### Install OpenVINO and Cuda\n\n\tThe following commands should do everything to download and install OpenCV 4.5.0 with Cuda and OpenVINO on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_all\n\nIf you need static opencv libraries\n\n\tmake install_all BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0-openvino\n    cuda information:\n      Device 0:  \"GeForce MX150\"  2003Mb, sm_61, Driver/Runtime ver.10.0/10.0\n\n#### Complete Install\n\nIf you have already done the \"Quick Install\" as described above, you do not need to run any further commands. For the curious, or for custom installations, here are the details for each of the steps that are performed when you run `make install`.\n\n##### Install required packages\n\nFirst, you need to change the current directory to the location of the GoCV repo, so you can access the `Makefile`:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNext, you need to update the system, and install any required packages:\n\n\tmake deps\n\n#### Download source\n\nNow, download the OpenCV 4.5.0 and OpenCV Contrib source code:\n\n\tmake download\n\n#### Build\n\nBuild everything. This will take quite a while:\n\n\tmake build\n\nIf you need static opencv libraries\n\n\tmake build BUILD_SHARED_LIBS=OFF\n\n#### Install\n\nOnce the code is built, you are ready to install:\n\n\tmake sudo_install\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n#### Cleanup extra files\n\nAfter the installation is complete, you can remove the extra files and folders:\n\n\tmake clean\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CPPFLAGS=\"-I/usr/local/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/lib -lopencv_core -lopencv_face -lopencv_videoio -lopencv_imgproc -lopencv_highgui -lopencv_imgcodecs -lopencv_objdetect -lopencv_features2d -lopencv_video -lopencv_dnn -lopencv_xfeatures2d\"\n\nPlease note that you will need to run these 2 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n### Docker\n\nThe project now provides `Dockerfile` which lets you build [GoCV](https://gocv.io/) Docker image which you can then use to build and run `GoCV` applications in Docker containers. The `Makefile` contains `docker` target which lets you build Docker image with a single command:\n\n```\nmake docker\n```\n\nBy default Docker image built by running the command above ships [Go](https://golang.org/) version `1.13.5`, but if you would like to build an image which uses different version of `Go` you can override the default value when running the target command:\n\n```\nmake docker GOVERSION='1.13.5'\n```\n\n#### Running GUI programs in Docker on macOS\n\nSometimes your `GoCV` programs create graphical interfaces like windows eg. when you use `gocv.Window` type when you display an image or video stream. Running the programs which create graphical interfaces in Docker container on macOS is unfortunately a bit elaborate, but not impossible. First you need to satisfy the following prerequisites:\n* install [xquartz](https://www.xquartz.org/). You can also install xquartz using [homebrew](https://brew.sh/) by running `brew cask install xquartz`\n* install [socat](https://linux.die.net/man/1/socat) `brew install socat`\n\nNote, you will have to log out and log back in to your machine once you have installed `xquartz`. This is so the X window system is reloaded.\n\nOnce you have installed all the prerequisites you need to allow connections from network clients to `xquartz`. Here is how you do that. First run the following command to open `xquart` so you can configure it:\n\n```shell\nopen -a xquartz\n```\nClick on *Security* tab in preferences and check the \"Allow connections\" box:\n\n![app image](./images/xquartz.png)\n\nNext, you need to create a TCP proxy using `socat` which will stream [X Window](https://en.wikipedia.org/wiki/X_Window_System) data into `xquart`. Before you start the proxy you need to make sure that there is no process listening in port `6000`. The following command should **not** return any results:\n\n```shell\nlsof -i TCP:6000\n```\nNow you can start a local proxy which will proxy the X Window traffic into xquartz which acts a your local X server:\n\n```shell\nsocat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\\"$DISPLAY\\\"\n```\n\nYou are now finally ready to run your `GoCV` GUI programs in Docker containers. In order to make everything work you must set `DISPLAY` environment variables as shown in a sample command below:\n\n```shell\ndocker run -it --rm -e DISPLAY=docker.for.mac.host.internal:0 your-gocv-app\n```\n\n**Note, since Docker for MacOS does not provide any video device support, you won't be able run GoCV apps which require camera.**\n\n### Alpine 3.7 Docker image\n\nThere is a Docker image with Alpine 3.7 that has been created by project contributor [@denismakogon](https://github.com/denismakogon). You can find it located at [https://github.com/denismakogon/gocv-alpine](https://github.com/denismakogon/gocv-alpine).\n\n## Raspbian\n\n### Installation\n\nWe have a special installation for the Raspberry Pi that includes some hardware optimizations. You use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Raspbian:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_raspi\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n## macOS\n\n### Installation\n\nYou can install OpenCV 4.5.0 using Homebrew.\n\nIf you already have an earlier version of OpenCV (3.4.x) installed, you should probably remove it before installing the new version:\n\n\tbrew uninstall opencv\n\nYou can then install OpenCV 4.5.0:\n\n\tbrew install opencv\n\n### pkgconfig Installation\npkg-config is used to determine the correct flags for compiling and linking OpenCV.\nYou can install it by using Homebrew:\n\n    brew install pkgconfig\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CXXFLAGS=\"--std=c++11\"\n\texport CGO_CPPFLAGS=\"-I/usr/local/Cellar/opencv/4.5.0/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/Cellar/opencv/4.5.0/lib -lopencv_stitching -lopencv_superres -lopencv_videostab -lopencv_aruco -lopencv_bgsegm -lopencv_bioinspired -lopencv_ccalib -lopencv_dnn_objdetect -lopencv_dpm -lopencv_face -lopencv_photo -lopencv_fuzzy -lopencv_hfs -lopencv_img_hash -lopencv_line_descriptor -lopencv_optflow -lopencv_reg -lopencv_rgbd -lopencv_saliency -lopencv_stereo -lopencv_structured_light -lopencv_phase_unwrapping -lopencv_surface_matching -lopencv_tracking -lopencv_datasets -lopencv_dnn -lopencv_plot -lopencv_xfeatures2d -lopencv_shape -lopencv_video -lopencv_ml -lopencv_ximgproc -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_flann -lopencv_xobjdetect -lopencv_imgcodecs -lopencv_objdetect -lopencv_xphoto -lopencv_imgproc -lopencv_core\"\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n## Windows\n\n### Installation\n\nThe following assumes that you are running a 64-bit version of Windows 10.\n\nIn order to build and install OpenCV 4.5.0 on Windows, you must first download and install MinGW-W64 and CMake, as follows.\n\n#### MinGW-W64\n\nDownload and run the MinGW-W64 compiler installer from [https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/).\n\nThe latest version of the MinGW-W64 toolchain is `7.3.0`, but any version from `7.X` on should work.\n\nChoose the options for \"posix\" threads, and for \"seh\" exceptions handling, then install to the default location `c:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2`.\n\nAdd the `C:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2\\mingw64\\bin` path to your System Path.\n\n#### CMake\n\nDownload and install CMake [https://cmake.org/download/](https://cmake.org/download/) to the default location. CMake installer will add CMake to your system path.\n\n#### OpenCV 4.5.0 and OpenCV Contrib Modules\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Windows:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\twin_build_opencv.cmd\n\nIt might take up to one hour.\n\nLast, add `C:\\opencv\\build\\install\\x64\\mingw\\bin` to your System Path.\n\n### Verifying the installation\n\nChange the current directory to the location of the GoCV repo:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\nNow you should be able to build or run any of the command examples:\n\n\tgo run cmd\\version\\main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, OpenCV is expected to be in `C:\\opencv\\build\\install\\include`. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nDue to the way OpenCV produces DLLs, including the version in the name, using this method is required if you're using a different version of OpenCV.\n\nFor example:\n\n\tset CGO_CXXFLAGS=\"--std=c++11\"\n\tset CGO_CPPFLAGS=-IC:\\opencv\\build\\install\\include\n\tset CGO_LDFLAGS=-LC:\\opencv\\build\\install\\x64\\mingw\\lib -lopencv_core412 -lopencv_face412 -lopencv_videoio412 -lopencv_imgproc412 -lopencv_highgui412 -lopencv_imgcodecs412 -lopencv_objdetect412 -lopencv_features2d412 -lopencv_video412 -lopencv_dnn412 -lopencv_xfeatures2d412 -lopencv_plot412 -lopencv_tracking412 -lopencv_img_hash412\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv cmd\\version\\main.go\n\n## Android\n\nThere is some work in progress for running GoCV on Android using Gomobile. For information on how to install OpenCV/GoCV for Android, please see:\nhttps://gist.github.com/ogero/c19458cf64bd3e91faae85c3ac887481\n\nSee original discussion here:\nhttps://github.com/hybridgroup/gocv/issues/235\n\n## Profiling\n\nSince memory allocations for images in GoCV are done through C based code, the go garbage collector will not clean all resources associated with a `Mat`.  As a result, any `Mat` created *must* be closed to avoid memory leaks.\n\nTo ease the detection and repair of the resource leaks, GoCV provides a `Mat` profiler that records when each `Mat` is created and closed.  Each time a `Mat` is allocated, the stack trace is added to the profile.  When it is closed, the stack trace is removed. See the [runtime/pprof documentation](https://golang.org/pkg/runtime/pprof/#Profile).\n\nIn order to include the MatProfile custom profiler, you MUST build or run your application or tests using the `-tags matprofile` build tag. For example:\n\n\tgo run -tags matprofile cmd/version/main.go\n\nYou can get the profile's count at any time using:\n\n```go\ngocv.MatProfile.Count()\n```\n\nYou can display the current entries (the stack traces) with:\n\n```go\nvar b bytes.Buffer\ngocv.MatProfile.WriteTo(&b, 1)\nfmt.Print(b.String())\n```\n\nThis can be very helpful to track down a leak.  For example, suppose you have\nthe following nonsense program:\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tfmt.Printf(\"initial MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tleak()\n\n\tfmt.Printf(\"final MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tvar b bytes.Buffer\n\tgocv.MatProfile.WriteTo(&b, 1)\n\tfmt.Print(b.String())\n}\n```\n\nRunning this program produces the following output:\n\n```\ninitial MatProfile count: 0\nfinal MatProfile count: 1\ngocv.io/x/gocv.Mat profile: total 1\n1 @ 0x40b936c 0x40b93b7 0x40b94e2 0x40b95af 0x402cd87 0x40558e1\n#\t0x40b936b\tgocv.io/x/gocv.newMat+0x4b\t/go/src/gocv.io/x/gocv/core.go:153\n#\t0x40b93b6\tgocv.io/x/gocv.NewMat+0x26\t/go/src/gocv.io/x/gocv/core.go:159\n#\t0x40b94e1\tmain.leak+0x21\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:11\n#\t0x40b95ae\tmain.main+0xae\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:16\n#\t0x402cd86\truntime.main+0x206\t\t/usr/local/Cellar/go/1.11.1/libexec/src/runtime/proc.go:201\n```\n\nWe can see that this program would leak memory.  As it exited, it had one `Mat` that was never closed.  The stack trace points to exactly which line the allocation happened on (line 11, the `gocv.NewMat()`).\n\nFurthermore, if the program is a long running process or if GoCV is being used on a web server, it may be helpful to install the HTTP interface )). For example:\n\n```go\npackage main\n\nimport (\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"time\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tgo func() {\n\t\tticker := time.NewTicker(time.Second)\n\t\tfor {\n\t\t\t<-ticker.C\n\t\t\tleak()\n\t\t}\n\t}()\n\n\thttp.ListenAndServe(\"localhost:6060\", nil)\n}\n\n```\n\nThis will leak a `Mat` once per second.  You can see the current profile count and stack traces by going to the installed HTTP debug interface: [http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat](http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat?debug=1).\n\n\n## How to contribute\n\nPlease take a look at our [CONTRIBUTING.md](./CONTRIBUTING.md) document to understand our contribution guidelines.\n\nThen check out our [ROADMAP.md](./ROADMAP.md) document to know what to work on next.\n\n## Why this project exists\n\nThe [https://github.com/go-opencv/go-opencv](https://github.com/go-opencv/go-opencv) package for Go and OpenCV does not support any version above OpenCV 2.x, and work on adding support for OpenCV 3 had stalled for over a year, mostly due to the complexity of [SWIG](http://swig.org/). That is why we started this project.\n\nThe GoCV package uses a C-style wrapper around the OpenCV 4 C++ classes to avoid having to deal with applying SWIG to a huge existing codebase. The mappings are intended to match as closely as possible to the original OpenCV project structure, to make it easier to find things, and to be able to figure out where to add support to GoCV for additional OpenCV image filters, algorithms, and other features.\n\nFor example, the [OpenCV `videoio` module](https://github.com/opencv/opencv/tree/master/modules/videoio) wrappers can be found in the GoCV package in the `videoio.*` files.\n\nThis package was inspired by the original https://github.com/go-opencv/go-opencv project, the blog post https://medium.com/@peterleyssens/using-opencv-3-from-golang-5510c312a3c and the repo at https://github.com/sensorbee/opencv thank you all!\n\n## License\n\nLicensed under the Apache 2.0 license. Copyright (c) 2017-2020 The Hybrid Group.\n\nLogo generated by GopherizeMe - https://gopherize.me\n"
 },
 {
  "repo": "bijection/sistine",
  "language": "Python",
  "readme_contents": "# Project Sistine\n\n![Sistine * 3/2](splash.png)\n\nWe turned a MacBook into a touchscreen using only $1 of hardware and a little bit of computer vision. The proof-of-concept, dubbed \u201cProject Sistine\u201d after our [recreation](https://www.anishathalye.com/media/2018/04/03/thumbnail.jpg) of the famous [painting](https://en.wikipedia.org/wiki/The_Creation_of_Adam) in the Sistine Chapel, was prototyped by [Anish Athalye](https://www.anishathalye.com/), [Kevin Kwok](https://twitter.com/antimatter15), [Guillermo Webster](https://twitter.com/biject), and [Logan Engstrom](https://github.com/lengstrom) in about 16 hours.\n\n## Basic Principle\n\nThe basic principle behind Sistine is simple. Surfaces viewed from an angle tend to look shiny, and you can tell if a finger is touching the surface by checking if it\u2019s touching its own reflection.\n\n![Hover versus touch](https://www.anishathalye.com/media/2018/04/03/explanation.png)\n\nKevin, back in middle school, noticed this phenomenon and built [ShinyTouch](https://antimatter15.com/project/shinytouch/), utilizing an external webcam to build a touch input system requiring virtually no setup. We wanted to see if we could miniaturize the idea and make it work without an external webcam. Our idea was to retrofit a small mirror in front of a MacBook\u2019s built-in webcam, so that the webcam would be looking down at the computer screen at a sharp angle. The camera would be able to see fingers hovering over or touching the screen, and we\u2019d be able to translate the video feed into touch events using computer vision.\n\n(Read the rest of our blog post, including a video demo and a high-level explanation of the algorithm, [here](https://www.anishathalye.com/2018/04/03/macbook-touchscreen/))\n\n## Installation (with Homebrew Python)\n\n* First, make sure you have [Mac Homebrew](https://brew.sh/) installed on your computer. If not, you can install it by running `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\n* Install Python 2 via Homebrew with `brew install python2`\n\n* Install OpenCV 3 via Homebrew with `brew install opencv3`\n\n* Install PyObjC via Pip with `pip2 install pyobjc`\n\n## Running\n\nRun `python2 sistine.py`\n\n## License\n\nCopyright (c) 2016-2018 Anish Athalye, Kevin Kwok, Guillermo Webster, and Logan\nEngstrom. Released under the MIT License. See [LICENSE.md][license] for\ndetails.\n\n[license]: LICENSE.md\n"
 },
 {
  "repo": "peterbraden/node-opencv",
  "language": "C++",
  "readme_contents": "# node-opencv\n\n[![Build Status](https://secure.travis-ci.org/peterbraden/node-opencv.svg)](http://travis-ci.org/peterbraden/node-opencv)\n\n[OpenCV](http://opencv.org) bindings for Node.js. OpenCV is\nthe defacto computer vision library - by interfacing with it natively in node,\nwe get powerful real time vision in js.\n\nPeople are using node-opencv to fly control quadrocoptors, detect faces from\nwebcam images and annotate video streams. If you're using it for something\ncool, I'd love to hear about it!\n\n## Install\n\nYou'll need OpenCV 2.3.1 or newer installed before installing node-opencv.\n\n## Specific for macOS\nInstall OpenCV using brew\n```bash\nbrew install pkg-config\nbrew install opencv@2\nbrew link --force opencv@2\n```\n\n\n## Specific for Windows\n1. Download and install OpenCV (Be sure to use a 2.4 version) @\nhttp://opencv.org/releases.html\nFor these instructions we will assume OpenCV is put at C:\\OpenCV, but you can\nadjust accordingly.\n\n2. If you haven't already, create a system variable called OPENCV_DIR and set it\n   to C:\\OpenCV\\build\\x64\\vc12\n\n   Make sure the \"x64\" part matches the version of NodeJS you are using.\n\n   Also add the following to your system PATH\n        ;%OPENCV_DIR%\\bin\n\n3. Install Visual Studio 2013. Make sure to get the C++ components.\n   You can use a different edition, just make sure OpenCV supports it, and you\n   set the \"vcxx\" part of the variables above to match.\n\n4. Download peterbraden/node-opencv fork\ngit clone https://github.com/peterbraden/node-opencv\n\n5. run npm install\n\n```bash\n$ npm install opencv\n```\n\n## Examples\nRun the examples from the parent directory.\n\n### Face Detection\n\n```javascript\ncv.readImage(\"./examples/files/mona.png\", function(err, im){\n  im.detectObject(cv.FACE_CASCADE, {}, function(err, faces){\n    for (var i=0;i<faces.length; i++){\n      var x = faces[i]\n      im.ellipse(x.x + x.width/2, x.y + x.height/2, x.width/2, x.height/2);\n    }\n    im.save('./out.jpg');\n  });\n})\n```\n\n\n## API Documentation\n\n### Matrix\n\nThe [matrix](http://opencv.jp/opencv-2svn_org/cpp/core_basic_structures.html#mat) is the most useful\nbase data structure in OpenCV. Things like images are just matrices of pixels.\n\n#### Creation\n\n```javascript\nnew Matrix(rows, cols)\n```\n\nOr if you're thinking of a Matrix as an image:\n\n```javascript\nnew Matrix(height, width)\n```\n\nOr you can use opencv to read in image files. Supported formats are in the OpenCV docs, but jpgs etc are supported.\n\n```javascript\ncv.readImage(filename, function(err, mat){\n  ...\n})\n\ncv.readImage(buffer, function(err, mat){\n  ...\n})\n```\n\nIf you need to pipe data into an image, you can use an ImageDataStream:\n\n```javascript\nvar s = new cv.ImageDataStream()\n\ns.on('load', function(matrix){\n  ...\n})\n\nfs.createReadStream('./examples/files/mona.png').pipe(s);\n```\n\nIf however, you have a series of images, and you wish to stream them into a\nstream of Matrices, you can use an ImageStream. Thus:\n\n```javascript\nvar s = new cv.ImageStream()\n\ns.on('data', function(matrix){\n   ...\n})\n\nardrone.createPngStream().pipe(s);\n```\n\nNote: Each 'data' event into the ImageStream should be a complete image buffer.\n\n\n\n#### Accessing Data\n\n```javascript\nvar mat = new cv.Matrix.Eye(4,4); // Create identity matrix\n\nmat.get(0,0) // 1\n\nmat.row(0)  // [1,0,0,0]\nmat.col(3)  // [0,0,0,1]\n```\n\n##### Save\n\n```javascript\nmat.save('./pic.jpg')\n```\n\nor:\n\n```javascript\nvar buff = mat.toBuffer()\n```\n\n#### Image Processing\n\n```javascript\nim.convertGrayscale()\nim.canny(5, 300)\nim.houghLinesP()\n```\n\n\n#### Simple Drawing\n\n```javascript\nim.ellipse(x, y)\nim.line([x1,y1], [x2, y2])\n```\n\n#### Object Detection\n\nThere is a shortcut method for\n[Viola-Jones Haar Cascade](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) object\ndetection. This can be used for face detection etc.\n\n```javascript\nmat.detectObject(haar_cascade_xml, opts, function(err, matches){})\n```\n\nFor convenience in face detection, cv.FACE_CASCADE is a cascade that can be used for frontal face detection.\n\nAlso:\n\n```javascript\nmat.goodFeaturesToTrack\n```\n\n#### Contours\n\n```javascript\nmat.findCountours\nmat.drawContour\nmat.drawAllContours\n```\n\n### Using Contours\n\n`findContours` returns a `Contours` collection object, not a native array. This object provides\nfunctions for accessing, computing with, and altering the contours contained in it.\nSee [relevant source code](src/Contours.cc) and [examples](examples/)\n\n```javascript\nvar contours = im.findContours();\n\n// Count of contours in the Contours object\ncontours.size();\n\n// Count of corners(verticies) of contour `index`\ncontours.cornerCount(index);\n\n// Access vertex data of contours\nfor(var c = 0; c < contours.size(); ++c) {\n  console.log(\"Contour \" + c);\n  for(var i = 0; i < contours.cornerCount(c); ++i) {\n    var point = contours.point(c, i);\n    console.log(\"(\" + point.x + \",\" + point.y + \")\");\n  }\n}\n\n// Computations of contour `index`\ncontours.area(index);\ncontours.arcLength(index, isClosed);\ncontours.boundingRect(index);\ncontours.minAreaRect(index);\ncontours.isConvex(index);\ncontours.fitEllipse(index);\n\n// Destructively alter contour `index`\ncontours.approxPolyDP(index, epsilon, isClosed);\ncontours.convexHull(index, clockwise);\n```\n\n#### Face Recognization\n\nIt requires to `train` then `predict`. For acceptable result, the face should be cropped, grayscaled and aligned, I ignore this part so that we may focus on the api usage.\n\n** Please ensure your OpenCV 3.2+ is configured with contrib. MacPorts user may `port install opencv +contrib` **\n\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst cv = require('opencv');\n\nfunction forEachFileInDir(dir, cb) {\n  let f = fs.readdirSync(dir);\n  f.forEach(function (fpath, index, array) {\n    if (fpath != '.DS_Store')\n     cb(path.join(dir, fpath));\n  });\n}\n\nlet dataDir = \"./_training\";\nfunction trainIt (fr) {\n  // if model existe, load it\n  if ( fs.existsSync('./trained.xml') ) {\n    fr.loadSync('./trained.xml');\n    return;\n  }\n\n  // else train a model\n  let samples = [];\n  forEachFileInDir(dataDir, (f)=>{\n      cv.readImage(f, function (err, im) {\n          // Assume all training photo are named as id_xxx.jpg\n          let labelNumber = parseInt(path.basename(f).substring(3));\n          samples.push([labelNumber, im]);\n      })\n  })\n\n  if ( samples.length > 3 ) {\n    // There are async and sync version of training method:\n    // .train(info, cb)\n    //     cb : standard Nan::Callback\n    //     info : [[intLabel,matrixImage],...])\n    // .trainSync(info)\n    fr.trainSync(samples);\n    fr.saveSync('./trained.xml');\n  }else {\n    console.log('Not enough images uploaded yet', cvImages)\n  }\n}\n\nfunction predictIt(fr, f){\n  cv.readImage(f, function (err, im) {\n    let result = fr.predictSync(im);\n    console.log(`recognize result:(${f}) id=${result.id} conf=${100.0-result.confidence}`);\n  });\n}\n\n//using defaults: .createLBPHFaceRecognizer(radius=1, neighbors=8, grid_x=8, grid_y=8, threshold=80)\nconst fr = new cv.FaceRecognizer();\ntrainIt(fr);\nforEachFileInDir('./_bench', (f) => predictIt(fr, f));\n```\n\n## Test\n\nUsing [tape](https://github.com/substack/tape). Run with command:\n\n`npm test`.\n\n## Contributing\n\nI (@peterbraden) don't spend much time maintaining this library, it runs\nprimarily on contributor support. I'm happy to accept most PR's if the tests run\ngreen, all new functionality is tested, and there are no objections in the PR.\n\nBecause I haven't got much time for maintenance, I'd prefer to keep an absolute\nminimum of dependencies.\n\n\n## MIT License\nThe library is distributed under the MIT License - if for some reason that\ndoesn't work for you please get in touch.\n"
 },
 {
  "repo": "kelaberetiv/TagUI",
  "language": "JavaScript",
  "readme_contents": "<img src=\"https://raw.githubusercontent.com/kelaberetiv/TagUI/master/src/media/tagui_logo.png\" height=\"111\" align=\"right\">\n\n# TagUI\n\n### TagUI is a command-line tool for digital process automation (RPA)\n\n### [Download TagUI v6](https://tagui.readthedocs.io/en/latest/setup.html)&ensp;|&ensp;[Visit documentation](https://tagui.readthedocs.io/en/latest/index.html)&ensp;|&ensp;[User feedback](https://forms.gle/mieY66xTN4NNm5Gq5)&ensp;|&ensp;[We're hiring](https://nuscareers.taleo.net/careersection/2/jobdetail.ftl?job=00CMO)\n\nWrite flows in simple TagUI language and automate your web, mouse and keyboard interactions on the screen.\n\nTagUI is free to use and open-source. It's easy to setup and use, and works on Windows, macOS and Linux.\n\nIn TagUI language, you use steps like `click` and `type` to interact with identifiers, which include web identifiers, image snapshots, screen coordinates, or even text using OCR. Below is a sample flow to download a report:\n\n```\nhttps://www.typeform.com\n\nclick login\ntype username as user@gmail.com\ntype password as 12345678\nclick btnlogin\n\ndownload https://admin.typeform.com/xxx to report.csv\n```\n```\n// besides web identifiers, images of UI elements can be used\nclick login_button.png\ntype username_box.png as user@gmail.com\n```\n```\n// (x,y) coordinates of user-interface elements can also be used\nclick (1200,200)\ntype (800,400) as user@gmail.com\n```\n\n# v6 Features\n\n### TagUI live mode\nYou can run live mode directly for faster development by running `tagui live` on the command line.\n\n### Click text using OCR\nTagUI can now click on the screen with visual automation just using text input, by using OCR technology.\n\n```\nclick v6 Features using ocr\n```\n\n### Deploy flows to run when double clicked\nYou can now create a shortcut for a flow, which can be moved to your desktop and double-clicked to run the flow. The flow will be run with all the options used when creating the shortcut.\n\n```\n$ tagui my_flow.tag -deploy\nOR\n$ tagui my_flow.tag -d\n```\n\n### Running flows with options can be done with abbreviations\nFor example, you can now do ``tagui my_flow.tag -h`` instead of ``tagui my_flow.tag -headless``.\n\n# Migrating to v6\n\n### Mandatory .tag file extension\nAll flow files must have a .tag extension.\n\n### Options must be used with a leading hyphen (-)\nWhen running a flow with options, prefix a - to the options.\n\nBefore v6:\n```\n$ tagui my_flow.tag headless\n```\n\nAfter v6:\n```\n$ tagui my_flow.tag -headless\nOR\n$ tagui my_flow.tag -h\n```\n\n### Change in syntax for echo, dump, write steps\nThe echo, dump and write steps are now consistent with the other steps. They no longer require quotes surrounding the string input. Instead, variables now need to be surrounded by backticks.\n\nBefore v6:\n```\necho 'This works!' some_text_variable\n```\n\nAfter v6:\n```\necho This works! `some_text_variable`\n```\n\n### if and loop code blocks can use indentation instead of curly braces {}\nThis increases readability and ease of use. Just indent your code within the if and loop code blocks. \n\nBefore v6:\n```\nif some_condition\n{\ndo_some_step_A\ndo_some_step_B\n}\n```\n\nAfter v6:\n```\nif some_condition\n  do_some_step_A\n  do_some_step_B\n```\n\n# TagUI v5.11\n\n### [Visit TagUI v5.11 homepage & documentation](https://github.com/kelaberetiv/TagUI/tree/pre_v6)\n\n# Credits\n- [TagUI v3](https://github.com/kensoh/TagUI/tree/before_aisg) - Ken Soh from Singapore\n- [SikuliX](http://sikulix.com) - Raimund Hocke from Germany\n- [CasperJS](http://casperjs.org) - Nicolas Perriault from France\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia\n- [SlimerJS](https://slimerjs.org) - Laurent Jouanneau from France\n\nThis project  is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG-RP-2019-050). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n"
 },
 {
  "repo": "esimov/pigo",
  "language": "Go",
  "readme_contents": "<h1 align=\"center\"><img alt=\"pigo-logo\" src=\"https://user-images.githubusercontent.com/883386/55795932-8787cf00-5ad1-11e9-8c3e-8211ba9427d8.png\" height=240/></h1>\r\n\r\n[![Build Status](https://travis-ci.org/esimov/pigo.svg?branch=master)](https://travis-ci.org/esimov/pigo)\r\n[![GoDoc](https://godoc.org/github.com/golang/gddo?status.svg)](https://godoc.org/github.com/esimov/pigo/core)\r\n[![license](https://img.shields.io/github/license/esimov/pigo)](./LICENSE)\r\n[![release](https://img.shields.io/badge/release-v1.4.2-blue.svg)](https://github.com/esimov/pigo/releases/tag/v1.4.2)\r\n[![snapcraft](https://img.shields.io/badge/snapcraft-v1.3.0-green.svg)](https://snapcraft.io/pigo)\r\n\r\nPigo is a pure Go face detection, pupil/eyes localization and facial landmark points detection library based on ***Pixel Intensity Comparison-based Object detection*** paper (https://arxiv.org/pdf/1305.4537.pdf).\r\n\r\n| Rectangle face marker | Circle face marker\r\n|:--:|:--:\r\n| ![rectangle](https://user-images.githubusercontent.com/883386/40916662-2fbbae1a-6809-11e8-8afd-d4ed40c7d4e9.png) | ![circle](https://user-images.githubusercontent.com/883386/40916683-447088a8-6809-11e8-942f-3112c10bede3.png) |\r\n\r\n### Motivation\r\nI've intended to implement this face detection method because all of the existing solutions for face detection in the Go ecosystem are only bindings to some C/C++ libraries like OpenCV, but installing OpenCV on various platforms is cumbersome.\r\n\r\nThe library does not require any third party modules or applications to be installed. However in case you wish to try the real time, webcam based face detection you might need to have Python2 and OpenCV installed, but **the core API does not require any third party module or external dependency**.\r\n\r\n### Key features\r\n- [x] Does not require OpenCV or any 3rd party modules to be installed\r\n- [x] High processing speed\r\n- [x] There is no need for image preprocessing prior detection\r\n- [x] There is no need for the computation of integral images, image pyramid, HOG pyramid or any other similar data structure\r\n- [x] The face detection is based on pixel intensity comparison encoded in the binary file tree structure\r\n- [x] Fast detection of in-plane rotated faces\r\n- [x] The library can detect even faces with eyeglasses\r\n- [x] [Pupils/eyes localization](#pupils--eyes-localization)\r\n- [x] [Facial landmark points detection](#facial-landmark-points-detection)\r\n- [x] **[Webassembly support \ud83c\udf89](#wasm-webassembly-support)**\r\n\r\n### Todo\r\n- [ ] Features detection and description\r\n\r\n**The library can also detect in plane rotated faces.** For this reason a new `-angle` parameter have been included into the command line utility. The command below will generate the following result (see the table below for all the supported options).\r\n\r\n```bash\r\n$ pigo -in input.jpg -out output.jpg -cf cascade/facefinder -angle=0.8 -iou=0.01\r\n```\r\n\r\n| Input file | Output file\r\n|:--:|:--:\r\n| ![input](https://user-images.githubusercontent.com/883386/50761018-015db180-1272-11e9-93d9-d3693cae9d66.jpg) | ![output](https://user-images.githubusercontent.com/883386/50761024-03277500-1272-11e9-9c20-2568b87a2344.png) |\r\n\r\n\r\nNote: In case of in plane rotated faces the angle value should be adapted to the provided image.\r\n\r\n### Pupils / eyes localization\r\n\r\nStarting from **v1.2.0** Pigo offer pupils/eyes localization capabilites. The implementation is based on [Eye pupil localization with an ensemble of randomized trees](https://www.sciencedirect.com/science/article/abs/pii/S0031320313003294).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/puploc\r\n\r\n![puploc](https://user-images.githubusercontent.com/883386/62784340-f5b3c100-bac6-11e9-865e-a2b4b9520b08.png)\r\n\r\n### Facial landmark points detection\r\n\r\n**v1.3.0** marks a new milestone in the library evolution, Pigo being able for facial landmark points detection. The implementation is based on [Fast Localization of Facial Landmark Points](https://arxiv.org/pdf/1403.6888.pdf).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/facial_landmark\r\n\r\n![flp_example](https://user-images.githubusercontent.com/883386/66802771-3b0cc880-ef26-11e9-9ee3-7e9e981ef3f7.png)\r\n\r\n## Install\r\n\r\n**Important note: for the Webassembly demo at least Go 1.13 is required!**\r\n\r\nInstall Go, set your `GOPATH`, and make sure `$GOPATH/bin` is on your `PATH`.\r\n\r\n```bash\r\n$ export GOPATH=\"$HOME/go\"\r\n$ export PATH=\"$PATH:$GOPATH/bin\"\r\n```\r\nNext download the project and build the binary file.\r\n\r\n```bash\r\n$ go get -u -f github.com/esimov/pigo/cmd/pigo\r\n$ go install\r\n```\r\n\r\n### Binary releases\r\nIn case you do not have installed or do not wish to install Go, you can obtain the binary file from the [releases](https://github.com/esimov/pigo/releases) folder.\r\n\r\nThe library can be accessed as a snapcraft function too.\r\n\r\n<a href=\"https://snapcraft.io/pigo\"><img src=\"https://raw.githubusercontent.com/snapcore/snap-store-badges/master/EN/%5BEN%5D-snap-store-white-uneditable.png\" alt=\"snapcraft pigo\"></a>\r\n\r\n## API\r\nBelow is a minimal example of using the face detection API.\r\n\r\nFirst you need to load and parse the binary classifier, then convert the image to grayscale mode,\r\nand finally to run the cascade function which returns a slice containing the row, column, scale and the detection score.\r\n\r\n```Go\r\ncascadeFile, err := ioutil.ReadFile(\"/path/to/cascade/file\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %v\", err)\r\n}\r\n\r\nsrc, err := pigo.GetImage(\"/path/to/image\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Cannot open the image file: %v\", err)\r\n}\r\n\r\npixels := pigo.RgbToGrayscale(src)\r\ncols, rows := src.Bounds().Max.X, src.Bounds().Max.Y\r\n\r\ncParams := pigo.CascadeParams{\r\n\tMinSize:     20,\r\n\tMaxSize:     1000,\r\n\tShiftFactor: 0.1,\r\n\tScaleFactor: 1.1,\r\n\r\n\tImageParams: pigo.ImageParams{\r\n\t\tPixels: pixels,\r\n\t\tRows:   rows,\r\n\t\tCols:   cols,\r\n\t\tDim:    cols,\r\n\t},\r\n}\r\n\r\npigo := pigo.NewPigo()\r\n// Unpack the binary file. This will return the number of cascade trees,\r\n// the tree depth, the threshold and the prediction from tree's leaf nodes.\r\nclassifier, err := pigo.Unpack(cascadeFile)\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %s\", err)\r\n}\r\n\r\nangle := 0.0 // cascade rotation angle. 0.0 is 0 radians and 1.0 is 2*pi radians\r\n\r\n// Run the classifier over the obtained leaf nodes and return the detection results.\r\n// The result contains quadruplets representing the row, column, scale and detection score.\r\ndets := classifier.RunCascade(cParams, angle)\r\n\r\n// Calculate the intersection over union (IoU) of two clusters.\r\ndets = classifier.ClusterDetections(dets, 0.2)\r\n```\r\n\r\n**A note about imports**:  in order to decode the image you will need to import `image/jpeg` or `image/png` (depending on the provided image type) and the Pigo library as well, otherwise you will get a `\"Image: Unkown format\"` error. See the following example:\r\n```Go\r\nimport (\r\n    _ \"image/jpeg\"\r\n    pigo \"github.com/esimov/pigo/core\"\r\n)\r\n```\r\n\r\n## Usage\r\nA command line utility is bundled into the library to detect faces in static images.\r\n\r\n```bash\r\n$ pigo -in input.jpg -out out.jpg -cf cascade/facefinder\r\n```\r\n\r\n### Supported flags:\r\n\r\n```bash\r\n$ pigo --help\r\n\r\n\u250c\u2500\u2510\u252c\u250c\u2500\u2510\u250c\u2500\u2510\r\n\u251c\u2500\u2518\u2502\u2502 \u252c\u2502 \u2502\r\n\u2534  \u2534\u2514\u2500\u2518\u2514\u2500\u2518\r\n\r\nGo (Golang) Face detection library.\r\n    Version: 1.4.2\r\n\r\n  -angle float\r\n    \t0.0 is 0 radians and 1.0 is 2*pi radians\r\n  -cf string\r\n    \tCascade binary file\r\n  -flpc string\r\n    \tFacial landmark points cascade directory\r\n  -in string\r\n    \tSource image (default \"-\")\r\n  -iou float\r\n    \tIntersection over union (IoU) threshold (default 0.2)\r\n  -json string\r\n    \tOutput the detection points into a json file\r\n  -mark\r\n    \tMark detected eyes (default true)\r\n  -marker string\r\n    \tDetection marker: rect|circle|ellipse (default \"rect\")\r\n  -max int\r\n    \tMaximum size of face (default 1000)\r\n  -min int\r\n    \tMinimum size of face (default 20)\r\n  -out string\r\n    \tDestination image (default \"-\")\r\n  -plc string\r\n    \tPupils/eyes localization cascade file\r\n  -scale float\r\n    \tScale detection window by percentage (default 1.1)\r\n  -shift float\r\n    \tShift detection window by percentage (default 0.1)\r\n```\r\n\r\n**Important notice:** In case the `plc` flag is not empty and the provided path is a valid file it will run the pupil/eyes detection method. The same is true for the `flpc` flag, only that in this case you need to provide the directory to the landmark point cascades found under `cascades/lps`.\r\n\r\n### CLI command examples\r\nYou can also use the `stdin` and `stdout` pipe commands:\r\n\r\n```bash\r\n$ cat input/source.jpg | pigo > -in - -out - >out.jpg -cf=/path/to/cascade\r\n```\r\n\r\n`in` and `out` default to `-` so you can also use:\r\n```bash\r\n$ cat input/source.jpg | pigo >out.jpg -cf=/path/to/cascade\r\n$ pigo -out out.jpg < input/source.jpg -cf=/path/to/cascade\r\n```\r\nUsing the `empty` string as value for the `-out` flag will skip the image generation part. This, combined with the `-json` flag will encode the detection results into the specified json file. You can also use the pipe `-` value for the `-json` flag to output the detection coordinates to the standard (`stdout`) output.\r\n\r\n## Real time face detection (running as a shared object)\r\n\r\nIn case you wish to test the library real time face detection capabilities using a webcam, the `examples` folder contains a  web and a few Python examples. Prior running it you need to have Python2 and OpenCV2 installed.\r\n\r\nSelect one of the few Python files provided in the `examples` folder and simply run them. Each of them will execute the exported Go binary file as a shared library. This is also a proof of concept how Pigo can be integrated into different programming languages. I have provided examples only for Python, since this was the only viable way to access the webcam, the Go ecosystem suffering badly from a comprehensive, cross platform and widely available library for accessing the webcam.\r\n\r\n## WASM (Webassembly) support \ud83c\udf89\r\n\r\nStarting from version **v1.4.0** the library has been ported to [**WASM**](http://webassembly.org/). This gives the library a huge performance gain in terms of real time face detection capabilities. Form more details check the subpage description: https://github.com/esimov/pigo/tree/master/wasm.\r\n\r\n## Benchmark results\r\n\r\nBelow are the benchmark results obtained running Pigo against [GoCV](https://github.com/hybridgroup/gocv) using the same conditions.\r\n\r\n```\r\n    BenchmarkGoCV-4   \t       3\t 414122553 ns/op\t     704 B/op\t       1 allocs/op\r\n    BenchmarkPIGO-4   \t      10\t 173664832 ns/op\t       0 B/op\t       0 allocs/op\r\n    PASS\r\n    ok  \tgithub.com/esimov/gocv-test\t4.530s\r\n```\r\nThe code used for the above test can be found under the following link: https://github.com/esimov/pigo-gocv-benchmark\r\n\r\n## Author\r\n\r\n* Endre Simo ([@simo_endre](https://twitter.com/simo_endre))\r\n\r\n## License\r\n\r\nCopyright \u00a9 2019 Endre Simo\r\n\r\nThis software is distributed under the MIT license. See the [LICENSE](https://github.com/esimov/pigo/blob/master/LICENSE) file for the full license text.\r\n"
 },
 {
  "repo": "justadudewhohacks/opencv4nodejs",
  "language": "C++",
  "readme_contents": "opencv4nodejs\n=============\n\n![opencv4nodejs](https://user-images.githubusercontent.com/31125521/37272906-67187fdc-25d8-11e8-9704-40e9e94c1e80.jpg)\n\n[![Build Status](https://travis-ci.org/justadudewhohacks/opencv4nodejs.svg?branch=master)](http://travis-ci.org/justadudewhohacks/opencv4nodejs)\n[![Build status](https://ci.appveyor.com/api/projects/status/cv3o65nrosh1udbb/branch/master?svg=true)](https://ci.appveyor.com/project/justadudewhohacks/opencv4nodejs/branch/master)\n[![Coverage](https://codecov.io/github/justadudewhohacks/opencv4nodejs/coverage.svg?branch=master)](https://codecov.io/gh/justadudewhohacks/opencv4nodejs)\n[![npm download](https://img.shields.io/npm/dm/opencv4nodejs.svg?style=flat)](https://www.npmjs.com/package/opencv4nodejs)\n[![node version](https://img.shields.io/badge/node.js-%3E=_6-green.svg?style=flat)](http://nodejs.org/download/)\n[![Slack](https://slack.bri.im/badge.svg)](https://slack.bri.im/)\n\n**opencv4nodejs allows you to use the native OpenCV library in nodejs. Besides a synchronous API the package provides an asynchronous API, which allows you to build non-blocking and multithreaded computer vision tasks. opencv4nodejs supports OpenCV 3 and OpenCV 4.**\n\n**The ultimate goal of this project is to provide a comprehensive collection of nodejs bindings to the API of OpenCV and the OpenCV-contrib modules. To get an overview of the currently implemented bindings, have a look at the [type declarations](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/lib/typings) of this package. Furthermore, contribution is highly appreciated. If you want to add missing bindings check out the <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/CONTRIBUTING.md\"><b>contribution guide</b>.**\n\n* **[Examples](#examples)**\n* **[How to install](#how-to-install)**\n* **[Usage with Docker](#usage-with-docker)**\n* **[Usage with Electron](#usage-with-electron)**\n* **[Usage with NW.js](#usage-with-nwjs)**\n* **[Quick Start](#quick-start)**\n* **[Async API](#async-api)**\n* **[With TypeScript](#with-typescript)**\n* **[External Memory Tracking (v4.0.0)](#external-mem-tracking)**\n<a name=\"examples\"></a>\n\n# Examples\n\nSee <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples\"><b>examples</b></a> for implementation.\n\n### Face Detection\n\n![face0](https://user-images.githubusercontent.com/31125521/29702727-c796acc4-8972-11e7-8043-117dd2761833.jpg)\n![face1](https://user-images.githubusercontent.com/31125521/29702730-c79d3904-8972-11e7-8ccb-e8c467244ad8.jpg)\n\n### Face Recognition with the OpenCV face module\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-opencv-for-face-recognition-37fa7cb860e8\"><b>Node.js + OpenCV for Face Recognition</b></a>.\n\n![facerec](https://user-images.githubusercontent.com/31125521/35453007-eac9d516-02c8-11e8-9c4d-a77c01ae1f77.jpg)\n\n### Face Landmarks with the OpenCV face module\n\n![facelandmarks](https://user-images.githubusercontent.com/31125521/39297394-af14ae26-4943-11e8-845a-a06cbfa28d5a.jpg)\n\n### Face Recognition with <a href=\"https://github.com/justadudewhohacks/face-recognition.js\"><b>face-recognition.js</b></a>\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-face-recognition-js-simple-and-robust-face-recognition-using-deep-learning-ea5ba8e852\"><b>Node.js + face-recognition.js : Simple and Robust Face Recognition using Deep Learning</b></a>.\n\n[![IMAGE ALT TEXT](https://user-images.githubusercontent.com/31125521/35453884-055f3bde-02cc-11e8-8fa6-945f320652c3.jpg)](https://www.youtube.com/watch?v=ArcFHpX-usQ \"Nodejs Face Recognition using face-recognition.js and opencv4nodejs\")\n\n### Hand Gesture Recognition\nCheck out <a href=\"https://medium.com/@muehler.v/simple-hand-gesture-recognition-using-opencv-and-javascript-eb3d6ced28a0\"><b>Simple Hand Gesture Recognition using OpenCV and JavaScript</b></a>.\n\n![gesture-rec_sm](https://user-images.githubusercontent.com/31125521/30052864-41bd5680-9227-11e7-8a62-6205f3d99d5c.gif)\n\n### Object Recognition with Deep Neural Networks\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-meets-opencvs-deep-neural-networks-fun-with-tensorflow-and-caffe-ff8d52a0f072\"><b>Node.js meets OpenCV\u2019s Deep Neural Networks\u200a\u2014\u200aFun with Tensorflow and Caffe</b></a>.\n\n#### Tensorflow Inception\n\n![husky](https://user-images.githubusercontent.com/31125521/32703295-f6b0e7ee-c7f3-11e7-8039-b3ada21810a0.jpg)\n![car](https://user-images.githubusercontent.com/31125521/32703296-f6cea892-c7f3-11e7-8aaa-9fe48b88fe05.jpeg)\n![banana](https://user-images.githubusercontent.com/31125521/32703297-f6e932ca-c7f3-11e7-9a66-bbc826ebf007.jpg)\n\n\n#### Single Shot Multibox Detector with COCO\n\n![dishes-detection](https://user-images.githubusercontent.com/31125521/32703228-eae787d4-c7f2-11e7-8323-ea0265deccb3.jpg)\n![car-detection](https://user-images.githubusercontent.com/31125521/32703229-eb081e36-c7f2-11e7-8b26-4d253b4702b4.jpg)\n\n### Machine Learning\nCheck out <a href=\"https://medium.com/@muehler.v/machine-learning-with-opencv-and-javascript-part-1-recognizing-handwritten-letters-using-hog-and-88719b70efaa\"><b>Machine Learning with OpenCV and JavaScript: Recognizing Handwritten Letters using HOG and SVM</b></a>.\n\n![resulttable](https://user-images.githubusercontent.com/31125521/30635645-5a466ea8-9df3-11e7-8498-527e1293c4fa.png)\n\n### Object Tracking\n\n![trackbgsubtract](https://user-images.githubusercontent.com/31125521/29702733-c7b59864-8972-11e7-996b-d28cb508f3b8.gif)\n![trackbycolor](https://user-images.githubusercontent.com/31125521/29702735-c8057686-8972-11e7-9c8d-13e30ab74628.gif)\n\n### Feature Matching\n\n![matchsift](https://user-images.githubusercontent.com/31125521/29702731-c79e3142-8972-11e7-947e-db109d415469.jpg)\n\n### Image Histogram\n\n![plotbgr](https://user-images.githubusercontent.com/31125521/29995016-1b847970-8fdf-11e7-9316-4eb0fd550adc.jpg)\n![plotgray](https://user-images.githubusercontent.com/31125521/29995015-1b83e06e-8fdf-11e7-8fa8-5d18326b9cd3.jpg)\n\n### Boiler plate for combination of opencv4nodejs, express and websockets.\n\n[opencv4nodejs-express-websockets](https://github.com/Mudassir-23/opencv4nodejs-express-websockets) - Boilerplate express app for getting started on opencv with nodejs and to live stream the video through websockets.\n\n### Automating lights by people detection through classifier\n\nCheck out <a href=\"https://medium.com/softway-blog/automating-lights-with-computer-vision-nodejs-fb9b614b75b2\"><b>Automating lights with Computer Vision & NodeJS</b></a>.\n\n![user-presence](https://user-images.githubusercontent.com/34403479/70385871-8d62e680-19b7-11ea-855c-3b2febfdbd72.png)\n\n<a name=\"how-to-install\"></a>\n\n# How to install\n\n``` bash\nnpm install --save opencv4nodejs\n```\n\nNative node modules are built via node-gyp, which already comes with npm by default. However, node-gyp requires you to have python installed. If you are running into node-gyp specific issues have a look at known issues with [node-gyp](https://github.com/nodejs/node-gyp) first.\n\n**Important note:** node-gyp won't handle whitespaces properly, thus make sure, that the path to your project directory does **not contain any whitespaces**. Installing opencv4nodejs under \"C:\\Program Files\\some_dir\" or similar will not work and will fail with: \"fatal error C1083: Cannot open include file: 'opencv2/core.hpp'\"!**\n\nOn Windows you will furthermore need Windows Build Tools to compile OpenCV and opencv4nodejs. If you don't have Visual Studio or Windows Build Tools installed, you can easily install the VS2015 build tools:\n\n``` bash\nnpm install --global windows-build-tools\n```\n\n## Installing OpenCV Manually\n\nSetting up OpenCV on your own will require you to set an environment variable to prevent the auto build script to run:\n\n``` bash\n# linux and osx:\nexport OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n# on windows:\nset OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n```\n\n### Windows\n\nYou can install any of the OpenCV 3 or OpenCV 4 <a href=\"https://github.com/opencv/opencv/releases/\"><b>releases</b></a> manually or via the [Chocolatey](https://chocolatey.org/) package manager:\n\n``` bash\n# to install OpenCV 4.1.0\nchoco install OpenCV -y -version 4.1.0\n```\n\nNote, this will come without contrib modules. To install OpenCV under windows with contrib modules you have to build the library from source or you can use the auto build script.\n\nBefore installing opencv4nodejs with an own installation of OpenCV you need to expose the following environment variables:\n- *OPENCV_INCLUDE_DIR* pointing to the directory with the subfolder *opencv2* containing the header files\n- *OPENCV_LIB_DIR* pointing to the lib directory containing the OpenCV .lib files\n\nAlso you will need to add the OpenCV binaries to your system path:\n- add an environment variable *OPENCV_BIN_DIR* pointing to the binary directory containing the OpenCV .dll files\n- append `;%OPENCV_BIN_DIR%;` to your system path variable\n\nNote: Restart your current console session after making changes to your environment.\n\n### MacOSX\n\nUnder OSX we can simply install OpenCV via brew:\n\n``` bash\nbrew update\nbrew install opencv@4\nbrew link --force opencv@4\n```\n\n### Linux\n\nUnder Linux we have to build OpenCV from source manually or using the auto build script.\n\n## Installing OpenCV via Auto Build Script\n\nThe auto build script comes in form of the [opencv-build](https://github.com/justadudewhohacks/npm-opencv-build) npm package, which will run by default when installing opencv4nodejs. The script requires you to have git and a recent version of cmake installed.\n\n### Auto Build Flags\n\nYou can customize the autobuild flags using *OPENCV4NODEJS_AUTOBUILD_FLAGS=<flags>*.\nFlags must be space-separated.\n\nThis is an advanced customization and you should have knowledge regarding the OpenCV compilation flags. Flags added by default are listed [here](https://github.com/justadudewhohacks/npm-opencv-build/blob/master/src/constants.ts#L44-L82).\n\n### Installing a Specific Version of OpenCV\n\nYou can specify the Version of OpenCV you want to install via the script by setting an environment variable:\n`export OPENCV4NODEJS_AUTOBUILD_OPENCV_VERSION=4.1.0`\n\n### Installing only a Subset of OpenCV modules\n\nIf you only want to build a subset of the OpenCV modules you can pass the *-DBUILD_LIST* cmake flag via the *OPENCV4NODEJS_AUTOBUILD_FLAGS* environment variable. For example `export OPENCV4NODEJS_AUTOBUILD_FLAGS=-DBUILD_LIST=dnn` will build only modules required for `dnn` and reduces the size and compilation time of the OpenCV package.\n\n## Configuring Environments via package.json\n\nIt's possible to specify build environment variables by inserting them into the `package.json` as follows:\n\n```json\n{\n  \"name\": \"my-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"opencv4nodejs\": \"^X.X.X\"\n  },\n  \"opencv4nodejs\": {\n    \"disableAutoBuild\": 1,\n    \"opencvIncludeDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\include\",\n    \"opencvLibDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\lib\",\n    \"opencvBinDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\bin\"\n  }\n}\n```\n\nThe following environment variables can be passed:\n\n- autoBuildBuildCuda\n- autoBuildFlags\n- autoBuildOpencvVersion\n- autoBuildWithoutContrib\n- disableAutoBuild\n- opencvIncludeDir\n- opencvLibDir\n- opencvBinDir\n\n<a name=\"usage-with-docker\"></a>\n\n# Usage with Docker\n\n### [opencv-express](https://github.com/justadudewhohacks/opencv-express) - example for opencv4nodejs with express.js and docker\n\nOr simply pull from [justadudewhohacks/opencv-nodejs](https://hub.docker.com/r/justadudewhohacks/opencv-nodejs/) for opencv-3.2 + contrib-3.2 with opencv4nodejs globally installed:\n\n``` docker\nFROM justadudewhohacks/opencv-nodejs\n```\n\n**Note**: The aforementioned Docker image already has ```opencv4nodejs``` installed globally. In order to prevent build errors during an ```npm install```, your ```package.json``` should not include ```opencv4nodejs```, and instead should include/require the global package either by requiring it by absolute path or setting the ```NODE_PATH``` environment variable to ```/usr/lib/node_modules``` in your Dockerfile and requiring the package as you normally would.\n\nDifferent OpenCV 3.x base images can be found here: https://hub.docker.com/r/justadudewhohacks/.\n\n<a name=\"usage-with-electron\"></a>\n\n# Usage with Electron\n\n### [opencv-electron](https://github.com/justadudewhohacks/opencv-electron) - example for opencv4nodejs with electron\n\nAdd the following script to your package.json:\n``` python\n\"electron-rebuild\": \"electron-rebuild -w opencv4nodejs\"\n```\n\nRun the script:\n``` bash\n$ npm run electron-rebuild\n```\n\nRequire it in the application:\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"usage-with-nwjs\"></a>\n\n# Usage with NW.js\n\nAny native modules, including opencv4nodejs, must be recompiled to be used with [NW.js](https://nwjs.io/). Instructions on how to do this are available in the **[Use Native Modules](http://docs.nwjs.io/en/latest/For%20Users/Advanced/Use%20Native%20Node%20Modules/)** section of the the NW.js documentation.\n\nOnce recompiled, the module can be installed and required as usual:\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"quick-start\"></a>\n\n# Quick Start\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n### Initializing Mat (image matrix), Vec, Point\n\n``` javascript\nconst rows = 100; // height\nconst cols = 100; // width\n\n// empty Mat\nconst emptyMat = new cv.Mat(rows, cols, cv.CV_8UC3);\n\n// fill the Mat with default value\nconst whiteMat = new cv.Mat(rows, cols, cv.CV_8UC1, 255);\nconst blueMat = new cv.Mat(rows, cols, cv.CV_8UC3, [255, 0, 0]);\n\n// from array (3x3 Matrix, 3 channels)\nconst matData = [\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]],\n  [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]]\n];\nconst matFromArray = new cv.Mat(matData, cv.CV_8UC3);\n\n// from node buffer\nconst charData = [255, 0, ...];\nconst matFromArray = new cv.Mat(Buffer.from(charData), rows, cols, cv.CV_8UC3);\n\n// Point\nconst pt2 = new cv.Point(100, 100);\nconst pt3 = new cv.Point(100, 100, 0.5);\n\n// Vector\nconst vec2 = new cv.Vec(100, 100);\nconst vec3 = new cv.Vec(100, 100, 0.5);\nconst vec4 = new cv.Vec(100, 100, 0.5, 0.5);\n```\n\n### Mat and Vec operations\n\n``` javascript\nconst mat0 = new cv.Mat(...);\nconst mat1 = new cv.Mat(...);\n\n// arithmetic operations for Mats and Vecs\nconst matMultipliedByScalar = mat0.mul(0.5);  // scalar multiplication\nconst matDividedByScalar = mat0.div(2);       // scalar division\nconst mat0PlusMat1 = mat0.add(mat1);          // addition\nconst mat0MinusMat1 = mat0.sub(mat1);         // subtraction\nconst mat0MulMat1 = mat0.hMul(mat1);          // elementwise multiplication\nconst mat0DivMat1 = mat0.hDiv(mat1);          // elementwise division\n\n// logical operations Mat only\nconst mat0AndMat1 = mat0.and(mat1);\nconst mat0OrMat1 = mat0.or(mat1);\nconst mat0bwAndMat1 = mat0.bitwiseAnd(mat1);\nconst mat0bwOrMat1 = mat0.bitwiseOr(mat1);\nconst mat0bwXorMat1 = mat0.bitwiseXor(mat1);\nconst mat0bwNot = mat0.bitwiseNot();\n```\n\n### Accessing Mat data\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\nconst matGray = new cv.Mat(..., cv.CV_8UC1);\n\n// get pixel value as vector or number value\nconst vec3 = matBGR.at(200, 100);\nconst grayVal = matGray.at(200, 100);\n\n// get raw pixel value as array\nconst [b, g, r] = matBGR.atRaw(200, 100);\n\n// set single pixel values\nmatBGR.set(50, 50, [255, 0, 0]);\nmatBGR.set(50, 50, new Vec(255, 0, 0));\nmatGray.set(50, 50, 255);\n\n// get a 25x25 sub region of the Mat at offset (50, 50)\nconst width = 25;\nconst height = 25;\nconst region = matBGR.getRegion(new cv.Rect(50, 50, width, height));\n\n// get a node buffer with raw Mat data\nconst matAsBuffer = matBGR.getData();\n\n// get entire Mat data as JS array\nconst matAsArray = matBGR.getDataAsArray();\n```\n\n### IO\n\n``` javascript\n// load image from file\nconst mat = cv.imread('./path/img.jpg');\ncv.imreadAsync('./path/img.jpg', (err, mat) => {\n  ...\n})\n\n// save image\ncv.imwrite('./path/img.png', mat);\ncv.imwriteAsync('./path/img.jpg', mat,(err) => {\n  ...\n})\n\n// show image\ncv.imshow('a window name', mat);\ncv.waitKey();\n\n// load base64 encoded image\nconst base64text='data:image/png;base64,R0lGO..';//Base64 encoded string\nconst base64data =base64text.replace('data:image/jpeg;base64','')\n                            .replace('data:image/png;base64','');//Strip image type prefix\nconst buffer = Buffer.from(base64data,'base64');\nconst image = cv.imdecode(buffer); //Image is now represented as Mat\n\n// convert Mat to base64 encoded jpg image\nconst outBase64 =  cv.imencode('.jpg', croppedImage).toString('base64'); // Perform base64 encoding\nconst htmlImg='<img src=data:image/jpeg;base64,'+outBase64 + '>'; //Create insert into HTML compatible <img> tag\n\n// open capture from webcam\nconst devicePort = 0;\nconst wCap = new cv.VideoCapture(devicePort);\n\n// open video capture\nconst vCap = new cv.VideoCapture('./path/video.mp4');\n\n// read frames from capture\nconst frame = vCap.read();\nvCap.readAsync((err, frame) => {\n  ...\n});\n\n// loop through the capture\nconst delay = 10;\nlet done = false;\nwhile (!done) {\n  let frame = vCap.read();\n  // loop back to start on end of stream reached\n  if (frame.empty) {\n    vCap.reset();\n    frame = vCap.read();\n  }\n\n  // ...\n\n  const key = cv.waitKey(delay);\n  done = key !== 255;\n}\n```\n\n### Useful Mat methods\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\n\n// convert types\nconst matSignedInt = matBGR.convertTo(cv.CV_32SC3);\nconst matDoublePrecision = matBGR.convertTo(cv.CV_64FC3);\n\n// convert color space\nconst matGray = matBGR.bgrToGray();\nconst matHSV = matBGR.cvtColor(cv.COLOR_BGR2HSV);\nconst matLab = matBGR.cvtColor(cv.COLOR_BGR2Lab);\n\n// resize\nconst matHalfSize = matBGR.rescale(0.5);\nconst mat100x100 = matBGR.resize(100, 100);\nconst matMaxDimIs100 = matBGR.resizeToMax(100);\n\n// extract channels and create Mat from channels\nconst [matB, matG, matR] = matBGR.splitChannels();\nconst matRGB = new cv.Mat([matR, matB, matG]);\n```\n\n### Drawing a Mat into HTML Canvas\n\n``` javascript\nconst img = ...\n\n// convert your image to rgba color space\nconst matRGBA = img.channels === 1\n  ? img.cvtColor(cv.COLOR_GRAY2RGBA)\n  : img.cvtColor(cv.COLOR_BGR2RGBA);\n\n// create new ImageData from raw mat data\nconst imgData = new ImageData(\n  new Uint8ClampedArray(matRGBA.getData()),\n  img.cols,\n  img.rows\n);\n\n// set canvas dimensions\nconst canvas = document.getElementById('myCanvas');\ncanvas.height = img.rows;\ncanvas.width = img.cols;\n\n// set image data\nconst ctx = canvas.getContext('2d');\nctx.putImageData(imgData, 0, 0);\n```\n\n### Method Interface\n\nOpenCV method interface from official docs or src:\n``` c++\nvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT);\n```\n\ntranslates to:\n\n``` javascript\nconst src = new cv.Mat(...);\n// invoke with required arguments\nconst dst0 = src.gaussianBlur(new cv.Size(5, 5), 1.2);\n// with optional paramaters\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, 0.8, cv.BORDER_REFLECT);\n// or pass specific optional parameters\nconst optionalArgs = {\n  borderType: cv.BORDER_CONSTANT\n};\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, optionalArgs);\n```\n\n<a name=\"async-api\"></a>\n\n# Async API\n\nThe async API can be consumed by passing a callback as the last argument of the function call. By default, if an async method is called without passing a callback, the function call will yield a Promise.\n\n### Async Face Detection\n\n``` javascript\nconst classifier = new cv.CascadeClassifier(cv.HAAR_FRONTALFACE_ALT2);\n\n// by nesting callbacks\ncv.imreadAsync('./faceimg.jpg', (err, img) => {\n  if (err) { return console.error(err); }\n\n  const grayImg = img.bgrToGray();\n  classifier.detectMultiScaleAsync(grayImg, (err, res) => {\n    if (err) { return console.error(err); }\n\n    const { objects, numDetections } = res;\n    ...\n  });\n});\n\n// via Promise\ncv.imreadAsync('./faceimg.jpg')\n  .then(img =>\n    img.bgrToGrayAsync()\n      .then(grayImg => classifier.detectMultiScaleAsync(grayImg))\n      .then((res) => {\n        const { objects, numDetections } = res;\n        ...\n      })\n  )\n  .catch(err => console.error(err));\n\n// using async await\ntry {\n  const img = await cv.imreadAsync('./faceimg.jpg');\n  const grayImg = await img.bgrToGrayAsync();\n  const { objects, numDetections } = await classifier.detectMultiScaleAsync(grayImg);\n  ...\n} catch (err) {\n  console.error(err);\n}\n```\n\n<a name=\"with-typescript\"></a>\n\n# With TypeScript\n\n``` javascript\nimport * as cv from 'opencv4nodejs'\n```\n\nCheck out the TypeScript [examples](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples/typed).\n\n<a name=\"external-mem-tracking\"></a>\n\n# External Memory Tracking (v4.0.0)\n\nSince version 4.0.0 was released, external memory tracking has been enabled by default. Simply put, the memory allocated for Matrices (cv.Mat) will be manually reported to the node process. This solves the issue of inconsistent Garbage Collection, which could have resulted in spiking memory usage of the node process eventually leading to overflowing the RAM of your system, prior to version 4.0.0.\n\nNote, that in doubt this feature can be **disabled** by setting an environment variable `OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING` before requiring the module:\n\n``` bash\nexport OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // linux\nset OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // windows\n```\n\nOr directly in your code:\n``` javascript\nprocess.env.OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING = 1\nconst cv = require('opencv4nodejs')\n```\n\n\n"
 },
 {
  "repo": "jrosebr1/imutils",
  "language": "Python",
  "readme_contents": "# imutils\nA series of convenience functions to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and ***both*** Python 2.7 and Python 3.\n\nFor more information, along with a detailed code review check out the following posts on the [PyImageSearch.com](http://www.pyimagesearch.com) blog:\n\n- [http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/](http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/)\n- [http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/)\n- [http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/](http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/)\n\n## Installation\nProvided you already have NumPy, SciPy, Matplotlib, and OpenCV already installed, the `imutils` package is completely `pip`-installable:\n\n<pre>$ pip install imutils</pre>\n\n## Finding function OpenCV functions by name\nOpenCV can be a big, hard to navigate library, especially if you are just getting started learning computer vision and image processing. The `find_function` method allows you to quickly search function names across modules (and optionally sub-modules) to find the function you are looking for.\n\n#### Example:\nLet's find all function names that contain the text `contour`:\n\n<pre>import imutils\nimutils.find_function(\"contour\")</pre>\n\n#### Output:\n<pre>1. contourArea\n2. drawContours\n3. findContours\n4. isContourConvex</pre>\n\nThe `contourArea` function could therefore be accessed via: `cv2.contourArea`\n\n\n## Translation\nTranslation is the shifting of an image in either the *x* or *y* direction. To translate an image in OpenCV you would need to supply the *(x, y)*-shift, denoted as *(t<sub>x</sub>, t<sub>y</sub>)* to construct the translation matrix *M*:\n\n![Translation equation](docs/images/translation_eq.png?raw=true)\n\nAnd from there, you would need to apply the `cv2.warpAffine` function.\n\nInstead of manually constructing the translation matrix *M* and calling `cv2.warpAffine`, you can simply make a call to the `translate` function of `imutils`.\n\n#### Example:\n<pre># translate the image x=25 pixels to the right and y=75 pixels up\ntranslated = imutils.translate(workspace, 25, -75)</pre>\n\n#### Output:\n\n<img src=\"docs/images/translation.png?raw=true\" alt=\"Translation example\" style=\"max-width: 500px;\">\n\n## Rotation\nRotating an image in OpenCV is accomplished by making a call to `cv2.getRotationMatrix2D` and `cv2.warpAffine`. Further care has to be taken to supply the *(x, y)*-coordinate of the point the image is to be rotated about. These calculation calls can quickly add up and make your code bulky and less readable. The `rotate` function in `imutils` helps resolve this problem.\n\n#### Example:\n<pre># loop over the angles to rotate the image\nfor angle in xrange(0, 360, 90):\n\t# rotate the image and display it\n\trotated = imutils.rotate(bridge, angle=angle)\n\tcv2.imshow(\"Angle=%d\" % (angle), rotated)</pre>\n\n#### Output:\n\n<img src=\"docs/images/rotation.png?raw=true\" alt=\"Rotation example\" style=\"max-width: 500px;\">\n\n## Resizing\nResizing an image in OpenCV is accomplished by calling the `cv2.resize` function. However, special care needs to be taken to ensure that the aspect ratio is maintained.  This `resize` function of `imutils` maintains the aspect ratio and provides the keyword arguments `width` and `height` so the image can be resized to the intended width/height while (1) maintaining aspect ratio and (2) ensuring the dimensions of the image do not have to be explicitly computed by the developer.\n\nAnother optional keyword argument, `inter`, can be used to specify interpolation method as well.\n\n#### Example:\n<pre># loop over varying widths to resize the image to\nfor width in (400, 300, 200, 100):\n\t# resize the image and display it\n\tresized = imutils.resize(workspace, width=width)\n\tcv2.imshow(\"Width=%dpx\" % (width), resized)</pre>\n\n#### Output:\n\n<img src=\"docs/images/resizing.png?raw=true\" alt=\"Resizing example\" style=\"max-width: 500px;\">\n\n## Skeletonization\nSkeletonization is the process of constructing the \"topological skeleton\" of an object in an image, where the object is presumed to be white on a black background. OpenCV does not provide a function to explicitly construct the skeleton, but does provide the morphological and binary functions to do so.\n\nFor convenience, the `skeletonize` function of `imutils` can be used to construct the topological skeleton of the image.\n\nThe first argument, `size` is the size of the structuring element kernel. An optional argument, `structuring`, can be used to control the structuring element -- it defaults to `cv2.MORPH_RECT`\t, but can be any valid structuring element.\n\n#### Example:\n<pre># skeletonize the image\ngray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nskeleton = imutils.skeletonize(gray, size=(3, 3))\ncv2.imshow(\"Skeleton\", skeleton)</pre>\n\n#### Output:\n\n<img src=\"docs/images/skeletonization.png?raw=true\" alt=\"Skeletonization example\" style=\"max-width: 500px;\">\n\n## Displaying with Matplotlib\nIn the Python bindings of OpenCV, images are represented as NumPy arrays in BGR order. This works fine when using the `cv2.imshow` function. However, if you intend on using Matplotlib, the `plt.imshow` function assumes the image is in RGB order. A simple call to `cv2.cvtColor` will resolve this problem, or you can use the `opencv2matplotlib` convenience function.\n\n#### Example:\n<pre># INCORRECT: show the image without converting color spaces\nplt.figure(\"Incorrect\")\nplt.imshow(cactus)\n\n# CORRECT: convert color spaces before using plt.imshow\nplt.figure(\"Correct\")\nplt.imshow(imutils.opencv2matplotlib(cactus))\nplt.show()</pre>\n\n#### Output:\n\n<img src=\"docs/images/matplotlib.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## URL to Image\nThis the `url_to_image` function accepts a single parameter: the `url` of the image we want to download and convert to a NumPy array in OpenCV format. This function performs the download in-memory. The `url_to_image` function has been detailed [here](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/) on the PyImageSearch blog.\n\n#### Example:\n<pre>url = \"http://pyimagesearch.com/static/pyimagesearch_logo_github.png\"\nlogo = imutils.url_to_image(url)\ncv2.imshow(\"URL to Image\", logo)\ncv2.waitKey(0)</pre>\n\n#### Output:\n\n<img src=\"docs/images/url_to_image.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Checking OpenCV Versions\nOpenCV 3 has finally been released! But with the major release becomes backward compatibility issues (such as with the `cv2.findContours` and `cv2.normalize` functions). If you want your OpenCV 3 code to be backwards compatible with OpenCV 2.4.X, you'll need to take special care to check which version of OpenCV is currently being used and then take appropriate action. The `is_cv2()` and `is_cv3()` are simple functions that can be used to automatically determine the OpenCV version of the current environment.\n\n#### Example:\n<pre>print(\"Your OpenCV version: {}\".format(cv2.__version__))\nprint(\"Are you using OpenCV 2.X? {}\".format(imutils.is_cv2()))\nprint(\"Are you using OpenCV 3.X? {}\".format(imutils.is_cv3()))</pre>\n\n#### Output:\n<pre>Your OpenCV version: 3.0.0\nAre you using OpenCV 2.X? False\nAre you using OpenCV 3.X? True</pre>\n\n## Automatic Canny Edge Detection\nThe Canny edge detector requires two parameters when performing hysteresis. However, tuning these two parameters to obtain an optimal edge map is non-trivial, especially when working with a dataset of images. Instead, we can use the `auto_canny` function which uses the median of the grayscale pixel intensities to derive the upper and lower thresholds. You can read more about the `auto_canny` function [here](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/).\n\n#### Example:\n<pre>gray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nedgeMap = imutils.auto_canny(gray)\ncv2.imshow(\"Original\", logo)\ncv2.imshow(\"Automatic Edge Map\", edgeMap)</pre>\n\n#### Output:\n\n<img src=\"docs/images/auto_canny.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## 4-point Perspective Transform\nA common task in computer vision and image processing is to perform a 4-point perspective transform of a ROI in an image and obtain a top-down, \"birds eye view\" of the ROI. The `perspective` module takes care of this for you. A real-world example of applying a 4-point perspective transform can be bound in this blog on on [building a kick-ass mobile document scanner](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/).\n\n#### Example\nSee the contents of `demos/perspective_transform.py`\n\n#### Output:\n\n<img src=\"docs/images/perspective_transform.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Sorting Contours\nThe contours returned from `cv2.findContours` are unsorted. By using the `contours` module the the `sort_contours` function we can sort a list of contours from left-to-right, right-to-left, top-to-bottom, and bottom-to-top, respectively.\n\n#### Example:\nSee the contents of `demos/sorting_contours.py`\n\n#### Output:\n\n<img src=\"docs/images/sorting_contours.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## (Recursively) Listing Paths to Images\nThe `paths` sub-module of `imutils` includes a function to recursively find images based on a root directory.\n\n#### Example:\nAssuming we are in the `demos` directory, let's list the contents of the `../demo_images`:\n\n<pre>from imutils import paths\nfor imagePath in paths.list_images(\"../demo_images\"):\n\tprint imagePath</pre>\n\n#### Output:\n<pre>../demo_images/bridge.jpg\n../demo_images/cactus.jpg\n../demo_images/notecard.png\n../demo_images/pyimagesearch_logo.jpg\n../demo_images/shapes.png\n../demo_images/workspace.jpg</pre>\n"
 },
 {
  "repo": "nuno-faria/tiler",
  "language": "Python",
  "readme_contents": "![title](images/title_stripes.png)\n\n\ud83d\udc77 Build images with images.\n\n\n## About\n\nTiler is a tool to create an image using all kinds of other smaller images (tiles). It is different from other mosaic tools since it can adapt to tiles with multiple shapes and sizes (i.e. not limited to squares).\n\nAn image can be built out of circles, lines, waves, cross stitches, legos, minecraft blocks, paper clips, letters, ... The possibilities are endless!\n\n\n## Installation\n\n- Clone the repo: `git clone https://github.com/nuno-faria/tiler.git`;\n- Install Python 3;\n- Install pip (optional, to install the dependencies);\n- Install dependencies: `pip install -r requirements.txt`\n\n## Usage\n\n- Make a folder with the tiles (and only the tiles) to build the image;\n    - The script `gen_tiles.py` can help in this task; it builds tiles with multiple colors based on the source tile (note: its recommended for the source file to have an RGB color of (240,240,240)). It is used as `python gen_tiles.py path/to/image` and creates a folder with a 'gen_' prefix in the same path as the base image.\n- Run `python tiler.py path/to/image path/to/tiles_folder/`.\n\n## Configuration\n\nAll configurations can be changed in the `conf.py` file.\n\n#### `gen_tiles.py`\n\n- `DEPTH` - number of divisions in each color channel (ex: DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `ROTATIONS` - list of rotations, in degrees, to apply over the original image (ex: [0, 90]).\n\n#### `tiler.py`\n\n- `COLOR_DEPTH` - number of divisions in each color channel (ex: COLOR_DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `RESIZING_SCALES` - scale to apply to each tile (ex: [1, 0.75, 0.5, 0.25]);\n- `PIXEL_SHIFT` - number of pixels shifted to create each box (ex: (5,5)); if None, shift will be the same as the tile dimension);\n    <img src=\"images/pixel_shift.png\" width=\"100%\">\n- `OVERLAP_TILES` - if tiles can overlap;\n- `RENDER` - render image as its being built;\n- `POOL_SIZE` - multiprocessing pool size;\n- `IMAGE_TO_TILE` - image to tile (ignored if passed as the 1st arg);\n- `TILES_FOLDER` - folder with tiles (ignored if passed as the 2nd arg);\n- `OUT` - result image filename.\n\n\n## Examples\n\n### Circles\n\n#### Various sizes\n\n<img src=\"images/cake_circles.png\" width=\"40%\">\n\n[Original](https://www.flaticon.com/free-icon/cake_1102780) cake image by [pongsakornred](https://www.flaticon.com/authors/pongsakornred) from [FLATICON](https://www.flaticon.com).\n\n#### Fixed\n\n- 10x10\n<img src=\"images/cake_circles_simple.png\" width=\"40%\">\n<img src=\"images/starry_night_circles_10x10.png\" width=\"100%\">\n- 25x25\n<img src=\"images/starry_night_circles_25x25.png\" width=\"100%\">\n- 50x50\n<img src=\"images/starry_night_circles_50x50.png\" width=\"100%\">\n\n\n### Paper clips\n\n<img src=\"images/cake_clips.png\" width=\"40%\">\n\n\n### Cross stitch (times)\n\n<img src=\"images/cake_times.png\" width=\"40%\">\n\n<img src=\"images/starry_night_times.png\" width=\"100%\">\n\n\n### Hearts\n\n<img src=\"images/heart_hearts.png\" width=\"40%\">\n\n\n### Legos\n\n<img src=\"images/cake_lego.png\" width=\"40%\">\n<img src=\"images/starry_night_lego.png\" width=\"100%\">\n\n\n### Minecraft blocks\n\n<img src=\"images/cake_minecraft.png\" width=\"40%\">\n<img src=\"images/starry_night_minecraft.png\" width=\"100%\">\n\n\n### Stripes (lines)\n\n<img src=\"images/cake_stripes.png\" width=\"50%\">\n\n\n### At\n\n<img src=\"images/github_logo_at.png\" width=\"50%\">\n"
 },
 {
  "repo": "soruly/trace.moe",
  "language": "PHP",
  "readme_contents": "# trace.moe\n\n[![License](https://img.shields.io/github/license/soruly/trace.moe.svg)](https://github.com/soruly/trace.moe/blob/master/LICENSE)\n[![Discord](https://img.shields.io/discord/437578425767559188.svg)](https://discord.gg/K9jn6Kj)\n[![Donate](https://img.shields.io/badge/donate-patreon-orange.svg)](https://www.patreon.com/soruly)\n\nThe website of trace.moe (whatanime.ga)\n\nImage Reverse Search for Anime Scenes\n\nUse anime screenshots to search where this scene is taken from.\n\nIt tells you which anime, which episode, and exactly which moment this scene appears in Japanese Anime.\n\n## Demo\n\nDemo image\n\n![](https://images.plurk.com/2FKxneXP64qiKwjlUA7sKj.jpg)\n\nSearch result tells you which moment it appears.\n\n![](https://addons.cdn.mozilla.net/user-media/previews/full/209/209947.png)\n\n## How does it work\n\ntrace.moe uses [sola](https://github.com/soruly/sola) to index video and work with liresolr. This repo only include the webapp for trace.moe, which demonstrate how to integrate anilist info, and how thumbnail/video previews are generated. If you want to make your own video scene search engine, please refer to sola instead.\n\nTo learn more, read the presentation slides below\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga.slide) May 2016\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2017.slide) Jun 2017\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2018.slide) Jun 2018\n\n[Presentation slides](https://github.com/soruly/slides/blob/master/2019-COSCUP-trace.moe.md) Aug 2019\n\nSystem Overview\n\n![](https://pbs.twimg.com/media/CstZmrxUIAAi8La.jpg)\n\nYou may find some other related repo here\n\n- [sola](https://github.com/soruly/sola)\n- [LireSolr](https://github.com/soruly/liresolr)\n- [anilist-crawler](https://github.com/soruly/anilist-crawler)\n- [trace.moe-WebExtension](https://github.com/soruly/trace.moe-WebExtension)\n- [trace.moe-telegram-bot](https://github.com/soruly/trace.moe-telegram-bot)\n\n## Official API Docs (Beta)\n\nhttps://soruly.github.io/trace.moe/\n\n## Mobile Apps (3rd party)\n\nWhatAnime by Andr\u00e9e Torres\nhttps://play.google.com/store/apps/details?id=com.maddog05.whatanime\nSource: https://github.com/maddog05/whatanime-android\n\nWhatAnime - \u4ee5\u56fe\u641c\u756a by Mystery0 (Simplified Chinese)\nhttps://play.google.com/store/apps/details?id=pw.janyo.whatanime\nSource: https://github.com/JanYoStudio/WhatAnime\n\n## Integrating search with trace.moe\n\nTo add trace.moe as a search option for your site, pass the image URL via query string like this\n\n```\nhttps://trace.moe/?url=http://searchimageurl\n```\n\nYou can also specify playback options like this\n\n```\nhttps://trace.moe/?autoplay=0&loop&mute=1&url=http://searchimageurl\n```\n\nPlayback URL params:\n\n| param    | value  | default (not set in URL param) | set with empty or other value |\n| -------- | ------ | ------------------------------ | ----------------------------- |\n| autoplay | 0 or 1 | 1                              | 1                             |\n| mute     | 0 or 1 | 0                              | 1                             |\n| loop     | 0 or 1 | 0                              | 1                             |\n\nThe `auto` URL parameter is no longer used, it would always search automatically when there is `?url=` param.\n\nNote that the server cannot access private image URLs.\nIn that case, users has to copy and paste (Ctrl+V/Cmd+V) the image directly, or save and upload the file.\n"
 },
 {
  "repo": "anandpawara/Real_Time_Image_Animation",
  "language": "Python",
  "readme_contents": "# Real time Image Animation\nThe Project is real time application in opencv using first order model\n\n# Steps to setup\n\n## Step 1: Create virtual environment\n\n**Python version** : python v3.7.3 or higher\n\n**create virual environment** : ```pip install virtualenv```\n\n**activate virtual environment** : ```virtualenv env```\n\n## Step 2: Activate virtual environment\n\n**For windows** : ```env/Script/activate```\n\n**For Linux** : ```source env/bin/activate```\n\n## Step 3 : Install required modules\n\n**Install modules** : ``` pip install -r requirements.txt ```\n\n**Install pytorch and torchvision** : ```pip install torch===1.0.0 torchvision===0.2.1 -f https://download.pytorch.org/whl/cu100/torch_stable.html ```\n\n## Step 4 : Download cascade file ,weights and model and save in folder named extract\n\n```gdown --id 1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK```\nThe file is also availible via direct link on Google's Drive:\nhttps://drive.google.com/uc?id=1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK\n\n**On Linux machine** : ```unzip checkpoints.zip```\n\nIf on windows platfrom unzip checkpoints.zip using unzipping software like 7zip.\n\n**Delete zip file** : ```rm checkpoints.zip```\n\n## Step 5 : Run the project\n\n**Run application from live camera** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar```\n\n**Run application from video file** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint -v path_to_video_file```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar -v .\\video_input\\test1.mp4 ```\n\n![test demo](animate.gif)\n\n### TODO:\nTkinter version\n\nNeed work on face alignments\n\nFuture plans adding deepfake voice and merging with video\n\nCredits\n=======\n```\n@InProceedings{Siarohin_2019_NeurIPS,\n  author={Siarohin, Aliaksandr and Lathuili\u00e8re, St\u00e9phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},\n  title={First Order Motion Model for Image Animation},\n  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},\n  month = {December},\n  year = {2019},\n  url = {https://github.com/AliaksandrSiarohin/first-order-model}\n}\n```\n- Original Project\n    * [AliaksandrSiarohin](https://github.com/AliaksandrSiarohin/first-order-model)\n\n    If you like this project give your support to original author of this project by giving github star to author's project\n\n- video explanation to the project <br/>\n    * [Video explanation by original author](https://www.youtube.com/watch?v=u-0cQ-grXBQ)\n    * [Two min papers](https://www.youtube.com/watch?v=mUfJOQKdtAk)    \n\n- try project on google colab\n    * [youtube link](https://www.youtube.com/watch?v=RsOJJd1q6Bg&feature=youtu.be)\n    * [link to colab version](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)\n\n    For any valueable feedback feel free to contact me on [linkedin](https://www.linkedin.com/in/anand-pawara-8045/)\n\n"
 },
 {
  "repo": "HuTianQi/SmartOpenCV",
  "language": "C++",
  "readme_contents": "# SmartOpenCV\n\n![SmartOpenCV](art/logo.png)  \n### \u524d\u8a00\n\n:fire: :fire: :fire: \u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u4ee5\u53ca\u7ec8\u7aef\u8bbe\u5907\u786c\u4ef6\u6c34\u5e73\u7684\u4e0d\u65ad\u63d0\u5347\uff0c\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u76f4\u63a5\u8fd0\u884c\u667a\u80fd\u7cfb\u7edf\u6210\u4e3a\u53ef\u80fd\uff0c\u7aef\u4fa7\u667a\u80fd\u5177\u5907\u4f4e\u5ef6\u65f6\uff0c\u9690\u79c1\u5b89\u5168\u7b49\u7279\u70b9\u3002\u540c\u65f6\u964d\u4f4e\u4e86\u4e91\u7aef\u667a\u80fd\u5b58\u5728\u7684\u7f51\u7edc\u4f20\u8f93\u4e0d\u53ef\u9760\u98ce\u9669\uff0c\u4f7f\u5f97\u7aef\u4fa7\u667a\u80fd\u8d8a\u6765\u8d8a\u5f97\u5230\u91cd\u89c6\u3002\u7aef\u4fa7\u667a\u80fd\u6bd4\u8f83\u6210\u719f\u7684\u9886\u57df\u5c31\u662fNLP\u4ee5\u53caCV\u3002\u5728CV\u9886\u57dfOpenCV\u4f5c\u4e3a\u5f00\u6e90\u4e14\u5f3a\u5927\u7684\u8de8\u5e73\u53f0\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u5728\u56fe\u50cf\u5904\u7406\u4ee5\u53ca\u56fe\u50cf\u8bc6\u522b\u65b9\u5411\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u4f46\u662f\u5728Android\u5e73\u53f0OpenCV\u5b98\u65b9SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u8bf8\u591a\u7f3a\u9677\u3002\n\n### SmartOpenCV\u662f\u4ec0\u4e48\nSmartOpenCV\u662f\u4e00\u4e2aOpenCV\u5728Android\u7aef\u7684\u589e\u5f3a\u5e93\uff0c\u89e3\u51b3\u4e86OpenCV Android SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u7684\u8bf8\u591a\u95ee\u9898\uff0c\u800c\u4e14\u65e0\u9700\u4fee\u6539OpenCV SDK\u6e90\u7801\uff0c\u4e0eOpenCV\u7684SDK\u89e3\u8026\uff0c\u53ea\u9700\u66ff\u6362xml\u4e2d\u539fOpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\u8fbe\u5230\u5177\u5907OpenCV\u5b98\u65b9SDK\u7684\u539f\u529f\u80fd\u4ee5\u53caSmartOpenCV\u7684\u589e\u5f3a\u529f\u80fd\u3002\n\n### OpenCV\u5b98\u65b9SDK\u5b58\u5728\u7684\u95ee\u9898\n\nOpenCV Android\u7aefSDK\u867d\u7136\u5f88\u5bb9\u6613\u4e0a\u624b\u548c\u4f7f\u7528\uff0c\u4f46\u662f\u9884\u89c8\u5b58\u5728\u5f88\u591a\u95ee\u9898\uff0c\u5e38\u89c1\u95ee\u9898\u5982\u4e0b\uff1a\n\n- **\u9ed8\u8ba4\u6a2a\u5c4f\u663e\u793a\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u63a5\u53e3\u4fee\u6539\u9884\u89c8\u65b9\u5411**\n\n- **\u9884\u89c8\u7ed8\u5236\u5b58\u5728\u9ed1\u8fb9**\uff1aOpenCV\u9ed8\u8ba4\u7ed8\u5236\u7b97\u6cd5\u5728\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\u5230Canvas\u65f6\u5b58\u5728\u4e00\u5b9a\u7684\u504f\u79fb\uff0c\u5728\u89c6\u89c9\u4e0a\u8868\u73b0\u5c31\u662f\u9884\u89c8\u5e27\u53ea\u4f1a\u5360SurfaceView\u63a7\u4ef6\u7684\u4e00\u90e8\u5206\u533a\u57df\uff0c\u504f\u79fb\u90e8\u5206\u533a\u57df\u4f1a\u663e\u793a\u4e3a\u9ed1\u8272\n\n  ```java\n  if (mScale != 0) {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2),\n                      (int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2 + mScale * mCacheBitmap.getWidth()),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2 + mScale * mCacheBitmap.getHeight())), null);\n  } else {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((canvas.getWidth() - mCacheBitmap.getWidth()) / 2,\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2,\n                      (canvas.getWidth() - mCacheBitmap.getWidth()) / 2 + mCacheBitmap.getWidth(),\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2 + mCacheBitmap.getHeight()), null);\n  }\n  ```\n\n  \n\n- **\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\u4e0d\u7b26\u5408\u5b9e\u9645\u573a\u666f\u8981\u6c42**\uff1a\u5bf9\u4e8e\u9884\u89c8\u5e27\u5927\u5c0f\u7684\u9009\u62e9\uff0cOpenCV\u9ed8\u8ba4\u7b97\u6cd5\u662f\u9009\u62e9**\u5c0f\u4e8e**\u9884\u89c8\u63a7\u4ef6(\u6216\u8bbe\u7f6e\u7684\u6700\u5927\u5e27\u5927\u5c0f)\u7684\u6700\u5927\u9884\u89c8\uff0c\u8fd9\u5c06\u5bfc\u81f4\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u9884\u89c8\u56fe\u50cf\u7684\u663e\u793a\u4e0d\u80fd\u94fa\u6ee1\u6574\u4e2a\u63a7\u4ef6\u751a\u81f3\u8fdc\u5c0f\u4e8e\u63a7\u4ef6\u5927\u5c0f\uff0c \u5728\u7edd\u5927\u90e8\u5206\u4e1a\u52a1\u573a\u666f\u4e0b\uff0c\u8fd9\u79cd\u7b97\u6cd5\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\n\n  ```java\n  protected Size calculateCameraFrameSize(List<?> supportedSizes, ListItemAccessor accessor, int surfaceWidth, int surfaceHeight) {\n          int calcWidth = 0;\n          int calcHeight = 0;\n  \n          int maxAllowedWidth = (mMaxWidth != MAX_UNSPECIFIED && mMaxWidth < surfaceWidth)? mMaxWidth : surfaceWidth;\n          int maxAllowedHeight = (mMaxHeight != MAX_UNSPECIFIED && mMaxHeight < surfaceHeight)? mMaxHeight : surfaceHeight;\n  \n          for (Object size : supportedSizes) {\n              int width = accessor.getWidth(size);\n              int height = accessor.getHeight(size);\n              Log.d(TAG, \"trying size: \" + width + \"x\" + height);\n  \n              if (width <= maxAllowedWidth && height <= maxAllowedHeight) {\n                  if (width >= calcWidth && height >= calcHeight) {\n                      calcWidth = (int) width;\n                      calcHeight = (int) height;\n                  }\n              }\n          }\n          if ((calcWidth == 0 || calcHeight == 0) && supportedSizes.size() > 0)\n          {\n              Log.i(TAG, \"fallback to the first frame size\");\n              Object size = supportedSizes.get(0);\n              calcWidth = accessor.getWidth(size);\n              calcHeight = accessor.getHeight(size);\n          }\n  \n          return new Size(calcWidth, calcHeight);\n      }\n  ```\n\n  \n\n\n### SmartOpenCV\u7684\u7279\u70b9\n\n- **\u6613\u4f7f\u7528**\uff1a\u5982\u679c\u4f60\u9879\u76ee\u4e2d\u4e4b\u524d\u4f7f\u7528\u7684\u662fOpenCV\u7684\u5b98\u65b9SDK\uff0c\u90a3\u4e48\u5f15\u5165SmartOpenCV\u540e\u53ea\u9700\u5c06xml\u6587\u4ef6\u4e2d\u7684`JavaCameraView`/`JavaCamera2View`\u66ff\u6362\u4e3aSmartOpenCV\u7684`CamerPreview`/`Camera2Preview`\u5373\u53ef\u8fbe\u5230\u4e0e\u4f7f\u7528\u5b98\u65b9SDK\u76f8\u540c\u7684\u6548\u679c\n\n- **\u529f\u80fd\u589e\u5f3a**\uff1a\n  1. \u9884\u89c8\u81ea\u9002\u5e94\uff1a\u81ea\u52a8\u6839\u636e\u524d\u540e\u6444\u50cf\u5934\uff0c\u6a2a\u7ad6\u5c4f\u4ee5\u53ca\u4e0d\u540c\u6444\u50cf\u5934\u53c2\u6570\u6765\u8c03\u6574\u4e0e\u9002\u914d\u9884\u89c8\u65b9\u5411\u4ee5\u53ca\u5927\u5c0f\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5199\u4efb\u4f55\u989d\u5916\u4ee3\u7801\n  2. \u53ef\u6269\u5c55\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u7ed8\u5236\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u573a\u666f\u81ea\u5b9a\u4e49\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\n  3. \u53ef\u6269\u5c55\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\n  4. \u652f\u6301**USB\u6444\u50cf\u5934**\uff1aUSB\u6444\u50cf\u5934\u4f5c\u4e3a\u5916\u8bbe\u63a5\u5165\u8bbe\u5907\uff0c\u548c\u624b\u673a/\u5e73\u677f\u7b49\u79fb\u52a8\u8bbe\u5907\u5185\u7f6e\u6444\u50cf\u5934\u5b58\u5728\u5dee\u5f02\uff0cSDK\u5185\u90e8\u5728\u5904\u7406\u79fb\u52a8\u8bbe\u5907\u6444\u50cf\u5934\u7684\u903b\u8f91\u65f6\u4e5f\u517c\u5bb9\u4e86\u5bf9\u95f8\u673a\u7b49\u7684USB\u6444\u50cf\u5934\u7684\u5904\u7406\n  \n- **\u63d0\u4f9b\u66f4\u53cb\u597d\u7684API\u63a5\u53e3**\uff1a\u5728\u7ee7\u627fOpenCV\u5b98\u65b9\u63a5\u53e3\u7684\u540c\u65f6\uff0cSmartOpenCV\u5c06\u4f17\u591a\u7e41\u6742\u64cd\u4f5c\u7edf\u4e00\u901a\u8fc7CameraConfiguration\u6765\u914d\u7f6e\uff0c\u63d0\u4f9b\u66f4\u53cb\u597d\u7684Fluent API\u63a5\u53e3\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\n\n- **\u4e0d\u76f4\u63a5\u4f9d\u8d56\u5b98\u65b9SDK\uff0c\u65b9\u4fbf\u5347\u7ea7\u5b98\u65b9SDK**\uff1a\u4e0eOpenCV\u5b98\u65b9SDK\u89e3\u8026\uff0c\u53ea\u8981\u5b98\u65b9SDK\u5185\u90e8\u6838\u5fc3\u903b\u8f91\u672a\u505a\u4fee\u6539\uff0c\u90a3\u4e48SmartOpenCV\u53ef\u4ee5\u517c\u5bb9\u6240\u6709\u7248\u672c\u7684\u5b98\u65b9SDK\uff0c\u4f7f\u7528SmartOpenCV\u540e\u5982\u679c\u4ee5\u540e\u6253\u7b97\u5347\u7ea7\u4f9d\u8d56\u7684OpenCV\u4e3a\u66f4\u65b0\u7248\u672c\uff0c\u53ea\u9700\u5c06OpenCV\u7684\u4f9d\u8d56\u66f4\u65b0\u4e3a\u65b0\u7248\u672c\u5373\u53ef\uff0c\u4ee3\u7801\u65e0\u9700\u505a\u4efb\u4f55\u6539\u52a8\n\n### \u6548\u679c\u5bf9\u6bd4\n#### \u4ee5\u4eba\u8138\u8bc6\u522b\u4e3a\u4f8b\n\n|            | \u6a2a\u5c4f                                                         | \u7ad6\u5c4f   |\n| ---------- | ------------------------------------------------------------ | ------ |\n| OpenCV | <div align=center>**\u5373\u4f7f\u5bbd\u4e0e\u9ad8\u90fd\u8bbe\u7f6e\u4e3amatch_parent\u4e5f\u65e0\u6cd5\u5168\u5c4f\uff0c\u5b58\u5728\u9ed1\u8fb9**  <img src=\"./art/screenshort/opencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\"/></div>  |<div align=center> **\u5b58\u5728\u9ed1\u8fb9\uff0c\u4e14\u9ed8\u8ba4\u4e0d\u652f\u6301\u7ad6\u5c4f**  <img src=\"./art/screenshort/opencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\" /></div>  |\n| SmartOpenCV | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\" /></div> | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\"/></div> |\n\n### Demo\u5bf9\u6bd4\u4f53\u9a8c\n[smartopencv-app-debug.apk](demo/smartopencv-app-debug.apk)  \n[opencv-app-debug.apk](demo/opencv-app-debug.apk)  \n\n### Integration\n\nStep1\uff1a\u5728\u9879\u76ee\u6839\u76ee\u5f55\u7684build.gradle\u4e2d\u6dfb\u52a0\u5bf9jitpack\u4ed3\u5e93\u7684\u914d\u7f6e\n\n```\nallprojects {\n    repositories {\n        ...\n        maven { url 'https://jitpack.io' }\n    }\n}\n```\n\nStep2\uff1a\u5728\u9700\u8981\u4f7f\u7528`SmartOpenCV`\u5e93\u7684\u6a21\u5757\u4e2d\u6dfb\u52a0\u4f9d\u8d56\n\n```\ndependencies {\n\timplementation('com.github.HuTianQi:SmartOpenCV:1.0.1') { // \u7248\u672c\u53f7\u5efa\u8bae\u4f7f\u7528\u5df2release\u7684\u6700\u65b0\u7248\u672c\n        exclude module: 'openCVLibrary411' // \u7531\u4e8e\u76ee\u524d\u591a\u6a21\u5757\u4f9d\u8d56\u65f6jitpack\u6253\u5305\u5b58\u5728bug\uff0c\u6392\u9664\u6253\u5305\u65f6\u4f9d\u8d56\u7684\u8be5\u6a21\u5757\n    }\n}\n```\n\n\n### Usage\n\n#### \u57fa\u7840\u7528\u6cd5\n\n\u5728\u9879\u76ee\u4e2d\u9700\u8981\u4f7f\u7528\u9884\u89c8\u7684xml\u4e2d\u7528SmartOpenCV\u7684`CameraPreview`/`Camera2Preview`\u66ff\u6362OpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\uff0c\u5c31\u8fd9\u4e48\u7b80\u5355\uff0c\u5176\u4f59\u7684\u4ec0\u4e48\u90fd\u4e0d\u7528\u505a\n\n```xml\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <!--<org.opencv.android.JavaCameraView-->\n    <!--android:id=\"@+id/fd_activity_surface_view\"-->\n    <!--android:layout_width=\"match_parent\"-->\n    <!--android:layout_height=\"match_parent\" />-->\n\n    <tech.huqi.smartopencv.core.preview.CameraPreview\n        android:id=\"@+id/fd_activity_surface_view\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</LinearLayout>\n```\n\n#### \u9ad8\u7ea7\u7528\u6cd5\n\n\u5982\u679c\u6253\u7b97\u901a\u8fc7SmartOpenCV\u63d0\u4f9b\u7684\u63a5\u53e3\u6765\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\uff0c\u90a3\u4e48\u8c03\u7528`SmartOpenCV.getInstance().init()`\u4f20\u5165\u524d\u9762\u83b7\u53d6\u7684\u9884\u89c8\u63a7\u4ef6\u5bf9\u8c61\u5373\u53ef\uff0c\u7528\u6cd5\u5982\u4e0b\uff1a\n\n```java\nSmartOpenCV.getInstance().init(mOpenCvCameraView, new CameraConfiguration.Builder()\n    .debug(true)\n    .cameraIndex(0)      // \u8bbe\u7f6e\u6444\u50cf\u5934\u7d22\u5f15,\u4e3b\u8981\u7528\u4e8e\u591a\u6444\u50cf\u5934\u8bbe\u5907\uff0c\u4f18\u5148\u7ea7\u4f4e\u4e8efrontCamera\n    .keepScreenOn(false) // \u662f\u5426\u4fdd\u6301\u5c4f\u5e55\u5e38\u4eae\n    .frontCamera(true)   // \u662f\u5426\u4f7f\u7528\u524d\u7f6e\u6444\u50cf\u5934\n    .openCvDefaultDrawStrategy(false)      // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u56fe\u50cf\u7ed8\u5236\u7b56\u7565\n    .openCvDefaultPreviewCalculator(false) // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n    .landscape(false)     // \u662f\u5426\u6a2a\u5c4f\u663e\u793a\n    .enableFpsMeter(true) // \u5f00\u542f\u9884\u89c8\u5e27\u7387\u7684\u663e\u793a\n    .usbCamera(false)     // \u662f\u5426\u4f7f\u7528USB\u6444\u50cf\u5934\uff0c\u5f53\u8bbe\u5907\u63a5\u5165\u7684\u662fUSB\u6444\u50cf\u5934\u65f6\u5c06\u5176\u8bbe\u7f6e\u4e3atrue\n    .maxFrameSize(400, 300)     // \u8bbe\u7f6e\u9884\u89c8\u5e27\u7684\u6700\u5927\u5927\u5c0f\n    .cvCameraViewListener(this) // \u8bbe\u7f6eOpenCV\u56de\u8c03\u76d1\u542c\u5668\n    .previewSizeCalculator(new IPreviewSizeCalculator() { // \u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n        @Override\n        public Size calculateCameraFrameSize(List<Size> supportedSizes, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u6539\u5199\u89c8\u5e27\u5927\u5c0f\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n            return new Size(1080,1920); \n        }\n    })\n    .drawStrategy(new IDrawStrategy() { // \u81ea\u5b9a\u4e49\u7ed8\u5236\u7b56\u7565\n        @Override\n        public void drawBitmap(Canvas canvas, Bitmap frameBitmap, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n        }\n    })\n    .build());\n```\n\n\n\n### LICENSE\n[LICENSE](LICENSE)  \n\n### \u516c\u4f17\u53f7\n![\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\u4ea4\u6d41\u53cd\u9988](art/wx_gzh.jpg)  \n"
 },
 {
  "repo": "Roujack/mathAI",
  "language": "Python",
  "readme_contents": "# mathAI\n\n\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c\u3002\n**\u8bf7\u67e5\u770b\u7cfb\u7edf\u6587\u6863\u8bf4\u660e\u6765\u8fd0\u884c\u7a0b\u5e8f\u3002\u6ce8\u610f\uff0c\u8fd9\u662f\u4e00\u4e2a\u534a\u5f00\u6e90\u7684\u9879\u76ee\uff0c\u76ee\u524d\u4e0a\u4f20\u7684\u7248\u672c\u53ea\u80fd\u5904\u7406\u7b80\u5355\u7684\u4e00\u7ef4\u52a0\u51cf\u4e58\u9664\u7b97\u672f\u8868\u8fbe\u5f0f\uff08\u5982\u679c\u60f3\u8981\u8bc6\u522b\u66f4\u52a0\u590d\u6742\u7684\u8868\u8fbe\u5f0f\uff0c\u53ef\u4ee5\u53c2\u8003\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u7684\u8bba\u6587\uff09\u3002\u53ef\u4ee5\u53c2\u8003\u7684\u4ee3\u7801\u662f\u524d\u9762\u5b57\u7b26\u8bc6\u522b\u90e8\u5206\u4ee5\u53ca\u6574\u4e2a\u7b97\u6cd5\u5904\u7406\u6846\u67b6\u3002**\n![image](https://github.com/Roujack/mathAI/blob/master/test.png)\n\n\u6574\u4e2a\u7a0b\u5e8f\u4f7f\u7528python\u5b9e\u73b0\uff0c\u5177\u4f53\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u4e86\u56fe\u50cf\u9884\u5904\u7406\u3001\u5b57\u7b26\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u679c\u8f93\u51fa\u3002\n\n\u672c\u7a0b\u5e8f\u4f7f\u7528opencv\u5bf9\u8f93\u5165\u7684\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5e76\u5c06\u5b57\u7b26\u88c1\u526a\u51fa\u6765\u518d\u5f52\u4e00\u5316\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u77e9\u9635\u3002\u6211\u5728TensorFlow\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2alenet5\n\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u8bc6\u522b\u6570\u5b66\u5b57\u7b26\uff0c\u8bad\u7ec3\u4f7f\u7528CHROME\u6570\u636e\u96c6\u3002\u5bf9\u4e8e\u6570\u5b66\u516c\u5f0f\u7684\u8bc6\u522b\uff0c\u4e3b\u8981\u662f\u5c06\u8bc6\u522b\u51fa\u7684\u72ec\u7acb\u7684\u5b57\u7b26\u7ec4\u7ec7\u6210\u8ba1\u7b97\u673a\u80fd\u591f\n\u7406\u89e3\u7684\u6570\u5b66\u516c\u5f0f\uff08\u8fd9\u91cc\u7684\u6570\u5b66\u516c\u5f0f\u5c31\u662f\u7eaf\u5b57\u7b26\u7684\u53ef\u6c42\u89e3\u7684\u6570\u5b66\u8ba1\u7b97\u9898\uff09\u3002\u5927\u6982\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u7f16\u8bd1\u539f\u7406\u7684\u7b97\u7b26\u4f18\u5148\u6cd5\u548c\u9012\u5f52\u4e0b\u964d\u6cd5\u8fdb\u884c\u5b9e\u73b0\u3002\n\u7136\u540e\u6839\u636e\u5c5e\u6027\u6587\u6cd5\u7684\u503c\u4f20\u9012\u601d\u60f3\uff0c\u5c06\u6570\u5b66\u516c\u5f0f\u7684\u503c\u8ba1\u7b97\u51fa\u6765\u3002\u6700\u540e\u4f7f\u7528python\u7684matlibplot\u5e93\u628a\u8ba1\u7b97\u8fc7\u7a0b\u548c\u7b54\u6848\u6253\u5370\u51fa\u6765\u3002\n\n\u4f18\u70b9\uff1a\u8fd9\u662f\u4e00\u6574\u5957\u62cd\u7167\u505a\u9898\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u540c\u65f6\u80fd\u591f\u5904\u7406\u591a\u79cd\u591a\u6837\u7684\u8ba1\u7b97\u9898\uff0c\u76ee\u524d\u5e02\u9762\u4e0a\u8fd8\u6ca1\u6709\u770b\u5230\u5b9e\u73b0\u3002OCR\u6280\u672f\u5982\u6b64\u6210\u719f\u7684\u4eca\u5929\u5b57\u7b26\u8bc6\u522b\n\u5df2\u7ecf\u4e0d\u7b97\u6709\u6311\u6218\u7684\u4e1c\u897f\u4e86\u3002\n\u7f3a\u70b9\uff1a\u5b57\u7b26\u7a7a\u95f4\u5173\u7cfb\u5224\u65ad\u53ea\u7528\u4e86\u4eba\u7c7b\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u56fe\u50cf\u9884\u5904\u7406\u4e0d\u591f\u9c81\u68d2\uff0c\u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784\u8bc6\u522b\u7b97\u6cd5\u4e0d\u591f\u5b8c\u7f8e\uff08\u53ef\u4ee5\u8003\u8651\u4f7f\u7528\u4e8c\u7ef4\u6587\u6cd5\u6765\u505a\uff09\u3002\n\u7cfb\u7edf\u8fd8\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002\n"
 },
 {
  "repo": "amusi/AI-Job-Notes",
  "language": null,
  "readme_contents": "# AI-Job-Notes\nAI\u7b97\u6cd5\u5c97\u6c42\u804c\u653b\u7565\uff1a\u6db5\u76d6\u6821\u62db\u65f6\u95f4\u8868\u3001\u51c6\u5907\u653b\u7565\u3001\u5237\u9898\u6307\u5357\u3001\u5185\u63a8\u3001AI\u516c\u53f8\u6e05\u5355\u548c\u7b54\u7591\u7b49\u8d44\u6599\n\nAI\u7b97\u6cd5\u5c97\u65b9\u5411\uff1a\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u50cf\u5904\u7406\u548cSLAM\u7b49\n\n>  \u6ce8\uff1a\u5982\u679c\u4f60\u770b\u5230\u8fd9\u7bc7\u6587\u7ae0\uff0c\u4e14\u6709\u4e00\u4e9b\u7591\u95ee\u6216\u8005\u60f3\u63d0\u4f9b\u4e00\u4e9b\u8d44\u6e90\uff0c\u6b22\u8fce\u63d0\u4ea4issues\uff01\n\n# \u76ee\u5f55\n\n<!-- MarkdownTOC depth=4 -->\n\n- [1 \u6821\u62db\u65f6\u95f4\u8868](#Scheduled)\n- [2 \u51c6\u5907\u653b\u7565](#Strategy)\n- [3 AI \u9762\u7ecf\u548c\u5237\u9898\u6307\u5357](#Coding)\n- [4 \u5185\u63a8](#Recommend)\n- [5 \u7b80\u5386\u6a21\u677f](#Resume)\n- [6 AI \u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09](#Company)\n- [7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5](#Salary)\n- [8 \u7b54\u7591\uff08\u542b130\u4e2a\u95ee\u7b54\uff09](#Q&A)\n\n<a name=\"Scheduled\"></a>\n\n## 1 \u6821\u62db\u65f6\u95f4\u8868\n\n![](imgs/\u6821\u62db\u65f6\u95f4\u8868.png)\n\n\u4ee5\u4eca\u5e74(2020)\u4e3a\u4f8b\uff0c\u9ed8\u8ba4\u4e3a2021\u5c4a\u5b66\u751f\uff082020\u5c4a\u5b66\u751f\u79f0\u4e3a\u4e0a\u5c4a\uff09\n\n| \u65f6\u95f4           | \u4efb\u52a1                                |\n| -------------- | ----------------------------------- |\n| 2020\u5e743\u6708~6\u6708  | \u627e\u6691\u671f\u5b9e\u4e60/\u4e0a\u5c4a\u6625\u62db\uff08\u8865\u62db\uff09         |\n| 2020\u5e746\u6708~8\u6708  | \u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09              |\n| 2020\u5e749\u6708~11\u6708 | \u79cb\u62db\u6b63\u5f0f\u6279\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09 |\n\n### 1.1 \u6691\u671f\u5b9e\u4e60\n\n2020\u5e743\u6708~6\u6708\uff1a\u6691\u671f\u5b9e\u4e60\u3002\n\n\u5b9e\u4e60\u4e00\u822c\u5206\u6210\u4e24\u79cd\uff1a\n\n- \u65e5\u5e38\u5b9e\u4e60\n- \u6691\u671f\u5b9e\u4e60\n\n![](imgs/\u5b9e\u4e60.png)\n\n**\u65e5\u5e38\u5b9e\u4e60**\uff1a\u65e5\u5e38\u5b9e\u4e60\u662f\u4efb\u4f55\u65f6\u5019\u90fd\u53ef\u4ee5\u627e\u7684\uff0c\u901a\u5e38\u662f\u6839\u636e\u5177\u4f53\u90e8\u95e8\u7684\u9700\u6c42\uff0c\u7531\u516c\u53f8HR\u3001\u90e8\u95e8\u4e3b\u7ba1\u6216\u8005\u90e8\u95e8\u5458\u5de5\u53d1\u5e03\u62db\u8058\u6d88\u606f\uff0c\u76f8\u5bf9\u8f83\u4e3a\u96f6\u6563\u4e5f\u6bd4\u8f83\u7075\u6d3b\u3002\n\n**\u6691\u671f\u5b9e\u4e60**\uff1a\u5f88\u591a\u516c\u53f8\uff0c\u7279\u522b\u662f\u5927\u516c\u53f8\uff08\u5982BAT\u7b49\u5927\u5382\uff09\uff0c\u90fd\u4f1a\u7ec4\u7ec7\u4e13\u9879\u7684**\u6691\u671f\u5b9e\u4e60\u751f**\u62db\u8058\u6d3b\u52a8\u3002\u4e00\u65b9\u9762\u662f\u9488\u5bf9\u5728\u6821\u5b66\u751f\u7684\u60c5\u51b5\uff08\u5f88\u591a\u5b66\u751f\u53ea\u6709\u6691\u671f\u624d\u6709\u5047\u671f\uff0c\u6216\u8005\u5bfc\u5e08\u6691\u5047\u624d\u653e\u4eba\uff09\uff0c\u53e6\u4e00\u65b9\u9762\u5c31\u662f\u4e3a\u4e86\u79cb\u5b63\u6821\u62db\uff08\u5927\u89c4\u6a21\u62db\u8058\uff09\u5438\u5f15\u4eba\u624d\u3002\u6691\u671f\u5b9e\u4e60\u5177\u6709\u5f88\u5927\u7684\u610f\u4e49\uff0c\u5bf9\u5b66\u751f\u6765\u8bf4\uff0c\u6700\u76f4\u63a5\u7684\u597d\u5904\u5c31\u662f\u8f6c\u6b63\u673a\u4f1a\u3002\u6691\u671f\u5b9e\u4e60\uff0c\u4e00\u822c6\u6708\u5e95\u5de6\u53f3\u5b9e\u4e60\u5165\u804c\uff08\u4e5f\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u7684\u65f6\u95f4\uff0c\u63d0\u524d\u5165\u804c\uff09\uff0c\u4e00\u822c8\u6708\u5e95\u62169\u6708\u4efd\u4f1a\u6709\u4e13\u9879\u6691\u671f\u5b9e\u4e60\u7b54\u8fa9\uff0c\u6839\u636e\u7efc\u5408\u8868\u73b0\uff0c\u7b54\u8fa9\u901a\u8fc7\u540e\u5c31\u53ef\u4ee5\u57fa\u672c\u7ed3\u675f\u79cb\u62db\u4e86\u3002\n\n\u6ce8\uff1a\u8fd9\u91cc\u5efa\u8bae\u5728\u8fdb\u5165\u516c\u53f8\u53c2\u52a0\u6691\u671f\u5b9e\u4e60\u7684\u671f\u95f4\uff0c\u4e5f\u8981\u53c2\u52a0\u79cb\u62db\u63d0\u524d\u6279\u548c\u79cb\u62db\u6b63\u5f0f\u6279\uff0c\u5e76\u591a\u6295\u9012\u4e00\u4e9b\u516c\u53f8\uff0c\u5373\u4f7f\u5728\u5b9e\u4e60\uff0c\u6240\u8c13\u7684\u5f88\u5fd9\uff0c\u6ca1\u65f6\u95f4\u51c6\u5907\u79cb\u62db\u4e86\uff0c\u90a3\u4e5f\u8981\u591a\u6295\u3002\u6691\u671f\u5b9e\u4e60\u7684\u53e6\u4e00\u4e2a\u597d\u5904\u662f\u589e\u52a0\u53ef\u8d35\u7684\u5b9e\u4e60\u7ecf\u9a8c\uff0c\u7b80\u5386\u4f1a\u597d\u770b\u5f88\u591a\u3002\n\n### 1.2 \u79cb\u62db\u63d0\u524d\u6279\n\n**2020\u5e746\u6708~8\u6708\uff1a\u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09**\n\n\u636e\u6211\u4e86\u89e3\u4e0a\u5c4a\u6253\u54cd\u79cb\u62db\u7b2c\u4e00\u67aa\u7684\u662f\u5927\u7586(DJI)\u79d1\u6280\uff0c\u5176\u57286\u6708\u5e95\u5c31\u5df2\u7ecf\u7ed3\u675f\u7b80\u5386\u6295\u9012\u4e86\uff0c\u7136\u540eBAT\u7b49\u5927\u5382\u90fd\u662f7\u6708\u4efd\u5f00\u59cb\u3002\u8fd9\u65f6\u5019\u7684\u6821\u62db\uff0c\u7edd\u5927\u90e8\u5206\u90fd\u662f\u5185\u63a8/\u63d0\u524d\u6279\uff0c\u800c\u4e0d\u662f\u6b63\u5f0f\u6279\uff0c\u5927\u5bb6\u4e00\u5b9a\u8981\u73cd\u60dc\u8fd9\u4e2a\u65f6\u95f4\u70b9\uff1a6\u6708~8\u6708\u3002\u867d\u7136\u6211\u8c03\u4f83\u7740\u8bf4\u795e\u4ed9\u6253\u67b6\uff0c\u4f46\u8fd8\u662f\u8981\u6ce8\u610f\u8fd9\u65f6\u5019\u6027\u4ef7\u6bd4\u7279\u522b\u9ad8\u3002\u4e00\u65b9\u9762\u662f\u85aa\u8d44\u666e\u904d\u9ad8\uff0c\u901a\u5e38\u4e00\u4e9bSP/SSP Offer\u90fd\u662f\u8fd9\u4e2a\u8282\u70b9\u53d1\u51fa\u6765\u7684\uff0c\u53e6\u4e00\u65b9\u9762\u662f\u6295\u9012\u7684\u4eba\u6570\u8fd8\u4e0d\u662f\u5f88\u591a\uff0c\u56e0\u4e3a\u6709\u4e9b\u4eba\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e2a\u63d0\u524d\u6279\u7684\u91cd\u8981\u6027\uff0c\u8001\u60f3\u7740\u591a\u51c6\u5907\u4e00\u70b9\uff0c\u5230\u79cb\u62db\u6b63\u5f0f\u6279\u518d\u5927\u5e72\u4e00\u573a\u3002\n\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a\u53c2\u4e0e\u79cb\u62db\u63d0\u524d\u6279\u7684\u5927\u4f6c\u7279\u522b\u591a\uff0c\u540c\u65f6\u5c97\u4f4dhc\u5e76\u4e0d\u591a\uff08\u56e0\u4e3a\u4f01\u4e1a\u8981\u8003\u8651\u6b63\u5f0f\u6279\u7684\u60c5\u51b5\uff0c\u4f1a\u63a7\u5236\u62db\u8058\u4eba\u6570\uff09\uff0c\u6240\u4ee5\u6211\u628a\u79cb\u62db\u63d0\u524d\u6279\u6bd4\u4f5c\uff1a\u795e\u4ed9\u6253\u67b6\u3002\u53e6\u5916\uff0c\u79cb\u62db\u63d0\u524d\u6279\u5927\u591a\u4ee5\u5185\u63a8\u4e3a\u4e3b\uff0c\u540e\u9762\u7ae0\u8282\u4e2d\u6211\u4f1a\u8bf4\u5230\u5982\u4f55\u83b7\u53d6\u62db\u8058\u4fe1\u606f\u4ee5\u53ca\u5982\u4f55\u5185\u63a8\u3002\n\n\u6ce8\uff1a\u63d0\u53d6\u6279\u6302\u4e86\uff0c\u6b63\u5f0f\u6279\u53ef\u4ee5\u518d\u7ee7\u7eed\u6295\uff08\u5177\u4f53\u770b\u4e0d\u540c\u516c\u53f8\u7684\u62db\u8058\u4ecb\u7ecd\uff09\u3002\n\n### 1.3 \u79cb\u62db\u6b63\u5f0f\u6279\n\n**2020\u5e749\u6708~11\u6708\uff1a\u79cb\u62db\u6b63\u5f0f\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09**\n\n\u6709\u53e5\u8bdd\u53eb\u505a\u91d1\u4e5d\u94f6\u5341\uff0c\u4e5f\u5c31\u662f9\u6708\u4efd\u7684 Offer \u6bd410\u6708\u4efd\u7684 Offer \u66f4\u53ef\u8d35\uff0c\u8fd9\u8bdd\u5176\u5b9e\u5f88\u6709\u9053\u7406\uff0c\u6240\u4ee5\u5927\u5bb6\u53ef\u4ee5\u8111\u8865\u52307\u30018\u6708\u4efd\u7684 Offer \u5c5e\u4e8e\u4ec0\u4e48 level \u4e86\u3002\u8fd9\u65f6\u5019\u4e5f\u5f88\u8003\u9a8c\u5927\u5bb6\u7684\u5fc3\u6001\uff0c\u6bd4\u59829\u6708\u4efd\u621610\u6708\u4efd\u4e86\uff0c\u5982\u679c\u4f60\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u518d\u770b\u770b\u8eab\u8fb9\u5df2\u7ecf\u62ff\u5230Offer\u7684\u540c\u5b66\uff0c\u4e00\u5b9a\u53d8\u6210\u67e0\u6aac\u7cbe\u3002\n\n\u6240\u4ee5 Amusi \u8fd9\u91cc\u5f3a\u70c8\u5efa\u8bae\u4e00\u5b9a\u8981\u628a\u63e1\u4f4f 1.2\u8282\u4e2d\u7684**\u79cb\u62db\u63d0\u524d\u6279 **\u3002\u5f53\u7136\u4e86\uff0c\u5982\u679c9\u6708\u4efd\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u5fc3\u6001\u5343\u4e07\u522b\u5d29\uff0c\u7ee7\u7eed\u6295\u7ee7\u7eed\u5e72\uff0c\u8bb0\u4f4f\u4e00\u53e5\u8bdd\uff1a\u591a\u6295\u51c6\u6ca1\u9519\uff01\u5176\u5b9e\u5927\u90e8\u5206\u540c\u5b66\u90fd\u662f9\u6708\u300110\u6708\u624d\u9646\u7eed\u6536\u5230Offer\u7684\uff0c\u6240\u4ee5\u4f60\u591a\u6295\u7ee7\u7eed\u52aa\u529b\uff0c\u6536\u83b7\u80af\u5b9a\u4f1a\u6709\u7684\u3002\n\n<a name=\"Strategy\"></a>\n\n## 2 \u51c6\u5907\u653b\u7565\n\n\u51c6\u5907\u653b\u7565\uff0c\u6211\u6ca1\u6709\u5177\u4f53\u7684\u65b9\u6848\uff0c\u56e0\u4e3a\u8fd9\u5c31\u597d\u50cf\u662f\u5b66\u4e60\u8ba1\u5212\u4e00\u6837\uff0c\u6bcf\u4e2a\u4eba\u90fd\u8981\u81ea\u5df1\u7684\u4e60\u60ef\uff0c\u6211\u7684\u4f60\u5e76\u4e0d\u4e00\u5b9a\u9002\u7528\u3002\u6240\u4ee5\u6211\u5c31\u7528\u4e00\u4e2a\u7cbe\u7b80\u7684\u516c\u53f8\u6765\u4ecb\u7ecd\u3002\n~~\u516c\u5f0f1.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60(\u53ef\u9009)+\u7ade\u8d5b(\u53ef\u9009)+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)~~\n\n\u516c\u5f0f2.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60+\u7ade\u8d5b+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)\n\n<a name=\"Coding\"></a>\n\n## 3 AI\u9762\u7ecf\u548c\u5237\u9898\u6307\u5357\n\n### 3.1 \u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\n\n\u8be6\u89c1\uff1a[\u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\uff08\u542b\u6570\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cSLAM\u7b49\u65b9\u5411\uff09](<https://github.com/amusi/Deep-Learning-Interview-Book>)\n\n**Deep Learning Interview Book** \u90e8\u5206\u5185\u5bb9\u5982\u4e0b\uff1a\n\n- \ud83d\ude03 [\u81ea\u6211\u4ecb\u7ecd](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D.md)\n- \ud83d\udd22 [\u6570\u5b66](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E5%AD%A6.md)\n- \ud83c\udf93 [\u673a\u5668\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd5 [\u6df1\u5ea6\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd7 [\u5f3a\u5316\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udc40 [\u8ba1\u7b97\u673a\u89c6\u89c9](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.md)\n- \ud83d\udcf7 [\u4f20\u7edf\u56fe\u50cf\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.md)\n- \ud83c\udc04\ufe0f [\u81ea\u7136\u8bed\u8a00\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.md)\n- \ud83c\udfc4 [SLAM](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/SLAM.md)\n- \ud83d\udc65 [\u63a8\u8350\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.md)\n- \ud83d\udcca [\u6570\u636e\u7ed3\u6784\u4e0e\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.md)\n- \ud83d\udc0d [\u7f16\u7a0b\u8bed\u8a00\uff1aC/C++/Python](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.md)\n- \ud83c\udf86 [\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6.md)\n- \u270f\ufe0f [\u9762\u8bd5\u7ecf\u9a8c](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C.md)\n- \ud83d\udca1 [\u9762\u8bd5\u6280\u5de7](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E6%8A%80%E5%B7%A7.md)\n- \ud83d\udce3 [\u5176\u5b83\uff08\u8ba1\u7b97\u673a\u7f51\u7edc/Linux\u7b49\uff09](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%85%B6%E5%AE%83.md)\n\n### 3.2 \u5237\u9898\u6307\u5357\n\n\u5237\u9898\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b66\u4e60\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\uff0c\u953b\u70bc\u7f16\u7a0b\u80fd\u529b\u548c\u719f\u6089\u5237\u9898\u6280\u5de7\n\n\u5237\u9898\u5efa\u8bae\uff1a\u5148\u5237[\u300a\u5251\u6307Offer\u300b](https://www.nowcoder.com/ta/coding-interviews)\uff0866\u9898\uff09\uff0c\u518d\u5237 [LeetCode](https://leetcode.com/)\uff08\u76ee\u524dLeetCode\u5df2\u7ecf\u6709900+\u9898\uff0c\u53ef\u4ee5\u6839\u636e\u7c7b\u522b\u6765\u5237\uff0c\u4f46\u5f3a\u70c8\u5efa\u8bae\u5148\u5237\u5b8c [LeetCode \u9762\u8bd5\u9ad8\u9891\u9898](https://leetcode.com/problemset/top-interview-questions/)\uff09\n\n> \u6ce8\uff1a\u6839\u636e 2020 \u5e74\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0cLeetCode \u5efa\u8bae\u81f3\u5c11\u5237200-300\u9898\uff0c\u6240\u4ee52020\u5e74\uff082021\u5c4a\uff09\u627e\u5de5\u4f5c\u7684\u540c\u5b66\u4e00\u5b9a\u8981\u52aa\u529b\u5237\u8d77\u6765\u4e86\uff01\n\n#### 3.2.1 \u5237\u9898\u7f16\u7a0b\u8bed\u8a00\n\n- C/C++\n- Python\n- JAVA\uff08\u4e0d\u63a8\u8350\uff09\n\n> \u6ce8\uff1a\u5982\u679c\u65f6\u95f4\u5145\u88d5\uff0c\u800c\u4e14\u6709 C++ \u57fa\u7840\uff0c\u90a3\u4e48\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 C++\u548c Python \u540c\u65f6\u5237\u9898\u3002\n>\n> \u6839\u636e 2019 \u5e74(2020\u5c4a)\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0c\u4f1a C++ \u7684\u540c\u5b66\u5177\u6709\u6709\u4e00\u5b9a\u4f18\u52bf\u3002\n\n#### 3.2.2 \u4e66\u7c4d\u63a8\u8350\n\n| \u4e66\u7c4d                                                         | \u8c46\u74e3\u8bc4\u5206 | \u63a8\u8350\u6307\u6570 |\n| ------------------------------------------------------------ | -------- | -------- |\n| [\u300a\u5251\u6307Offer\u300b](https://book.douban.com/subject/25910559/)   | 9.1      | \u2606\u2606\u2606\u2606\u2606    |\n| [\u300a\u6570\u636e\u7ed3\u6784(C++\u8bed\u8a00\u7248)\u300b](https://book.douban.com/subject/25859528/) | 9.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u7b97\u6cd5\u56fe\u89e3\u300b](https://book.douban.com/subject/26979890/)    | 8.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u5927\u8bdd\u6570\u636e\u7ed3\u6784\u300b](https://book.douban.com/subject/6424904/) | 7.9      | \u2606\u2606\u2606      |\n| [\u300a\u7b97\u6cd5\u300b(\u7b2c\u56db\u7248)](https://book.douban.com/subject/19952400/) | 9.4      | \u2606\u2606\u2606      |\n\n>  \u6ce8\uff1a\u5176\u5b9e\u8fd8\u6709\u5f88\u591a\u65b9\u5411\u6ca1\u6709\u6d89\u53ca\uff0c\u5982linux\u3001\u6570\u636e\u5e93\uff0c\u4f46\u6682\u65f6\u5148\u63a8\u8350\u8fd9\u4e9b\uff0c\u540e\u9762\u518d\u8865\u5145\n\n#### 3.2.3 \u5728\u7ebf\u5237\u9898\u7f51\u7ad9\n\n- [LeetCode(\u82f1\u6587)](https://leetcode.com/)\n- [LeetCode(\u4e2d\u6587)](https://leetcode-cn.com/)\n- [\u725b\u5ba2\u7f51](https://www.nowcoder.com/)\uff1a\u63a8\u8350\u5251\u6307Offer\u548c\u5404\u5927\u516c\u53f8\u5f80\u5e74\u9898\u5e93\uff0c\u725b\u5ba2\u7f51\u7684\u4f18\u52bf\u5728\u4e8e\u5f88\u591a\u516c\u53f8\u90fd\u4f1a\u4f7f\u7528\u5176\u4f5c\u4e3a\u5728\u7ebf\u5237\u9898\u5e73\u53f0\uff0c\u6240\u4ee5\u5728\u8fd9\u4e0a\u9762\u5237\u9898\uff0c\u6709\u5229\u4e8e\u61c2\u5f97\u8f93\u5165\u8f93\u51fa\u7b49\"\u5957\u8def\"\n\n#### 3.2.4 \u5237\u9898\u65b9\u6cd5\n\n- \u300a\u5251\u6307Offer\u300b\u5168\u5237\u5b8c\n- LeetCode\u9009\u62e9\u6027\u5237\uff1a\u53ef\u4ee5\u7c7b\u522b\u6765\u5237\u9898\uff0c\u5982\u6570\u7ec4\u7c7b\u3001\u94fe\u8868\u7c7b\uff0c\u6216\u8005\u9762\u8bd5\u9ad8\u9891\u7c7b\n\n#### 3.2.5 \u5237\u9898\u65f6\u95f4\n\n\u73b0\u5728\u8d77~2020-10-15\n\n#### 3.2.6 \u5237\u9898\u91cd\u8981\u6027\n\n\u6b63\u5e38\u6821\u62db\u6d41\u7a0b\u90fd\u8981\u8fdb\u884c\u5728\u7ebf\u7b14\u8bd5\uff0c\u9762\u8bd5\u4e2d\u4e5f\u53ef\u80fd\u4f1a\u624b\u6495\u4ee3\u7801\uff0c\u6240\u4ee5\u5237\u9898\u5341\u5206\u5f71\u54cd\u9762\u8bd5\u7ed3\u679c\u3002\n\n<a name=\"Recommend\"></a>\n\n## 4 \u5185\u63a8\n\n\u56fd\u5185\u516c\u53f8\u4eba\u5de5\u667a\u80fd\u65b9\u5411\u5c97\u4f4d\u7684\u5185\u63a8\u673a\u4f1a\uff0c\u542b\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u65b9\u5411\u3002\n\n[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4e3b\u8981\u4ee5\u5168\u804c\u3001\u5b9e\u4e60\u548c\u6821\u62db\u4e3a\u4e3b\uff0c\u5e76\u4e14\u5168\u90fd\u662f\u5185\u63a8\u65b9\u5f0f\uff01\n\n- [\u5b9e\u4e60\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%AE%9E%E4%B9%A0/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u3001\u4eac\u4e1c\u548c\u5546\u6c64\u7b49\u516c\u53f8\n\n- [\u5168\u804c\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%85%A8%E8%81%8C/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u7b49\u516c\u53f8\n\n\u6ce8\uff1a2020\u5e746\u6708\u5f00\u59cb\uff0c[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4f1a\u63a8\u51fa\u5927\u91cf\u6821\u62db\u5185\u63a8\u8d44\u6e90\uff0c\u6b22\u8fcestar/fork/watching\u3002\n\n### 4.1 \u5185\u63a8\u7684\u91cd\u8981\u6027\n\n\u5185\u63a8\uff0c\u771f\u7684\u592a\u91cd\u8981\u4e86\u3002\u5176\u5b9e\u73b0\u5728\u627e\u5b9e\u4e60\u4e5f\u4e00\u6837\uff0c\u5185\u63a8\u7684\u91cd\u8981\u6027\u5c31\u63d0\u9192\u51fa\u6765\u4e86\uff0c\u6bd4\u5982\u6211\u8fd9\u8fb9\u7684\u8d44\u6e90\u5c31\u53ef\u4ee5\u5185\u63a8\u5230BAT\u3001\u5546\u6c64\u3001\u65f7\u89c6\u7b49\u516c\u53f8\uff0c\u4e00\u822c\u5e38\u89c4\u64cd\u4f5c\u662f\u7f51\u4e0a\u6295\u9012\u7b80\u5386\uff0c\u800c\u5feb\u901f\u76f4\u63a5\u7684\u5c31\u662f\u5c06\u7b80\u5386\u9001\u5230leader/\u4e3b\u7ba1\u90a3\u91cc\u3002\u800c\u4e14\u5185\u63a8\u662f\u5efa\u7acb\u5728\u4e00\u79cd\u4e92\u4fe1\u7684\u57fa\u7840\u4e0a(\u867d\u7136\u4e0d\u5927)\uff0c\u8be5\u8d70\u7684\u6d41\u7a0b\u8fd8\u662f\u8981\u8d70\uff0c\u4f46\u65e0\u5f62\u4e2d\u589e\u5927\u4e86\u9762\u8bd5\u901a\u8fc7\u6982\u7387\u3002\u4f60\u8981\u77e5\u9053\uff0c\u5f88\u591a\u4eba\u7684\u7b80\u5386\u5728\u5b98\u7f51\u6216\u8005\u5176\u4ed6\u7b2c\u4e09\u65b9\u62db\u8058\u7f51\u7ad9\u4e0a\u5c31\u76f4\u63a5\u5361\u6b7b\u4e86\u3002\n\n### 4.2 \u5982\u4f55\u5185\u63a8\uff1f\n\n\u5185\u63a8\u7684\u65b9\u5f0f\u5f88\u591a\uff0c\u6bd4\u5982\uff1a\n\n1. \u5f3a\u5173\u8054\uff1a\u76f4\u63a5\u627e\u5df2\u7ecf\u6bd5\u4e1a\u7684\u5e08\u5144\u5e08\u59d0\u6216\u670b\u53cb\u5185\u63a8\uff08\u7f3a\u70b9\u662f\u8eab\u8fb9\u670b\u53cb\u53bb\u7684\u4f01\u4e1a\u6709\u9650\uff0c\u5f88\u591a\u4eba\u662f\u7b2c\u4e00\u6279\u4ece\u4e8b\u7b97\u6cd5\u5c97\u7684\uff0c\u53ef\u80fd\u90fd\u6ca1\u6709\u5e08\u5144\u5e08\u59d0\u641e\u8fd9\u4e2a\uff09\n2. \u5e38\u89c4\u64cd\u4f5c\uff1a\u4e0a\u725b\u5ba2\u7f51\u8bba\u575b\u770b\u4f01\u4e1a\u4eba\u5458\u53d1\u5185\u63a8\u5e16\u5b50\u3001\u5173\u6ce8\u4e00\u4e9b\u62db\u8058\u516c\u4f17\u53f7\uff08\u8fd9\u91cc\u6211\u5c31\u4e0d\u63a8\u8350\uff0c\u56e0\u4e3a\u5f88\u591a\u516c\u4f17\u53f7\u90fd\u5f88\u6709\u5957\u8def\uff0c\u5185\u63a8\u4e00\u4e2a\u4f01\u4e1a\uff0c\u8fd8\u8981\u8f6c\u53d1\u6587\u7ae0\u5230\u5176\u5b83\u7fa4\u91cc\uff0c\u7136\u540e\u622a\u56fe\u7ed9\u4ed6\u4eec\uff0c\u53ef\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u4eba\uff0c\u4e3a\u4e86\u5185\u63a8\uff0c\u53ea\u80fd\u8fd9\u4e48\u5e72\uff09\n3. Amusi \u5185\u63a8\u3002\u8fd9\u91cc\u611f\u89c9\u50cf\u4f3c\u6253\u5e7f\u544a\u4e00\u6837\uff0c\u4f46\u786e\u5b9e\u662f\u4e00\u4e2a\u65b9\u5f0f\uff0c\u56e0\u4e3a\u6211\u624b\u91cc\u8d44\u6e90\u633a\u591a\u7684\uff0c\u5f88\u591a\u516c\u53f8\u7684\u4eba\u90fd\u8ba4\u8bc6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5185\u63a8\u3002\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u5173\u6ce8\u4e00\u4e0b\u8fd9\u4e2a\u6c42\u804c\u7fa4[\u300c2020AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4\u300d](https://t.zsxq.com/VFUZR3n) \u6216\u8005 [AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend)\n\n![](imgs/2020\u5e74AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4.png)\n\n<a name=\"Resume\"></a>\n\n## 5 \u7b80\u5386\u6a21\u677f\n\n\u63d0\u4f9b\u4e86\u4e09\u4efd\u7b80\u5386\u6a21\u677f\uff0c\u8be6\u89c1\uff1a[AI \u7b97\u6cd5\u5c97\u7b80\u5386\u6a21\u677f](https://github.com/amusi/AI-Job-Resume)\n\n![](imgs/Resume-Demo.png)\n\n<a name=\"Company\"></a>\n\n## 6 AI\u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09\n\n\u9996\u5148 AI > CV\uff0c\u6240\u4ee5\u63d0\u4f9bCV\u5c97\u7684\u516c\u53f8\u80af\u5b9a\u5c31\u63d0\u4f9b AI\u5c97\u3002\u4f46\u81f3\u4e8e\u8fd9\u4e9b\u516c\u53f8\u662f\u5426\u8fd8\u6709 NLP\u3001\u673a\u5668\u5b66\u4e60\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u63a8\u8350\u7b97\u6cd5\u548c SLAM\u7b49\u5c97\u4f4d\uff0c\u8fd9\u4e2a\u9700\u8981\u5927\u5bb6\u81ea\u884c\u53bb\u5b98\u7f51\u8fdb\u884c\u4e86\u89e3\u3002\n\n\u8350\u8bfb\uff1a[\u56fd\u5185\u63d0\u4f9b\u8ba1\u7b97\u673a\u89c6\u89c9(CV)\u7b97\u6cd5\u5c97\u4f4d\u7684\u516c\u53f8\u540d\u5355(\u542b\u5916\u4f01\u548c\u56fd\u5185\u516c\u53f8)](https://github.com/amusi/CV-Company-List )\n\n**\u5317\u4eac\u63d0\u4f9bCV\u7b97\u6cd5\u5c97\u7684\u516c\u53f8\u540d\u5355**\n\n![](/imgs/Beijing.png)\n\n\u66f4\u591a\u57ce\u5e02\u4fe1\u606f\uff08\u4e0a\u6d77\uff0c\u6df1\u5733\uff0c\u676d\u5dde\uff0c\u5357\u4eac\uff0c\u5e7f\u5dde\u548c\u6210\u90fd\u7b49\uff09\u8be6\u89c1\uff1ahttps://github.com/amusi/CV-Jobs\n\n<a name=\"Salary\"></a>\n\n## 7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5\n\n\u4eca\u5e74\u662f19\u5e74\uff0c\u6240\u4ee5\u8fd9\u91cc\u4ee52020\u5c4a\u4e3a\u4f8b\u3002\u6211\u662f18\u5e74\u627e\u7684\u5de5\u4f5c\uff0c\u4f46\u5e94\u8be5\u662f2019\u5c4a\u7684\uff0c\u518d\u6b21\u5f3a\u8c03\u4e00\u4e0b\u65f6\u95f4\u4e0d\u8981\u641e\u6df7\u4e86\uff0c\u6240\u4ee5\u6211\u8fd9\u91cc\u8bf4\u8bf42019\u5c4aAI\u7b97\u6cd5\u5c97\u7684\u85aa\u8d44\u60c5\u51b5\u3002\n\n\u6211\u53ea\u4ee5**\u7855\u58eb\u53ca\u4e00\u7ebf\u5de6\u53f3\u57ce\u5e02**\u4e3a\u4f8b\uff08\u5317\u4e0a\u5e7f\u6df1\u3001\u5357\u4eac\u3001\u676d\u5dde\u7b49\uff09\uff0c\u56e0\u4e3a\u50cf\u6b66\u6c49\u3001\u6210\u90fd\uff0c\u4f60\u5373\u4f7f\u627e\u7684AI\u7b97\u6cd5\u5c97\uff0c\u4f46\u57ce\u5e02\u4e0d\u4e00\u6837\uff0c\u85aa\u8d44\u8fd8\u662f\u591a\u5c11\u6709\u533a\u522b\uff0c\u660e\u663e\u4e0d\u80fd\u53ea\u770bMoney\uff0c\u4e0d\u8003\u8651\u57ce\u5e02\u5927\u73af\u5883\u3002\n\n- **\u767d\u83dc\u4ef7\uff1a25w~30w**\n\n- **SP\uff1a30w~40w**\n\n- **SSP\uff1a40w+**\n\n\u8bf4\u5e74\u85aa\u6709\u70b9\u7b3c\u7edf\uff0c\u6211\u518d\u8bf4\u7ec6\u4e00\u70b9\uff0c\u5927\u5bb6\u4e5f\u53ef\u4ee5\u63d0\u53d6\u719f\u6089\u4e00\u4e0b\u3002\n\n\u4e00\u822c\u4f01\u4e1a\u85aa\u8d44\u6784\u6210\u662f\uff1a\n\n- \u5e74\u85aa = \u6708\u85aa*12 + \u5e74\u7ec8\u5956\n\n\u5e74\u7ec8\u5956\u4e00\u822c\u662f2~5\u4e2a\u6708\u7684\u85aa\u8d44\uff0c\u5927\u6982\u662f3\u4e2a\u6708\n\n\u6240\u4ee5\uff0c\u5e74\u85aa=\u6708\u85aa*15\n\n\u5982\u679c\u4f60\u6708\u85aa2w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f30w=2*15\uff08\u767d\u83dc\u7684Top\uff0cSP\u7684Down\uff09\n\n\u5982\u679c\u4f60\u6708\u85aa2.7w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f40.5w=2.7*15\uff08SP\u7684Top\uff0cSSP\u7684Down\uff09\n\n\u8fd9\u91ccpo\u4e00\u5f20\u5f88\u5168\u5f88\u5168\u7684\u9ad8\u85aa\u56fe\uff0c\u6765\u81eaOfferShow\n\n\u6ce8\uff1a\u8ddfhr\u8c08\u85aa\u8d44\u7684\u65f6\u5019\uff0c\u5982\u679c\u5979/\u4ed6\u95ee\u4f60\uff1a\u4f60\u7684\u5e0c\u671b\u85aa\u8d44\u662f\u591a\u5c11\uff1f\uff01\u8fd9\u65f6\u5019\u4f60\u4e00\u5b9a\u8981\u5f80\u9ad8\u4e86\u8981\uff0c\u81f3\u5c11\u6bd4\u4f60\u60f3\u8981\u7684\u9ad830%\u3002\u542c\u6211\u7684\uff0c\u6ca1\u6709\u9519\uff0c\u4e0d\u7136...\n\n![](imgs/salary.png)\n\n<a name=\"Q&A\"></a>\n\n## 8 \u7b54\u7591\n\n130\u4e2a\u95ee\u7b54\u8bf7\u6233\u2014> [Q&A](Q&A.md)"
 },
 {
  "repo": "CodecWang/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "# \u9762\u5411\u521d\u5b66\u8005\u7684OpenCV-Python\u6559\u7a0b\n\n- \u6559\u7a0b\u5730\u5740: [http://codec.wang/#/opencv/](http://codec.wang/#/opencv/)\n- \u672c\u4ed3\u5e93\u4e3a\u6559\u7a0b\u4e2d\u6240\u7528\u5230\u7684\u6e90\u7801\u3001\u56fe\u7247\u548c\u97f3\u89c6\u9891\u7d20\u6750\u7b49\n\n![](http://cos.codec.wang/opencv-python-tutorial-amend-new-cover.png)\n\n## \u76ee\u5f55\n\n### \u5165\u95e8\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u7b80\u4ecb\u4e0e\u5b89\u88c5](http://codec.wang/#/opencv/start/01-introduction-and-installation) | \u4e86\u89e3\u548c\u5b89\u88c5OpenCV-Python |\n| [\u756a\u5916\u7bc7: \u4ee3\u7801\u6027\u80fd\u4f18\u5316](http://codec.wang/#/opencv/start/extra-01-code-optimization) | \u5ea6\u91cf\u8fd0\u884c\u65f6\u95f4/\u63d0\u5347\u6548\u7387\u7684\u51e0\u79cd\u65b9\u5f0f |\n| [\u57fa\u672c\u5143\u7d20: \u56fe\u7247](http://codec.wang/#/opencv/start/02-basic-element-image) | \u56fe\u7247\u7684\u8f7d\u5165/\u663e\u793a\u548c\u4fdd\u5b58 |\n| [\u756a\u5916\u7bc7: \u65e0\u635f\u4fdd\u5b58\u548cMatplotlib\u4f7f\u7528](http://codec.wang/#/opencv/start/extra-02-high-quality-save-and-matplotlib) | \u9ad8\u4fdd\u771f\u4fdd\u5b58\u56fe\u7247\u3001Matplotlib\u5e93\u7684\u7b80\u5355\u4f7f\u7528 |\n| [\u6253\u5f00\u6444\u50cf\u5934](http://codec.wang/#/opencv/start/03-open-camera) | \u6253\u5f00\u6444\u50cf\u5934\u6355\u83b7\u56fe\u7247/\u5f55\u5236\u89c6\u9891/\u64ad\u653e\u672c\u5730\u89c6\u9891 |\n| [\u756a\u5916\u7bc7: \u6ed1\u52a8\u6761](http://codec.wang/#/opencv/start/extra-03-trackbar) | \u6ed1\u52a8\u6761\u7684\u4f7f\u7528 |\n| [\u56fe\u50cf\u57fa\u672c\u64cd\u4f5c](http://codec.wang/#/opencv/start/04-basic-operations) | \u8bbf\u95ee\u50cf\u7d20\u70b9/ROI/\u901a\u9053\u5206\u79bb\u5408\u5e76/\u56fe\u7247\u5c5e\u6027 |\n| [\u989c\u8272\u7a7a\u95f4\u8f6c\u6362](http://codec.wang/#/opencv/start/05-changing-colorspaces) | \u989c\u8272\u7a7a\u95f4\u8f6c\u6362/\u8ffd\u8e2a\u7279\u5b9a\u989c\u8272\u7269\u4f53 |\n| [\u9608\u503c\u5206\u5272](http://codec.wang/#/opencv/start/06-image-thresholding) | \u9608\u503c\u5206\u5272/\u4e8c\u503c\u5316 |\n| [\u756a\u5916\u7bc7: Otsu\u9608\u503c\u6cd5](http://codec.wang/#/opencv/start/extra-04-otsu-thresholding) | \u53cc\u5cf0\u56fe\u7247/Otsu\u81ea\u52a8\u9608\u503c\u6cd5 |\n| [\u56fe\u50cf\u51e0\u4f55\u53d8\u6362](http://codec.wang/#/opencv/start/07-image-geometric-transformation) | \u65cb\u8f6c/\u5e73\u79fb/\u7f29\u653e/\u7ffb\u8f6c |\n| [\u756a\u5916\u7bc7: \u4eff\u5c04\u53d8\u6362\u4e0e\u900f\u89c6\u53d8\u6362](http://codec.wang/#/opencv/start/extra-05-warpaffine-warpperspective) | \u57fa\u4e8e2\u00d73\u7684\u4eff\u5c04\u53d8\u6362/\u57fa\u4e8e3\u00d73\u7684\u900f\u89c6\u53d8\u6362 |\n| [\u7ed8\u56fe\u529f\u80fd](http://codec.wang/#/opencv/start/08-drawing-function) | \u753b\u7ebf/\u753b\u5706/\u753b\u77e9\u5f62/\u6dfb\u52a0\u6587\u5b57 |\n| [\u756a\u5916\u7bc7: \u9f20\u6807\u7ed8\u56fe](http://codec.wang/#/opencv/start/extra-06-drawing-with-mouse) | \u7528\u9f20\u6807\u5b9e\u65f6\u7ed8\u56fe |\n| [\u6311\u6218\u7bc7: \u753b\u52a8\u6001\u65f6\u949f](http://codec.wang/#/opencv/start/challenge-01-draw-dynamic-clock) | / |\n| [\u6311\u6218\u7bc7: PyQt5\u7f16\u5199GUI\u754c\u9762](http://codec.wang/#/opencv/start/challenge-02-create-gui-with-pyqt5) | / |\n\n### \u57fa\u7840\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u56fe\u50cf\u6df7\u5408](http://codec.wang/#/opencv/basic/09-image-blending) | \u7b97\u6570\u8fd0\u7b97/\u6df7\u5408/\u6309\u4f4d\u8fd0\u7b97 |\n| [\u756a\u5916\u7bc7: \u4eae\u5ea6\u4e0e\u5bf9\u6bd4\u5ea6](http://codec.wang/#/opencv/basic/extra-07-contrast-and-brightness) | \u8c03\u6574\u56fe\u7247\u7684\u4eae\u5ea6\u548c\u5bf9\u6bd4\u5ea6 |\n| [\u5e73\u6ed1\u56fe\u50cf](http://codec.wang/#/opencv/basic/10-smoothing-images) | \u5377\u79ef/\u6ee4\u6ce2/\u6a21\u7cca/\u964d\u566a |\n| [\u756a\u5916\u7bc7: \u5377\u79ef\u57fa\u7840-\u56fe\u7247\u8fb9\u6846](http://codec.wang/#/opencv/basic/extra-08-padding-and-convolution) | \u4e86\u89e3\u5377\u79ef/\u6ee4\u6ce2\u7684\u57fa\u7840\u77e5\u8bc6/\u7ed9\u56fe\u7247\u6dfb\u52a0\u8fb9\u6846 |\n| [\u8fb9\u7f18\u68c0\u6d4b](http://codec.wang/#/opencv/basic/11-edge-detection) | Canny/Sobel\u7b97\u5b50 |\n| [\u756a\u5916\u7bc7: \u56fe\u50cf\u68af\u5ea6](http://codec.wang/#/opencv/basic/extra-09-image-gradients) | \u4e86\u89e3\u56fe\u50cf\u68af\u5ea6\u548c\u8fb9\u7f18\u68c0\u6d4b\u7684\u76f8\u5173\u6982\u5ff5 |\n| [\u8150\u8680\u4e0e\u81a8\u80c0](http://codec.wang/#/opencv/basic/12-erode-and-dilate) | \u5f62\u6001\u5b66\u64cd\u4f5c/\u8150\u8680/\u81a8\u80c0/\u5f00\u8fd0\u7b97/\u95ed\u8fd0\u7b97 |\n| [\u8f6e\u5ed3](http://codec.wang/#/opencv/basic/13-contours) | \u5bfb\u627e/\u7ed8\u5236\u8f6e\u5ed3 |\n| [\u756a\u5916\u7bc7: \u8f6e\u5ed3\u5c42\u7ea7](http://codec.wang/#/opencv/basic/extra-10-contours-hierarchy) | \u4e86\u89e3\u8f6e\u5ed3\u95f4\u7684\u5c42\u7ea7\u5173\u7cfb |\n| [\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/14-contour-features) | \u9762\u79ef/\u5468\u957f/\u6700\u5c0f\u5916\u63a5\u77e9\\(\u5706\\)/\u5f62\u72b6\u5339\u914d |\n| [\u756a\u5916\u7bc7: \u51f8\u5305\u53ca\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/extra-11-convex-hull) | \u8ba1\u7b97\u51f8\u5305/\u4e86\u89e3\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81 |\n| [\u76f4\u65b9\u56fe](http://codec.wang/#/opencv/basic/15-histograms) | \u8ba1\u7b97\u7ed8\u5236\u76f4\u65b9\u56fe/\u5747\u8861\u5316 |\n| [\u6a21\u677f\u5339\u914d](http://codec.wang/#/opencv/basic/16-template-matching) | \u56fe\u4e2d\u627e\u5c0f\u56fe |\n| [\u970d\u592b\u53d8\u6362](http://codec.wang/#/opencv/basic/17-hough-transform) | \u63d0\u53d6\u76f4\u7ebf/\u5706 |\n| [\u6311\u6218\u4efb\u52a1: \u8f66\u9053\u68c0\u6d4b](http://codec.wang/#/opencv/basic/challenge-03-lane-road-detection) | / |\n\n> \u5982\u679c\u60a8\u89c9\u5f97\u5199\u7684\u4e0d\u9519\u7684\u8bdd\uff0c\u6b22\u8fce\u6253\u8d4f\uff0c\u6211\u4f1a\u52aa\u529b\u5199\u51fa\u66f4\u597d\u7684\u5185\u5bb9\uff01\u270a\ud83e\udd1f\n\n![](http://cos.codec.wang/wechat_alipay_pay_pic.png)\n\n"
 },
 {
  "repo": "shimat/opencvsharp",
  "language": "C#",
  "readme_contents": "# OpenCvSharp [![CircleCI Status](https://circleci.com/gh/shimat/opencvsharp/tree/master.svg?style=svg)](https://circleci.com/gh/shimat/opencvsharp/tree/master) [![Appveyor Build status](https://ci.appveyor.com/api/projects/status/dvjexft02s6b3re6/branch/master?svg=true)](https://ci.appveyor.com/project/shimat/opencvsharp/branch/master) [![Github Actions Ubuntu Status](https://github.com/shimat/opencvsharp/workflows/Ubuntu%2018.04/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![Github Actions MacOS Status](https://github.com/shimat/opencvsharp/workflows/macOS%2010.15/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![GitHub license](https://img.shields.io/github/license/shimat/opencvsharp.svg)](https://github.com/shimat/opencvsharp/blob/master/LICENSE) \n\nWrapper of OpenCV for .NET\n\nOld versions of OpenCvSharp are stored in [opencvsharp_2410](https://github.com/shimat/opencvsharp_2410).\n\n## NuGet\n\n| Package | Description | Link |\n|---------|-------------|------|\n|**OpenCvSharp4**| OpenCvSharp core libraries | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.svg)](https://badge.fury.io/nu/OpenCvSharp4) |\n|**OpenCvSharp4.WpfExtensions**| WPF Extensions | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions.svg)](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions) |\n|**OpenCvSharp4.Windows**| All-in-one package for Windows (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.Windows.svg)](https://badge.fury.io/nu/OpenCvSharp4.Windows) |\n|**OpenCvSharp4.runtime.win**| Native bindings for Windows x64/x86 (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.win.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.win) |\n|**OpenCvSharp4.runtime.uwp**| Native bindings for UWP (Universal Windows Platform) x64/x86/ARM | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp) |\n|**OpenCvSharp4.runtime.ubuntu.18.04-x64**| Native bindings for Ubuntu 18.04 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64) |\n|**OpenCvSharp4.runtime.osx.10.15-x64**| Native bindings for macOS 10.15 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.osx.10.15-x64.svg)](https://www.nuget.org/packages/OpenCvSharp4.runtime.osx.10.15-x64/) |\n|(beta packages)| Development Build Package    | https://ci.appveyor.com/nuget/opencvsharp |\n\nNative binding (OpenCvSharpExtern.dll / libOpenCvSharpExtern.so) is required to work OpenCvSharp. To use OpenCvSharp, you should add both `OpenCvSharp4` and `OpenCvSharp4.runtime.*` packages to your project. Currently, native bindings for Windows, UWP, Ubuntu 18.04 and macOS are released.\n\nPackages named OpenCvSharp3-* and OpenCvSharp-* are deprecated.\n> [OpenCvSharp3-AnyCPU](https://www.nuget.org/packages/OpenCvSharp3-AnyCPU/) / [OpenCvSharp3-WithoutDll](https://www.nuget.org/packages/OpenCvSharp3-WithoutDll/) / [OpenCvSharp-AnyCPU](https://www.nuget.org/packages/OpenCvSharp-AnyCPU/) /  [OpenCvSharp-WithoutDll](https://www.nuget.org/packages/OpenCvSharp-WithoutDll/)\n\n## Docker images\nhttps://hub.docker.com/u/shimat\n- [shimat/ubuntu18-dotnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/ubuntu18-dotnetcore3.1-opencv4.5.0)\n- [shimat/appengine-aspnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/appengine-aspnetcore3.1-opencv4.5.0)\n\n\n## Installation\n\n### Windows (except UWP)\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.win` NuGet packages to your project. You can use `OpenCvSharp4.Windows` instead.\n\n### UWP\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.uwp` NuGet packages to your project. Note that `OpenCvSharp4.runtime.win` and `OpenCvSharp4.Windows` don't work for UWP. \n\n### Ubuntu 18.04\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.ubuntu.18.04.x64` NuGet packages to your project.\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\ndotnet add package OpenCvSharp4.runtime.ubuntu.18.04-x64\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Google AppEngine Flexible (Ubuntu 16.04)\nSome Docker images are provided to use OpenCvSharp with AppEngine Flexible. The native binding (libOpenCvSharpExtern) is already built in the docker image and you don't need to worry about it.\n```\nFROM shimat/appengine-aspnetcore3.1-opencv4.5.0:20201030\n\nADD ./ /app \nENV ASPNETCORE_URLS=http://*:${PORT} \n\nWORKDIR /app \nENTRYPOINT [ \"dotnet\", \"YourAspNetCoreProject.dll\" ]\n```\n\n### Ubuntu 18.04 Docker image\nYou can use the `shimat/ubuntu18-dotnetcore3.1-opencv4.5.0` docker image.\nThis issue may be helpful: https://github.com/shimat/opencvsharp/issues/920\n\n### Downloads\nIf you do not use NuGet, get DLL files from the [release page](https://github.com/shimat/opencvsharp/releases).\n\n## Target OpenCV\n* [OpenCV 4.5.0](http://opencv.org/) with [opencv_contrib](https://github.com/opencv/opencv_contrib)\n\n## Requirements\n* [.NET Framework 4.6.1](http://www.microsoft.com/ja-jp/download/details.aspx?id=1639) / [.NET Core 2.0](https://www.microsoft.com/net/download) / [Mono](http://www.mono-project.com/Main_Page)\n* (Windows) [Visual C++ 2019 Redistributable Package](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\n* (Windows Server) Media Foundation\n```\nPS1> Install-WindowsFeature Server-Media-Foundation\n```\n* (Ubuntu, Mac) You must pre-install all the dependency packages needed to build OpenCV. Many packages such as libjpeg must be installed in order to work OpenCV. \nhttps://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n\n\n**OpenCvSharp won't work on Unity and Xamarin platform.** For Unity, please consider using [OpenCV for Unity](https://www.assetstore.unity3d.com/en/#!/content/21088) or some other solutions.\n\n**OpenCvSharp does not support CUDA.** If you want to use the CUDA features, you need to customize the native bindings yourself.\n\n## Usage\nFor more details, see **[samples](https://github.com/shimat/opencvsharp_samples/)** and **[Wiki](https://github.com/shimat/opencvsharp/wiki)** pages.\n\n**Always remember to release Mat instances! The `using` syntax is useful.**\n```C#\n// C# 8\n// Edge detection by Canny algorithm\nusing OpenCvSharp;\n\nclass Program \n{\n    static void Main() \n    {\n        using var src = new Mat(\"lenna.png\", ImreadModes.Grayscale);\n \u00a0 \u00a0 \u00a0 \u00a0using var dst = new Mat();\n        \n        Cv2.Canny(src, dst, 50, 200);\n        using (new Window(\"src image\", src)) \n        using (new Window(\"dst image\", dst)) \n        {\n            Cv2.WaitKey();\n        }\n    }\n}\n```\n\n## Features\n* OpenCvSharp is modeled on the native OpenCV C/C++ API style as much as possible.\n* Many classes of OpenCvSharp implement IDisposable. There is no need to manage unsafe resources. \n* OpenCvSharp does not force object-oriented programming style on you. You can also call native-style OpenCV functions.\n* OpenCvSharp provides functions for converting from `Mat` into `Bitmap`(GDI+) or `WriteableBitmap`(WPF).\n\n## Code samples\nhttps://github.com/shimat/opencvsharp_samples/\n\n## Documents\nhttps://shimat.github.io/opencvsharp_docs/index.html\n\n## OpenCvSharp Build Instructions\n### Windows\n- Install Visual Studio 2019 or later\n  - VC++ features are required.\n- Run `download_opencv_windows.ps1` to download OpenCV libs and headers from https://github.com/shimat/opencv_files. Those lib files are precompiled by the owner of OpenCvSharp using AppVeyor CI.\n```\n.\\download_opencv_windows.ps1\n```\n- Build OpenCvSharp\n  - Open `OpenCvSharp.sln` and build\n  \n#### How to customize OpenCV binaries yourself\nIf you want to use some OpenCV features that are not provided by default in OpenCvSharp (e.g. GPU), you will have to build OpenCV yourself. The binary files of OpenCV for OpenCvSharp for Windows are created in the [opencv_files](https://github.com/shimat/opencv_files) repository. See the README.\n\n- `git clone --recursive https://github.com/shimat/opencv_files`\n- Edit `build_windows.ps1` or `build_uwp.ps1` to customize the CMake parameters .\n- Run the PowerShell script.\n\n### Ubuntu 18.04\n- Build OpenCV with opencv_contrib. \n  - https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n- Install .NET Core SDK. https://docs.microsoft.com/ja-jp/dotnet/core/install/linux-package-manager-ubuntu-1804\n- Get OpenCvSharp source files\n```\ngit clone https://github.com/shimat/opencvsharp.git\ncd opencvsharp\ngit fetch --all --tags --prune && git checkout ${OPENCVSHARP_VERSION}\n```\n\n- Build native wrapper `OpenCvSharpExtern`\n```\ncd opencvsharp/src\nmkdir build\ncd build\ncmake -D CMAKE_INSTALL_PREFIX=${YOUR_OPENCV_INSTALL_PATH} ..\nmake -j \nmake install\n```\nYou should add reference to `opencvsharp/src/build/OpenCvSharpExtern/libOpenCvSharpExtern.so`\n```\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/home/shimat/opencvsharp/src/build/OpenCvSharpExtern\"\n```\n\n- Add `OpenCvSharp4` NuGet package to your project\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Older Ubuntu\nRefer to the [Dockerfile](https://github.com/shimat/opencvsharp/blob/master/docker/google-appengine-ubuntu.16.04-x64/Dockerfile) and [Wiki pages](https://github.com/shimat/opencvsharp/wiki).\n\n## Donations\nIf you find the OpenCvSharp library useful and would like to show your gratitude by donating, here are some donation options. Thank you.\n\nhttps://github.com/sponsors/shimat\n"
 },
 {
  "repo": "mapillary/OpenSfM",
  "language": "JavaScript",
  "readme_contents": "OpenSfM ![Docker workflow](https://github.com/mapillary/opensfm/workflows/Docker%20CI/badge.svg)\n=======\n\n## Overview\nOpenSfM is a Structure from Motion library written in Python. The library serves as a processing pipeline for reconstructing camera poses and 3D scenes from multiple images. It consists of basic modules for Structure from Motion (feature detection/matching, minimal solvers) with a focus on building a robust and scalable reconstruction pipeline. It also integrates external sensor (e.g. GPS, accelerometer) measurements for geographical alignment and robustness. A JavaScript viewer is provided to preview the models and debug the pipeline.\n\n<p align=\"center\">\n  <img src=\"https://docs.opensfm.org/_images/berlin_viewer.jpg\" />\n</p>\n\nCheckout this [blog post with more demos](http://blog.mapillary.com/update/2014/12/15/sfm-preview.html)\n\n\n## Getting Started\n\n* [Building the library][]\n* [Running a reconstruction][]\n* [Documentation][]\n\n\n[Building the library]: https://docs.opensfm.org/building.html (OpenSfM building instructions)\n[Running a reconstruction]: https://docs.opensfm.org/using.html (OpenSfM usage)\n[Documentation]: https://docs.opensfm.org  (OpenSfM documentation)\n\n## License\nOpenSfM is BSD-style licensed, as found in the LICENSE file.  See also the Facebook Open Source [Terms of Use][] and [Privacy Policy][]\n\n[Terms of Use]: https://opensource.facebook.com/legal/terms (Facebook Open Source - Terms of Use)\n[Privacy Policy]: https://opensource.facebook.com/legal/privacy (Facebook Open Source - Privacy Policy)\n"
 },
 {
  "repo": "makelove/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "- \u6211\u5728B\u7ad9\u505a\u89c6\u9891\u535a\u5ba2VLoger\uff0c\u6b22\u8fce\u5927\u5bb6\u6765\u6367\u573a\u3002\u4e0d\u53ea\u662fOpenCV\n    - \u7a0b\u5e8f\u5458\u8d5a\u94b1\u6307\u5357 https://space.bilibili.com/180948619\n\n# [OpenCV-Python-Tutorial](https://github.com/makelove/OpenCV-Python-Tutorial)\n- \u6709\u670b\u53cb\u53cd\u6620\u8bf4\u4e0b\u8f7drepo\u6162\uff0c\u56e0\u4e3adata\u6709\u4e00\u4e9b\u89c6\u9891sample\n    - \u73b0\u57282020-8-15\u628arepo\u538b\u7f29\uff0c\u4e0a\u4f20\u5230\u767e\u5ea6\u4e91\u76d8\n        - \u94fe\u63a5: https://pan.baidu.com/s/1jpjpfum5EMpNrZoEHGvn1g \u63d0\u53d6\u7801: 8cab\n\n## [OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf](OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf)\n- \u8fd9\u4e2arepo\u662f\u8fd9\u672c\u4e66PDF\u7684\u6240\u6709\u6e90\u4ee3\u7801\uff0c\u51e0\u4e4e\u90fd\u88ab\u6d4b\u8bd5\u8fc7\uff0c\u80fd\u6b63\u5e38\u8fd0\u884c\u3002\u7a0b\u5e8f\u4f7f\u7528\u7684\u56fe\u7247\u548c\u89c6\u9891\uff0c\u90fd\u5728data\u6587\u4ef6\u5185\u3002\n\n### \u5e73\u65f6\u4f1a\u6dfb\u52a0\u4e00\u4e9b\u6709\u8da3\u7684\u4ee3\u7801\uff0c\u5b9e\u73b0\u67d0\u79cd\u529f\u80fd\u3002\n- \u5b98\u7f51 https://opencv.org/\n- \u5b98\u65b9\u6587\u6863api https://docs.opencv.org/4.0.0/\n- \u5b98\u65b9\u82f1\u6587\u6559\u7a0b http://docs.opencv.org/3.2.0/d6/d00/tutorial_py_root.html\n\n## \u8fd0\u884c:\u5b98\u65b9samples/demo.py \u4f1a\u6709\u5f88\u591a\u6709\u8da3\u7684\u4f8b\u5b50\uff0c\u4ecb\u7ecd\u4f60\u53bb\u4e86\u89e3OpenCV\u7684\u529f\u80fd\u3002\n\n\n~~python 2.7 \u5206\u652f\u88ab\u5e9f\u5f03\u4e86\uff0c\u4e0d\u518d\u66f4\u65b0~~\n\n~~# \u6dfb\u52a0\u4e86 Python3.6\u5206\u652f,\n\u8be5\u5206\u652f\u662f\u4f7f\u7528 opencv3.2+Python3.6~~\n\n## \u628a\u539f\u6765\u7684master\u5206\u652f\u6539\u4e3apython2.7\u5206\u652f\uff0cpython3.6\u5206\u652f\u6539\u4e3amaster\u5206\u652f\n* git clone https://github.com/makelove/OpenCV-Python-Tutorial.git\n* ~~git checkout python3.6~~\n\n##### \u5efa\u8bae\u4f7f\u7528PyCharm\u6765\u7f16\u5199/\u8c03\u8bd5Python\u4ee3\u7801\n\n## \u5f00\u53d1\u73af\u5883\n* macOS Mojave 10.14\n* Python 3.6.1\n* OpenCV 3.2.0\n* PyCharm 2018.3\n\n\n### VMware \u865a\u62df\u673a\n\u5982\u679c\u5b89\u88c5OpenCV\u6709\u95ee\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528VMware \u865a\u62df\u673a\u5b89\u88c5Ubuntu\u7cfb\u7edf\uff0c\u672c\u4eba\u53ef\u4ee5\u5e2e\u4f60\u4eec\u5b89\u88c5\u4e00\u4e2a\uff0c\u518d\u5171\u4eab\u5230\u767e\u5ea6\u4e91\n\n### \u6811\u8393\u6d3e3b\n\u672c\u4eba\u6709\u4e00\u5757\u3010\u6811\u8393\u6d3e3b\u3011\u5f00\u53d1\u677f\uff0c\u4e5f\u5b89\u88c5\u4e86OpenCV3\uff0c\u5f88\u597d\u7528\uff0c\u5efa\u8bae\u4f60\u4eec\u4e5f\u4e70\u4e00\u5757\u6765\u73a9\u4e00\u73a9\u3002\n\n### \u6444\u50cf\u5934\n* MacBook pro\u81ea\u5e26\n* \u6dd8\u5b9d\uff0c[130W\u50cf\u7d20\u9ad8\u6e05\u6444\u50cf\u5934\u6a21\u7ec4 720P 1280x720 USB2.0\u514d\u9a71 \u5fae\u8ddd\u6a21\u5757](https://s.click.taobao.com/gOB3ACw)\n* \u6dd8\u5b9d\uff0c[\u6811\u8393\u6d3e3\u4ee3B Raspberry Pi USB\u6444\u50cf\u5934\uff0c\u514d\u9a71\u52a8](https://s.click.taobao.com/kTu2ACw) \u4e0d\u597d\u7528\uff0c\u53ef\u89c6\u89d2\u5ea6\u592a\u5c0f\uff01\n* Kinect for Xbox360 Slim\uff0c AUX\u63a5\u53e3\u4e0d\u80fd\u76f4\u63a5\u63d2\u5165\u7535\u8111\uff0c\u9700\u8981\u8d2d\u4e70\u7535\u6e90\u9002\u914d\u5668 [\u6dd8\u5b9d](https://s.click.taobao.com/t?e=m%3D2%26s%3DuOhQTZaHKEQcQipKwQzePOeEDrYVVa64LKpWJ%2Bin0XLjf2vlNIV67rEUhWAGPPKrYFMBzHxYoCOlldgrEKAMDfvtTsPa%2Bvw8FDXjhIkoffd7RTQd3LKg2nJi6DFpZGNc%2Bht3wBcxEogkdIkZMKiRbrUG0ypJDuSgXlTpbZcV4j5YC7K2OdchcA%3D%3D&scm=null&pvid=null&app_pvid=59590_11.9.33.73_524_1585572680125&ptl=floorId%3A17741&originalFloorId%3A17741&app_pvid%3A59590_11.9.33.73_524_1585572680125&union_lens=lensId%3APUB%401585572666%400b1a25a5_48ac_1712b7ede03_179a%40023mXY9mmpUNuNySUoJofoOt)\n\n## \u6559\u7a0b\u8d44\u6e90\n- http://www.learnopencv.com/\n- http://www.pyimagesearch.com/\n- [YouTube\u4e0asentex\u7684OpenCV\u89c6\u9891\u6559\u7a0b](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq)\n- B\u7ad9 [OpenCV YouTube](https://search.bilibili.com/all?keyword=OpenCV%20YouTube)\n- [\u5b98\u65b9\u6559\u7a0b](https://opencv.org/courses/)\n\n## \u65b0\u95fbNews https://opencv.org/news.html\n- \u4e2d\u6587\u8bba\u575b http://www.opencv.org.cn/\n- [OpenCV 3.3\u53d1\u5e03\u4e86](http://opencv.org/opencv-3-3.html) \n    1. \u4e3b\u8981\u6d88\u606f\u662f\u6211\u4eec\u5c06DNN\u6a21\u5757\u4eceopencv_contrib\u63a8\u5e7f\u5230\u4e3b\u5b58\u50a8\u5e93\uff0c\u6539\u8fdb\u548c\u52a0\u901f\u4e86\u5f88\u591a\u3002\u4e0d\u518d\u9700\u8981\u5916\u90e8BLAS\u5b9e\u73b0\u3002\u5bf9\u4e8eGPU\uff0c\u4f7f\u7528Halide\uff08http://halide-lang.org\uff09\u8fdb\u884c\u5b9e\u9a8c\u6027DNN\u52a0\u901f\u3002\u6709\u5173\u8be5\u6a21\u5757\u7684\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728\u6211\u4eec\u7684wiki\u4e2d\u627e\u5230\uff1a[OpenCV\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV)\u3002\n    2. OpenCV\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u6807\u5fd7ENABLE_CXX11\u6784\u5efa\u4e3aC ++ 11\u5e93\u3002\u6dfb\u52a0\u4e86C ++ 11\u7a0b\u5e8f\u5458\u7684\u4e00\u4e9b\u5f88\u9177\u7684\u529f\u80fd\u3002\n    3. \u7531\u4e8e\u201c\u52a8\u6001\u8c03\u5ea6\u201d\u529f\u80fd\uff0c\u6211\u4eec\u8fd8\u5728OpenCV\u7684\u9ed8\u8ba4\u7248\u672c\u4e2d\u542f\u7528\u4e86\u4e0d\u5c11AVX / AVX2\u548cSSE4.x\u4f18\u5316\u3002DNN\u6a21\u5757\u8fd8\u5177\u6709\u4e00\u4e9bAVX / AVX2\u4f18\u5316\u3002\nIntel Media SDK\u73b0\u5728\u53ef\u4ee5\u88ab\u6211\u4eec\u7684videoio\u6a21\u5757\u7528\u6765\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u7684\u89c6\u9891\u7f16\u7801/\u89e3\u7801\u3002\u652f\u6301MPEG1 / 2\uff0c\u4ee5\u53caH.264\u3002\n    4. \u5d4c\u5165OpenCV Intel IPP\u5b50\u96c6\u5df2\u4ece2015.12\u5347\u7ea7\u52302017.2\u7248\u672c\uff0c\u4ece\u800c\u5728\u6211\u4eec\u7684\u6838\u5fc3\u548cimgproc perf\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e8615\uff05\u7684\u901f\u5ea6\u3002\n    5. 716\u62c9\u8bf7\u6c42\u5df2\u7ecf\u5408\u5e76\uff0c588\u6211\u4eec\u7684\u9519\u8bef\u8ddf\u8e2a\u5668\u4e2d\u7684\u95ee\u9898\u5df2\u7ecf\u5173\u95ed\uff0c\u56e0\u4e3aOpenCV 3.2\u3002\u53e6\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u4e25\u683c\u7684\u9759\u6001\u5206\u6790\u4eea\u5de5\u5177\u8fd0\u884cOpenCV\uff0c\u5e76\u4fee\u590d\u4e86\u68c0\u6d4b\u5230\u7684\u95ee\u9898\u3002\u6240\u4ee5OpenCV 3.3\u5e94\u8be5\u662f\u975e\u5e38\u7a33\u5b9a\u548c\u53ef\u9760\u7684\u91ca\u653e\u3002\n    6. \u6709\u5173OpenCV 3.3\u7684\u66f4\u6539\u548c\u65b0\u529f\u80fd\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95eehttps://github.com/opencv/opencv/wiki/ChangeLog\u3002\n    7. [\u4e0b\u8f7dOpenCV 3.3](https://github.com/opencv/opencv/releases/tag/3.3.0)\n    8. [\u5b89\u88c5OpenCV 3.3](http://www.linuxfromscratch.org/blfs/view/cvs/general/opencv.html)\n- OpenCV 4.0\u53d1\u5e03\u4e86 https://opencv.org/opencv-4-0-0.html\n\n## \u600e\u6837\u7ffb\u5899\uff1f\u4f7f\u7528Google\u641c\u7d22\u5f15\u64ce\uff0c\u89c2\u770bYouTube\u89c6\u9891\u6559\u7a0b\n- shadowsocks\n    - \u65b9\u4fbf\uff0c\u968f\u5730\u968f\u65f6\u7ffb\u5899\n    - \u624b\u673a\u4f7f\u75284G\u4fe1\u53f7\u4e0a\u7f51\uff0c\u4e5f\u53ef\u4ee5\u3002\n    - \u5f3a\u70c8\u63a8\u8350\uff01\n    - \u8d2d\u7269\u56fd\u5916\u670d\u52a1\u5668\uff0c\u642d\u5efa\u4e5f\u5f88\u5bb9\u6613\n        - \u53c2\u8003 https://isweic.com/build-shadowsocks-python-server/\n        - pip install shadowsocks\n        - \u8fd0\u884c\n            - shell\u7a97\u53e3\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb\n                - 8388\u662f\u7aef\u53e3\u53f7\uff0cpassword\u662f\u5bc6\u7801\uff0caes-256-cfb\u662f\u52a0\u5bc6\u7c7b\u578b\uff0c\u901a\u8fc7Ctrl+C\u7ed3\u675f\n            - \u540e\u53f0\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb --user nobody -d start\n                - \u7ed3\u675f\u540e\u53f0\u8fd0\u884c\n                    - ssserver -d stop\n            - \u68c0\u67e5\u8fd0\u884c\u65e5\u5fd7\n                - less /var/log/shadowsocks.log\n- [Lantern\u84dd\u706f](https://github.com/getlantern/lantern/releases/tag/latest)\n    - \u672c\u4eba\u4e0d\u4f7f\u7528\u84dd\u706f\u4e86\u3002\n    1. \u53ef\u4ee5\u514d\u8d39\u4f7f\u7528\uff0c\u4f46\u7528\u5b8c800m\u6d41\u91cf\u540e\u4f1a\u9650\u901f\uff0c\u8fd8\u80fd\u6b63\u5e38\u4f7f\u7528\uff0c\u5c31\u662f\u6709\u70b9\u6162\n    2. \u4e13\u4e1a\u7248\u4e0d\u8d35\uff0c2\u5e74336\u5143\uff0c\u6bcf\u59290.46\u5143\u3002[Lantern\u84dd\u706f\u4e13\u4e1a\u7248\u8d2d\u4e70\u6d41\u7a0b](https://github.com/getlantern/forum/issues/3863)\n    3. \u9080\u8bf7\u597d\u53cb\u6765\u83b7\u5f97\u66f4\u591a\u7684\u4e13\u4e1a\u7248\u4f7f\u7528\u65f6\u95f4\u3002\u6211\u7684\u9080\u8bf7\u7801\uff1aGW2362\n    \n## \u66f4\u65b0\n- [\u7834\u89e3\u9a8c\u8bc1\u7801](my06-\u9a8c\u8bc1\u7801\u8bc6\u522b/solving_captchas_code_examples/README.md)\n    \n## \u6350\u8d60\u6253\u8d4f  \n- OpenCV\u95ee\u7b54\u7fa41,QQ\u7fa4\u53f7:187436093\n- \u5fae\u4fe1  \n    - <img src=\"data/wechat_donate.jpg\" width = \"200\" height = \"200\" alt=\"wechat_donate\"  />\n\n\n- \u652f\u4ed8\u5b9d\n    - <img src=\"data/alipay_donate.jpg\" width = \"200\" height = \"200\" alt=\"alipay_donate\"  />\n \n- \u798f\u5229\n    - \u514d\u8d39\u56fd\u5185\u670d\u52a1\u5668\uff0c\u4f46\u9700\u8981\u4ea4\u62bc\u91d1\uff0c\u968f\u65f6\u5168\u989d\u539f\u8def\u9000\u8fd8\n        - \u6709\u9700\u8981\u7684\u670b\u53cb\u8bf7\u52a0\u5165QQ\u7fa4\uff0c\u53d1\u3010\u624b\u673a\u53f7\u3011\u7ed9\u7fa4\u4e3b\n        - ![free_server](data/free_server.jpeg)"
 },
 {
  "repo": "MasteringOpenCV/code",
  "language": "C++",
  "readme_contents": "==============================================================================\r\nMastering OpenCV with Practical Computer Vision Projects\r\n==============================================================================\r\nFull source-code for the book.\r\n--------------------------------------------------------------------------------\r\n\r\n    Source-Code:    https://github.com/MasteringOpenCV/code\r\n    Book:           http://www.packtpub.com/cool-projects-with-opencv/book\r\n    Copyright:      Packt Publishing 2012.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nTo build & run the projects for the book:\r\n--------------------------------------------------------------------------------\r\n- Install OpenCV (versions between 2.4.2 to 2.4.11 are supported, whereas OpenCV 3.0 is not yet supported). eg: go to \"http://opencv.org/\", click on\r\n  Downloads, download the latest OpenCV 2.4 version (including prebuilt library), and extract\r\n  it to \"C:\\OpenCV\" for Windows or \"~/OpenCV\" for Linux. In OpenCV v2.4.3, the\r\n  prebuilt OpenCV library is in \"C:\\OpenCV\\build\" or \"~/OpenCV/build\", such as\r\n  \"C:\\OpenCV\\build\\x64\\vc9\" for MS Visual Studio 2008 (or \"vs10\" folder for MS \r\n  Visual Studio 2010, or the \"x86\" parent folder for 32-bit Windows).\r\n\r\n- Install all the source code of the book. eg: extract the code to\r\n  \"C:\\MasteringOpenCV\" for Windows or \"~/MasteringOpenCV\" for Linux.\r\n  \r\n- Install CMake v2.8 or later from \"http://www.cmake.org/\".\r\n\r\nEach chapter of the book is for a separate project. Therefore there are 9\r\nprojects for the 9 chapters (remember that Chapter 9 is an online chapter that\r\ncan be downloaded from \"http://www.packtpub.com/cool-projects-with-opencv/book\").\r\nYou can run each project separately, they each contain a README.md text file\r\ndescribing how to build that project, using CMake in most cases, because CMake\r\ncan be used with many compilers and many operating systems.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nChapters:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android, by Shervin Emami.\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad, by Khvedchenia Ievgen.\r\n- Ch3) Marker-less Augmented Reality, by Khvedchenia Ievgen.\r\n- Ch4) Exploring Structure from Motion using OpenCV, by Roy Shilkrot.\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks, by David Escriv\u00e1.\r\n- Ch6) Non-rigid Face Tracking, by Jason Saragih.\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT, by Daniel L\u00e9lis Baggio.\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces, by Shervin Emami.\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect, by Naureen Mahmood.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nWhat you need for this book:\r\n--------------------------------------------------------------------------------\r\nYou don't need to have special knowledge in computer vision to read this book,\r\nbut you should have good C/C++ programming skills and basic experience with\r\nOpenCV before reading this book. Readers without experience in OpenCV may wish to\r\nread the book Learning OpenCV for an introduction to the OpenCV features, or read\r\n\"OpenCV 2 Cookbook\" for examples on how to use OpenCV with recommended C/C++\r\npatterns, because \"Mastering OpenCV with Practical Computer Vision Projects\" will\r\nshow you how to solve real problems, assuming you are already familiar with the\r\nbasics of OpenCV and C/C++ development.\r\n\r\nIn addition to C/C++ and OpenCV experience, you will also need a computer, and an\r\nIDE of your choice (such as Visual Studio, XCode, Eclipse, or QtCreator, running\r\non Windows, Mac or Linux). Some chapters have further requirements, particularly:\r\n\r\n- To develop the Android app, you will need an Android device, Android\r\n  development tools, and basic Android development experience.\r\n- To develop the iOS app, you will need an iPhone, iPad, or iPod Touch device,\r\n  iOS development tools (including an Apple computer, XCode IDE, and an Apple\r\n  Developer Certificate), and basic iOS and Objective-C development experience.\r\n- Several desktop projects require a webcam connected to your computer. Any\r\n  common USB webcam should suffice, but a webcam of at least 1 megapixel may be\r\n  desirable.\r\n- CMake is used in most projects, including OpenCV itself, to build across\r\n  operating systems and compilers. A basic understanding of build systems is\r\n  required, and knowledge of cross-platform building is recommended.\r\n- An understanding of linear algebra is expected, such as basic vector and matrix\r\n  operations and eigen decomposition.\r\n\r\nPer-chapter Requirements:\r\n- Ch1: webcam (for desktop app), or Android development system (for Android app).\r\n- Ch2: iOS development system (to build an iOS app).\r\n- Ch3: OpenGL built into OpenCV.\r\n- Ch4: PCL (http://pointclouds.org/) and SSBA (http://www.inf.ethz.ch/personal/chzach/opensource.html).\r\n- Ch5: nothing.\r\n- Ch6: nothing, but requires training data for execution.\r\n- Ch7: nothing.\r\n- Ch8: webcam.\r\n- Ch9: Kinect depth sensor.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nScreenshots:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android:\r\n![Ch1) Cartoonifier and Skin Changer for Android](https://raw.github.com/MasteringOpenCV/code/master/Chapter1_AndroidCartoonifier/screenshot.png)\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad:\r\n![Ch2) Marker-based Augmented Reality on iPhone or iPad](https://raw.github.com/MasteringOpenCV/code/master/Chapter2_iPhoneAR/screenshot.png)\r\n- Ch3) Marker-less Augmented Reality:\r\n![Ch3) Marker-less Augmented Reality](https://raw.github.com/MasteringOpenCV/code/master/Chapter3_MarkerlessAR/screenshot.png)\r\n- Ch4) Exploring Structure from Motion using OpenCV:\r\n![Ch4) Exploring Structure from Motion using OpenCV](https://raw.github.com/MasteringOpenCV/code/master/Chapter4_StructureFromMotion/screenshot.png)\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks:\r\n![Ch5) Number Plate Recognition using SVM and Neural Networks](https://raw.github.com/MasteringOpenCV/code/master/Chapter5_NumberPlateRecognition/screenshot.png)\r\n- Ch6) Non-rigid Face Tracking:\r\n![Ch6) Non-rigid Face Tracking](https://raw.github.com/MasteringOpenCV/code/master/Chapter6_NonRigidFaceTracking/screenshot.png)\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT:\r\n![Ch7) 3D Head Pose Estimation using AAM and POSIT](https://raw.github.com/MasteringOpenCV/code/master/Chapter7_HeadPoseEstimation/screenshot.png)\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces:\r\n![Ch8) Face Recognition using Eigenfaces or Fisherfaces](https://raw.github.com/MasteringOpenCV/code/master/Chapter8_FaceRecognition/screenshot.png)\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect:\r\n![Ch9) Developing Fluid Wall using the Microsoft Kinect](https://raw.github.com/MasteringOpenCV/code/master/Chapter9_FluidInteractionUsingKinect/screenshot.png)\r\n\r\n\r\n"
 },
 {
  "repo": "kongqw/OpenCVForAndroid",
  "language": "Java",
  "readme_contents": "# OpenCV 3.2.0\n\n\u8fd0\u884c\u524d\u8bf7\u5148\u5b89\u88c5[OpenCV Manager(\u5fc5\u987b)](https://github.com/kongqw/FaceDetectLibrary/tree/opencv3.2.0/OpenCVManager)\u3002\n\n\u672c\u793a\u4f8b\u57fa\u4e8eOpenCV 3.2.0 \u7248\u672c\u5728Android\u5e73\u53f0\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u548c\u76ee\u6807\u8ffd\u8e2a\n\n## \u76ee\u6807\u68c0\u6d4b\n\n- \u4eba\u8138\u68c0\u6d4b\n- \u773c\u775b\u68c0\u6d4b\n- \u5fae\u7b11\u68c0\u6d4b\n- \u4e0a\u534a\u8eab\u68c0\u6d4b\n- \u4e0b\u534a\u8eab\u68c0\u6d4b\n- \u5168\u8eab\u68c0\u6d4b\n\n## \u76ee\u6807\u8ffd\u8e2a\n\n- CamShift\u7b97\u6cd5\u5b9e\u73b0\u76ee\u6807\u8ffd\u8e2a\n\n### \u6548\u679c\u56fe\n\n\u76ee\u6807\u68c0\u6d4b\n\n![\u76ee\u6807\u68c0\u6d4b](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectDetecting.gif)\n\n\u76ee\u6807\u8ffd\u8e2a\n\n![\u76ee\u6807\u8ffd\u8e2a](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectTracking.gif)\n\n## \u4eba\u8138\u8bc6\u522b\n\n\u4eba\u8138\u8bc6\u522b\uff08\u5bf9\u6bd4\uff09\u8bf7\u5207\u6362\u5230[master](https://github.com/kongqw/FaceDetectLibrary/tree/master)\u5206\u652f\uff0c\u57fa\u4e8eOpenCV 2.4.11\u3002\n"
 },
 {
  "repo": "oreillymedia/Learning-OpenCV-3_examples",
  "language": "C++",
  "readme_contents": "# Learning OpenCV 3\n\n## INTRO\n\nThis is the example code that accompanies Learning OpenCV 3 by Adrian Kaehler and Gary Bradski ([9781491937990](http:*shop.oreilly.com/product/0636920044765.do)).\n\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n\n\n## NOTES\n\nFor default suggestions of how the run the code, it assumes you put your build directory under `Learning-OpenCV-3_examples` directory. \n\nThus, from the `Learning-OpenCV-3_examples` directory:\n\n```\t\n  mkdir build\n  cd build\n  cmake ..\n  make -j\n```\n\n#### Docker\nFor your interest, included here is an Ubuntu _Docker_ file that\n* Shares a directory with the host operating system\n* Shares the first camera between both systems\n* Loads Ubuntu 16.04 \n* Loads all dependencies for OpenCV 3.2 and opencv_contrib\n* Loads and builds OpenCV 3.2 and opencv_contrib into a build directory \n  * executable files end up in `opencv-3.2.0/build/bin`\n* Next, it `git clones` the code (and Docker file) for Learning OpenCV 3 and builds it\n  * executable files end up in `Learning_OpenCV-3_examples/build`\n* To get to the top level directory, just type: `cd`\n\n\n## CONTENTS:\n\n### SPECIAL FILES:\n\n* README.md       -- this readme file\n* Dockerfile      -- complete self contained opencv environment using Ubuntu 16-04\n* CMakeLists.txt  -- how to buld everything here \n\n### EXERCISES:\n\n* Exercises at end of Chapter 5\n* Exercises at end of Chapter 7\n* Exercises_8_1.cpp Exercises at end of Chapter 8\n* Exercises_9_1-2-10-11-12-15-16.cpp Exercises at end of Chapter 8\n* Exercises_9_4.cpp Exercises at end of Chapter 9\n* Exercises_9_5.cpp Exercises at end of Chapter 9\n* Exercises at end of Chapter 11\n* Exercises_13_1-2-11.cpp\tExercises for Chapter 13\n* Exercises_13_9.cpp\t\n\n### EXAMPLES:\n\n* Example 2-1. A simple OpenCV program that loads an image from disk and displays it\n* Example 2-2. Same as Example 2-1 but employing the \u201cusing namespace\u201d directive\n* Example 2-3. A simple OpenCV program for playing a video file from disk\n* Example 2-4. Adding a trackbar slider to the basic viewer window for moving around\n* Example 2-5. Loading and then smoothing an image before it is displayed on the screen\n* Example 2-6. Using cv::pyrDown() to create a new image that is half the width and\n* Example 2-7. The Canny edge detector writes its output to a single-channel (grayscale) image\n* Example 2-8. Combining the pyramid down operator (twice) and the Canny\n* Example 2-9. Getting and setting pixels in Example 2-8\n* Example 2-10. The same object can load videos from a camera or a file\n* Example 2-11. A complete program to read in a color video and write out the log-polar-\n* Example 4-1. Summation of a multidimensional array, done plane by plane\n* Example 4-2. Summation of two arrays using the N-ary operator\n* Example 4-3. Printing all of the nonzero elements of a sparse array\n* Example 4-4. A better way to print a matrix\n* Example 5-1. Complete program to alpha-blend the ROI starting at (0,0) in src2 with the ROI starting at (x,y) in src1\n* Example 7-1. Using the default random number generator to generate a pair of integers\n* Example 8-1. Unpacking a four-character code to identify a video codec\n* Example 8-2. Using cv::FileStorage to create a .yml data file\n* Example 8-3. Using cv::FileStorage to read a .yml file\n* Example 9-1. Creating a window and displaying an image in that window\n* Example 9-2. Toy program for using a mouse to draw boxes on the screen\n* Example 9-3. Using a trackbar to create a \u201cswitch\u201d tha t the user can turn on and off;\n* Example 9-4. Slightly modified code from the OpenCV documentation that draws a\n* Example 9-5. An example program ch4_qt.cpp, which takes a single argument\n* Example 9-6. The QMoviePlayer object header file QMoviePlayer.hpp\n* Example 9-7. The QMoviePlayer object source file: QMoviePlayer.cpp\n* Example 9-8. An example program which takes a single argument\n* Example 9-9. The WxMoviePlayer object header file WxMoviePlayer.hpp\n* Example 9-10. The WxMoviePlayer object source file WxMoviePlayer.cpp\n* Example 9-11. An example header file for our custom View class\n* Example 10-1. Using cv::threshold() to sum three channels of an image\n* Example 10-2. Alternative method to combine and threshold image planes\n* Example 10-3. Threshold versus adaptive threshold\n* Example 11-1. An affine transformation.\n* Example 11-2. Code for perspective transformation\n* Example 11-3. Log-polar transform example\n* Example 12-1. Using cv::dft() and cv::idft() to accelerate the computation of\n* Example 12-2. Using cv::HoughCircles() to return a sequence of circles found in a\n* **EXTRA** Example 12-3. Using GrabCut for background removal\n* **EXTRA** Example 12-4. Using GrabCut for background removal\n* Example 13-1. Histogram computation and display\n* Example 13-2. Creating signatures from histograms for EMD; note that this code is the\n* Example 13-3. Template matching\n* Example 14-1. Finding contours based on a trackbar\u2019s location; the contours are\n* Example 14-2. Finding and drawing contours on an input image\n* Example 14-3. Drawing labeled connected components\n* Example 14-4. Using the shape context distance extractor\n* Example 15-1. Reading out the RGB values of all pixels in one row of a video and\n* Example 15-2. Learning a background model to identify foreground pixels\n* Example 15-3. Computing the on and off-diagonal elements of a variance/covariance model\n* Example 15-4. Codebook algorithm implementation\n* Example 15-5. Cleanup using connected components\n* **EXTRA** Example 15-6, using OpenCV's background subtractor class.  Modified by Gary Bradski, 6/4/2017\n* Example 16-1. Pyramid L-K optical flow\n* **EXTRA** Example 16-2. 2D Feature detectors and 2D Extra Features framework\n* Example 17-1. Kalman filter example code\n* Example 17-2. Farneback optical flow example code\n* Example 18-1. Reading a chessboard\u2019s width and height, reading them and calibrating \n* **EXTRA** Example 18-1. From disk. Reading a chessboard\u2019s width and height, reading them and calibrating \n* Example 19-1. Bird\u2019s - eye view\n* Example 19-2. Computing the fundamental matrix using RANSAC\n* Example 19-3. Stereo calibration, rectification, and correspondence\n* Example 19-4. Two-dimensional line fitting\n* Example 20-01. Using K-means\n* Example 20-02. Using the Mahalanobis distance for classification\n* Example 21-1. Creating and training a decision tree\n* Example 22-1. Detecting and drawing faces\n\n### IMAGES:\n\n* box.png\n* box_in_scene.png\n* checkerboard9x6.png\n* example_16-01-imgA.png\n* example_16-01-imgB.png\n* faces.png\n* BlueCup.jpg\n* HandIndoorColor.jpg\n* HandOutdoorColor.jpg\n* HandOutdoorSunColor.jpg\n* adrian.jpg\n* faceScene.jpg\n* faceTemplate.jpg\n* fruits.jpg\n* stuff.jpg\n\n### MOVIES:\n\n* test.avi\n* tree.avi\n\n### CLASSIFIERS:\n\n* haarcascade_frontalcatface.xml           #Cat faces!\n* haarcascade_frontalcatface_extended.xml\n* haarcascade_frontalface_alt.xml\n\n### DIRECTORIES:\n\n* birdseye     -- where the images are of checkerboards on the floor\n* build        -- you will make and build things in this directory\n* calibration  -- checkerboard images to calibrate on\n* muchroom     -- machine learning database\n* shape_sample -- silhoette shapes to recognize\n* stereoData   -- left, right image pairs of checkboards to calibrate and view on\n\n\n## LINKS:\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n"
 },
 {
  "repo": "changwookjun/StudyBook",
  "language": null,
  "readme_contents": "# Study E-Book(ComputerVision DeepLearning MachineLearning Math NLP Python ReinforcementLearning)\n\nContents  \n* [Computer Vision Books](https://github.com/changwookjun/StudyBook/tree/master/ComputerVisionBooks)   \n  + [Machine Learning for OpenCV.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Machine%20Learning%20for%20OpenCV.pdf)   \n  + [Computer Vision- Algorithms and Applications.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Computer%20Vision-%20Algorithms%20and%20Applications.pdf)   \n* [Deep Learning Books](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks)   \n  + [Deep Learning - Josh Patterson & Adam Gibson.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20-%20Josh%20Patterson%20%26%20Adam%20Gibson.pdf)   \n  + [Deep Learning with Python A Hands-on Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20with%20Python%20A%20Hands-on%20Introduction.pdf)   \n  + [Fundamentals of Deep Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Fundamentals%20of%20Deep%20Learning.pdf)   \n  + [Introduction to Deep Learning Using R.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Introduction%20to%20Deep%20Learning%20Using%20R.pdf)   \n  + [Learning TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Learning%20TensorFlow.pdf)   \n  + [deeplearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning.pdf)   \n  + [deeplearningbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook.pdf)   \n  + [deeplearningbook_bookmarked.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook_bookmarked.pdf)   \n  + [oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf)   \n  + [CS 20_Tensorflow for Deep Learning Research](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research)     \n      - [01 _ Lecture slide _ Overview of Tensorflow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/01%20_%20Lecture%20slide%20_%20Overview%20of%20Tensorflow.pdf)     \n      - [02_Lecture slide_TensorFlow Operations.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/02_Lecture%20slide_TensorFlow%20Operations.pdf)     \n      - [03 _ Lecture slide _ Basic Models in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/03%20_%20Lecture%20slide%20_%20Basic%20Models%20in%20TensorFlow.pdf)     \n      - [04 Eager Execution + word2vec.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/04%20Eager%20Execution%20%2B%20word2vec.pdf)     \n      - [05_Slide_Managing your experiment.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/05_Slide_Managing%20your%20experiment.pdf)     \n      - [06_Introduction to Computer Vision and convolutional network.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/06_Introduction%20to%20Computer%20Vision%20and%20convolutional%20network.pdf)     \n      - [07 _ Covnets in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/07%20_%20Covnets%20in%20TensorFlow.pdf)     \n      - [08_Style transfer.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/08_Style%20transfer.pdf)     \n      - [10_Lecture_Slides_VAE in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/10_Lecture_Slides_VAE%20in%20TensorFlow.pdf)     \n      - [11 _ Slides _ Introduction to RNNs.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/11%20_%20Slides%20_%20Introduction%20to%20RNNs.pdf)     \n      - [12_Slides_Machine Translation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/12_Slides_Machine%20Translation.pdf)    \n      - [14_Slides_A TensorFlow Chatbot.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/14_Slides_A%20TensorFlow%20Chatbot.pdf)    \n      - [16_Slides_Tensor2Tensor.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/16_Slides_Tensor2Tensor.pdf)    \n      - [CS20_intro_to_RL.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/CS20_intro_to_RL.pdf)   \n      - [march9guestlecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/march9guestlecture.pdf)    \n  + [DeepLearning_chapter-wise-pdf](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf)     \n      - [table-of-contents.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B1%5Dtable-of-contents.pdf)  \n      - [acknowledgements.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B2%5Dacknowledgements.pdf)  \n      - [notation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B3%5Dnotation.pdf)  \n      - [chapter-1-introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B4%5Dchapter-1-introduction.pdf)  \n      - [part-1-basics.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B5%5Dpart-1-basics.pdf)  \n      - [part-1-chapter-2.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B6%5Dpart-1-chapter-2.pdf)  \n      - [part-1-chapter-3.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B7%5Dpart-1-chapter-3.pdf)  \n      - [part-1-chapter-4.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B8%5Dpart-1-chapter-4.pdf)  \n      - [part-1-chapter-5.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B9%5Dpart-1-chapter-5.pdf)  \n      - [part-2-deep-network-modern-practices.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B10%5Dpart-2-deep-network-modern-practices.pdf)  \n      - [part-2-chapter-6.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B11%5Dpart-2-chapter-6.pdf)  \n      - [part-2-chapter-7.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B12%5Dpart-2-chapter-7.pdf)  \n      - [part-2-chapter-8.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B13%5Dpart-2-chapter-8.pdf)  \n      - [part-2-chapter-9.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B14%5Dpart-2-chapter-9.pdf)        \n      - [part-2-chapter-10.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B15%5Dpart-2-chapter-10.pdf)        \n      - [part-2-chapter-11.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B16%5Dpart-2-chapter-11.pdf)        \n      - [part-2-chapter-12.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B17%5Dpart-2-chapter-12.pdf)  \n      - [part-3-deep-learning-research.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B18%5Dpart-3-deep-learning-research.pdf) \n      - [part-3-chapter-13.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B19%5Dpart-3-chapter-13.pdf) \n      - [part-3-chapter-14.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B20%5Dpart-3-chapter-14.pdf) \n      - [part-3-chapter-15.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B21%5Dpart-3-chapter-15.pdf) \n      - [part-3-chapter-16.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B22%5Dpart-3-chapter-16.pdf) \n      - [part-3-chapter-17.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B23%5Dpart-3-chapter-17.pdf) \n      - [part-3-chapter-18.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B24%5Dpart-3-chapter-18.pdf) \n      - [part-3-chapter-19.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B25%5Dpart-3-chapter-19.pdf) \n      - [part-3-chapter-20.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B26%5Dpart-3-chapter-20.pdf) \n      - [bibliography.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B27%5Dbibliography.pdf) \n      - [index.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B28%5Dindex.pdf) \n  + [d2l-en.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/d2l-en.pdf) \n  + [Dive into DeepLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Dive_into_Deep_Learning.pdf)   \n  + [ee559 Deep learning](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/ee559-Deeplearning)     \n  + [Hands-on-Machine-Learning-with-Scikit-2E.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Hands-on-Machine-Learning-with-Scikit-2E.pdf)  \n  + [deeplearning_2019_spring.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning_2019_spring.pdf)    \n  + [Deep-Learning-with-PyTorch.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep-Learning-with-PyTorch.pdf)  \n\n* [Machine Learning Books](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks)      \n  + [30_03_atelierdatamining.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/30_03_atelierdatamining.pdf)  \n  + [Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)  \n  + [Building Machine Learning Systems with Python, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Building%20Machine%20Learning%20Systems%20with%20Python%2C%202nd%20Edition.pdf)   \n  + [MATLAB Machine Learning by Michael Paluszek.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/MATLAB%20Machine%20Learning%20by%20Michael%20Paluszek.pdf)  \n  + [Machine Learning in Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine%20Learning%20in%20Python.pdf)  \n  + [Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine_Learning.pdf)  \n  + [Mastering Feature Engineering.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Feature%20Engineering.pdf)  \n  + [Mastering Machine Learning with scikit-learn, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Machine%20Learning%20with%20scikit-learn%2C%202nd%20Edition.pdf)  \n  + [NG_MLY.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Ng_MLY.pdf) \n  + [Practical Machine Learning A New Look at Anomaly Detection.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Practical%20Machine%20Learning%20A%20New%20Look%20at%20Anomaly%20Detection.pdf) \n  + [Practical Machine Learning with H2O.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Data%20Analytics.pdf) \n  + [Python Real World Machine Learning - Prateek Joshi.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Real%20World%20Machine%20Learning%20-%20Prateek%20Joshi.pdf) \n  + [Gaussian Processes for Machine Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/RW.pdf) \n  + [The Elements of Statistical Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/ESLII_print12.pdf) \n  + [Foundations of Data Science.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science.pdf) \n  + [cs229-cheatsheet](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks/cs229-cheatsheet) \n  + [Automatic_Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Automatic_Machine_Learning.pdf) \n  + [DataScienceHandbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/DataScienceHandbook.pdf) \n  + [Python Data Science Handbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/pythondatasciencehandbook.pdf) \n  + [Foundations of Data Science(Microsoft).pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science(Microsoft).pdf) \n  + [Bayesian_Data_Analysis_Third_edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bayesian_Data_Analysis_Third_edition.pdf)  \n  + [Joseph K. Blitzstein, Jessica Hwang-Introduction to Probability.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Joseph%20K.%20Blitzstein%2C%20Jessica%20Hwang-Introduction%20to%20Probability.pdf)   \n  \n* [Math Books](https://github.com/changwookjun/StudyBook/tree/master/MathBooks)     \n  + [MIT18_657F15_LecNote.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/MIT18_657F15_LecNote.pdf) \n  + [Mathematics for Machine Learnin.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Mathematics%20for%20Machine%20Learnin.pdf) \n  + [mathandcomp.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mathandcomp.pdf) \n  + [Introduction to Applied Linear Algebra.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Introduction%20to%20Applied%20Linear%20Algebra.pdf) \n  + [matrixcookbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/matrixcookbook.pdf) \n  + [mml-book](https://github.com/changwookjun/StudyBook/tree/master/MathBooks/mml-book)   \n  + [MATHEMATICS FOR MACHINE LEARNING.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mml-book.pdf)  \n  + [LINEAR ALGEBRA.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/LINEAR%20ALGEBRA.pdf)  \n\n* [NLP Books](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks)     \n  + [Applied Text Analysis with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Applied%20Text%20Analysis%20with%20Python.pdf) \n  + [Natural Language Processing with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Natural%20Language%20Processing%20with%20Python.pdf) \n  + [Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from your Data.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Text%20Analytics%20with%20Python%20A%20Practical%20Real-World%20Approach%20to%20Gaining%20Actionable%20Insights%20from%20your%20Data.pdf) \n  + [The Text Mining HandBook.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/The%20Text%20Mining%20HandBook.pdf) \n  + [eisenstein-nlp-notes.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/eisenstein-nlp-notes.pdf)\n  + [oxford-cs-deepnlp-2017](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks/oxford-cs-deepnlp-2017)\n    - [Lecture 1a - Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201a%20-%20Introduction.pdf)    \n    - [Lecture 1b - Deep Neural Networks Are Our Friends.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201b%20-%20Deep%20Neural%20Networks%20Are%20Our%20Friends.pdf)   \n    - [Lecture 2a- Word Level Semantics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202a-%20Word%20Level%20Semantics.pdf) \n    - [Lecture 2b - Overview of the Practicals.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202b%20-%20Overview%20of%20the%20Practicals.pdf) \n    - [Lecture 3 - Language Modelling and RNNs Part 1.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%203%20-%20Language%20Modelling%20and%20RNNs%20Part%201.pdf) \n    - [Lecture 4 - Language Modelling and RNNs Part 2.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%204%20-%20Language%20Modelling%20and%20RNNs%20Part%202.pdf)   \n    - [Lecture 5 - Text Classification.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%205%20-%20Text%20Classification.pdf)   \n    - [Lecture 6 - Nvidia RNNs and GPUs.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%206%20-%20Nvidia%20RNNs%20and%20GPUs.pdf)   \n    - [Lecture 7 - Conditional Language Modeling.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%207%20-%20Conditional%20Language%20Modeling.pdf)   \n    - [Lecture 8 - Conditional Language Modeling with Attention.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)   \n    - [Lecture 9 - Speech Recognition.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%209%20-%20Speech%20Recognition.pdf)   \n    - [Lecture 10 - Text to Speech.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2010%20-%20Text%20to%20Speech.pdf)   \n    - [Lecture 11 - Question Answering.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2011%20-%20Question%20Answering.pdf)   \n    - [Lecture 12- Memory Lecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2012-%20Memory%20Lecture.pdf)    \n    - [Lecture 13 - Linguistics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2013%20-%20Linguistics.pdf)      \n  + [Speech and Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Speech_and_Language_Processing.pdf) \n  + [Embeddings in Natural Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Embeddings%20in%20Natural%20Language%20Processing.pdf)  \n\n\n* [Python Books](https://github.com/changwookjun/StudyBook/tree/master/PythonBooks)   \n  + [Learn Python The Hard Way 3rd Edition free pdf download.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Learn%20Python%20The%20Hard%20Way%203rd%20Edition%20free%20pdf%20download.pdf)\n  + [SciPy and NumPy.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/SciPy%20and%20NumPy.pdf)\n  + [ScipyLectures-simple.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/ScipyLectures-simple.pdf)\n  + [Shaw Z.A. - Learn Python the Hard Way, 2nd Edition [2011, PDF, ENG].pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Shaw%20Z.A.%20-%20Learn%20Python%20the%20Hard%20Way%2C%202nd%20Edition%20%5B2011%2C%20PDF%2C%20ENG%5D.pdf)\n  + [Understanding GIL.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Understanding%20GIL.pdf)\n  + [scipy-ref-0.17.0.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/scipy-ref-0.17.0.pdf)\n  \n* [Reinforcement Learning Books](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks)  \n  + [RLAlgsInMDPs.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLAlgsInMDPs.pdf)\n  + [RLbook2018.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLbook2018.pdf)\n  + [Dissecting Reinforcement Learning](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning)    \n    - [Dissecting Reinforcement Learning-Part1.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part1.pdf)   \n    - [Dissecting Reinforcement Learning-Part2.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part2.pdf) \n    - [Dissecting Reinforcement Learning-Part3.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part3.pdf) \n    - [Dissecting Reinforcement Learning-Part4.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part4.pdf) \n    - [Dissecting Reinforcement Learning-Part5.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part5.pdf)   \n    - [Dissecting Reinforcement Learning-Part6.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part6.pdf)   \n    - [Dissecting Reinforcement Learning-Part7.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part7.pdf)  \n  + [AI_CRASH_COURSE.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AI_CRASH_COURSE.pdf)   \n  \n  + [UCL Course on RL d.silver](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver)\n    - [Lecture 1: Introduction to Reinforcement Learning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/intro_RL.pdf)  \n    - [Lecture 2: Markov Decision Processes](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MDP.pdf)      \n    - [Lecture 3: Planning by Dynamic Programming](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/DP.pdf)  \n    - [Lecture 4: Model-Free Prediction](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MC-TD.pdf)      \n    - [Lecture 5: Model-Free Control](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/control.pdf)      \n    - [Lecture 6: Value Function Approximation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/FA.pdf)      \n    - [Lecture 7: Policy Gradient Methods](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/pg.pdf)      \n    - [Lecture 8: Integrating Learning and Planning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/dyna.pdf)      \n    - [Lecture 9: Exploration and Exploitation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/XX.pdf)      \n    - [Lecture 10: Case Study: RL in Classic Games](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/games.pdf)  \n    - [Lecture 11: Case Study: Deep RL](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/deep_rl_tutorial.pdf)      \n    - [Video-lectures available here](https://www.youtube.com/watch?v=2pWv7GOvuf0)          \n  + [AnIntroductiontoDeepReinforcementLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AnIntroductiontoDeepReinforcementLearning.pdf)  \n\n# Author\nChangWookJun / @changwookjun (changwookjun@gmail.com)\n"
 },
 {
  "repo": "ivanseidel/Is-Now-Illegal",
  "language": "JavaScript",
  "readme_contents": "# Is Now Illegal!\nA NERD protest against Trump's Immigration ban \ud83d\udeab\nGo to [IsNowIllegal.com](http://isnowillegal.com) and type what you want to make illegal!\n\n## What's this?\nA webapp that gives you the Donald J. Trump power. Pretend you are Trump for a few seconds, make something illegal, share with friends and have fun!\n\n## Donate\n\nThe server costs are too high and we will shutdown very soon if we don't get enough donations. For real. \ud83d\ude14\nPlease click to [Donate via Patreon](https://www.patreon.com/isnowillegal) or contact us below.\n\n## Who made this?\n![](https://github.com/ivanseidel.png?size=100)\nIvan Seidel ([github](https://github.com/ivanseidel))\n\n![](https://github.com/brunolemos.png?size=100)\nBruno Lemos ([github](https://github.com/brunolemos), [twitter](https://twitter.com/brunolemos))\n\n![](https://github.com/joaopedrovbs.png?size=100)\nJo\u00e3o Pedro ([github](https://github.com/joaopedrovbs))\n\nSee full list of [contributors](https://github.com/ivanseidel/Is-Now-Illegal/graphs/contributors)."
 },
 {
  "repo": "andrewssobral/bgslibrary",
  "language": "C++",
  "readme_contents": "## BGSLibrary\nA Background Subtraction Library\n\n[![Release](https://img.shields.io/badge/Release-3.0.0-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![License: GPL v3](https://img.shields.io/badge/License-MIT-blue.svg)](http://www.gnu.org/licenses/gpl-3.0) [![Platform: Windows, Linux, OS X](https://img.shields.io/badge/Platform-Windows%2C%20Linux%2C%20OS%20X-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![OpenCV](https://img.shields.io/badge/OpenCV-2.4.x%2C%203.x%2C%204.x-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Wrapper: Python, MATLAB](https://img.shields.io/badge/Wrapper-Java%2C%20Python%2C%20MATLAB-orange.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Algorithms](https://img.shields.io/badge/Algorithms-43-red.svg)](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n\n<p align=\"center\">\n<a href=\"https://youtu.be/_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgs_giphy2.gif\" border=\"0\" />\n</a>\n</p>\n\nLast page update: **06/08/2019**\n\nLibrary Version: **3.0.0** (see **[Build Status](https://github.com/andrewssobral/bgslibrary/wiki/Build-status)** and **[Release Notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)** for more info)\n\nThe **BGSLibrary** was developed early 2012 by [Andrews Sobral](http://andrewssobral.wixsite.com/home) to provide an easy-to-use C++ framework (wrappers for Python, Java and MATLAB are also available) for foreground-background separation in videos based on [OpenCV](http://www.opencv.org/). The bgslibrary is compatible with OpenCV 2.4.x, 3.x and 4.x, and compiles under Windows, Linux, and Mac OS X. Currently the library contains **43** algorithms. The source code is available under the [MIT license](https://opensource.org/licenses/MIT), the library is available free of charge to all users, academic and commercial.\n\n* [List of available algorithms](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n* [Algorithms benchmark](https://github.com/andrewssobral/bgslibrary/wiki/Algorithms-benchmark)\n* [Which algorithms really matter?](https://github.com/andrewssobral/bgslibrary/wiki/Which-algorithms-really-matter%3F)\n* [Library architecture](https://github.com/andrewssobral/bgslibrary/wiki/Library-architecture)\n\n* Installation instructions\n\nYou can either install BGSLibrary via [pre-built binary package](https://github.com/andrewssobral/bgslibrary/releases) or build it from source via:\n\n`git clone --recursive https://github.com/andrewssobral/bgslibrary.git`\n\n* * [Windows installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions---Windows)\n\n* * [Ubuntu / OS X installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions-Ubuntu-or-OSX)\n\nSupported Compilers are:\n\n    GCC 4.8 and above\n    Clang 3.4 and above\n    MSVC 2015, 2017, 2019\n\nOther compilers might work, but are not officially supported.\nThe bgslibrary requires some features from the ISO C++ 2014 standard.\n\n* Graphical User Interface:\n\n*  * [C++ QT](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-QT) ***(Official)***\n*  * [C++ MFC](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-MFC) ***(Deprecated)***\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-Java) ***(Obsolete)***\n\n* Wrappers:\n\n*  * [Python](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Python)\n*  * [MATLAB](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-MATLAB)\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Java)\n\n* [Docker images](https://github.com/andrewssobral/bgslibrary/wiki/Docker-images)\n* [How to integrate BGSLibrary in your own CPP code](https://github.com/andrewssobral/bgslibrary/wiki/How-to-integrate-BGSLibrary-in-your-own-CPP-code)\n* [How to contribute](https://github.com/andrewssobral/bgslibrary/wiki/How-to-contribute)\n* [List of collaborators](https://github.com/andrewssobral/bgslibrary/wiki/List-of-collaborators)\n* [Release notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)\n\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/andrewssobral/bgslibrary.svg)](https://starchart.cc/andrewssobral/bgslibrary)\n\n\nCitation\n--------\n\nIf you use this library for your publications, please cite it as:\n```\n@inproceedings{bgslibrary,\nauthor    = {Sobral, Andrews},\ntitle     = {{BGSLibrary}: An OpenCV C++ Background Subtraction Library},\nbooktitle = {IX Workshop de Vis\u00e3o Computacional (WVC'2013)},\naddress   = {Rio de Janeiro, Brazil},\nyear      = {2013},\nmonth     = {Jun},\nurl       = {https://github.com/andrewssobral/bgslibrary}\n}\n```\nA chapter about the BGSLibrary has been published in the handbook on [Background Modeling and Foreground Detection for Video Surveillance](https://sites.google.com/site/backgroundmodeling/).\n```\n@incollection{bgslibrarychapter,\nauthor    = {Sobral, Andrews and Bouwmans, Thierry},\ntitle     = {BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation},\nbooktitle = {Background Modeling and Foreground Detection for Video Surveillance},\npublisher = {CRC Press, Taylor and Francis Group.}\nyear      = {2014},\n}\n```\n\n\nDownload PDF:\n* Sobral, Andrews. BGSLibrary: An OpenCV C++ Background Subtraction Library. IX Workshop de Vis\u00e3o Computacional (WVC'2013), Rio de Janeiro, Brazil, Jun. 2013. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in brazilian-portuguese containing an english abstract).\n\n* Sobral, Andrews; Bouwmans, Thierry. \"BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation\". Chapter on the handbook \"Background Modeling and Foreground Detection for Video Surveillance\", CRC Press, Taylor and Francis Group, 2014. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in english).\n\n\nSome references\n---------------\n\nSome algorithms of the BGSLibrary were used successfully in the following papers: \n\n* (2014) Sobral, Andrews; Vacavant, Antoine. A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos. Computer Vision and Image Understanding (CVIU), 2014. ([Online](http://dx.doi.org/10.1016/j.cviu.2013.12.005)) ([PDF](http://www.researchgate.net/publication/259340906_A_comprehensive_review_of_background_subtraction_algorithms_evaluated_with_synthetic_and_real_videos))\n\n* (2013) Sobral, Andrews; Oliveira, Luciano; Schnitman, Leizer; Souza, Felippe. (**Best Paper Award**) Highway Traffic Congestion Classification Using Holistic Properties. In International Conference on Signal Processing, Pattern Recognition and Applications (SPPRA'2013), Innsbruck, Austria, Feb 2013. ([Online](http://dx.doi.org/10.2316/P.2013.798-105)) ([PDF](http://www.researchgate.net/publication/233427564_HIGHWAY_TRAFFIC_CONGESTION_CLASSIFICATION_USING_HOLISTIC_PROPERTIES))\n\n\nVideos\n------\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_qt_gui_video.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=Ccqa9KBO9_U\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_youtube.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n"
 },
 {
  "repo": "opentrack/opentrack",
  "language": "C++",
  "readme_contents": "## Intro\n\n[<img src=\"https://ci.appveyor.com/api/projects/status/n0j9h38jnif5qbe9/branch/unstable?svg=true\"/>](https://ci.appveyor.com/project/sthalik/opentrack/branch/unstable)\n\nopentrack project home is located at <<http://github.com/opentrack/opentrack>>.\n\nFor the latest **downloads** visit <<https://github.com/opentrack/opentrack/releases>> Download an `.exe` installer or a `.7z` archive. Currently installers and portable versions for Windows are available for each release. It supports [USB stick truly \"portable\" installations](https://github.com/opentrack/opentrack/wiki/portable-mode-for-USB-sticks)\n\nPlease first refer to <<https://github.com/opentrack/opentrack/wiki>>\nfor [new user guide](https://github.com/opentrack/opentrack/wiki/Quick-Start-Guide-(WIP)), [frequent answers](https://github.com/opentrack/opentrack/wiki/common-issues), specific tracker/filter\ndocumentation. See also the [gameplay video](https://www.youtube.com/watch?v=XI73ul_FnBI) with opentrack set up.\n\n## Looking for railway planning software?\n\n**Railway planning software** <<http://opentrack.ch>> had the name `opentrack` first. Apologies for the long-standing naming conflict.\n\n## Usage\n\n`opentrack` is an application dedicated to tracking user's head\nmovements and relaying the information to games and flight simulation\nsoftware.\n\n`opentrack` allows for output shaping, filtering, and operating with many input and output devices and protocols; the codebase runs Microsoft Windows, Apple OSX (currently unmaintained), and GNU/Linux.\n\nDon't be afraid to submit an **issue/feature request** if you have any problems! We're a friendly bunch.\n\n## Tracking input\n\n- PointTracker by Patrick Ruoff, freetrack-like light sources\n- Oculus Rift DK1, DK2, CV, and legacy/knockoff versions (Windows only)\n- Paper [marker support](https://github.com/opentrack/opentrack/wiki/Aruco-tracker)\n  via the ArUco library <<https://github.com/opentrack/aruco>>\n- Razer Hydra\n- Relaying via UDP from a different computer\n- Relaying UDP via FreePIE-specific Android app\n- Joystick analog axes (Windows)\n- Windows Phone [tracker](https://github.com/ZanderAdam/OpenTrack.WindowsPhone/wiki) over opentrack UDP protocol\n- Arduino with custom firmware\n- Intel RealSense 3D cameras (Windows)\n- BBC micro:bit, LEGO, sensortag support via Smalltalk<sup>[(1)](https://en.wikipedia.org/wiki/Smalltalk)[(2)](https://en.wikipedia.org/wiki/Alan_Kay)</sup>\n  [S2Bot](http://www.picaxe.com/Teaching/Other-Software/Scratch-Helper-Apps/)\n- Wiimote (Windows)\n\n## Output protocols\n\n- SimConnect for newer Microsoft Flight Simulator (Windows)\n- freetrack implementation (Windows)\n- Relaying UDP to another computer\n- Virtual joystick output (Windows, Linux, OSX)\n- Wine freetrack glue protocol (Linux, OSX)\n- X-Plane plugin (Linux)\n- Tablet-like mouse output (Windows)\n- FlightGear\n- FSUIPC for Microsoft Flight Simulator 2002/2004 (Windows)\n- SteamVR through a bridge (Windows; see <<https://github.com/r57zone/OpenVR-OpenTrack>> by @r57zone)\n\n## Credits, in chronological order\n\n- Stanis\u0142aw Halik (maintainer)\n- Wim Vriend -- author of [FaceTrackNoIR](http://facetracknoir.sourceforge.net/) that served as the initial codebase for `opentrack`. While the  code was almost entirely rewritten, we still hold on to many of `FaceTrackNoIR`'s ideas.\n- Chris Thompson (aka mm0zct, Rift and Razer Hydra author and maintainer)\n- Patrick Ruoff (PT tracker author)\n- Xavier Hallade (Intel RealSense tracker author and maintainer)\n- furax49 (hatire tracker author)\n- Michael Welter (contributor)\n- Alexander Orokhovatskiy (Russian translation; profile repository maintenance; providing hardware; translating reports from the Russian community)\n- Attila Csipa (Micro:Bit author)\n- Eike \"e4z9\" (OSX joystick output driver)\n- Wei Shuai (Wiimote tracker)\n- St\u00e9phane Lenclud (Kinect Face Tracker, Easy Tracker)\n\n## Thanks\n\n- uglyDwarf (high CON)\n- Andrzej Czarnowski (FreePIE tracker and\n  [Google Cardboard](https://github.com/opentrack/opentrack/wiki/VR-HMD-goggles-setup-----google-cardboard,-colorcross,-opendive)\n  assistance, testing)\n- Wim Vriend (original codebase author and maintainer)\n- Ryan Spicer (OSX tester, contributor)\n- Ries van Twisk (OSX tester, OSX Build Fixes, contributor)\n- Donovan Baarda (filtering/control theory expert)\n- Mathijs Groothuis (@MathijsG, dozens of bugs and other issues reported; NL translation)\n- The Russian community from the [IL-2 Sturmovik forums](https://forum.il2sturmovik.ru/) (reporting bugs, requesting important features)\n- OpenCV authors and maintainers <<https://github.com/opencv/opencv/>>.\n\n## Contributing\n\nCode, translations, \n\nPlease see [basic rules for contributing](https://github.com/opentrack/opentrack/blob/unstable/CONTRIBUTING.md). There's also a guide for [working with core code](https://github.com/opentrack/opentrack/wiki/Hacking-opentrack). For writing input and output modules you don't need this guide except for \n\n## License and warranty\n\nAlmost all code is licensed under the [ISC license](https://en.wikipedia.org/wiki/ISC_license). There are very few proprietary dependencies. There is no copyleft code. See individual files for licensing and authorship information.\n\nSee [WARRANTY.txt](WARRANTY.txt) for applying warranty terms (that is, disclaiming possible pre-existing warranty) that are in force unless the software author specifies their own warranty terms. In short, we disclaim all possible warranty and aren't responsible for any possible damage or losses.\n\nThe code is held to a high-quality standard and written with utmost care; consider this a promise without legal value. Despite doing the best we can not to injure users' equipment, software developers don't want to be dragged to courts for imagined or real issues. Disclaiming warranty is a standard practice in the field, even for expensive software like operating systems.\n\n## Building opentrack from source\n\nOn Windows, use either mingw-w64 or MS Visual Studio 2015 Update 3/newer. On other platforms use GNU or LLVM. Refer to [Visual C++ 2015 build instructions](https://github.com/opentrack/opentrack/wiki/Building-under-MS-Visual-C---2017-and-later).\n"
 },
 {
  "repo": "abhiTronix/vidgear",
  "language": "Python",
  "readme_contents": "<!--\r\n===============================================\r\nvidgear library source-code is deployed under the Apache 2.0 License:\r\n\r\nCopyright (c) 2019-2020 Abhishek Thakur(@abhiTronix) <abhi.una12@gmail.com>\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n===============================================\r\n-->\r\n\r\n<h1 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/vidgear.webp\" alt=\"VidGear\" title=\"Logo designed by Abhishek Thakur(@abhiTronix), under CC-BY-NC-SA 4.0 License\" width=\"80%\"/>\r\n</h1>\r\n<h2 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/tagline.svg\" alt=\"VidGear tagline\" width=\"40%\"/>\r\n</h2>\r\n\r\n<div align=\"center\">\r\n\r\n[Releases][release]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Gears][gears]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentation][docs]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Installation][installation]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[License](#license)\r\n\r\n[![Build Status][travis-cli]][travis] [![Codecov branch][codecov]][code] [![Build Status][appveyor]][app]\r\n\r\n[![Glitter chat][gitter-bagde]][gitter] [![PyPi version][pypi-badge]][pypi] [![Twitter][twitter-badge]][twitter-intent]\r\n\r\n[![Code Style][black-badge]][black]\r\n\r\n</div>\r\n\r\n&nbsp;\r\n\r\nVidGear is a high-performance Video Processing python library that provides an easy-to-use, highly extensible, **Multi-Threaded + Asyncio wrapper** around many state-of-the-art specialized libraries like *[OpenCV][opencv], [FFmpeg][ffmpeg], [ZeroMQ][zmq], [picamera][picamera], [starlette][starlette], [pafy][pafy] and [python-mss][mss]* at its backend, and enable us to flexibly exploit their internal parameters and methods, while silently delivering robust error-handling and unparalleled real-time performance.\r\n\r\nVidGear primarily focuses on simplicity, and thereby lets programmers and software developers to easily integrate and perform Complex Video Processing Tasks, in just a few lines of code.\r\n\r\n&nbsp;\r\n\r\nThe following **functional block diagram** clearly depicts the generalized functioning of VidGear APIs:\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/gears_fbd.webp\" alt=\"@Vidgear Functional Block Diagram\" />\r\n</p>\r\n\r\n&nbsp;\r\n\r\n# Table of Contents\r\n\r\n* [**TL;DR**](#tldr)\r\n* [**Getting Started**](#getting-started)\r\n* [**Gears: What are these?**](#gears)\r\n  * [**CamGear**](#camgear)\r\n  * [**PiGear**](#pigear)\r\n  * [**VideoGear**](#videogear)\r\n  * [**ScreenGear**](#screengear)\r\n  * [**WriteGear**](#writegear)\r\n  * [**StreamGear**](#streamgear)\r\n  * [**NetGear**](#netgear)\r\n  * [**WebGear**](#webgear)\r\n  * [**NetGear_Async**](#netgear_async)\r\n* [**Documentation**](#documentation)\r\n* [**Community Channel**](#community-channel)\r\n* [**Contributions & Support**](#contributions--support)\r\n  * [**Support**](#support)\r\n  * [**Contributors**](#contributors)\r\n* [**Citation**](#citation)\r\n* [**Copyright**](#copyright)\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# TL;DR\r\n  \r\n#### What is vidgear?\r\n\r\n> *\"VidGear is a High-Performance Framework that provides an all-in-one complete Video Processing solution for building real-time applications in python.\"*\r\n\r\n#### What does it do?\r\n\r\n> *\"VidGear can read, write, process, send & receive video frames from/to various devices in real-time.\"*\r\n\r\n#### What is its purpose?\r\n\r\n> *\"Built with simplicity in mind, VidGear lets programmers and software developers to easily integrate and perform complex Video Processing tasks in their existing or new applications, in just a [few lines of code][switch_from_cv]. Beneficial for both, if you're new to programming with Python language or already a pro at it.\"*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## Getting Started\r\n\r\nIf this is your first time using VidGear, head straight to the [Installation \u27b6][installation] to install VidGear.\r\n\r\nOnce you have VidGear installed, **checkout its well-documented [Gears \u27b6][gears]**\r\n\r\nAlso, if you're already familar with [OpenCV][opencv] library, then see [Switching from OpenCV \u27b6][switch_from_cv]\r\n\r\nOr, if you're just getting started with OpenCV with Python, then see [here \u27b6](https://abhitronix.github.io/vidgear/help/general_faqs/#im-new-to-python-programming-or-its-usage-in-computer-vision-how-to-use-vidgear-in-my-projects)\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## Gears: What are these?\r\n\r\n> **VidGear is built with multiple [Gears][gears] (APIs), each with some unique functionality.**\r\n\r\nEach of these APIs is exclusively designed to handle/control different device-specific video streams, network streams, and media encoders. These APIs provide an easy-to-use, highly extensible, multi-threaded and asyncio layer above state-of-the-art libraries under the hood to exploit their internal parameters and methods flexibly while providing robust error-handling and unparalleled performance. \r\n\r\n**These Gears can be classified as follows:**\r\n\r\n**A. VideoCapture Gears:**\r\n\r\n  * [**CamGear:**](#camgear) Multi-threaded API targeting various IP-USB-Cameras/Network-Streams/YouTube-Video-URLs.\r\n  * [**PiGear:**](#pigear) Multi-threaded API targeting  various Raspberry Pi Camera Modules.\r\n  * [**ScreenGear:**](#screengear) Multi-threaded ultra-fast Screencasting.    \r\n  * [**VideoGear:**](#videogear) Common API with internal [Video Stabilizer](/gears/stabilizer/overview/) wrapper.\r\n\r\n**B. VideoWriter Gears:**\r\n\r\n  * [**WriteGear:**](#writegear) Handles Flexible Lossless Video Encoding and Compression.\r\n\r\n**C. Streaming Gears:**\r\n\r\n  * [**StreamGear**](#streamgear): Handles Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats.\r\n\r\n\r\n**D. Network Gears:**\r\n\r\n  * [**NetGear:**](#netgear) Handles high-performance video-frames & data transfer between interconnecting systems over the network.\r\n\r\n  * **Asynchronous I/O Network Gears:**\r\n\r\n    * [**WebGear:**](#webgear) ASGI Video Server that can send live video-frames to any web browser on the network.\r\n    * [**NetGear_Async:**](#netgear_async) Immensely Memory-efficient Asyncio video-frames network messaging framework.\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## CamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/camgear.webp\" alt=\"CamGear Functional Block Diagram\" width=\"45%\"/>\r\n</p>\r\n\r\n> *CamGear can grab ultra-fast frames from diverse range of devices/streams, which includes almost any IP/USB Cameras, multimedia video file format ([_upto 4k tested_][test-4k]), various network stream protocols such as `http(s), rtp, rstp, rtmp, mms, etc.`, plus support for live Gstreamer's stream pipeline and YouTube video/live-streams URLs.*\r\n\r\nCamGear provides a flexible, high-level multi-threaded wrapper around `OpenCV's` [VideoCapture class][opencv-vc] with access almost all of its available parameters and also employs [`pafy`][pafy] python APIs for live [YouTube streaming][youtube-doc]. Furthermore, CamGear relies exclusively on [**Threaded Queue mode**][TQM-doc] for ultra-fast, error-free and synchronized frame handling.\r\n\r\n### CamGear API Guide:\r\n\r\n[**>>> Usage Guide**][camgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## VideoGear\r\n\r\n> *VideoGear API provides a special internal wrapper around VidGear's exclusive [**Video Stabilizer**][stablizer-doc] class.*\r\n\r\nVideoGear also act as a Common API, that provides an internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs and their parameters, with a special `enablePiCamera` boolean flag.\r\n\r\nVideoGear is basically ideal when you need to switch to different video sources without changing your code much. Also, it enables easy stabilization for various video-streams _(real-time or not)_  with minimum effort and using way fewer lines of code.\r\n\r\n\r\n**Below is a snapshot of a VideoGear Stabilizer in action  (_See its detailed usage [here][stablizer-doc-ex]_):**\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://github.com/abhiTronix/Imbakup/raw/master/Images/stabilizer.gif\" alt=\"VideoGear Stabilizer in action!\"/>\r\n  <br>\r\n  <sub><i>Original Video Courtesy <a href=\"http://liushuaicheng.org/SIGGRAPH2013/database.html\" title=\"opensourced video samples database\">@SIGGRAPH2013</a></i></sub>\r\n</p>\r\n\r\n**Code to generate above result:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import VideoGear\r\nimport numpy as np\r\nimport cv2\r\n\r\n# open any valid video stream with stabilization enabled(`stabilize = True`)\r\nstream_stab = VideoGear(source = \"test.mp4\", stabilize = True).start()\r\n\r\n# open same stream without stabilization for comparison\r\nstream_org = VideoGear(source = \"test.mp4\").start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read stabilized frames\r\n    frame_stab = stream_stab.read()\r\n\r\n    # check for stabilized frame if Nonetype\r\n    if frame_stab is None:\r\n        break\r\n\r\n    # read un-stabilized frame\r\n    frame_org = stream_org.read()\r\n\r\n    # concatenate both frames\r\n    output_frame = np.concatenate((frame_org, frame_stab), axis=1)\r\n\r\n    # put text over concatenated frame\r\n    cv2.putText(\r\n        output_frame, \"Before\", (10, output_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n    cv2.putText(\r\n        output_frame, \"After\", (output_frame.shape[1] // 2 + 10, output_frame.shape[0] - 10),\r\n        cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Stabilized Frame\", output_frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close both video streams\r\nstream_org.stop()\r\nstream_stab.stop()\r\n```\r\n\r\n### VideoGear API Guide:\r\n\r\n[**>>> Usage Guide**][videogear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## PiGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/picam2.webp\" alt=\"PiGear\" width=\"50%\" />\r\n</p>\r\n\r\n> *PiGear is similar to CamGear but made to support various Raspberry Pi Camera Modules *(such as [OmniVision OV5647 Camera Module][OV5647-picam] and [Sony IMX219 Camera Module][IMX219-picam])*.*\r\n\r\nPiGear provides a flexible multi-threaded wrapper around complete [picamera](https://picamera.readthedocs.io/en/release-1.13/index.html) python library, and also provides us the ability to exploit almost all of its parameters like `brightness, saturation, sensor_mode, iso, exposure, etc.` effortlessly. Furthermore, PiGear supports multiple camera modules, such as in case of Raspberry Pi Compute module IO boards.\r\n\r\nBest of all, PiGear provides excellent error-handling with features like a **Threaded Internal Timer** - that keeps active track of any frozen-threads/hardware-failures robustly, and exit safely if it does occurs, i.e. If you're running PiGear API in your script, and someone accidentally pulls Camera module cable out, instead of going into possible kernel panic, PiGear will exit safely to save resources. \r\n\r\n\r\n**Code to open picamera stream with variable parameters in PiGear API:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import PiGear\r\nimport cv2\r\n\r\n# add various Picamera tweak parameters to dictionary\r\noptions = {\"hflip\": True, \"exposure_mode\": \"auto\", \"iso\": 800, \"exposure_compensation\": 15, \"awb_mode\": \"horizon\", \"sensor_mode\": 0}\r\n\r\n# open pi video stream with defined parameters\r\nstream = PiGear(resolution = (640, 480), framerate = 60, logging = True, **options).start() \r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n### PiGear API Guide:\r\n\r\n[**>>> Usage Guide**][pigear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## ScreenGear\r\n\r\n> *ScreenGear is designed exclusively for ultra-fast Screencasting, that means it can grab frames from your monitor in real-time, either by define an area on the computer screen, or full-screen, at the expense of inconsiderable latency. ScreenGear also seamlessly support frame capturing from multiple monitors.*\r\n\r\nScreenGear implements a multi-threaded wrapper around [**python-mss**][mss] python library API and also supports a easy and flexible direct internal parameter manipulation. \r\n\r\n**Below is a snapshot of a ScreenGear API in action:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/screengear.gif\" alt=\"ScreenGear in action!\"/>\r\n</p>\r\n\r\n**Code to generate the above results:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import ScreenGear\r\nimport cv2\r\n\r\n# open video stream with default parameters\r\nstream = ScreenGear().start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n\r\n### ScreenGear API Guide:\r\n\r\n[**>>> Usage Guide**][screengear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n## WriteGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/writegear.webp\" alt=\"WriteGear Functional Block Diagram\" width=\"70%\" />\r\n</p>\r\n\r\n> *WriteGear handles various powerful Writer Tools that provide us the freedom to do almost anything imagine with multimedia files.*\r\n\r\nWriteGear API provides a complete, flexible and robust wrapper around [**FFmpeg**][ffmpeg], a leading multimedia framework. With WriteGear, we can process real-time frames into a lossless compressed video-file with any suitable specification in just few easy lines of codes. These specifications include setting video/audio properties such as `bitrate, codec, framerate, resolution, subtitles,  etc.`, and also performing complex tasks such as multiplexing video with audio in real-time _(see this [doc][live-audio-doc])_, while handling all errors robustly. \r\n\r\nBest of all, WriteGear grants the complete freedom to play with any FFmpeg parameter with its exclusive **Custom Commands function** _(see this [doc][custom-command-doc])_, without relying on any Third-party library.\r\n\r\nIn addition to this, WriteGear also provides flexible access to [**OpenCV's VideoWriter API**][opencv-writer] which provides some basic tools for video frames encoding but without compression.\r\n\r\n**WriteGear primarily operates in the following two modes:**\r\n\r\n  * **Compression Mode:** In this mode, WriteGear utilizes powerful [**FFmpeg**][ffmpeg] inbuilt encoders to encode lossless multimedia files. This mode provides us the ability to exploit almost any parameter available within FFmpeg, effortlessly and flexibly, and while doing that it robustly handles all errors/warnings quietly. *You can find more about this mode [here \u27b6][cm-writegear-doc]*\r\n\r\n  * **Non-Compression Mode:**  In this mode, WriteGear utilizes basic [**OpenCV's inbuilt VideoWriter API**][opencv-vw] tools. This mode also supports all parameters manipulation available within VideoWriter API, but it lacks the ability to manipulate encoding parameters and other important features like video compression, audio encoding, etc. *You can learn about this mode [here \u27b6][ncm-writegear-doc]*\r\n\r\n### WriteGear API Guide:\r\n\r\n[**>>> Usage Guide**][writegear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## StreamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/streamgear_flow.webp\" alt=\"NetGear API\" width=80%/>\r\n</p>\r\n\r\n\r\n> *StreamGear automates transcoding workflow for generating Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats (such as MPEG-DASH) in just few lines of python code.*\r\n\r\nStreamGear provides a standalone, highly extensible and flexible wrapper around [**FFmpeg**][ffmpeg] - a leading multimedia framework, for generating chunked-encoded media segments of the content.\r\n\r\nSteamGear API automatically transcodes source videos/audio files & real-time frames, and breaks them into a sequence of multiple smaller chunks/segments (typically 2-4 seconds in length) at different quality levels _(i.e. different bitrates or spatial resolutions)_. It also creates a Manifest file _(such as MPD in-case of DASH)_ that describes these segment information _(timing, URL, media characteristics like video resolution and bit rates)_, and is provided to the client prior to the streaming session. Thereby, segments are served on a web server and can be downloaded through HTTP standard compliant GET requests. This makes it possible to stream videos at different quality levels, and to switch in the middle of a video from one quality level to another one \u2013 if bandwidth permits \u2013 on a per segment basis.\r\n\r\n\r\nSteamGear currently only supports [**MPEG-DASH**](https://www.encoding.com/mpeg-dash/) _(Dynamic Adaptive Streaming over HTTP, ISO/IEC 23009-1)_ , but other adaptive streaming technologies such as Apple HLS, Microsoft Smooth Streaming, will be added soon.\r\n\r\n**StreamGear primarily works in two independent modes for transcoding which serves different purposes:**\r\n\r\n  * **Single-Source Mode:** In this mode, StreamGear transcodes entire video/audio file _(as opposed to frames by frame)_ into a sequence of multiple smaller chunks/segments for streaming. This mode works exceptionally well, when you're transcoding lossless long-duration videos(with audio) for streaming and required no extra efforts or interruptions. But on the downside, the provided source cannot be changed or manipulated before sending onto FFmpeg Pipeline for processing.  This mode can be easily activated by assigning suitable video path as input to `-video_source` attribute, during StreamGear initialization. ***Learn more about this mode [here \u27b6][ss-mode-doc]***\r\n\r\n  * **Real-time Frames Mode:** When no valid input is received on `-video_source` attribute, StreamGear API activates this mode where it directly transcodes video-frames _(as opposed to a entire file)_, into a sequence of multiple smaller chunks/segments for streaming. In this mode, StreamGear supports real-time [`numpy.ndarray`](https://numpy.org/doc/1.18/reference/generated/numpy.ndarray.html#numpy-ndarray) frames, and process them over FFmpeg pipeline. But on the downside, audio has to added manually _(as separate source)_ for streams. ***Learn more about this mode [here \u27b6][rtf-mode-doc]***\r\n\r\n\r\n### StreamGear API Guide:\r\n\r\n[**>>> Usage Guide**][streamgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/netgear.webp\" alt=\"NetGear API\" width=65%/>\r\n</p>\r\n\r\n> *NetGear is exclusively designed to transfer video frames synchronously and asynchronously between interconnecting systems over the network in real-time.*\r\n\r\nNetGear implements a high-level wrapper around [**PyZmQ**][pyzmq] python library that contains python bindings for [ZeroMQ][zmq] - a high-performance asynchronous distributed messaging library that provides a message queue, but unlike message-oriented middleware, its system can run without a dedicated message broker. \r\n\r\nNetGear provides seamless support for [*Bidirectional data transmission*][netgear_bidata_doc] between receiver(client) and sender(server) through bi-directional synchronous messaging patterns such as zmq.PAIR _(ZMQ Pair Pattern)_ & zmq.REQ/zmq.REP _(ZMQ Request/Reply Pattern)_. \r\n\r\nNetGear also supports real-time [*Frame Compression capabilities*][netgear_compression_doc] for optimizing performance while sending the frames directly over the network, by encoding the frame before sending it and decoding it on the client's end automatically in real-time. \r\n\r\nFor security, NetGear implements easy access to ZeroMQ's powerful, smart & secure Security Layers, that enables [*Strong encryption on data*][netgear_security_doc], and unbreakable authentication between the Server and the Client with the help of custom certificates/keys and brings easy, standardized privacy and authentication for distributed systems over the network. \r\n\r\nBest of all, NetGear can robustly handle [*Multiple Server-Systems*][netgear_multi_server_doc] and [*Multiple Client-Systems*][netgear_multi_client_doc] and at once, thereby providing access to seamless Live Streaming of the multiple device in a network at the same time.\r\n\r\n\r\n**NetGear as of now seamlessly supports three ZeroMQ messaging patterns:**\r\n\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear API Guide:\r\n\r\n[**>>> Usage Guide**][netgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## WebGear\r\n\r\n> *WebGear is a powerful [ASGI](https://asgi.readthedocs.io/en/latest/) Video-streamer API, that is built upon [Starlette](https://www.starlette.io/) - a lightweight ASGI framework/toolkit, which is ideal for building high-performance asyncio services.*\r\n\r\nWebGear API provides a highly extensible and flexible asyncio wrapper around [Starlette][starlette] ASGI application, and provides easy access to its complete framework. Thereby, WebGear API can flexibly interact with the Starlette's ecosystem of shared middleware and mountable applications, and its various [Response classes](https://www.starlette.io/responses/), [Routing tables](https://www.starlette.io/routing/), [Static Files](https://www.starlette.io/staticfiles/), [Templating engine(with Jinja2)](https://www.starlette.io/templates/), etc. \r\n\r\nIn layman's terms, WebGear can acts as powerful **Video Streaming Server** that transfers live video-frames to any web browser on a network. It addition to this, WebGear API also provides a special internal wrapper around [VideoGear](#videogear), which itself provides internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs thereby granting it exclusive power for streaming frames incoming from any device/source, such as streaming [Stabilization enabled Video][stabilize_webgear_doc] in real-time.\r\n\r\n**Below is a snapshot of a WebGear Video Server in action on the Mozilla Firefox browser:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/webgear.gif\" alt=\"WebGear in action!\" width=\"70%\" />\r\n  <br>\r\n  <sub><i>WebGear Video Server at <a href=\"http://localhost:8000/\" title=\"default address\">http://localhost:8000/</a> address.</i></sub>\r\n</p>\r\n\r\n**Code to generate the above result:**\r\n\r\n```python\r\n# import required libraries\r\nimport uvicorn\r\nfrom vidgear.gears.asyncio import WebGear\r\n\r\n#various performance tweaks\r\noptions = {\"frame_size_reduction\": 40, \"frame_jpeg_quality\": 80, \"frame_jpeg_optimize\": True, \"frame_jpeg_progressive\": False}\r\n\r\n#initialize WebGear app  \r\nweb = WebGear(source = \"foo.mp4\", logging = True, **options)\r\n\r\n#run this app on Uvicorn server at address http://localhost:8000/\r\nuvicorn.run(web(), host='localhost', port=8000)\r\n\r\n#close app safely\r\nweb.shutdown()\r\n```\r\n\r\n### WebGear API Guide:\r\n\r\n[**>>> Usage Guide**][webgear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear_Async \r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/zmq_asyncio.webp\" alt=\"WebGear in action!\" width=\"70%\"/>\r\n</p>\r\n\r\n> _NetGear_Async can generate double performance as compared to [NetGear API](#netgear) at about 1/3rd of memory consumption, and also provide complete server-client handling with various options to use variable protocols/patterns similar to NetGear, but it doesn't support any [NetGear's Exclusive Modes][netgear-exm] yet._\r\n\r\nNetGear_Async is an asyncio videoframe messaging framework, built on [`zmq.asyncio`][asyncio-zmq], and powered by high-performance asyncio event loop called [**`uvloop`**][uvloop] to achieve unmatchable high-speed and lag-free video streaming over the network with minimal resource constraints. Basically, this API is able to transfer thousands of frames in just a few seconds without causing any significant load on your system. \r\n\r\nNetGear_Async provides complete server-client handling and options to use variable protocols/patterns similar to [NetGear API](#netgear) but doesn't support any [NetGear Exclusive modes][netgear-exm] yet. Furthermore, NetGear_Async allows us to  define our own custom Server Source to manipulate frames easily before sending them across the network(see this [doc][netgear_Async-cs] example).\r\n\r\nNetGear_Async as of now supports [all four ZeroMQ messaging patterns](#attributes-and-parameters-wrench):\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_ \r\n* [**`zmq.PUSH/zmq.PULL`**][zmq-pull-push] _(ZMQ Push/Pull Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear_Async API Guide:\r\n\r\n[**>>> Usage Guide**][netgear_async-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Documentation\r\n\r\nThe complete documentation for all VidGear APIs can be found in the link below:\r\n\r\n* [**Documentation - English**][docs]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n# Contributions & Support\r\n\r\nContributions are welcome, We'd love your contribution to VidGear in order to fix bugs or to implement new features!  \r\n\r\nPlease see our **[Contribution Guidelines](contributing.md)** for more details.\r\n\r\n### Support\r\n\r\n**VidGear is free, but rely on your support.** \r\n\r\nSending a donation using link below is **extremely** helpful in keeping VidGear development alive:\r\n\r\n[![ko-fi][kofi-badge]][kofi]\r\n\r\n### Contributors\r\n\r\n<a href=\"https://github.com/abhiTronix/vidgear/graphs/contributors\">\r\n  <img src=\"https://contributors-img.web.app/image?repo=abhiTronix/vidgear\" />\r\n</a>\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Community Channel\r\n\r\nIf you've come up with some new idea, or looking for the fastest way troubleshoot your problems, then *join our [Gitter community channel \u27b6][gitter]*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# Citation\r\n\r\nHere is a Bibtex entry you can use to cite this project in a publication:\r\n\r\n\r\n```BibTeX\r\n@misc{vidgear,\r\n    author = {Abhishek Thakur},\r\n    title = {vidgear},\r\n    howpublished = {\\url{https://github.com/abhiTronix/vidgear}},\r\n    year = {2019-2020}\r\n  }\r\n```\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Copyright\r\n\r\n**Copyright \u00a9 abhiTronix 2019-2020**\r\n\r\nThis library is released under the **[Apache 2.0 License][license]**.\r\n\r\n\r\n\r\n\r\n<!--\r\nBadges\r\n-->\r\n\r\n[appveyor]:https://img.shields.io/appveyor/ci/abhitronix/vidgear.svg?style=for-the-badge&logo=appveyor\r\n[codecov]:https://img.shields.io/codecov/c/github/abhiTronix/vidgear/testing?style=for-the-badge&logo=codecov\r\n[travis-cli]:https://img.shields.io/travis/com/abhiTronix/vidgear/testing?logo=travis&style=for-the-badge\r\n[prs-badge]:https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAABC0lEQVRYhdWVPQoCMRCFX6HY2ghaiZUXsLW0EDyBrbWtN/EUHsHTWFnYyCL4gxibVZZlZzKTnWz0QZpk5r0vIdkF/kBPAMOKeddE+CQPKoc5Yt5cTjBMdQSwDQToWgBJAn3jmhqgltapAV6E6b5U17MGGAUaUj07TficMfIBZDV6vxowBm1BP9WbSQE4o5h9IjPJmy73TEPDDxVmoZdQrQ5jRhly9Q8tgMUXkIIWn0oG4GYQfAXQzz1PGoCiQndM7b4RgJay/h7zBLT3hASgoKjamQJMreKf0gfuAGyYtXEIAKcL/Dss15iq6ohXghozLYiAMxPuACwtIT4yeQUxAaLrZwAoqGRKGk7qDSYTfYQ8LuYnAAAAAElFTkSuQmCC\r\n[twitter-badge]:https://img.shields.io/badge/Tweet-Now-blue.svg?style=for-the-badge&logo=twitter\r\n[pypi-badge]:https://img.shields.io/pypi/v/vidgear.svg?style=for-the-badge&logo=pypi\r\n[gitter-bagde]:https://img.shields.io/badge/Chat-Gitter-blue.svg?style=for-the-badge&logo=gitter\r\n[Coffee-badge]:https://abhitronix.github.io/img/vidgear/orange_img.png\r\n[kofi-badge]:https://www.ko-fi.com/img/githubbutton_sm.svg\r\n[black-badge]:https://img.shields.io/badge/code%20style-black-000000.svg?style=for-the-badge&logo=github\r\n\r\n\r\n<!--\r\nInternal URLs\r\n-->\r\n\r\n[release]:https://github.com/abhiTronix/vidgear/releases/latest\r\n[pypi]:https://pypi.org/project/vidgear/\r\n[gitter]:https://gitter.im/vidgear/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\r\n[twitter-intent]:https://twitter.com/intent/tweet?url=https%3A%2F%2Fabhitronix.github.io%2Fvidgear&via%20%40abhi_una12&text=Checkout%20VidGear%20-%20A%20High-Performance%20Video-Processing%20Python%20Framework.&hashtags=vidgear%20%23videoprocessing%20%23python%20%23threaded%20%23asyncio\r\n[coffee]:https://www.buymeacoffee.com/2twOXFvlA\r\n[kofi]: https://ko-fi.com/W7W8WTYO\r\n[license]:https://github.com/abhiTronix/vidgear/blob/master/LICENSE\r\n[travis]:https://travis-ci.com/github/abhiTronix/vidgear\r\n[app]:https://ci.appveyor.com/project/abhiTronix/vidgear\r\n[code]:https://codecov.io/gh/abhiTronix/vidgear\r\n\r\n[test-4k]:https://github.com/abhiTronix/vidgear/blob/e0843720202b0921d1c26e2ce5b11fadefbec892/vidgear/tests/benchmark_tests/test_benchmark_playback.py#L65\r\n[bs_script_dataset]:https://github.com/abhiTronix/vidgear/blob/testing/scripts/bash/prepare_dataset.sh\r\n\r\n[faq]:https://abhitronix.github.io/vidgear/help/get_help/#frequently-asked-questions\r\n[doc-vidgear-purpose]:https://abhitronix.github.io/vidgear/help/motivation/#why-is-vidgear-a-thing\r\n[live-audio-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/usage/#using-compression-mode-with-live-audio-input\r\n[ffmpeg-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/ffmpeg_install/\r\n[youtube-doc]:https://abhitronix.github.io/vidgear/gears/camgear/usage/#using-camgear-with-youtube-videos\r\n[TQM-doc]:https://abhitronix.github.io/vidgear/bonus/TQM/#threaded-queue-mode\r\n[camgear-doc]:https://abhitronix.github.io/vidgear/gears/camgear/overview/\r\n[stablizer-doc]:https://abhitronix.github.io/vidgear/gears/stabilizer/overview/\r\n[stablizer-doc-ex]:https://abhitronix.github.io/vidgear/gears/videogear/usage/#using-videogear-with-video-stabilizer-backend\r\n[videogear-doc]:https://abhitronix.github.io/vidgear/gears/videogear/overview/\r\n[pigear-doc]:https://abhitronix.github.io/vidgear/gears/pigear/overview/\r\n[cm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/overview/\r\n[ncm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/non_compression/overview/\r\n[screengear-doc]:https://abhitronix.github.io/vidgear/gears/screengear/overview/\r\n[streamgear-doc]:https://abhitronix.github.io/vidgear/gears/streamgear/overview/\r\n[writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/introduction/\r\n[netgear-doc]:https://abhitronix.github.io/vidgear/gears/netgear/overview/\r\n[webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/overview/\r\n[netgear_async-doc]:https://abhitronix.github.io/vidgear/gears/netgear_async/overview/\r\n[drop35]:https://github.com/abhiTronix/vidgear/issues/99\r\n[custom-command-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/cciw/\r\n[advanced-webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/\r\n[netgear_bidata_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/bidirectional_mode/\r\n[netgear_compression_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/compression/\r\n[netgear_security_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/secure_mode/\r\n[netgear_multi_server_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_server/\r\n[netgear_multi_client_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_client/\r\n[netgear-exm]: https://abhitronix.github.io/vidgear/gears/netgear/overview/#modes-of-operation\r\n[stabilize_webgear_doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/#using-webgear-with-real-time-video-stabilization-enabled\r\n[netgear_Async-cs]: https://abhitronix.github.io/vidgear/gears/netgear_async/usage/#using-netgear_async-with-a-custom-sourceopencv\r\n[installation]:https://abhitronix.github.io/vidgear/installation/\r\n[gears]:https://abhitronix.github.io/vidgear/gears\r\n[switch_from_cv]:https://abhitronix.github.io/vidgear/switch_from_cv/\r\n[ss-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#a-single-source-mode\r\n[rtf-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#b-real-time-frames-mode\r\n[docs]: https://abhitronix.github.io/vidgear\r\n\r\n<!--\r\nExternal URLs\r\n-->\r\n[asyncio-zmq]:https://pyzmq.readthedocs.io/en/latest/api/zmq.asyncio.html\r\n[uvloop]: https://github.com/MagicStack/uvloop\r\n[uvloop-ns]: https://github.com/MagicStack/uvloop/issues/14\r\n[ffmpeg]:https://www.ffmpeg.org/\r\n[flake8]: https://flake8.pycqa.org/en/latest/\r\n[black]: https://github.com/psf/black\r\n[pytest]:https://docs.pytest.org/en/latest/\r\n[opencv-writer]:https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5\r\n[OpenCV-windows]:https://www.learnopencv.com/install-opencv3-on-windows/\r\n[OpenCV-linux]:https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/\r\n[OpenCV-pi]:https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/\r\n[starlette]:https://www.starlette.io/\r\n[uvicorn]:http://www.uvicorn.org/\r\n[daphne]:https://github.com/django/daphne/\r\n[hypercorn]:https://pgjones.gitlab.io/hypercorn/\r\n[prs]:http://makeapullrequest.com\r\n[opencv]:https://github.com/opencv/opencv\r\n[picamera]:https://github.com/waveform80/picamera\r\n[pafy]:https://github.com/mps-youtube/pafy\r\n[pyzmq]:https://github.com/zeromq/pyzmq\r\n[zmq]:https://zeromq.org/\r\n[mss]:https://github.com/BoboTiG/python-mss\r\n[pip]:https://pip.pypa.io/en/stable/installing/\r\n[opencv-vc]:https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#a57c0e81e83e60f36c83027dc2a188e80\r\n[OV5647-picam]:https://github.com/techyian/MMALSharp/doc/OmniVision-OV5647-Camera-Module\r\n[IMX219-picam]:https://github.com/techyian/MMALSharp/doc/Sony-IMX219-Camera-Module\r\n[opencv-vw]:https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html\r\n[yt-dl]:https://github.com/ytdl-org/youtube-dl/\r\n[numpy]:https://github.com/numpy/numpy\r\n[zmq-pair]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pair.html\r\n[zmq-req-rep]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/client_server.html\r\n[zmq-pub-sub]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pubsub.html\r\n[zmq-pull-push]: https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pushpull.html#push-pull\r\n[picamera-setting]:https://picamera.readthedocs.io/en/release-1.13/quickstart.html"
 },
 {
  "repo": "tebelorg/RPA-Python",
  "language": "Python",
  "readme_contents": "# RPA for Python :snake:\n\n[**Use Cases**](#use-cases)&ensp;|&ensp;[**API Reference**](#api-reference)&ensp;|&ensp;[**About & Credits**](#about--credits)&ensp;|&ensp;[**PyCon Video**](https://www.youtube.com/watch?v=F2aQKWx_EAE)&ensp;|&ensp;[**v1.27**](https://github.com/tebelorg/RPA-Python/releases)\n\n>_This tool was previously known as TagUI for Python. [More details](https://github.com/tebelorg/RPA-Python/issues/100) on the name change, which is backward compatible so existing scripts written with `import tagui as t` and `t.function()` will continue to work._\n\n![RPA for Python demo in Jupyter notebook](https://raw.githubusercontent.com/tebelorg/Tump/master/tagui_python.gif)\n\nTo install this Python package for RPA (robotic process automation) -\n```\npip install rpa\n```\n\nTo use it in Jupyter notebook, Python script or interactive shell -\n```python\nimport rpa as r\n```\n\nNotes on different operating systems and optional visual automation mode -\n- :rainbow_flag: **Windows -** if visual automation is cranky, try setting your display zoom level to recommended % or 100%\n- :apple: **macOS -** Catalina update introduces tighter app security, see solutions for [PhantomJS](https://github.com/tebelorg/RPA-Python/issues/79) and [Java popups](https://github.com/tebelorg/RPA-Python/issues/78)\n- :penguin: **Linux -** visual automation mode requires special setup on Linux, see how to [install OpenCV and Tesseract](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html)\n\n# Use Cases\n\nRPA for Python's simple and powerful API makes robotic process automation fun! You can use it to quickly automate repetitive time-consuming tasks, whether the tasks involve websites, desktop applications, or the command line.\n\n#### WEB AUTOMATION&ensp;:spider_web:\n```python\nr.init()\nr.url('https://www.google.com')\nr.type('//*[@name=\"q\"]', 'decentralization[enter]')\nprint(r.read('result-stats'))\nr.snap('page', 'results.png')\nr.close()\n```\n\n#### VISUAL AUTOMATION&ensp;:see_no_evil:\n```python\nr.init(visual_automation = True)\nr.dclick('outlook_icon.png')\nr.click('new_mail.png')\n...\nr.type('message_box.png', 'message')\nr.click('send_button.png')\nr.close()\n```\n\n#### OCR AUTOMATION&ensp;\ud83e\uddff\n```python\nr.init(visual_automation = True, chrome_browser = False)\nprint(r.read('pdf_window.png'))\nprint(r.read('image_preview.png'))\nr.hover('anchor_element.png')\nprint(r.read(r.mouse_x(), r.mouse_y(), r.mouse_x() + 400, r.mouse_y() + 200))\nr.close()\n```\n\n#### KEYBOARD AUTOMATION&ensp;:musical_keyboard:\n```python\nr.init(visual_automation = True, chrome_browser = False)\nr.keyboard('[cmd][space]')\nr.keyboard('safari[enter]')\nr.keyboard('[cmd]t')\nr.keyboard('joker[enter]')\nr.wait(2.5)\nr.snap('page.png', 'results.png')\nr.close()\n```\n\n#### MOUSE AUTOMATION&ensp;:mouse:\n```python\nr.init(visual_automation = True)\nr.type(600, 300, 'open source')\nr.click(900, 300)\nr.snap('page.bmp', 'results.bmp')\nr.hover('button_to_drag.bmp')\nr.mouse('down')\nr.hover(r.mouse_x() + 300, r.mouse_y())\nr.mouse('up')\nr.close()\n```\n\n# API Reference\n\nCheck out [sample Python script](https://github.com/tebelorg/RPA-Python/blob/master/sample.py), [RPA Challenge solution](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610518196), and [RedMart groceries example](https://github.com/tebelorg/RPA-Python/issues/24). To automate Chrome browser invisibly, see this [simple hack](https://github.com/tebelorg/RPA-Python/issues/133#issuecomment-634113838). To run 20-30X faster, without normal UI interaction delays, [see this advanced hack](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610532082).\n\n#### ELEMENT IDENTIFIERS\nAn element identifier helps to tell RPA for Python exactly which element on the user interface you want to interact with. For example, //\\*[@id=\"email\"] is an XPath pointing to the webpage element having the id attribute \"email\".\n\n- :globe_with_meridians: For web automation, the web element identifier can be XPath selector, CSS selector, or the following attributes - id, name, class, title, aria-label, text(), href, in decreasing order of priority. Recommend writing XPath manually or simply using attributes. There is automatic waiting for an element to appear before timeout happens, and error is returned that the element cannot be found. To change the default timeout of 10 seconds, use timeout() function.\n\n- :camera_flash: An element identifier can also be a .png or .bmp image snapshot representing the UI element (can be on desktop applications, terminal window or web browser). If the image file specified does not exist, OCR will be used to search for that text on the screen to act on the UI element containing the text, eg r.click('Submit Form.png'). Transparency (0% opacity) is supported in .png images. x, y coordinates of elements on the screen can be used as well.\n\n- :page_facing_up: A further image identifier example is an image of the window (PDF viewer, MS Word, textbox etc) with the center content of the image set as transparent. This allows using read() and snap() to perform OCR and save snapshots of application windows, containers, frames, textboxes with varying content. Also for read() and snap(), x1, y1, x2, y2 coordinates pair can be used to define the region of interest on the screen to perform OCR or capture snapshot.\n\n#### CORE FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\ninit()|visual_automation = False, chrome_browser = True|start TagUI, auto-setup on first run\nclose()||close TagUI, Chrome browser, SikuliX\npack()||for deploying package without internet\nupdate()||for updating package without internet\n\n>_to print and log debug info to rpa_python.log use debug(True), to switch off use debug(False)_\n\n#### BASIC FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nurl()|webpage_url (no parameter to return current URL)|go to web URL\nclick()|element_identifier (or x, y using visual automation)| left-click on element\nrclick()|element_identifier (or x, y using visual automation)|right-click on element\ndclick()|element_identifier (or x, y using visual automation)|double-click on element\nhover()|element_identifier (or x, y using visual automation)|move mouse to element\ntype()|element_identifier (or x, y), text_to_type ('[enter]', '[clear]')|enter text at element\nselect()|element_identifier (or x, y), option_value (or x, y)|choose dropdown option\nread()|element_identifier (page = web page) (or x1, y1, x2, y2)|fetch & return element text\nsnap()|element_identifier (page = web page), filename_to_save|save screenshot to file\nload()|filename_to_load|load & return file content\ndump()|text_to_dump, filename_to_save|save text to file\nwrite()|text_to_write, filename_to_save|append text to file\nask()|text_to_prompt|ask & return user input\n\n>_to wait for an element to appear until timeout() value, use hover(). to drag-and-drop, [you can do this](https://github.com/tebelorg/RPA-Python/issues/58#issuecomment-570778431)_\n\n#### PRO FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nkeyboard()|keys_and_modifiers (using visual automation)|send keystrokes to screen\nmouse()|'down' or 'up' (using visual automation)|send mouse event to screen\nwait()|delay_in_seconds (default 5 seconds)|explicitly wait for some time\ntable()|element_identifier (XPath only), filename_to_save|save basic HTML table to CSV\nupload()|element_identifier (CSS only), filename_to_upload|upload file to web element\ndownload()|download_url, filename_to_save(optional)|download from URL to file\nunzip()|file_to_unzip, unzip_location (optional)|unzip zip file to specified location\nframe()|main_frame id or name, sub_frame (optional)|set web frame, frame() to reset\npopup()|string_in_url (no parameter to reset to main page)|set context to web popup tab\nrun()|command_to_run (use ; between commands)|run OS command & return output\ndom()|statement_to_run (JS code to run in browser)|run code in DOM & return output\nvision()|command_to_run (Python code for SikuliX)|run custom SikuliX commands\ntimeout()|timeout_in_seconds (blank returns current timeout)|change wait timeout (default 10s)\n\nkeyboard() modifiers and special keys -\n>_[shift] [ctrl] [alt] [cmd] [win] [meta] [clear] [space] [enter] [backspace] [tab] [esc] [up] [down] [left] [right] [pageup] [pagedown] [delete] [home] [end] [insert] [f1] .. [f15] [printscreen] [scrolllock] [pause] [capslock] [numlock]_\n\n#### HELPER FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nexist()|element_identifier|return True or False if element exists before timeout\npresent()|element_identifier|return True or False if element is present now\ncount()|element_identifier|return number of web elements as integer\nclipboard()|text_to_put or no parameter|put text or return clipboard text as string\nmouse_xy()||return '(x,y)' coordinates of mouse as string\nmouse_x()||return x coordinate of mouse as integer\nmouse_y()||return y coordinate of mouse as integer\ntitle()||return page title of current web page as string\ntext()||return text content of current web page as string\ntimer()||return time elapsed in sec between calls as float\n\n>_to type large amount of text quickly, use clipboard() and keyboard() to paste instead of type()_\n\n# About & Credits\n\nTagUI is the leading open-source RPA software :robot: with thousands of active users. It was created in 2016-2017 when I left DBS Bank as a test automation engineer, to embark on a one-year sabbatical to Eastern Europe. Most of its code base was written in Novi Sad Serbia. My wife and I also spent a couple of months in Budapest Hungary, as well as Chiang Mai Thailand for visa runs. In 2018, I joined AI Singapore to continue development of TagUI.\n\nOver the past few months I take on a daddy role full-time, taking care of my newborn baby girl and wife :cowboy_hat_face:\ud83e\udd31. In between the nannying, I use my time pockets to create this Python package that's built on TagUI. I hope that RPA for Python and ML frameworks would be good friends, and `pip install rpa` would make life easier for Python users.\n\nLastly, at only ~1k lines of code, it would make my day to see developers of other languages port this project over to their favourite programming language. See ample comments in this [single-file package](https://github.com/tebelorg/RPA-Python/blob/master/tagui.py), and its intuitive architecture -\n\n![RPA for Python architecture](https://raw.githubusercontent.com/tebelorg/Tump/master/TagUI-Python/architecture.png)\n\nI would like to credit and express my appreciation below :bowing_man:, and you are invited to [connect on LinkedIn](https://www.linkedin.com/in/kensoh) -\n\n- [TagUI](https://github.com/kelaberetiv/TagUI/tree/pre_v6) - AI Singapore from Singapore / [@aisingapore](https://www.aisingapore.org)\n- [SikuliX](https://github.com/RaiMan/SikuliX1) - Raimund Hocke from Germany / [@RaiMan](https://github.com/RaiMan)\n- [CasperJS](https://github.com/casperjs/casperjs) - Nicolas Perriault from France / [@n1k0](https://github.com/n1k0)\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia / [@ariya](https://github.com/ariya)\n- [SlimerJS](https://github.com/laurentj/slimerjs) - Laurent Jouanneau from France / [@laurentj](https://github.com/laurentj)\n\n# License\nRPA for Python is open-source software released under Apache 2.0 license\n"
 },
 {
  "repo": "nagadomi/lbpcascade_animeface",
  "language": null,
  "readme_contents": "# lbpcascade_animeface\n\nThe face detector for anime/manga using OpenCV.\n\nOriginal release since 2011 at [OpenCV\u306b\u3088\u308b\u30a2\u30cb\u30e1\u9854\u691c\u51fa\u306a\u3089lbpcascade_animeface.xml](http://ultraist.hatenablog.com/entry/20110718/1310965532) (in Japanese)\n\n## Usage\n\nDownload and place the cascade file into your project directory.\n\n    wget https://raw.githubusercontent.com/nagadomi/lbpcascade_animeface/master/lbpcascade_animeface.xml\n\n### Python Example\n\n```python\nimport cv2\nimport sys\nimport os.path\n\ndef detect(filename, cascade_file = \"../lbpcascade_animeface.xml\"):\n    if not os.path.isfile(cascade_file):\n        raise RuntimeError(\"%s: not found\" % cascade_file)\n\n    cascade = cv2.CascadeClassifier(cascade_file)\n    image = cv2.imread(filename, cv2.IMREAD_COLOR)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.equalizeHist(gray)\n    \n    faces = cascade.detectMultiScale(gray,\n                                     # detector options\n                                     scaleFactor = 1.1,\n                                     minNeighbors = 5,\n                                     minSize = (24, 24))\n    for (x, y, w, h) in faces:\n        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n    cv2.imshow(\"AnimeFaceDetect\", image)\n    cv2.waitKey(0)\n    cv2.imwrite(\"out.png\", image)\n\nif len(sys.argv) != 2:\n    sys.stderr.write(\"usage: detect.py <filename>\\n\")\n    sys.exit(-1)\n    \ndetect(sys.argv[1])\n```\nRun\n\n    python detect.py imas.jpg\n\n![result](https://user-images.githubusercontent.com/287255/43184241-ed3f1af8-9022-11e8-8800-468b002c73d9.png)\n\n## Note\nI am providing similar project at https://github.com/nagadomi/animeface-2009. animeface-2009 is my original work that was made before libcascade_animeface. The detection accuracy is higher than this project. However, installation of that is a bit complicated. Also I am providing a face cropping script using animeface-2009.\n"
 },
 {
  "repo": "skvark/opencv-python",
  "language": "Shell",
  "readme_contents": "[![Downloads](http://pepy.tech/badge/opencv-python)](http://pepy.tech/project/opencv-python)\n\n## OpenCV on Wheels\n\n**Unofficial** pre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA. \n\n### Installation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n    There are four different packages (see options 1, 2, 3 and 4 below) and you should **SELECT ONLY ONE OF THEM**. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (`cv2`). If you installed multiple different packages in the same environment, uninstall them all with ``pip uninstall`` and reinstall only one package.\n\n    **a.** Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)\n\n    - Option 1 - Main modules package: ``pip install opencv-python``\n    - Option 2 - Full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n    **b.** Packages for server (headless) environments (such as Docker, cloud environments etc.), no GUI library dependencies\n\n    These packages are smaller than the two other packages above because they do not contain any GUI functionality (not compiled with Qt / other GUI components). This means that the packages avoid a heavy dependency chain to X11 libraries and you will have for example smaller Docker images as a result. You should always use these packages if you do not use `cv2.imshow` et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI.\n\n    - Option 3 - Headless main modules package: ``pip install opencv-python-headless``\n    - Option 4 - Headless full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python-headless`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n4. Import the package:\n\n    ``import cv2``\n\n    All packages contain haarcascade files. ``cv2.data.haarcascades`` can be used as a shortcut to the data folder. For example:\n\n    ``cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")``\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Pip install fails with ``Could not find a version that satisfies the requirement ...``?**\n\nA: Most likely the issue is related to too old pip and can be fixed by running ``pip install --upgrade pip``. Note that the wheel (especially manylinux) format does not currently support properly ARM architecture so there are no packages for ARM based platforms in PyPI. However, ``opencv-python`` packages for Raspberry Pi can be found from https://www.piwheels.org/.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions).\n\nIf you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/skvark/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\n## Documentation for opencv-python\n\n[![AppVeyor CI test status (Windows)](https://img.shields.io/appveyor/ci/skvark/opencv-python.svg?maxAge=3600&label=Windows)](https://ci.appveyor.com/project/skvark/opencv-python)\n[![Travis CI test status (Linux and macOS)](https://img.shields.io/travis/com/skvark/opencv-python/master?label=Linux%20%26%20macOS)](https://travis-ci.com/github/skvark/opencv-python/)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\n### CI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example ``appveyor.yml`` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n   -  OpenCV is included as submodule and the version is updated\n      manually by maintainers when a new OpenCV release has been made\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV\n\n   -  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix \n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build. \n\nSee the next section for more info about manual builds outside the CI environment.\n\n### Manual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/skvark/opencv-python.git`\n2. ``cd opencv-python``\n    - you can use `git` to checkout some other version of OpenCV in the `opencv` and `opencv_contrib` submodules if needed\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n    - this might take anything from 5 minutes to over 2 hours depending on your hardware\n6. You'll have the wheel file in the `dist` folder and you can do with that whatever you wish\n    - Optional: on Linux use some of the `manylinux` images as a build hosts if maximum portability is needed and run `auditwheel` for the wheel after build\n    - Optional: on macOS use ``delocate`` (same as ``auditwheel`` but for macOS) for better portability\n\n#### Source distributions\n\nSince OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples: \n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies. \n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\n### Licensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [3-clause BSD License](https://github.com/opencv/opencv/blob/master/LICENSE).\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux and MacOS wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\n### Versioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\n### Releases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\n### Development builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\n### Manylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\n### Supported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.6\n- 3.7\n- 3.8\n- 3.9\n\n### Backward compatibility\n\nStarting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\n"
 },
 {
  "repo": "atduskgreg/opencv-processing",
  "language": "Java",
  "readme_contents": "## OpenCV for Processing\n\n**A Processing library for the [OpenCV](http://opencv.org/) computer vision library.**\n\nOpenCV for Processing is based on OpenCV's official Java bindings. It attempts to provide convenient wrappers for common OpenCV functions that are friendly to beginners and feel familiar to the Processing environment.\n\nSee the included examples below for an overview of what's possible and links to the relevant example code. Complete documentation is available here:\n\n**[OpenCV for Processing reference](http://atduskgreg.github.io/opencv-processing/reference/)**\n\nOpenCV for Processing is based on the officially supported [OpenCV Java API](http://docs.opencv.org/java/), currently at version 2.4.5. In addition to using the wrapped functionality, you can import OpenCV modules and use any of its documented functions: [OpenCV javadocs](http://docs.opencv.org/java/). See the advanced examples (HistogramSkinDetection, DepthFromStereo, and Marker Detection) below for details. (This style of API was inspired by Kyle McDonald's [ofxCv addon](https://github.com/kylemcdonald/ofxCv) for OpenFrameworks.) \n\nContributions welcome.\n\n### Installing\n\nOpenCV for Processing currently supports Mac OSX, 32-bit and 64-bit Windows, 32- and 64-bit Linux. Android support is hopefully coming soon (pull requests welcome).\n\n_NB: When running on the Mac, make sure you have Processing set to 64-bit mode in the Preferences_\n\nSee [here](https://github.com/atduskgreg/opencv-processing/releases) for the latest release.\n\n### Examples\n\n#### LiveCamTest\n\nAccess a live camera and do image processing on the result, specifically face detection.\n\nCode: [LiveCamTest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/LiveCamTest/LiveCamTest.pde)\n\n_Note: There's a bug that prevents live camera access in current versions of Processing 2.0 on machines with a Retina display._\n\n#### FaceDetection\n\nDetect faces in images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8634017624/\" title=\"Screen Shot 2013-04-08 at 1.22.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8543/8634017624_35f7ef05ce.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-08 at 1.22.18 PM\"></a>\n\nCode: [FaceDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FaceDetection/FaceDetection.pde)\n\n#### BrightnessContrast\n\nAdjust the brightness and contrast of color and gray images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9155239258/\" title=\"brightness and contrast by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2841/9155239258_41a7df36c6.jpg\" width=\"500\" height=\"358\" alt=\"brightness and contrast\"></a>\n\nCode: [BrightnessContrast.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightnessContrast/BrightnessContrast.pde)\n\n#### FilterImages\n\nBasic filtering operations on images: threshold, blur, and adaptive thresholds.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8643666252/\" title=\"Screen Shot 2013-04-12 at 1.42.30 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8240/8643666252_be0da1c751.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-12 at 1.42.30 PM\"></a>\n\nCode: [FilterImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FilterImages/FilterImages.pde)\n\n#### FindContours\n\nFind contours in images and calculate polygon approximations of the contours (i.e., the closest straight line that fits the contour).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9024663015/\" title=\"contours with polygon approximations by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3719/9024663015_f419b117b1.jpg\" width=\"500\" height=\"208\" alt=\"contours with polygon approximations\"></a>\n\nCode: [FindContours.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindContours/FindContours.pde)\n\n#### FindEdges\n\nThree different edge-detection techniques: Canny, Scharr, and Sobel.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8635989723/\" title=\"Screen Shot 2013-04-10 at 2.03.59 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8109/8635989723_170b69dca0.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-10 at 2.03.59 AM\"></a>\n\nCode: [FindEdges.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindEdges/FindEdges.pde)\n\n#### FindLines\n\nFind straight lines in the image using Hough line detection.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9263329608/\" title=\"Hough line detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3781/9263329608_735ce228bb.jpg\" width=\"486\" height=\"500\" alt=\"Hough line detection\"></a>\n\nCode: [HoughLineDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HoughLineDetection/HoughLineDetection.pde)\n\n#### BrightestPoint\n\nFind the brightest point in an image.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9199572469/\" title=\"finding the brightest point by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7407/9199572469_4a25c83062.jpg\" width=\"500\" height=\"366\" alt=\"finding the brightest point\"></a>\n\nCode: [BrightestPoint.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightestPoint/BrightestPoint.pde)\n\n#### RegionOfInterest\n\nAssign a sub-section (or Region of Interest) of the image to be processed. Video of this example in action here: [Region of Interest demo on Vimeo](https://vimeo.com/69009345).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9077805277/\" title=\"region of interest by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3795/9077805277_084d87a3a5.jpg\" width=\"500\" height=\"358\" alt=\"region of interest\"></a>\n\nCode: [RegionOfInterest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/RegionOfInterest/RegionOfInterest.pde)\n\n#### ImageDiff\n\nFind the difference between two images in order to subtract the background or detect a new object in a scene.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8640005799/\" title=\"Screen Shot 2013-04-11 at 2.10.35 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8114/8640005799_44b48e01ae.jpg\" width=\"500\" height=\"409\" alt=\"Screen Shot 2013-04-11 at 2.10.35 PM\"></a>\n\nCode: [ImageDiff.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ImageDiff/ImageDiff.pde)\n\n#### DilationAndErosion\n\nThin (erode) and expand (dilate) an image in order to close holes. These are known as \"morphological\" operations.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9075875005/\" title=\"dilation and erosion by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2818/9075875005_8f7cde3ed7.jpg\" width=\"496\" height=\"500\" alt=\"dilation and erosion\"></a>\n\nCode: [DilationAndErosion.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DilationAndErosion/DilationAndErosion.pde)\n\n#### BackgroundSubtraction\n\nDetect moving objects in a scene. Use background subtraction to distinguish background from foreground and contour tracking to track the foreground objects.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9220336868/\" title=\"Background Subtraction by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7292/9220336868_bed3498528.jpg\" width=\"500\" height=\"369\" alt=\"Background Subtraction\"></a>\n\nCode: [BackgroundSubtraction.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BackgroundSubtraction/BackgroundSubtraction.pde)\n\n\n#### WorkingWithColorImages\n\nDemonstration of what you can do color images in OpenCV (threshold, blur, etc) and what you can't (lots of other operations).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9136033334/\" title=\"color operations: threshold and blur by atduskgreg, on Flickr\"><img src=\"http://farm6.staticflickr.com/5451/9136033334_3345dfa057.jpg\" width=\"500\" height=\"358\" alt=\"color operations: threshold and blur\"></a>\n\nCode: [WorkingWithColorImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WorkingWithColorImages/WorkingWithColorImages.pde)\n\n#### ColorChannels ####\n\nSeparate a color image into red, green, blue or hue, saturation, and value channels in order to work with the channels individually.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9246157901/\" title=\"ColorChannels by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2847/9246157901_08ccf19e7d.jpg\" width=\"488\" height=\"500\" alt=\"ColorChannels\"></a>\n\nCode: [ColorChannels](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ColorChannels/ColorChannels.pde)\n\n#### FindHistogram\n\nDemonstrates use of the findHistogram() function and the Histogram class to get and draw histograms for grayscale and individual color channels.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9174190443/\" title=\"gray, red, green, blue histograms by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7287/9174190443_224a740ce8.jpg\" width=\"500\" height=\"355\" alt=\"gray, red, green, blue histograms\"></a>\n\nCode: [FindHistogram.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindHistogram/FindHistogram.pde)\n\n#### HueRangeSelection\n\nDetect objects based on their color. Demonstrates the use of HSV color space as well as range-based image filtering.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9193745547/\" title=\"Hue-based color detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3799/9193745547_8f09e55a39.jpg\" width=\"500\" height=\"397\" alt=\"Hue-based color detection\"></a>\n\nCode: [HueRangeSelection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HueRangeSelection/HueRangeSelection.pde)\n\n#### CalibrationDemo (in progress)\n\nAn example of the process involved in calibrating a camera. Currently only detects the corners in a chessboard pattern.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8706849024/\" title=\"Screen Shot 2013-05-04 at 2.03.23 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8267/8706849024_f2d938ec51.jpg\" width=\"500\" height=\"382\" alt=\"Screen Shot 2013-05-04 at 2.03.23 AM\"></a>\n\nCode: [CalibrationDemo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/CalibrationDemo/CalibrationDemo.pde)\n\n#### HistogramSkinDetection\n\nA more advanced example. Detecting skin in an image based on colors in a region of color space. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8707167599/\" title=\"Screen Shot 2013-05-04 at 2.25.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8135/8707167599_d38fbdfe30.jpg\" width=\"500\" height=\"171\" alt=\"Screen Shot 2013-05-04 at 2.25.18 PM\"></a>\n\nCode: [HistogramSkinDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HistogramSkinDetection/HistogramSkinDetection.pde)\n\n#### DepthFromStereo\n\nAn advanced example. Calculates depth information from a pair of stereo images. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642493130/\" title=\"Screen Shot 2013-04-12 at 2.27.30 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8260/8642493130_f99dd76f3d.jpg\" width=\"500\" height=\"404\" alt=\"Screen Shot 2013-04-12 at 2.27.30 AM\"></a>\n\nCode: [DepthFromStereo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DepthFromStereo/DepthFromStereo.pde)\n\n#### WarpPerspective (in progress)\n\nUn-distort an object that's in perspective. Coming to the real API soon.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9279197332/\" title=\"Warp Perspective by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2861/9279197332_ca6beb3760.jpg\" width=\"500\" height=\"416\" alt=\"Warp Perspective\"></a>\n\nCode: [WarpPerspective.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WarpPerspective/WarpPerspective.pde)\n\n#### MarkerDetection\n\nAn in-depth advanced example. Detect a CV marker in an image, warp perspective, and detect the number stored in the marker. Many steps in the code. Uses many un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642309968/\" title=\"Screen Shot 2013-04-12 at 12.20.17 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8522/8642309968_257e397db2.jpg\" width=\"500\" height=\"225\" alt=\"Screen Shot 2013-04-12 at 12.20.17 AM\"></a>\n\nCode: [MarkerDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/MarkerDetection/MarkerDetection.pde)\n\n#### MorphologyOperations\n\nOpen and close an image, or do more complicated morphological transformations.\n\n<a href=\"https://flic.kr/p/tazj7r\" title=\"Morphology operations\"><img src=\"https://farm6.staticflickr.com/5340/17829980821_1734e8bab8_z_d.jpg\" width=\"640\" height=\"393\" alt=\"Morphology operations\"></a>\n\nCode: [MorphologyOperations.pde](examples/MorphologyOperations/MorphologyOperations.pde)\n"
 },
 {
  "repo": "QianMo/OpenCV3-Intro-Book-Src",
  "language": "C++",
  "readme_contents": "\u300aOpenCV3\u7f16\u7a0b\u5165\u95e8\u300b\u4e66\u672c\u914d\u5957\u6e90\u4ee3\u7801\n==============================\n#### \u300aIntroduction to OpenCV3 Programming\u300bBook Source Code<br>\n\n![](http://img.blog.csdn.net/20150325155409850)  \n<br>\u672c\u4e66\u6709OpenCV2\u3001OpenCV3\u4e24\u5957\u72ec\u7acb\u7684\u4e66\u672c\u914d\u5957\u793a\u4f8b\u7a0b\u5e8f\u4f9b\u9009\u62e9\u4f7f\u7528\u3002\n<br>  \u67094\u4e2a\u90e8\u520611\u7ae0\uff0c\u5171\u670995\u4e2a\u4e3b\u7ebf\u793a\u4f8b\u7a0b\u5e8f\uff0c\u4e3a\u65b9\u4fbf\u5927\u5bb6\u67e5\u9605\u548c\u5b66\u4e60\uff0c\u603b\u7ed3\u6210\u5982\u4e0b\u3002\n# \u6b63\u6587\u90e8\u5206\u6e90\u4ee3\u7801\n## \u7b2c\u4e00\u90e8\u5206 \u5feb\u901f\u4e0a\u624bOpenCV\n\t\t1\tOpenCV\u73af\u5883\u914d\u7f6e\u7684\u6d4b\u8bd5\u7528\u4f8b\t1.3.8\n\t\t2\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e00\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u663e\u793a\t1.4.1\n\t\t3\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e8c\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u8150\u8680\t1.4.2\n\t\t4\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e09\u4e2a\u7a0b\u5e8f\uff1ablur\u56fe\u50cf\u6a21\u7cca\t1.4.3\n\t\t5\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u56db\u4e2a\u7a0b\u5e8f\uff1acanny\u8fb9\u7f18\u68c0\u6d4b\t1.4.4\n\t\t6\t\u8bfb\u53d6\u5e76\u64ad\u653e\u89c6\u9891\t1.5.1\n\t\t7\t\u8c03\u7528\u6444\u50cf\u5934\u91c7\u96c6\u56fe\u50cf\t1.5.2\n\t\t8\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5f69\u8272\u76ee\u6807\u8ddf\u8e2a\uff1aCamshift\t2.1.1\n\t\t9\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5149\u6d41\uff1aoptical flow\t2.1.2\n\t\t10\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u70b9\u8ffd\u8e2a\uff1alkdemo\t2.1.3\n\t\t11\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u4eba\u8138\u8bc6\u522b\uff1aobjectDetection\t2.1.4\n\t\t12\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u652f\u6301\u5411\u91cf\u673a\u5f15\u5bfc\t2.1.5\n\t\t13\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u5904\u7406\u7ebf\u6027\u4e0d\u53ef\u5206\u6570\u636e\t2.1.5\n\t\t14\tprintf\u51fd\u6570\u7684\u7528\u6cd5\u793a\u4f8b\t2.6.2\n\t\t15\t\u7528imwrite\u51fd\u6570\u751f\u6210png\u900f\u660e\u56fe\t3.1.8\n\t\t16\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u56fe\u50cf\u7684\u8f7d\u5165\u3001\u663e\u793a\u4e0e\u8f93\u51fa\t3.1.9\n\t\t17\t\u4e3a\u7a0b\u5e8f\u754c\u9762\u6dfb\u52a0\u6ed1\u52a8\u6761\t3.2.1\n\t\t18\t\u9f20\u6807\u64cd\u4f5c\u793a\u4f8b\t3.3\n## \u7b2c\u4e8c\u90e8\u5206 \u521d\u63a2core\u7ec4\u4ef6\t\n\t\t19\t\u57fa\u7840\u56fe\u50cf\u5bb9\u5668Mat\u7c7b\u7684\u4f7f\u7528\t4.1.7\n\t\t20\t\u7528OpenCV\u8fdb\u884c\u57fa\u672c\u7ed8\u56fe\t4.3\n\t\t21\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e00\uff1a\u7528\u6307\u9488\u8bbf\u95ee\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t22\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e8c\uff1a\u7528\u8fed\u4ee3\u5668\u64cd\u4f5c\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t23\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e09\uff1a\u52a8\u6001\u5730\u5740\u8ba1\u7b97\t5.1.5\u30015.1.6\n\t\t24\t\u904d\u5386\u56fe\u50cf\u4e2d\u50cf\u7d20\u768414\u79cd\u65b9\u6cd5\t5.1.6\n\t\t25\t\u521d\u7ea7\u56fe\u50cf\u6df7\u5408\t5.2.4\n\t\t26\t\u591a\u901a\u9053\u56fe\u50cf\u6df7\u5408\t5.3.3\n\t\t27\t\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u3001\u4eae\u5ea6\u503c\u8c03\u6574\t5.4.3\n\t\t28\t\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\t5.5.8\n\t\t29\tXML\u548cYAML\u6587\u4ef6\u7684\u5199\u5165\t5.6.3\n\t\t30\tXML\u548cYAML\u6587\u4ef6\u7684\u8bfb\u53d6\t5.6.4\u3001\n## \u7b2c\u4e09\u90e8\u5206 \u638c\u63e1imgproc\u7ec4\u4ef6\t\n\t\t31\t\u65b9\u6846\u6ee4\u6ce2\uff1aboxFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t32\t\u5747\u503c\u6ee4\u6ce2\uff1ablur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t33\t\u9ad8\u65af\u6ee4\u6ce2\uff1aGaussianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t34\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u7ebf\u6027\u6ee4\u6ce2\t6.1.12\n\t\t35\t\u4e2d\u503c\u6ee4\u6ce2\uff1amedianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t36\t\u53cc\u8fb9\u6ee4\u6ce2\uff1abilateralFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t37\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u6ee4\u6ce2\t6.2.5\n\t\t38\t\u81a8\u80c0\uff1adilate\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t39\t\u8150\u8680\uff1aerode\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t40\t\u7efc\u5408\u793a\u4f8b\uff1a\u8150\u8680\u4e0e\u81a8\u80c0\t6.3.6\n\t\t41\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u81a8\u80c0\t6.4.8\n\t\t42\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u8150\u8680\t6.4.8\n\t\t43\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u5f00\u8fd0\u7b97\t6.4.8\n\t\t44\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u95ed\u8fd0\u7b97\t6.4.8\n\t\t45\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u68af\u5ea6\t6.4.8\n\t\t46\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9876\u5e3d\u201d\t6.4.8\n\t\t47\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9ed1\u5e3d\u201d\t6.4.8\n\t\t48\t\u7efc\u5408\u793a\u4f8b\uff1a\u5f62\u6001\u5b66\u6ee4\u6ce2\t6.4.9\n\t\t49\t\u6f2b\u6c34\u586b\u5145\u7b97\u6cd5\uff1afloodFill\u51fd\u6570\t6.5.3\n\t\t50\t\u7efc\u5408\u793a\u4f8b\uff1a\u6f2b\u6c34\u586b\u5145\t6.5.4\n\t\t51\t\u5c3a\u5bf8\u8c03\u6574\uff1aresize()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.5\n\t\t52\t\u5411\u4e0a\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrUp()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t53\t\u5411\u4e0b\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrDown()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t54\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u91d1\u5b57\u5854\u4e0e\u56fe\u7247\u5c3a\u5bf8\u7f29\u653e\t6.6.7\n\t\t55\t\u793a\u4f8b\u7a0b\u5e8f\uff1a\u57fa\u672c\u9608\u503c\u64cd\u4f5c\t6.7.3\n\t\t56\tCanny\u8fb9\u7f18\u68c0\u6d4b\t7.1.2\n\t\t57\tSobel \u7b97\u5b50\u7684\u4f7f\u7528\t7.1.3\n\t\t58\tLaplacian\u7b97\u5b50\u7684\u4f7f\u7528\t7.1.4\n\t\t59\tScharr\u6ee4\u6ce2\u5668\t7.1.5\n\t\t60\t\u7efc\u5408\u793a\u4f8b\uff1a\u8fb9\u7f18\u68c0\u6d4b\t7.1.6\n\t\t61\t\u6807\u51c6\u970d\u592b\u53d8\u6362\uff1aHoughLines()\u51fd\u6570\u7684\u4f7f\u7528\t7.2.4\n\t\t62\t\u7d2f\u8ba1\u6982\u7387\u970d\u592b\u53d8\u6362\uff1aHoughLinesP()\u51fd\u6570\t7.2.5\n\t\t63\t\u970d\u592b\u5706\u53d8\u6362\uff1aHoughCircles()\u51fd\u6570\t7.2.8\n\t\t64\t\u7efc\u5408\u793a\u4f8b\uff1a\u970d\u592b\u53d8\u6362\t7.2.9\n\t\t65\t\u5b9e\u73b0\u91cd\u6620\u5c04\uff1aremap()\u51fd\u6570\t7.3.3\n\t\t66\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u5b9e\u73b0\u591a\u79cd\u91cd\u6620\u5c04\t7.3.4\n\t\t67\t\u4eff\u5c04\u53d8\u6362\t7.4.5\n\t\t68\t\u76f4\u65b9\u56fe\u5747\u8861\u5316\t7.5.3\n\t\t69\t\u8f6e\u5ed3\u67e5\u627e\t8.1.3\n\t\t70\t\u67e5\u627e\u5e76\u7ed8\u5236\u8f6e\u5ed3\t8.1.4\n\t\t71\t\u51f8\u5305\u68c0\u6d4b\u57fa\u7840\t8.2.3\n\t\t72\t\u5bfb\u627e\u548c\u7ed8\u5236\u7269\u4f53\u7684\u51f8\u5305\t8.2.4\n\t\t73\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u77e9\u5f62\u8fb9\u754c\t8.3.6\n\t\t74\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u5706\u5f62\u8fb9\u754c\t8.3.7\n\t\t75\t\u4f7f\u7528\u591a\u8fb9\u5f62\u5305\u56f4\u8f6e\u5ed3\t8.3.8\n\t\t76\t\u56fe\u50cf\u8f6e\u5ed3\u77e9\t8.4.4\n\t\t77\t\u5206\u6c34\u5cad\u7b97\u6cd5\u7684\u4f7f\u7528\t8.5.2\n\t\t78\t\u5b9e\u73b0\u56fe\u50cf\u4fee\u8865\t8.6.2\n\t\t79\tH-S\u4e8c\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.3\n\t\t80\t\u4e00\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.4\n\t\t81\tRGB\u4e09\u8272\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.5\n\t\t82\t\u76f4\u65b9\u56fe\u5bf9\u6bd4\t9.3.2\n\t\t83\t\u53cd\u5411\u6295\u5f71\t9.4.7\n\t\t84\t\u6a21\u677f\u5339\u914d\t9.5.3\n\t\t\n## \u7b2c\u56db\u90e8\u5206 \u6df1\u5165featrue2d\u7ec4\u4ef6\t\n\t\t85\t\u5b9e\u73b0Harris\u89d2\u70b9\u68c0\u6d4b\uff1acornerHarris()\u51fd\u6570\u7684\u4f7f\u7528\t10.1.4\n\t\t86\tharris\u89d2\u70b9\u68c0\u6d4b\u4e0e\u7ed8\u5236\t10.1.5\n\t\t87\tShi-Tomasi\u89d2\u70b9\u68c0\u6d4b\t10.2.3\n\t\t88\t\u4e9a\u50cf\u7d20\u7ea7\u89d2\u70b9\u68c0\u6d4b\t10.3.3\n\t\t89\tSURF\u7279\u5f81\u70b9\u68c0\u6d4b\t11.1.6\n\t\t90\tSURF\u7279\u5f81\u63d0\u53d6\t11.2.3\n\t\t91\t\u4f7f\u7528FLANN\u8fdb\u884c\u7279\u5f81\u70b9\u5339\u914d\t11.3.3\n\t\t92\tFLANN\u7ed3\u5408SURF\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u548c\u5339\u914d\t11.3.4\n\t\t93\tSIFT\u914d\u5408\u66b4\u529b\u5339\u914d\u8fdb\u884c\u5173\u952e\u70b9\u63cf\u8ff0\u548c\u63d0\u53d6\t11.3.5\n\t\t94\t\u5bfb\u627e\u5df2\u77e5\u7269\u4f53\t11.4.3\n\t\t95\t\u5229\u7528ORB\u7b97\u6cd5\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u4e0e\u5339\u914d\t11.5.4\n\n\n\n# \u989d\u5916\u7684\u9644\u8d60\u7a0b\u5e8f\u4e00\u89c8\n\n\u9664\u4e66\u672c\u672c\u8eab\u7684\u793a\u4f8b\u7a0b\u5e8f\u4e4b\u5916\uff0c\u989d\u5916\u9644\u52a0\u4e86OpenCV2\u7248\u768421\u4e2a\u76f8\u8f83\u4e8e\u6b63\u6587\u4e3b\u7ebf\u7684\u793a\u4f8b\u4ee3\u7801\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u7684\u7a0b\u5e8f\u6e90\u4ee3\u7801\u3002\n\u73b0\u5c06\u9644\u52a0\u768421\u4e2a\u793a\u4f8b\u7a0b\u5e8f\u5217\u4e3e\u5982\u4e0b\uff1a\n\n\t\t1\t\u968f\u673a\u56fe\u5f62\u548c\u6587\u5b57\u751f\u6210\u793a\u4f8b\uff08randomtext\uff09\n\t\t2\t\u751f\u6210\u5f69\u8272\u8272\u6761\uff08gencolors\uff09\t\n\t\t3\t\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08kalman\uff09\t\n\t\t4\t\u6e10\u53d8\u8fc7\u6e21\u5404\u79cd\u56fe\u5f62\u6ee4\u6ce2\uff08median_blur\uff09\n\t\t5\t\u8ddd\u79bb\u53d8\u6362\uff08distanceTransform\uff09\n\t\t6\t\u628a\u56fe\u50cf\u6620\u5c04\u5230\u6781\u6307\u6570\u7a7a\u95f4\uff08Log Polar\uff09\n\t\t7\tfilter2D\u6ee4\u6ce2\u5668\u7684\u7528\u6cd5\t\n\t\t8\tgrabCut\u56fe\u50cf\u5206\u5272\u793a\u4f8b\t\n\t\t9\tMeanShift\u56fe\u50cf\u5206\u5272\u793a\u4f8b\n\t\t10\t\u7528\u6ed1\u52a8\u63a7\u5236\u56fe\u50cf\u76f4\u65b9\u56fe\n\t\t11\t\u627e\u5230\u56fe\u50cf\u6700\u5c0f\u7684\u5c01\u95ed\u8f6e\u5ed3\n\t\t12\tRetina\u7279\u5f81\u70b9\u68c0\u6d4b\n\t\t13\t\u6444\u50cf\u5934\u5e27\u6570\u68c0\u6d4b\n\t\t14\t\u89c6\u9891\u622a\u56fe\n\t\t15\t\u5bf9\u89c6\u9891\u7684\u5feb\u901f\u89d2\u70b9\u68c0\u6d4b\n\t\t16\t\u89c6\u9891\u7b80\u5355\u8272\u5f69\u68c0\u6d4b\n\t\t17\t\u8ddf\u8e2a\u5206\u5272\u89c6\u9891\u4e2d\u8fd0\u52a8\u7684\u7269\u4f53\n\t\t18\t\u89c6\u9891\u7684\u76f4\u65b9\u56fe\u53cd\u5411\u6295\u5f71\u3002\n\t\t19\t\u8ba1\u7b97\u89c6\u9891\u4e2d\u4e24\u4e2a\u56fe\u50cf\u533a\u57df\u7684\u76f8\u4f3c\u5ea6\n\t\t20\t\u89c6\u9891\u524d\u540e\u80cc\u666f\u5206\u79bb\n\t\t21\t\u7528\u9ad8\u65af\u80cc\u666f\u5efa\u6a21\u5206\u79bb\u80cc\u666f\n\n\n<br>\n\n\n# \u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\n\n[\u3010\u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\u3011](http://blog.csdn.net/poem_qianmo/article/details/44416709)\n\n![](http://img.blog.csdn.net/20150325202951885)  \n<br>Please Enjoy~\n\n"
 },
 {
  "repo": "ANYbotics/grid_map",
  "language": "C++",
  "readme_contents": "# Grid Map\n\n## Overview\n\nThis is a C++ library with [ROS] interface to manage two-dimensional grid maps with multiple data layers. It is designed for mobile robotic mapping to store data such as elevation, variance, color, friction coefficient, foothold quality, surface normal, traversability etc. It is used in the [Robot-Centric Elevation Mapping](https://github.com/anybotics/elevation_mapping) package designed for rough terrain navigation.\n\nFeatures:\n\n* **Multi-layered:** Developed for universal 2.5-dimensional grid mapping with support for any number of layers.\n* **Efficient map re-positioning:** Data storage is implemented as two-dimensional circular buffer. This allows for non-destructive shifting of the map's position (e.g. to follow the robot) without copying data in memory.\n* **Based on Eigen:** Grid map data is stored as [Eigen] data types. Users can apply available Eigen algorithms directly to the map data for versatile and efficient data manipulation.\n* **Convenience functions:** Several helper methods allow for convenient and memory safe cell data access. For example, iterator functions for rectangular, circular, polygonal regions and lines are implemented.\n* **ROS interface:** Grid maps can be directly converted to and from ROS message types such as PointCloud2, OccupancyGrid, GridCells, and our custom GridMap message. Conversion packages provide compatibility with [costmap_2d], [PCL], and [OctoMap] data types.\n* **OpenCV interface:** Grid maps can be seamlessly converted from and to [OpenCV] image types to make use of the tools provided by [OpenCV].\n* **Visualizations:** The *grid_map_rviz_plugin* renders grid maps as 3d surface plots (height maps) in [RViz]. Additionally, the *grid_map_visualization* package helps to visualize grid maps as point clouds, occupancy grids, grid cells etc.\n* **Filters:** The *grid_map_filters* provides are range of filters to process grid maps as a sequence of filters. Parsing of mathematical expressions allows to flexibly setup powerful computations such as thresholding, normal vectors, smoothening, variance, inpainting, and matrix kernel convolutions.\n\nThis is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.\n\nThe source code is released under a [BSD 3-Clause license](LICENSE).\n\n**Author: P\u00e9ter Fankhauser<br />\nAffiliation: [ANYbotics](https://www.anybotics.com/)<br />\nMaintainer: P\u00e9ter Fankhauser, pfankhauser@anybotics.com<br />**\nWith contributions by: Tanja Baumann, Jeff Delmerico, Remo Diethelm, Perry Franklin, Dominic Jud, Ralph Kaestner, Philipp Kr\u00fcsi, Alex Millane, Daniel Stonier, Elena Stumm, Martin Wermelinger, Christos Zalidis, Edo Jelavic, Ruben Grandia, Simone Arreghini, Magnus G\u00e4rtner\n\nThis projected was initially developed at ETH Zurich (Autonomous Systems Lab & Robotic Systems Lab).\n\n[This work is conducted as part of ANYmal Research, a community to advance legged robotics.](https://www.anymal-research.org/)\n\n![Grid map example in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin_example.png)\n\n## Publications\n\nIf you use this work in an academic context, please cite the following publication:\n\n> P. Fankhauser and M. Hutter,\n> **\"A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation\"**,\n> in Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1), A. Koubaa (Ed.), Springer, 2016. ([PDF](http://www.researchgate.net/publication/284415855))\n\n\n    @incollection{Fankhauser2016GridMapLibrary,\n      author = {Fankhauser, P{\\'{e}}ter and Hutter, Marco},\n      booktitle = {Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1)},\n      title = {{A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation}},\n      chapter = {5},\n      editor = {Koubaa, Anis},\n      publisher = {Springer},\n      year = {2016},\n      isbn = {978-3-319-26052-5},\n      doi = {10.1007/978-3-319-26054-9{\\_}5},\n      url = {http://www.springer.com/de/book/9783319260525}\n    }\n\n## Documentation\n\nAn introduction to the grid map library including a tutorial is given in [this book chapter](http://www.researchgate.net/publication/284415855).\n\nThe C++ API is documented here:\n* [grid_map_core](http://docs.ros.org/kinetic/api/grid_map_core/html/index.html)\n* [grid_map_ros](http://docs.ros.org/kinetic/api/grid_map_ros/html/index.html)\n* [grid_map_costmap_2d](http://docs.ros.org/kinetic/api/grid_map_costmap_2d/html/index.html)\n* [grid_map_cv](http://docs.ros.org/kinetic/api/grid_map_cv/html/index.html)\n* [grid_map_filters](http://docs.ros.org/kinetic/api/grid_map_filters/html/index.html)\n* [grid_map_octomap](http://docs.ros.org/kinetic/api/grid_map_octomap/html/index.html)\n* [grid_map_pcl](http://docs.ros.org/kinetic/api/grid_map_pcl/html/index.html)\n\n## Installation\n\n### Installation from Packages\n\nTo install all packages from the grid map library as Debian packages use\n\n    sudo apt-get install ros-$ROS_DISTRO-grid-map\n\n### Building from Source\n\n#### Dependencies\n\nThe *grid_map_core* package depends only on the linear algebra library [Eigen].\n\n    sudo apt-get install libeigen3-dev\n\nThe other packages depend additionally on the [ROS] standard installation (*roscpp*, *tf*, *filters*, *sensor_msgs*, *nav_msgs*, and *cv_bridge*). Other format specific conversion packages (e.g. *grid_map_cv*, *grid_map_pcl* etc.) depend on packages described below in *Packages Overview*.\n\n#### Building\n\nTo build from source, clone the latest version from this repository into your catkin workspace and compile the package using\n\n    cd catkin_ws/src\n    git clone https://github.com/anybotics/grid_map.git\n    cd ../\n    catkin_make\n\nTo maximize performance, make sure to build in *Release* mode. You can specify the build type by setting\n\n    catkin_make -DCMAKE_BUILD_TYPE=Release\n\n\n### Packages Overview\n\nThis repository consists of following packages:\n\n* ***grid_map*** is the meta-package for the grid map library.\n* ***grid_map_core*** implements the algorithms of the grid map library. It provides the `GridMap` class and several helper classes such as the iterators. This package is implemented without [ROS] dependencies.\n* ***grid_map_ros*** is the main package for [ROS] dependent projects using the grid map library. It provides the interfaces to convert grid maps from and to several [ROS] message types.\n* ***grid_map_demos*** contains several nodes for demonstration purposes.\n* ***grid_map_filters*** builds on the [ROS Filters] package to process grid maps as a sequence of filters.\n* ***grid_map_msgs*** holds the [ROS] message and service definitions around the [grid_map_msg/GridMap] message type.\n* ***grid_map_rviz_plugin*** is an [RViz] plugin to visualize grid maps as 3d surface plots (height maps).\n* ***grid_map_visualization*** contains a node written to convert GridMap messages to other [ROS] message types for example for  visualization in [RViz].\n\nAdditional conversion packages:\n\n* ***grid_map_costmap_2d*** provides conversions of grid maps from [costmap_2d] map types.\n* ***grid_map_cv*** provides conversions of grid maps from and to [OpenCV] image types.\n* ***grid_map_octomap*** provides conversions of grid maps from OctoMap ([OctoMap]) maps.\n* ***grid_map_pcl*** provides conversions of grid maps from Point Cloud Library ([PCL](http://pointclouds.org/)) polygon meshes and point clouds. For details, see the grid map pcl package [README](grid_map_pcl/README.md).\n\n### Unit Tests\n\nRun the unit tests with\n\n    catkin_make run_tests_grid_map_core run_tests_grid_map_ros\n\nor\n\n    catkin build grid_map --no-deps --verbose --catkin-make-args run_tests\n\nif you are using [catkin tools](http://catkin-tools.readthedocs.org/).\n\n## Usage\n\n### Demonstrations\n\nThe *grid_map_demos* package contains several demonstration nodes. Use this code to verify your installation of the grid map packages and to get you started with your own usage of the library.\n\n* *[simple_demo](grid_map_demos/src/simple_demo_node.cpp)* demonstrates a simple example for using the grid map library. This ROS node creates a grid map, adds data to it, and publishes it. To see the result in RViz, execute the command\n\n        roslaunch grid_map_demos simple_demo.launch\n\n* *[tutorial_demo](grid_map_demos/src/tutorial_demo_node.cpp)* is an extended demonstration of the library's functionalities. Launch the *tutorial_demo* with\n\n        roslaunch grid_map_demos tutorial_demo.launch\n\n* *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* showcases the usage of the grid map iterators. Launch it with\n\n        roslaunch grid_map_demos iterators_demo.launch\n\n* *[image_to_gridmap_demo](grid_map_demos/src/ImageToGridmapDemo.cpp)* demonstrates how to convert data from an [image](grid_map_demos/data/eth_logo.png) to a grid map. Start the demonstration with\n\n        roslaunch grid_map_demos image_to_gridmap_demo.launch\n\n    ![Image to grid map demo result](grid_map_demos/doc/image_to_grid_map_demo_result.png)\n    \n* *[grid_map_to_image_demo](grid_map_demos/src/GridmapToImageDemo.cpp)* demonstrates how to save a grid map layer to an image. Start the demonstration with\n\n        rosrun grid_map_demos grid_map_to_image_demo _grid_map_topic:=/grid_map _file:=/home/$USER/Desktop/grid_map_image.png\n\n* *[opencv_demo](grid_map_demos/src/opencv_demo_node.cpp)* demonstrates map manipulations with help of [OpenCV] functions. Start the demonstration with\n\n        roslaunch grid_map_demos opencv_demo.launch\n\n    ![OpenCV demo result](grid_map_demos/doc/opencv_demo_result.gif)\n\n* *[resolution_change_demo](grid_map_demos/src/resolution_change_demo_node.cpp)* shows how the resolution of a grid map can be changed with help of the [OpenCV] image scaling methods. The see the results, use\n\n        roslaunch grid_map_demos resolution_change_demo.launch\n\n* *[filters_demo](grid_map_demos/src/FiltersDemo.cpp)* uses a chain of [ROS Filters] to process a grid map. Starting from the elevation of a terrain map, the demo uses several filters to show how to compute surface normals, use inpainting to fill holes, smoothen/blur the map, and use math expressions to detect edges, compute roughness and traversability. The filter chain setup is configured in the [`filters_demo_filter_chain.yaml`](grid_map_demos/config/filters_demo_filter_chain.yaml) file. Launch the demo with\n\n        roslaunch grid_map_demos filters_demo.launch\n\n    [![Filters demo results](grid_map_demos/doc/filters_demo_preview.gif)](grid_map_demos/doc/filters_demo.gif)\n\n For more information about grid map filters, see [grid_map_filters](#grid_map_filters).\n\n* *[interpolation_demo](grid_map_demos/src/InterpolationDemo.cpp)* shows the result of different interpolation methods on the resulting surface. The start the demo, use\n\n        roslaunch grid_map_demos interpolation_demo.launch\n\n<img src=\"grid_map_core/doc/interpolationSineWorld.gif\" width=\"256\" height=\"252\">\n<img src=\"grid_map_core/doc/interpolationGaussWorld.gif\" width=\"256\" height=\"252\">\n\nThe user can play with different worlds (surfaces) and different interpolation settings in the [`interpolation_demo.yaml`](grid_map_demos/config/interpolation_demo.yaml) file. The visualization displays the ground truth in green and yellow color. The interpolation result is shown in red and purple colors. Also, the demo computes maximal and average interpolation errors, as well as the average time required for a single interpolation query.\n\nGrid map features four different interpolation methods (in order of increasing accuracy and increasing complexity):\n* **NN** - Nearest Neighbour (fastest, but least accurate).\n* **Linear** - Linear interpolation.\n* **Cubic convolution** - Piecewise cubic interpolation. Implemented using the cubic convolution algorithm.\n* **Cubic** - Cubic interpolation (slowest, but most accurate).\n\nFor more details check the literature listed in  [`CubicInterpolation.hpp`](grid_map_core/include/grid_map_core/CubicInterpolation.hpp) file.\n\n### Conventions & Definitions\n\n[![Grid map layers](grid_map_core/doc/grid_map_layers.png)](grid_map_core/doc/grid_map_layers.pdf)\n\n[![Grid map conventions](grid_map_core/doc/grid_map_conventions.png)](grid_map_core/doc/grid_map_conventions.pdf)\n\n\n### Iterators\n\nThe grid map library contains various iterators for convenience.\n\nGrid map | Submap | Circle | Line | Polygon\n:---: | :---: | :---: | :---: | :---:\n[![Grid map iterator](grid_map_core/doc/iterators/grid_map_iterator_preview.gif)](grid_map_core/doc/iterators/grid_map_iterator.gif) | [![Submap iterator](grid_map_core/doc/iterators/submap_iterator_preview.gif)](grid_map_core/doc/iterators/submap_iterator.gif) | [![Circle iterator](grid_map_core/doc/iterators/circle_iterator_preview.gif)](grid_map_core/doc/iterators/circle_iterator.gif) | [![Line iterator](grid_map_core/doc/iterators/line_iterator_preview.gif)](grid_map_core/doc/iterators/line_iterator.gif) | [![Polygon iterator](grid_map_core/doc/iterators/polygon_iterator_preview.gif)](grid_map_core/doc/iterators/polygon_iterator.gif)\n__Ellipse__ | __Spiral__\n[![Ellipse iterator](grid_map_core/doc/iterators/ellipse_iterator_preview.gif)](grid_map_core/doc/iterators/ellipse_iterator.gif) | [![Spiral iterator](grid_map_core/doc/iterators/spiral_iterator_preview.gif)](grid_map_core/doc/iterators/spiral_iterator.gif)\n\nUsing the iterator in a `for` loop is common. For example, iterate over the entire grid map with the `GridMapIterator` with\n\n    for (grid_map::GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        cout << \"The value at index \" << (*iterator).transpose() << \" is \" << map.at(\"layer\", *iterator) << endl;\n    }\n\nThe other grid map iterators follow the same form. You can find more examples on how to use the different iterators in the *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* node.\n\nNote: For maximum efficiency when using iterators, it is recommended to locally store direct access to the data layers of the grid map with `grid_map::Matrix& data = map[\"layer\"]` outside the `for` loop:\n\n    grid_map::Matrix& data = map[\"layer\"];\n    for (GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        const Index index(*iterator);\n        cout << \"The value at index \" << index.transpose() << \" is \" << data(index(0), index(1)) << endl;\n    }\n\nYou can find a benchmarking of the performance of the iterators in the `iterator_benchmark` node of the `grid_map_demos` package which can be run with\n\n    rosrun grid_map_demos iterator_benchmark\n\nBeware that while iterators are convenient, it is often the cleanest and most efficient to make use of the built-in [Eigen] methods. Here are some examples:\n\n* Setting a constant value to all cells of a layer:\n\n        map[\"layer\"].setConstant(3.0);\n\n* Adding two layers:\n\n        map[\"sum\"] = map[\"layer_1\"] + map[\"layer_2\"];\n\n* Scaling a layer:\n\n        map[\"layer\"] = 2.0 * map[\"layer\"];\n\n* Max. values between two layers:\n\n        map[\"max\"] = map[\"layer_1\"].cwiseMax(map[\"layer_2\"]);\n\n* Compute the root mean squared error:\n\n        map.add(\"error\", (map.get(\"layer_1\") - map.get(\"layer_2\")).cwiseAbs());\n        unsigned int nCells = map.getSize().prod();\n        double rootMeanSquaredError = sqrt((map[\"error\"].array().pow(2).sum()) / nCells);\n\n\n### Changing the Position of the Map\n\nThere are two different methods to change the position of the map:\n* `setPosition(...)`: Changes the position of the map without changing data stored in the map. This changes the corresponce between the data and the map frame.\n* `move(...)`: Relocates the grid map such that the corresponce between data and the map frame does not change. Data in the overlapping region before and after the position change remains stored. Data that falls outside of the map at its new position is discarded. Cells that cover previously unknown regions are emptied (set to nan). The data storage is implemented as two-dimensional circular buffer to minimize computational effort.\n\n`setPosition(...)` | `move(...)`\n:---: | :---:\n![Grid map iterator](grid_map_core/doc/setposition_method.gif) | ![Submap iterator](grid_map_core/doc/move_method.gif)|\n\n\n## Packages\n\n### grid_map_rviz_plugin\n\nThis [RViz] plugin visualizes a grid map layer as 3d surface plot (height map). A separate layer can be chosen as layer for the color information.\n\n![Grid map visualization in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin.png)\n\n\n### grid_map_visualization\n\nThis node subscribes to a topic of type [grid_map_msgs/GridMap] and publishes messages that can be visualized in [RViz]. The published topics of the visualizer can be fully configure with a YAML parameter file. Any number of visualizations with different parameters can be added. An example is [here](grid_map_demos/config/tutorial_demo.yaml) for the configuration file of the *tutorial_demo*.\n\nPoint cloud | Vectors | Occupancy grid | Grid cells\n--- | --- | --- | ---\n[![Point cloud](grid_map_visualization/doc/point_cloud_preview.jpg)](grid_map_visualization/doc/point_cloud.jpg) | [![Vectors](grid_map_visualization/doc/vectors_preview.jpg)](grid_map_visualization/doc/vectors.jpg) | [![Occupancy grid](grid_map_visualization/doc/occupancy_grid_preview.jpg)](grid_map_visualization/doc/occupancy_grid.jpg) | [![Grid cells](grid_map_visualization/doc/grid_cells_preview.jpg)](grid_map_visualization/doc/grid_cells.jpg)\n\n#### Parameters\n\n* **`grid_map_topic`** (string, default: \"/grid_map\")\n\n    The name of the grid map topic to be visualized. See below for the description of the visualizers.\n\n\n#### Subscribed Topics\n\n* **`/grid_map`** ([grid_map_msgs/GridMap])\n\n    The grid map to visualize.\n\n\n#### Published Topics\n\nThe published topics are configured with the [YAML parameter file](grid_map_demos/config/tutorial_demo.yaml). Possible topics are:\n\n* **`point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a point cloud. Select which layer to transform as points with the `layer` parameter.\n\n        name: elevation\n        type: point_cloud\n        params:\n         layer: elevation\n         flat: false # optional\n\n* **`flat_point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a \"flat\" point cloud, i.e. with all points at the same height *z*. This is convenient to visualize 2d maps or images (or even video streams) in [RViz] with help of its `Color Transformer`. The parameter `height` determines the desired *z*-position of the flat point cloud.\n\n        name: flat_grid\n        type: flat_point_cloud\n        params:\n         height: 0.0\n\n    Note: In order to omit points in the flat point cloud from empty/invalid cells, specify the layers which should be checked for validity with `setBasicLayers(...)`.\n\n* **`vectors`** ([visualization_msgs/Marker])\n\n    Visualizes vector data of the grid map as visual markers. Specify the layers which hold the *x*-, *y*-, and *z*-components of the vectors with the `layer_prefix` parameter. The parameter `position_layer` defines the layer to be used as start point of the vectors.\n\n        name: surface_normals\n        type: vectors\n        params:\n         layer_prefix: normal_\n         position_layer: elevation\n         scale: 0.06\n         line_width: 0.005\n         color: 15600153 # red\n\n* **`occupancy_grid`** ([nav_msgs/OccupancyGrid])\n\n    Visualizes a layer of the grid map as occupancy grid. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bound with `data_min` and `data_max`.\n\n        name: traversability_grid\n        type: occupancy_grid\n        params:\n         layer: traversability\n         data_min: -0.15\n         data_max: 0.15\n\n* **`grid_cells`** ([nav_msgs/GridCells])\n\n    Visualizes a layer of the grid map as grid cells. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bounds with `lower_threshold` and `upper_threshold`.\n\n        name: elevation_cells\n        type: grid_cells\n        params:\n         layer: elevation\n         lower_threshold: -0.08 # optional, default: -inf\n         upper_threshold: 0.08 # optional, default: inf\n\n* **`region`** ([visualization_msgs/Marker])\n\n    Shows the boundary of the grid map.\n\n        name: map_region\n        type: map_region\n        params:\n         color: 3289650\n         line_width: 0.003\n\n*Note: Color values are in RGB form as concatenated integers (for each channel value 0-255). The values can be generated like [this](http://www.wolframalpha.com/input/?i=BitOr%5BBitShiftLeft%5Br%2C16%5D%2C+BitShiftLeft%5Bg%2C8%5D%2C+b%5D+where+%7Br%3D0%2C+g%3D255%2C+b%3D0%7D) as an example for the color green (red: 0, green: 255, blue: 0).*\n\n### grid_map_filters\n\nThe *grid_map_filters* package containts several filters which can be applied a grid map to perform computations on the data in the layers. The grid map filters are based on [ROS Filters], which means that a chain of filters can be configured as a YAML file. Furthermore, additional filters can be written and made available through the ROS plugin mechanism, such as the [`InpaintFilter`](grid_map_cv/include/grid_map_cv/InpaintFilter.hpp) from the `grid_map_cv` package.\n\nSeveral basic filters are provided in the *grid_map_filters* package:\n\n* **`gridMapFilters/ThresholdFilter`**\n\n    Set values below/above a threshold to a specified value.\n\n        name: lower_threshold\n        type: gridMapFilters/ThresholdFilter\n        params:\n          layer: layer_name\n          lower_threshold: 0.0 # alternative: upper_threshold\n          set_to: 0.0 # # Other uses: .nan, .inf\n\n* **`gridMapFilters/MeanInRadiusFilter`**\n\n    Compute for each cell of a layer the mean value inside a radius.\n\n        name: mean_in_radius\n        type: gridMapFilters/MeanInRadiusFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.06 # in m.\n* **`gridMapFilters/MedianFillFilter`**\n\n    Compute for each _NaN_ cell of a layer the median (of finites) inside a patch with radius. \n    Optionally, apply median calculations for values that are already finite, the patch radius for these points is given by existing_value_radius. \n\n        name: median\n        type: gridMapFilters/MedianFillFilter\n        params:\n          input_layer: input\n          output_layer: output\n          fill_hole_radius: 0.11 # in m. \n          filter_existing_values: false # Default is false. If enabled it also does a median computation for existing values. \n          existing_value_radius: 0.2 # in m. Note that this option only has an effect if filter_existing_values is set true. \n    \n* **`gridMapFilters/NormalVectorsFilter`**\n\n    Compute the normal vectors of a layer in a map.\n\n        name: surface_normals\n        type: gridMapFilters/NormalVectorsFilter\n        params:\n          input_layer: input\n          output_layers_prefix: normal_vectors_\n          radius: 0.05\n          normal_vector_positive_axis: z\n\n* **`gridMapFilters/NormalColorMapFilter`**\n\n    Compute a new color layer based on normal vectors layers.\n\n        name: surface_normals\n        type: gridMapFilters/NormalColorMapFilter\n        params:\n          input_layers_prefix: normal_vectors_\n          output_layer: normal_color\n\n* **`gridMapFilters/MathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression with layers of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/MathExpressionFilter\n        params:\n          output_layer: output\n          expression: acos(normal_vectors_z) # Slope.\n          # expression: abs(elevation - elevation_smooth) # Surface roughness.\n          # expression: 0.5 * (1.0 - (slope / 0.6)) + 0.5 * (1.0 - (roughness / 0.1)) # Weighted and normalized sum.\n\n* **`gridMapFilters/SlidingWindowMathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression within a sliding window on a layer of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/SlidingWindowMathExpressionFilter\n        params:\n          input_layer: input\n          output_layer: output\n          expression: meanOfFinites(input) # Box blur\n          # expression: sqrt(sumOfFinites(square(input - meanOfFinites(input))) ./ numberOfFinites(input)) # Standard deviation\n          # expression: 'sumOfFinites([0,-1,0;-1,5,-1;0,-1,0].*elevation_inpainted)' # Sharpen with kernel matrix\n          compute_empty_cells: true\n          edge_handling: crop # options: inside, crop, empty, mean\n          window_size: 5 # in number of cells (optional, default: 3), make sure to make this compatible with the kernel matrix\n          # window_length: 0.05 # instead of window_size, in m\n\n* **`gridMapFilters/DuplicationFilter`**\n\n    Duplicate a layer of a grid map.\n\n        name: duplicate\n        type: gridMapFilters/DuplicationFilter\n        params:\n          input_layer: input\n          output_layer: output\n\n* **`gridMapFilters/DeletionFilter`**\n\n    Delete layers from a grid map.\n\n        name: delete\n        type: gridMapFilters/DeletionFilter\n        params:\n          layers: [color, score] # List of layers.\n\nAdditionally, the *grid_map_cv* package provides the following filters:\n\n* **`gridMapCv/InpaintFilter`**\n\n    Use OpenCV to inpaint/fill holes in a layer.\n\n        name: inpaint\n        type: gridMapCv/InpaintFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.05 # in m\n\n\n## Build Status\n\n### Devel Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idev__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idev__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdev__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdev__grid_map__ubuntu_bionic_amd64/) |\n| doc | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idoc__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idoc__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdoc__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdoc__grid_map__ubuntu_bionic_amd64/) |\n\n### Release Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary/) |\n| grid_map_core | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary/) |\n| grid_map_costmap_2d | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary/) |\n| grid_map_cv | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary/) |\n| grid_map_demos | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary/) |\n| grid_map_filters | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary/) |\n| grid_map_loader | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary/) |\n| grid_map_msgs | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary/) |\n| grid_map_octomap | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary/) |\n| grid_map_pcl | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary/) |\n| grid_map_ros | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary/) |\n| grid_map_rviz_plugin | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary/) |\n| grid_map_sdf | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary/) |\n| grid_map_visualization | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary/) |\n\n\n## Bugs & Feature Requests\n\nPlease report bugs and request features using the [Issue Tracker](https://github.com/anybotics/grid_map/issues).\n\n[ROS]: http://www.ros.org\n[RViz]: http://wiki.ros.org/rviz\n[Eigen]: http://eigen.tuxfamily.org\n[OpenCV]: http://opencv.org/\n[OctoMap]: https://octomap.github.io/\n[PCL]: http://pointclouds.org/\n[costmap_2d]: http://wiki.ros.org/costmap_2d\n[grid_map_msgs/GridMapInfo]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMapInfo.html\n[grid_map_msgs/GridMap]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMap.html\n[grid_map_msgs/GetGridMap]: http://docs.ros.org/api/grid_map_msgs/html/srv/GetGridMap.html\n[sensor_msgs/PointCloud2]: http://docs.ros.org/api/sensor_msgs/html/msg/PointCloud2.html\n[visualization_msgs/Marker]: http://docs.ros.org/api/visualization_msgs/html/msg/Marker.html\n[geometry_msgs/PolygonStamped]: http://docs.ros.org/api/geometry_msgs/html/msg/PolygonStamped.html\n[nav_msgs/OccupancyGrid]: http://docs.ros.org/api/nav_msgs/html/msg/OccupancyGrid.html\n[nav_msgs/GridCells]: http://docs.ros.org/api/nav_msgs/html/msg/GridCells.html\n[ROS Filters]: http://wiki.ros.org/filters\n[EigenLab]: https://github.com/leggedrobotics/EigenLab\n"
 },
 {
  "repo": "Breakthrough/PySceneDetect",
  "language": "Python",
  "readme_contents": "\n![PySceneDetect](https://raw.githubusercontent.com/Breakthrough/PySceneDetect/master/docs/img/pyscenedetect_logo_small.png)\n==========================================================\nVideo Scene Cut Detection and Analysis Tool\n----------------------------------------------------------\n\n[![Build Status](https://img.shields.io/travis/com/Breakthrough/PySceneDetect)](https://travis-ci.com/github/Breakthrough/PySceneDetect) [![PyPI Status](https://img.shields.io/pypi/status/scenedetect.svg)](https://pypi.python.org/pypi/scenedetect/) [![PyPI Version](https://img.shields.io/pypi/v/scenedetect?color=blue)](https://pypi.python.org/pypi/scenedetect/)  [![PyPI License](https://img.shields.io/pypi/l/scenedetect.svg)](http://pyscenedetect.readthedocs.org/en/latest/copyright/)\n\n\n### Latest Release: v0.5.4 (September 14, 2020)\n\n**Main Webpage**:  [py.scenedetect.com](http://py.scenedetect.com)\n\n**Documentation**:  [manual.scenedetect.com](http://manual.scenedetect.com)\n\n**Installation and Dependencies**: https://pyscenedetect.readthedocs.io/en/latest/download/\n\n----------------------------------------------------------\n\n**Quick Install**: To install PySceneDetect via `pip` with all dependencies:\n\n    pip install scenedetect[opencv]\n\nTo enable video splitting support, you will also need to have `mkvmerge` or `ffmpeg` installed on your system. See the documentation on [Video Splitting Support](https://pyscenedetect.readthedocs.io/en/latest/examples/video-splitting/) after installation for details.\n\nRequires Python modules `numpy`, OpenCV `cv2`, and (optional) `tqdm` for displaying progress.\n\n----------------------------------------------------------\n\n**Quick Start (Command Line)**:\n\nSplit the input video wherever a new scene is detected:\n\n    scenedetect -i video.mp4 detect-content split-video\n\nSkip the first 10 seconds of the input video, and output a list of scenes to the terminal:\n\n    scenedetect -i video.mp4 time -s 10s detect-content list-scenes\n\nTo show a summary of all other options and commands:\n\n    scenedetect help\n\nYou can find more examples [on the website](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/) or [in the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/cli/global_options.html).\n\n**Quick Start (Python API)**:\n\nIn the code example below, we create a function `find_scenes()` which will\nload a video, detect the scenes, and return a list of tuples containing the\n(start, end) timecodes of each detected scene.  Note that you can modify\nthe `threshold` argument to modify the sensitivity of the scene detection.\n\n```python\n# Standard PySceneDetect imports:\nfrom scenedetect import VideoManager\nfrom scenedetect import SceneManager\n\n# For content-aware scene detection:\nfrom scenedetect.detectors import ContentDetector\n\ndef find_scenes(video_path, threshold=30.0):\n    # Create our video & scene managers, then add the detector.\n    video_manager = VideoManager([video_path])\n    scene_manager = SceneManager()\n    scene_manager.add_detector(\n        ContentDetector(threshold=threshold))\n\n    # Base timestamp at frame 0 (required to obtain the scene list).\n    base_timecode = video_manager.get_base_timecode()\n\n    # Improve processing speed by downscaling before processing.\n    video_manager.set_downscale_factor()\n\n    # Start the video manager and perform the scene detection.\n    video_manager.start()\n    scene_manager.detect_scenes(frame_source=video_manager)\n\n    # Each returned scene is a tuple of the (start, end) timecode.\n    return scene_manager.get_scene_list(base_timecode)\n```\n\nTo get started, try printing the result from calling `find_scenes` on a small video clip:\n\n```python\n    scenes = find_scenes('video.mp4')\n    print(scenes)\n```\n\nSee [the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/api.html) for the full PySceneDetect API documentation.\n\n----------------------------------------------------------\n\nPySceneDetect is a command-line tool and Python library, which uses OpenCV to analyze a video to find scene changes or cuts.  If `ffmpeg` or `mkvmerge` is installed, the video can also be split into scenes automatically.  A frame-by-frame analysis can also be generated for a video, to help with determining optimal threshold values or detecting patterns/other analysis methods for a particular video.  See [the Usage documentation](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/) for details.\n\nThere are two main detection methods PySceneDetect uses: `detect-threshold` (comparing each frame to a set black level, useful for detecting cuts and fades to/from black), and `detect-content` (compares each frame sequentially looking for changes in content, useful for detecting fast cuts between video scenes, although slower to process).  Each mode has slightly different parameters, and is described in detail below.\n\nIn general, use `detect-threshold` mode if you want to detect scene boundaries using fades/cuts in/out to black.  If the video uses a lot of fast cuts between content, and has no well-defined scene boundaries, you should use the `detect-content` mode.  Once you know what detection mode to use, you can try the parameters recommended below, or generate a statistics file (using the `-s` / `--statsfile` flag) in order to determine the correct paramters - specifically, the proper threshold value.\n\nNote that PySceneDetect is currently in beta; see Current Features & Roadmap below for details.  For help or other issues, you can contact me on [my website](http://www.bcastell.com/about/), or we can chat in #pyscenedetect on Freenode.  Feel free to submit any bugs or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues) here on Github.\n\n\nUsage\n----------------------------------------------------------\n\n - [Basic Usage](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/)\n - [PySceneDetect Manual](https://pyscenedetect-manual.readthedocs.io/en/latest/), covers `scenedetect` command and Python API\n - [Example: Detecting and Splitting Scenes in Movie Clip](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/)\n\n\nCurrent Features & Roadmap\n----------------------------------------------------------\n\nYou can [view the latest features and version roadmap on Readthedocs](http://pyscenedetect.readthedocs.org/en/latest/features/).\nSee [`docs/changelog.md`](https://github.com/Breakthrough/PySceneDetect/blob/master/docs/changelog.md) for a list of changes in each version, or visit [the Releases page](https://github.com/Breakthrough/PySceneDetect/releases) to download a specific version.  Feel free to submit any bugs/issues or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues).\n\nAdditional features being planned or in development can be found [here (tagged as `feature`) in the issue tracker](https://github.com/Breakthrough/PySceneDetect/issues?q=is%3Aissue+is%3Aopen+label%3Afeature).  You can also find additional information about PySceneDetect at [http://www.bcastell.com/projects/PySceneDetect/](http://www.bcastell.com/projects/PySceneDetect/).\n\n\n----------------------------------------------------------\n\nLicensed under BSD 3-Clause (see the `LICENSE` file for details).\n\nCopyright (C) 2014-2020 Brandon Castellano.\nAll rights reserved.\n\n"
 },
 {
  "repo": "emgucv/emgucv",
  "language": "C#",
  "readme_contents": "==================================================================\n\nA cross platform .Net wrapper for the OpenCV image-processing library. Allows OpenCV functions to be called from .NET compatible languages. The wrapper can be compiled by Visual Studio, Xamarin Studio and Unity, it can run on Windows, Linux, Mac OS, iOS and Android.\n\nPlease visit our project webpage for more information:\nhttp://www.emgu.com/wiki/index.php/Main_Page\n\nBuild instructions can be found here:\nhttp://www.emgu.com/wiki/index.php/Download_And_Installation#Building_from_Git\n"
 },
 {
  "repo": "datitran/object_detector_app",
  "language": "Python",
  "readme_contents": "# Object-Detector-App\n\nA real-time object recognition application using [Google's TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) and [OpenCV](http://opencv.org/).\n\n## Getting Started\n1. `conda env create -f environment.yml`\n2. `python object_detection_app.py` / `python object_detection_multithreading.py`\n    Optional arguments (default value):\n    * Device index of the camera `--source=0`\n    * Width of the frames in the video stream `--width=480`\n    * Height of the frames in the video stream `--height=360`\n    * Number of workers `--num-workers=2`\n    * Size of the queue `--queue-size=5`\n    * Get video from HLS stream rather than webcam '--stream-input=http://somertmpserver.com/hls/live.m3u8'\n    * Send stream to livestreaming server '--stream-output=--stream=http://somertmpserver.com/hls/live.m3u8'\n\n## Tests\n```\npytest -vs utils/\n```\n\n## Requirements\n- [Anaconda / Python 3.5](https://www.continuum.io/downloads)\n- [TensorFlow 1.2](https://www.tensorflow.org/)\n- [OpenCV 3.0](http://opencv.org/)\n\n## Notes\n- OpenCV 3.1 might crash on OSX after a while, so that's why I had to switch to version 3.0. See open issue and solution [here](https://github.com/opencv/opencv/issues/5874).\n- Moving the `.read()` part of the video stream in a multiple child processes did not work. However, it was possible to move it to a separate thread.\n\n## Copyright\n\nSee [LICENSE](LICENSE) for details.\nCopyright (c) 2017 [Dat Tran](http://www.dat-tran.com/).\n"
 },
 {
  "repo": "RubensZimbres/Repo-2017",
  "language": "Python",
  "readme_contents": "# Python Codes in Data Science\n\nCodes in NLP, Deep Learning, Reinforcement Learning and Artificial Intelligence\n\n<b> Welcome to my GitHub repo. </b>\n\nI am a Data Scientist and I code in R, Python and Wolfram Mathematica. Here you will find some Machine Learning, Deep Learning, Natural Language Processing and Artificial Intelligence models I developed.\n\n<b> Outputs of the models can be seen at my portfolio: </b> https://drive.google.com/file/d/0B0RLknmL54khdjRQWVBKeTVxSHM/view?usp=sharing\n\n----------------\nKeras version used in models: keras==1.1.0\n\n<b> Autoencoder for Audio  </b> is a model where I compressed an audio file and used Autoencoder to reconstruct the audio file, for use in phoneme classification.\n\n<b> Collaborative Filtering  </b> is a Recommender System where the algorithm predicts a movie review based on genre of movie and similarity among people who watched the same movie.\n\n<b> Convolutional NN Lasagne  </b> is a Convolutional Neural Network model in Lasagne to solve the MNIST task.\n\n<b> Ensembled Machine Learning </b> is a .py file where 7 Machine Learning algorithms are used in a classification task with 3 classes and all possible hyperparameters of each algorithm are adjusted. Iris dataset of scikit-learn.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Ensembled.MachineLearning.png?raw=true>\n</p>\n\n<b> GAN Generative Adversarial  </b> are models of Generative Adversarial Neural Networks.\n\n<b> Hyperparameter Tuning RL  </b> is a model where hyperparameters of Neural Networks are adjusted via Reinforcement Learning. According to a reward, hyperparameter tuning (environment) is changed through a policy (mechanization of knowledge) using the Boston Dataset. Hyperparameters tuned are: learning rate, epochs, decay, momentum, number of hidden layers and nodes and initial weights.\n\n<b> Keras Regularization L2  </b> is a Neural Network model for regression made with Keras where a L2 regularization was applied to prevent overfitting.\n\n<b> Lasagne Neural Nets Regression  </b> is a Neural Network model based in Theano and Lasagne, that makes a linear regression with a continuous target variable and reaches 99.4% accuracy. It uses the DadosTeseLogit.csv sample file.\n\n<b> Lasagne Neural Nets + Weights  </b> is a Neural Network model based in Theano and Lasagne, where is possible to visualize weights between X1 and X2 to hidden layer. Can also be adapted to visualize weights between hidden layer and output. It uses the DadosTeseLogit.csv sample file.\n\n<b> Multinomial Regression  </b> is a regression model where target variable has 3 classes.\n\n<b> Neural Networks for Regression  </b> shows multiple solutions for a regression problem, solved with sklearn, Keras, Theano and Lasagne. It uses the Boston dataset sample file from sklearn and reaches more than 98% accuracy.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/HiddenLayers.jpg?raw=true>\n</p>\n\n<b> NLP + Naive Bayes Classifier  </b> is a model where movie reviews were labeled as positive and negative and the algorithm then classifies a totally new set of reviews using Logistic Regression, Decision Trees and Naive Bayes, reaching an accuracy of 92%.\n\n<b> NLP Anger Analysis  </b> is a Doc2Vec model associated with Word2Vec model to analyze level of anger using synonyms in consumer complaints of a U.S. retailer in Facebook posts.\n\n<b> NLP Consumer Complaint  </b> is a model where Facebook posts of a U.S. computer retailer were scraped, tokenized, lemmatized and applied Word2Vec. After that, t-SNE and Latent Dirichlet Allocation were developed in order to classify the arguments and weights of each keyword used by a consumer in his complaint. The code also analyzes frequency of words in 100 posts.\n\n<b> NLP Convolutional Neural Network </b> is a Convolutional Neural Network for Text in order to classify movie reviews.\n\n<b> NLP Doc2Vec  </b> is a Natural Language Procesing file where cosine similarity among phrases is measured through Doc2Vec.\n\n<b> NLP Document Classification  </b> is a code for Document Classification according to Latent Dirichlet Allocation.\n\n<b> NLP Facebook Analysis  </b> analyzes Facebook posts regarding Word Frequency and Topic Modelling using LDA.\n\n<b> NLP Facebook Scrap  </b> is a Python code for scraping data from Facebook.\n\n<b> NLP - Latent Dirichlet Allocation  </b> is a Natural Language Processing model where a Wikipedia page on Statistical Inference is classified regarding topics, using Latent Dirichlet Allocation with Gensim, NLTK, t-SNE and K-Means.\n\n<b> NLP Probabilistic ANN  </b> is a Natural Language Processing model where sentences are vectorized by Gensim and a probabilistic Neural Network model is deveoped using Gensim, for sentiment analysis.\n\n<b> NLP Semantic Doc2Vec + Neural Network  </b> is a model where positive and negative movie reviews were extracted and semantically classified with NLTK and BeautifulSoup, then labeled as positive or negative. Text was then used as an input for the Neural Network model training. After training, new sentences are entered in the Keras Neural Network model and then classified. It uses the zip file.\n\n<b> NLP Sentiment Positive  </b> is a model that identifies website content as positive, neutral or negative using BeautifulSoup and NLTK libraries, plotting the results.\n\n<b> NLP Twitter Analysis ID #  </b> is a model that extracts posts from Twitter based in ID of user or Hashtag.\n\n<b> NLP Twitter Scrap  </b> is a model that scraps Twitter data and shows the cleaned text as output.\n\n<b> NLP Twitter Streaming  </b> is a model of analysis of real-time data from Twitter (under development).\n\n<b> NLP Twitter Streaming Mood  </b> is a model where the evolution of mood Twitter posts is measured during a period of time.\n\n<b> NLP Wikipedia Summarization  </b> is a Python code that summarizes any given page in a few sentences.\n\n<b> NLP Word Frequency  </b> is a model that calculates the frequency of nouns, verbs, words in Facebook posts.\n\n<b> Probabilistic Neural Network  </b> is a Probabilistic Neural Network for Time Series Prediction.\n\n<b> REAL-TIME Twitter Analysis  </b> is a model where Twitter streaming is extracted, words and sentences tokenized, word embeddings were created, topic modeling was made and classified using K-Means. Then, NLTK SentimentAnalyzer was used to classify each sentence of the streaming into positive, neutral or negative. Accumulated sum was used to generate the plot and the code loops each 1 second, collecting new tweets.\n\n<b> RESNET-2  </b> is a Deep Residual Neural Network.\n\n<b> ROC Curve Multiclass  </b> is a .py file where Naive Bayes was used to solve the IRIS Dataset task and ROC curve of different classes are plotted.\n\n<b> SQUEEZENET  </b> is a simplified version of the AlexNet.\n\n<b> Stacked Machine Learning  </b> is a .py notebook where t-SNE, Principal Components Analysis and Factor Analysis were applied to reduce dimensionality of data. Classification performances were measured after applying K-Means.\n\n<b> Support Vector Regression  </b> is a SVM model for non linear regression in an artificial dataset.\n\n<b> Text-to-Speech  </b> is a .py file where Python speaks any given text and saves it as an audio .wav file.\n\n<b> Time Series ARIMA </b>  is a ARIMA model to forecast time series, with an error margin of 0.2%.\n\n<b> Time Series Prediction with Neural Networks - Keras </b>  is a Neural Network model to forecast time series, using Keras with an adaptive learning rate depending upon derivative of loss.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/ARIMA.10Period.png?raw=true> \n</p>\n\n<b> Variational Autoencoder  </b> is a VAE made with Keras.\n\n<b> Web Crawler  </b> is a code that scraps data from different URLs of a hotel website.\n\n<b> t-SNE Dimensionality Reduction  </b> is a t-SNE model for dimensionality reduction which is compared to Principal Components Analysis regarding its discriminatory power.\n\n<b> t-SNE PCA + Neural Networks  </b> is a model that compares performance or Neural Networks made after t-SNE, PCA and K-Means.\n\n<b> t-SNE PCA LDA embeddings </b> is a model where t-SNE, Principal Components Analysis, Linear Discriminant Analysis and Random Forest embeddings are compared in a task to classify clusters of similar digits.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Doc2Vec.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/t_SNE_Lk.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/RESNET_Me.jpg?raw=true>\n</p>\n\n"
 },
 {
  "repo": "go-opencv/go-opencv",
  "language": "Go",
  "readme_contents": "Go OpenCV binding\n==================\n\n[![Join the chat at https://gitter.im/lazywei/go-opencv](https://badges.gitter.im/lazywei/go-opencv.svg)](https://gitter.im/lazywei/go-opencv?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nA Golang binding for [OpenCV](http://opencv.org/).\n\nOpenCV 1.x C API bindings through CGO, and OpenCV 2+ C++ API ([`GoCV`](gocv/)) through SWIG.\n\n-------------------\n\n## Disclaimer\n\nThis is a fork of [chai's go-opencv](https://github.com/chai2010/opencv), which has only OpenCV1 support through CGO, and all credits for OpenCV1 wrapper (except files in `gocv/` folder) should mainly go to Chai. At the time of the fork (Dec 9, 2013) the original project was inactive and was hosted on Google Code, which was a little inconvenient for community contribution. Hence, I decided to host a fork on Github so people can contribute to this project easily. Since then, some patches were added by community, and some experimental OpenCV 2 wrappers were added as well. That means this fork went on a little bit divergent way comparing to the origin project. However, now the origin project seems to be active again and be moved to GitHub starting from Aug 25, 2014. Efforts to merge the two projects are very welcome.\n\n-------------------\n\n## Install\n\n### Linux & Mac OS X\n\nInstall Go and OpenCV, you might want to install both of them via `apt-get` or `homebrew`.\n\nYou can reference the [link](https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html) to install required packages.\n\n```\ngo get github.com/go-opencv/go-opencv\ncd $GOPATH/src/github.com/go-opencv/go-opencv/samples\ngo run hellocv.go\n```\n\n### Windows\n\n- Install Go and MinGw\n- install OpenCV-2.4.x to MinGW dir\n\n```\n# libopencv*.dll --> ${MinGWRoot}\\bin\n# libopencv*.lib --> ${MinGWRoot}\\lib\n# include\\opencv --> ${MinGWRoot}\\include\\opencv\n# include\\opencv2 --> ${MinGWRoot}\\include\\opencv2\n\ngo get github.com/go-opencv/go-opencv\ncd ${GoOpenCVRoot}/trunk/samples && go run hellocv.go\n```\n\n## [WIP] OpenCV2 (GoCV)\n\nAfter OpenCV 2.x+, the core team no longer develop and maintain C API. Therefore, CGO will not be used in CV2 binding. Instead, we are using SWIG for wrapping. The support for OpenCV2 is currently under development, and whole code will be placed under `gocv` package.\n\nIf you want to use CV2's API, please refer to the code under `gocv/` directory. There is no too many documents for CV2 wrapper yet, but you can still find the example usages in `*_test.go`.\n\nPlease also note that the basic data structures in OpenCV (e.g., `cv::Mat`, `cv::Point3f`) are wrapped partially for now. For more detail on how to use these types, please refer to [GoCV's README](gocv/README.md).\n\n*Requirement*: we will build the wrappers based on [mat64](https://godoc.org/github.com/gonum/matrix/mat64), given it is much easier to manipulate the underlaying data. In most case, it is not necessary to access the original CV data, e.g., `cv::Mat` can be converted from/to `*mat64.Dense`.\n\n## Example\n\n### OpenCV2's initCameraMatrix2D\n\n```go\npackage main\n\nimport . \"github.com/go-opencv/go-opencv/gocv\"\nimport \"github.com/gonum/matrix/mat64\"\n\nfunc main() {\n\n\tobjPts := mat64.NewDense(4, 3, []float64{\n\t\t0, 25, 0,\n\t\t0, -25, 0,\n\t\t-47, 25, 0,\n\t\t-47, -25, 0})\n\n\timgPts := mat64.NewDense(4, 2, []float64{\n\t\t1136.4140625, 1041.89208984,\n\t\t1845.33190918, 671.39581299,\n\t\t302.73373413, 634.79998779,\n\t\t1051.46154785, 352.76107788})\n\n\tcamMat := GcvInitCameraMatrix2D(objPts, imgPts)\n\tfmt.Println(camMat)\n}\n```\n\n\n### Resizing\n\n```go\npackage main\n\nimport opencv \"github.com/go-opencv/go-opencv/opencv\"\n\nfunc main() {\n\tfilename := \"bert.jpg\"\n\tsrcImg := opencv.LoadImage(filename)\n\tif srcImg == nil {\n\t\tpanic(\"Loading Image failed\")\n\t}\n\tdefer srcImg.Release()\n\tresized1 := opencv.Resize(srcImg, 400, 0, 0)\n\tresized2 := opencv.Resize(srcImg, 300, 500, 0)\n\tresized3 := opencv.Resize(srcImg, 300, 500, 2)\n\topencv.SaveImage(\"resized1.jpg\", resized1, nil)\n\topencv.SaveImage(\"resized2.jpg\", resized2, nil)\n\topencv.SaveImage(\"resized3.jpg\", resized3, nil)\n}\n```\n\n### Webcam\n\nYet another cool example is created by @saratovsource which demos how to use webcam:\n\n```\ncd samples\ngo run webcam.go\n```\n\n### More\n\nYou can find more samples at: https://github.com/go-opencv/go-opencv/tree/master/samples\n\n## How to contribute\n\n- Fork this repo\n- Clone the main repo, and add your fork as a remote\n\n  ```\n  git clone https://github.com/go-opencv/go-opencv.git\n  cd go-opencv\n  git remote rename origin upstream\n  git remote add origin https://github.com/your_github_account/go-opencv.git\n  ```\n\n- Create new feature branch\n\n  ```\n  git checkout -b your-feature-branch\n  ```\n\n- Commit your change and push it to your repo \n\n  ```\n  git commit -m 'new feature'\n  git push origin your-feature-branch\n  ```\n\n- Open a pull request!\n\n"
 },
 {
  "repo": "abidrahmank/OpenCV2-Python-Tutorials",
  "language": "Python",
  "readme_contents": "OpenCV2-Python-Guide\n====================\n\nThis repo contains tutorials on OpenCV-Python library using new cv2 interface\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n=======================================================================\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\nPlease try the examples with OpenCV 3x before sending any bug reports\n\nData files\n-----------\n\nThe input data used in these tutorials are given in **Data** folder\n\nOnline\n---------\n\n* **For official tutorials, please visit : http://docs.opencv.org/trunk/doc/py_tutorials/py_tutorials.html**\n* https://opencv-python-tutroals.readthedocs.org/en/latest/index.html - This is only for checking. May contain lots of errors, please stick to the official tutorials.\n\nOffline\n---------\nTo build docs from source,\n* Install sphinx\n* Download/Clone this repo and navigate to the base folder\n* run command : `make html` , html docs will be available in **build/html/** folder\n"
 },
 {
  "repo": "yangkun19921001/Blog",
  "language": "HTML",
  "readme_contents": "<p align=\"center\">\n<a href=\"https://github.com/yangkun19921001/Blog\" target=\"_blank\">\n\t<img src=\"https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200308171526.png\" width=\"300px\"/>\n</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"XXX\"><img src=\"https://img.shields.io/badge/\u9605\u8bfb-read-brightgreen.svg\" alt=\"\u5728\u7ebf\u9605\u8bfb\"></a>\n</p>\n\n\n\n   * [Blog](#blog)\n      * [\u9762\u8bd5](#\u9762\u8bd5)\n      * [Flutter \u7cfb\u5217](#flutter-\u7cfb\u5217)\n      * [\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217](#\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217)\n      * [Java \u6e90\u7801\u5206\u6790](#java-\u6e90\u7801\u5206\u6790)\n      * [Android \u6e90\u7801\u5206\u6790](#android-\u6e90\u7801\u5206\u6790)\n      * [\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790](#\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790)\n      * [\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217](#\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217)\n      * [\u8bbe\u8ba1\u6a21\u5f0f](#\u8bbe\u8ba1\u6a21\u5f0f)\n      * [\u9ad8\u7ea7 UI \u7cfb\u5217](#\u9ad8\u7ea7-ui-\u7cfb\u5217)\n      * [\u97f3\u89c6\u9891](#\u97f3\u89c6\u9891)\n      * [\u5f00\u6e90\u9879\u76ee](#\u5f00\u6e90\u9879\u76ee)\n      * [\u82f1\u8bed](#\u82f1\u8bed)\n      * [\u5173\u4e8e\u6211](#\u5173\u4e8e\u6211)\n\n# Blog\n\n\u7528\u4e8e\u8bb0\u5f55\u751f\u6d3b\u3001\u5b66\u4e60\u3001\u5de5\u4f5c\u7b49\u5185\u5bb9\u3002\n\n## \u9762\u8bd5\n\n- [Android \u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5b9d\u5178](https://github.com/yangkun19921001/Blog/blob/master/\u7b14\u8bd5\u9762\u8bd5/Android\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5fc5\u5907/README.md)\n- [1307 \u9875\u5b57\u8282\u8df3\u52a8 Android \u9762\u8bd5\u5168\u5957\u771f\u9898\u89e3\u6790\u5728\u4e92\u8054\u7f51\u706b\u4e86 \uff0c\u5b8c\u6574\u7248\u5f00\u653e\u4e0b\u8f7d](https://mp.weixin.qq.com/s/Crty_REXRVMEhI20XAeLGw)\n\n## Flutter \u7cfb\u5217\n\n- [Google \u4e3a\u4ec0\u4e48\u4ee5 Flutter \u4f5c\u4e3a\u539f\u751f\u7a81\u7834\u53e3](https://juejin.im/post/5c91f0f25188256b7463868e)\n- [Flutter (\u4e00) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3(\u53d8\u91cf\u3001\u5185\u7f6e\u7c7b\u578b\u3001\u51fd\u6570\u3001\u64cd\u4f5c\u7b26\u3001\u6d41\u7a0b\u63a7\u5236\u8bed\u53e5)](https://juejin.im/post/5c91ed15518825573578c31f)\n- [Flutter (\u4e8c) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 \uff08\u5f02\u5e38,\u7c7b,Mixin, \u6cdb\u578b,\u5e93\uff09](https://juejin.im/post/5c939b275188252d863cc797)\n- [Flutter (\u4e09) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 (\u5f02\u6b65,\u751f\u6210\u5668,\u9694\u79bb,\u5143\u6570\u636e,\u6ce8\u91ca)](https://juejin.im/post/5c962b356fb9a0710e47e361)\n- [Flutter (\u56db) \u57fa\u7840 Widgets\u3001Material Components Widget \u5168\u9762\u4ecb\u7ecd](https://juejin.im/post/5cbedc816fb9a03202221a37)\n\n## \u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217\n\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e00)\u5192\u6ce1\u4e0e\u9009\u62e9\u6392\u5e8f](https://juejin.im/post/5c9442cb5188252da9013153)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e8c)\u7ebf\u6027\u8868\u7684\u94fe\u5f0f\u5b58\u50a8\u7ed3\u6784](https://juejin.im/post/5c9449dd5188252da22508e3)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e09)\u6808\u4e0e\u6808\u7684\u5e94\u7528](https://juejin.im/post/5c9453965188252db02e4be6)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u56db)\u54c8\u5e0c\u8868\u7684\u601d\u60f3\u548c\u4e8c\u53c9\u6811\u5165\u95e8](https://juejin.im/post/5c9456f25188252d971438a9)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e94) \u5206\u6cbb\u6cd5 (\u4e8c\u5206\u67e5\u627e\u3001\u5feb\u901f\u6392\u5e8f\u3001\u5f52\u5e76\u6392\u5e8f)](https://juejin.im/post/5c945c245188252d863cc969)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u516d)\u4e8c\u53c9\u6392\u5e8f\u6811](https://juejin.im/post/5c9460e25188252d971438c4)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e03) huffman \u6811\u4e0e AVL \u6811](https://juejin.im/post/5c9464515188252d7e34df85)\n\n## Java \u6e90\u7801\u5206\u6790\n\n- [\u6e90\u7801\u5206\u6790 (\u4e00) ArrayList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c94695c5188252daa18f487)\n- [\u6e90\u7801\u5206\u6790 (\u4e8c) LinkedList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946b555188252d7941fef2)\n- [\u6e90\u7801\u5206\u6790 (\u4e09) Stack \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946d525188252d5f0fd9ee)\n- [\u9762\u8bd5\u5b98: \u6211\u5fc5\u95ee\u7684\u5bb9\u5668\u77e5\u8bc6\u70b9!](https://juejin.im/post/5e88afce518825085d6ced2e)\n\n## Android \u6e90\u7801\u5206\u6790\n\n- [\u4ece setContentView \u5165\u53e3\uff0c\u5168\u65b9\u4f4d\u5206\u6790 LayoutInflater](https://juejin.im/post/5d6a7f2be51d4561e43a6ce8)\n- [\u5206\u6790\u5e7f\u64ad \u7684 registerReceiver\u3001sendBroadcast\u3001 onReceive \u7cfb\u7edf\u5230\u5e95\u505a\u4e86\u4ec0\u4e48?](https://juejin.im/post/5d752aad518825346e5f2b31)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e00) SystemServer \u8fdb\u7a0b\u542f\u52a8](https://juejin.im/post/5db3f95ee51d4529e83947f9)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e8c) Launcher \u542f\u52a8](https://juejin.im/post/5db5565cf265da4d0f14053c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e09) \u5e94\u7528\u7a0b\u5e8f\u8fdb\u7a0b\u521b\u5efa\u5230\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u7684\u8fc7\u7a0b](https://juejin.im/post/5db599bc6fb9a0203b234b08)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u56db) Activity \u542f\u52a8](https://juejin.im/post/5db85da4e51d4529f73e27fb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e94) Service \u542f\u52a8](https://juejin.im/post/5dbb0507f265da4cf406f735)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516d) BroadcastReceiver \u542f\u52a8](https://juejin.im/post/5dbd5144e51d456eec1830af)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e03) ContentProvider \u542f\u52a8](https://juejin.im/post/5dbe8e6ce51d456f0006634a)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516b) ActivityManagerService](https://juejin.im/post/5dc4339c5188254e7a15585c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e5d) WindowManager](https://juejin.im/post/5dc7d729f265da4cf85d7feb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u5341) WindowManagerService \u7684\u7a97\u53e3\u7ba1\u7406](https://juejin.im/post/5dcab476f265da4d0a68e3ab)\n\n## \u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790\n\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e00) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d89e9c051882509662c5620)\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e8c) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5d8c83836fb9a04dec52f19d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Rxjava2 \u7684\u57fa\u672c\u6267\u884c\u6d41\u7a0b\u3001\u7ebf\u7a0b\u5207\u6362\u539f\u7406](https://juejin.im/post/5d9b489251882560e87e620e)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e00) \u540c\u6b65\u3001\u5f02\u6b65\u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d9ef57c51882514316fe33a)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e8c) \u62e6\u622a\u5668\u7684\u9b45\u529b](https://juejin.im/post/5da306965188252ba420a15d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e09) \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5da5dcd551882544432558f8)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Retrofit \u7f51\u7edc\u8bf7\u6c42\uff0c\u5305\u542b RxJava + Retrofit + OKhttp \u8bf7\u6c42\u8bb2\u89e3](https://juejin.im/post/5da802d051882508866e9463)\n\n## \u6027\u80fd\u4f18\u5316\u7cfb\u5217\n\n- [\u6027\u80fd\u4f18\u5316(\u4e00)APP \u542f\u52a8\u4f18\u5316\uff08\u4e0d\u6562\u8bf4\u79d2\u5f00\uff0c\u4f46\u662f\u6700\u7ec8\u4f18\u5316\u5b8c\u771f\u4e0d\u5230 1s\uff09](https://juejin.im/post/5cc19374e51d456e781f2036)\n- [\u6027\u80fd\u4f18\u5316(\u4e8c) UI \u7ed8\u5236\u4f18\u5316](https://juejin.im/post/5cc2dfc7e51d456e845b4260)\n- [\u6027\u80fd\u4f18\u5316(\u4e09)\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0,\u81f3\u5c11\u89e3\u51b3 APP \u4e2d 90 % \u7684\u5185\u5b58\u5f02\u5e38\u95ee\u9898](https://juejin.im/post/5cd82a3ee51d456e781f20ce)\n- [\u6027\u80fd\u4f18\u5316(\u56db) ubuntu \u5b8c\u7f8e\u7f16\u8bd1 libjpeg \u56fe\u50cf\u538b\u7f29\u5e93\uff0c\u5ab2\u7f8e\u5fae\u4fe1\u56fe\u7247\u538b\u7f29\u7b97\u6cd5](https://juejin.im/post/5ce15d0ce51d45106e5e6dac)\n- [\u6027\u80fd\u4f18\u5316 (\u4e94) \u957f\u56fe\u4f18\u5316\uff0c\u4eff\u5fae\u535a\u52a0\u8f7d\u957f\u56fe\u65b9\u5f0f](https://juejin.im/post/5ce96da06fb9a07ee4633f50)\n- [\u6027\u80fd\u4f18\u5316 (\u516d) \u8001\u677f\u95ee\u4f60\u54b1\u4eec APP \u8017\u7535\u91cf\uff0c\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u80fd\u77e5\u9053\u8fd8\u80fd\u505a\u51fa\u5bf9\u5e94\u4f18\u5316\u3002](https://juejin.im/post/5ce9088f6fb9a07ee4633ef3)\n- [\u6027\u80fd\u4f18\u5316 (\u4e03) APK \u52a0\u56fa\u4e4b Dex \u52a0\u89e3\u5bc6\uff0c\u53cd\u7f16\u8bd1\u90fd\u770b\u4e0d\u5230\u9879\u76ee\u4e3b\u8981\u4ee3\u7801\u3002](https://juejin.im/post/5cf3ee295188256aa76bb1e1)\n- [\u6027\u80fd\u4f18\u5316 (\u516b) APK \u52a0\u56fa\u4e4b\u52a8\u6001\u66ff\u6362 Application](https://juejin.im/post/5cf69d30f265da1b897abd53)\n- [\u6027\u80fd\u4f18\u5316 (\u4e5d) APP \u7a33\u5b9a\u6027\u4e4b\u70ed\u4fee\u590d\u539f\u7406\u63a2\u7d22](https://juejin.im/post/5cfce989f265da1b6c5f6991)\n- [\u6027\u80fd\u4f18\u5316 (\u5341) APP \u6301\u7eed\u8fd0\u884c\u4e4b\u8fdb\u7a0b\u4fdd\u6d3b\u5b9e\u73b0](https://juejin.im/post/5cffe4d4f265da1b695d55d4)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e00) ProGuard \u5bf9\u4ee3\u7801\u548c\u8d44\u6e90\u538b\u7f29](https://juejin.im/post/5d05dab06fb9a07ea9446e21)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e8c) APK \u6781\u9650\u538b\u7f29(\u8d44\u6e90\u8d8a\u591a,\u6548\u679c\u8d8a\u663e\u8457)](https://juejin.im/post/5d0627f7f265da1bd4247e76)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e09) \u6709\u4e86 breakpad , native \u5d29\u6e83\u518d\u4e5f\u4e0d\u6015\u4e86](https://juejin.im/post/5d811f82518825446d0d15e1)\n- [\u9762\u8bd5\u5b98: \u8bf4\u4e00\u4e0b\u4f60\u505a\u8fc7\u54ea\u4e9b\u6027\u80fd\u4f18\u5316?](https://juejin.im/post/5e7f12ba518825736d2780a0)\n\n## \u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217\n\n- [\u79fb\u52a8\u67b6\u6784 (\u4e00) \u67b6\u6784\u7b2c\u4e00\u6b65\uff0c\u5b66\u4f1a\u753b\u5404\u79cd UML \u56fe\u3002](https://juejin.im/post/5d2e048cf265da1b9163c7c8)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e8c) Android \u4e2d Handler \u67b6\u6784\u5206\u6790\uff0c\u5e76\u5b9e\u73b0\u81ea\u5df1\u7b80\u6613\u7248\u672c Handler \u6846\u67b6](https://juejin.im/post/5d30b4a8f265da1b855c8f45)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e09) AMS \u6e90\u7801\u5206\u6790](https://juejin.im/post/5d3463b4e51d45109725ff47)\n- [\u79fb\u52a8\u67b6\u6784 (\u56db) EventBus 3.1.1 \u6e90\u7801\u5206\u6790\u53ca\u5b9e\u73b0\u81ea\u5df1\u7684\u8f7b\u91cf\u7ea7 EventBus \u6846\u67b6\uff0c\u6839\u636e TAG \u53d1\u9001\u63a5\u6536\u4e8b\u4ef6\u3002](https://juejin.im/post/5d3c5b965188252c9c52beba)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e94) \u4ec5\u4ec5\u5bf9 Java Bean \u7684\u64cd\u4f5c\uff0c\u5c31\u80fd\u5b8c\u6210\u5bf9\u6570\u636e\u6301\u4e45\u5316\u3002](https://juejin.im/post/5d49a6c9518825056564a074)\n- [\u79fb\u52a8\u67b6\u6784 (\u516d) \u8f7b\u91cf\u7ea7\u8fdb\u7a0b\u95f4\u901a\u4fe1\u6846\u67b6\u8bbe\u8ba1](https://juejin.im/post/5d4fe70d518825168d37a740)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e03) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u7ec4\u4ef6\u5316\u6846\u67b6\u6a21\u578b](https://juejin.im/post/5d5bcb85f265da03e369839d)\n- [\u79fb\u52a8\u67b6\u6784 (\u516b) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u52a8\u6001\u5316\u52a0\u8f7d\u63d2\u4ef6\u6280\u672f\u6a21\u578b\u5b9e\u73b0](https://juejin.im/post/5d6246d36fb9a06b0f23ed6e)\n\n## \u8bbe\u8ba1\u6a21\u5f0f\n\n- [\u901a\u8fc7\u4ee3\u7801\u793a\u4f8b\u6765\u5b66\u4e60\u9762\u5411\u5bf9\u8c61\u516d\u5927\u539f\u5219](https://juejin.im/post/5d669bfc6fb9a06b1b19d25e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e00) \u901a\u8fc7\u7406\u8bba + \u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d\u5355\u4f8b\u6a21\u5f0f\u6765\u5b66\u4e60\u5355\u4f8b](https://juejin.im/post/5d6a8121e51d4561e6237193)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e8c ) \u7ed3\u5408\u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d Builder \u6765\u5b66\u4e60\u5efa\u9020\u8005\u6a21\u5f0f](https://juejin.im/post/5d6bcd0ee51d4561d41d2e36)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e09 ) \u539f\u578b\u6a21\u5f0f](https://juejin.im/post/5d6e7eaa5188250d9432b463)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u56db ) \u5de5\u5382\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7125d5f265da03d7283ce9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e94) \u62bd\u8c61\u5de5\u5382\u6a21\u5f0f](https://juejin.im/post/5d71278ef265da03d063c265)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516d) \u7b56\u7565\u6a21\u5f0f](https://juejin.im/post/5d7273abf265da03b31bf1ec)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e03) \u72b6\u6001\u6a21\u5f0f](https://juejin.im/post/5d738f40e51d4561c41fb8a6)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516b) \u8d23\u4efb\u94fe\u6a21\u5f0f](https://juejin.im/post/5d749589f265da03d871e36e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e5d) \u89c2\u5bdf\u8005\u6a21\u5f0f](https://juejin.im/post/5d7501f36fb9a06ac93cf457)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u5341 ) \u5907\u5fd8\u5f55\u6a21\u5f0f](https://juejin.im/post/5d77ab1de51d4561c83e7cd9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e00) \u8fed\u4ee3\u5668\u6a21\u5f0f](https://juejin.im/post/5d791e176fb9a06ae61ae3cc)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e8c) \u6a21\u677f\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7a759fe51d4561c02a25db)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e09) \u8bbf\u95ee\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b24b1e51d4561d41d2e96)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u56db) \u4e2d\u4ecb\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b63b3e51d4561ea1a94ed)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e94) \u4ee3\u7406\u6a21\u5f0f](https://juejin.im/post/5d7c6bc7f265da03f3338254)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516d) \u7ec4\u5408\u6a21\u5f0f](https://juejin.im/post/5d7cbda7f265da03d2116f64)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e03) \u9002\u914d\u5668\u3001\u88c5\u9970\u3001\u4eab\u5143\u6a21\u5f0f](https://juejin.im/post/5d7dfff751882539aa5ad79c)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516b) \u5916\u89c2\u3001\u6865\u63a5\u6a21\u5f0f](https://juejin.im/post/5d7e01f4f265da03b5747aac)\n\n## \u9ad8\u7ea7 UI \u7cfb\u5217\n\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e00) View \u7684\u57fa\u7840\u77e5\u8bc6\u4f60\u5fc5\u987b\u77e5\u9053](https://juejin.im/post/5dcff9d3f265da0bd20af0da)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e8c) \u6df1\u5165\u7406\u89e3 Android 8.0 View \u89e6\u6478\u4e8b\u4ef6\u5206\u53d1\u673a\u5236](https://juejin.im/post/5dd7a4796fb9a07a8f412d17)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e09) \u7406\u89e3 View \u5de5\u4f5c\u539f\u7406\u5e76\u5e26\u4f60\u5165\u81ea\u5b9a\u4e49 View \u95e8](https://juejin.im/post/5ddff234518825793218d2e4)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u56db) Paint \u6e32\u67d3/\u6ee4\u955c/xfermode \u4f7f\u7528](https://juejin.im/post/5de36c43f265da05de5881e8)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e94) \u770b\u5b8c\u8be5\u7bc7\u6587\u7ae0 Canvas \u4f60\u5e94\u8be5\u4f1a\u4e86](https://juejin.im/post/5de514fcf265da060115e02d)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u516d) PathMeasure \u5236\u4f5c\u8def\u5f84\u52a8\u753b](https://juejin.im/post/5de789dce51d4557e76a4a39)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e03) SVG \u57fa\u7840\u4f7f\u7528 + \u7ed8\u5236\u4e2d\u56fd\u5730\u56fe](https://juejin.im/post/5deb6d41e51d4558052f16ac)\n\n## \u97f3\u89c6\u9891\n\n- [\u97f3\u89c6\u9891\u4e4b\u8fdb\u7a0b\u95f4\u4f20\u9012 YUV \u683c\u5f0f\u89c6\u9891\u6d41\uff0c\u89e3\u51b3\u4e0d\u80fd\u540c\u65f6\u8c03\u7528 Camera \u95ee\u9898](https://juejin.im/post/5cf345ddf265da1b8c19731a)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e00) C \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5df8c917f265da339772a5d1)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e8c) C++ \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5e1347775188253a6c3966fd)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e09) JNI \u4ece\u5165\u95e8\u5230\u638c\u63e1](https://juejin.im/post/5e1606e0f265da5d2d0ffbdb)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u56db) \u4ea4\u53c9\u7f16\u8bd1\u52a8\u6001\u5e93\u3001\u9759\u6001\u5e93\u7684\u5165\u95e8\u5b66\u4e60](https://juejin.im/post/5e1ad6806fb9a02ff076e103)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e94) Shell \u811a\u672c\u5165\u95e8](https://juejin.im/post/5e1c0a4ce51d451c8771c487)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516d) FFmpeg 4.2.2 \u4ea4\u53c9\u7f16\u8bd1](https://juejin.im/post/5e1eace16fb9a02fec66474e)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e03) \u638c\u63e1\u97f3\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 AudioTrack\u3001OpenSL ES \u6e32\u67d3 PCM \u6570\u636e](https://juejin.im/post/5e3fcc5bf265da57685db2a9)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516b) \u638c\u63e1\u89c6\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 OpenGL ES 2.0 \u6e32\u67d3 YUV \u6570\u636e](https://juejin.im/post/5e4581476fb9a07cd80f15e0)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e5d) \u4ece 0 ~ 1 \u5f00\u53d1\u4e00\u6b3e Android \u7aef\u64ad\u653e\u5668(\u652f\u6301\u591a\u534f\u8bae\u7f51\u7edc\u62c9\u6d41/\u672c\u5730\u6587\u4ef6)](https://juejin.im/post/5e495ec1e51d452713551017)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341) \u57fa\u4e8e Nginx \u642d\u5efa(rtmp\u3001http)\u76f4\u64ad\u670d\u52a1\u5668](https://juejin.im/post/5e4ec66c5188254967067502)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e00) Android \u7aef\u5b9e\u73b0 rtmp \u63a8\u6d41](https://juejin.im/post/5e5d17276fb9a07cc01a29d3)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e8c) \u57fa\u4e8e FFmpeg + OpenSLES \u5b9e\u73b0\u97f3\u9891\u4e07\u80fd\u64ad\u653e\u5668](https://juejin.im/post/5eb1880be51d454de7772152)\n- [WebRTC \u5b66\u4e60\u8bb0\u5f55 (\u4e00) \u4e91\u670d\u52a1\u5668\u642d\u5efa AppRTC \u73af\u5883](https://juejin.im/post/5e8f4a606fb9a03c7a331bd3)\n\n## \u5f00\u6e90\u9879\u76ee\n\n- [\u70ed\u4fee\u590d DexEncryptionDecryption](https://github.com/yangkun19921001/DexEncryptionDecryption)\n- [\u56fe\u7247\u538b\u7f29 LIBJPEG_SAMPLE](https://github.com/yangkun19921001/LIBJPEG_SAMPLE)\n- [\u8fdb\u7a0b\u4fdd\u6d3b KeepAlive](https://github.com/yangkun19921001/KeepAlive)\n- [Java/Native \u5f02\u5e38\u6355\u83b7 YKCrash](https://github.com/yangkun19921001/YKCrash)\n- [\u63d2\u4ef6\u5316 YKPluginAPK](https://github.com/yangkun19921001/YKPluginAPK)\n- [YUV \u64ad\u653e YUVPlay](https://github.com/yangkun19921001/YUVPlay)\n- [\u9632\u5fae\u535a\u957f\u56fe\u52a0\u8f7d long_picture_view](https://github.com/yangkun19921001/long_picture_view)\n- [\u8fdb\u7a0b\u95f4\u901a\u4fe1 YKProBus](https://github.com/yangkun19921001/YKProBus)\n- [EventBus YEventBus](https://github.com/yangkun19921001/YEventBus)\n- [\u8fdb\u7a0b\u95f4\u5927\u6570\u636e\u4f20\u8f93 MemoryFileWriteBytesYUV](https://github.com/yangkun19921001/MemoryFileWriteBytesYUV)\n- [ Kotlin GitHub App](https://juejin.im/post/5dc294d5f265da4d4434afc9)\n- [Android rtmp\u63a8\u6d41\u3001\u62c9\u6d41](https://github.com/yangkun19921001/NDK_AV_SAMPLE/tree/master/ykav_common/src/main/cpp)\n\n## \u82f1\u8bed\n\n\u5f85\u66f4\u65b0...\n\n## \u5173\u4e8e\u6211\n\n- Email: yang1001yk@gmail.com\n- [\u4e2a\u4eba\u535a\u5ba2](https://www.devyk.top/)\n- [\u6398\u91d1](https://juejin.im/user/578259398ac2470061f3a3fb)\n- [GitHub](https://github.com/yangkun19921001)\n\n\u626b\u7801\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\uff0c\u8ba9\u6211\u4eec\u79bb\u5f97\u66f4\u8fdb\u4e00\u4e9b!\n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200328235020.jpg)\n\n\n\n## \u8d5e\u8d4f\n\n\u5982\u679c\u8fd9\u4e2a\u9762\u8bd5\u9898\u5e93\u5bf9\u4f60\u5f88\u6709\u5e2e\u52a9\uff0c\u53ef\u4ee5\u626b\u63cf\u4e0b\u65b9\u4e8c\u7ef4\u7801\u7ed9\u4f5c\u8005\u4e00\u70b9\u9f13\u52b1\u3002\u91d1\u989d\u968f\u610f, \u8c22\u8c22! \n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200330103229.png)\n\n\n\n\n\n\n\n"
 },
 {
  "repo": "ahmetozlu/tensorflow_object_counting_api",
  "language": "Python",
  "readme_contents": "# TensorFlow Object Counting API\nThe TensorFlow Object Counting API is an open source framework built on top of TensorFlow and Keras that makes it easy to develop object counting systems. ***Please contact if you need professional object detection & tracking & counting project with the super high accuracy.***\n\n## QUICK DEMO\n\n---\n### Cumulative Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=411>\n</p>\n\n---\n### Real-Time Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42237325-1f964e82-7f06-11e8-966b-dfde98701c66.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/42238435-77ac0d34-7f09-11e8-9609-e7c3c2c5af74.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42241094-14163cc8-7f12-11e8-83ed-68021b5e3b33.gif\" | width=430><img src=\"https://user-images.githubusercontent.com/22610163/42237904-d6a3ac22-7f07-11e8-88f8-5f21430d9503.gif\" | width=430>\n</p>\n\n---\n\n---\n### Object Tracking Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389634-4682ea00-19d3-11ea-84a2-3996a43e98fe.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/70389738-6bc42800-19d4-11ea-971f-f19cb5b90140.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389764-e9883380-19d4-11ea-8c54-80935811c3fa.gif\" | width=680>\n</p>\n\n- Tracking module was built on top of [this approach](https://github.com/kcg2015/Vehicle-Detection-and-Tracking).\n\n---\n\n### Object Counting On Single Image (TensorFlow implementation):\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/47524870-7c830e80-d8a4-11e8-8fd1-741193615a04.png\" | width=750></p>\n\n---\n\n### Object Counting based R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969852-0569b080-7983-11e9-8051-07d6766ca0e4.png\" | width=750></p>\n\n### Object Segmentation & Counting based Mask R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969871-1c100780-7983-11e9-9660-7b8571b01ff7.png\" | width=750></p>\n\n---\n\n### BONUS: Custom Object Counting Mode (TensorFlow implementation):\n\nYou can train TensorFlow models with your own training data to built your own custom object counter system! If you want to learn how to do it, please check one of the sample projects, which cover some of the theory of transfer learning and show how to apply it in useful projects, are given at below.\n\n**Sample Project#1: Smurf Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/smurf_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62861574-9d6e0080-bd0c-11e9-9e38-b63226df8aa1.gif\" | width=750>\n</p>\n\n**Sample Project#2: Barilla-Spaghetti Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counting_api_keras_tensorflow/barilla_spaghetti_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62903429-46e3df00-bd6b-11e9-9f97-4de477fa8769.png\" | width=750>  \n</p>\n\n---\n\n***The development is on progress! The API will be updated soon, the more talented and light-weight API will be available in this repo!***\n\n- ***Detailed API documentation and sample jupyter notebooks that explain basic usages of API will be added!***\n\n**You can find a sample project - case study that uses TensorFlow Object Counting API in [*this repo*](https://github.com/ahmetozlu/vehicle_counting_tensorflow).**\n\n---\n\n## USAGE\n\n### 1.) Usage of \"Cumulative Counting Mode\"\n\n#### 1.1) For detecting, tracking and counting *the pedestrians* with disabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"pedestrian counting\" case:*\n\n    fps = 30 # change it with your input video fps\n    width = 626 # change it with your input video width\n    height = 360 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 385 # roi line position\n    deviation = 1 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_x_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"pedestrian counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"pedestrian counting case-study\": [pedestrian_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/pedestrian_counting.py)**\n\n---\n\n**1.2)** For detecting, tracking and counting *the vehicles* with enabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"vehicle counting\" case:*\n\n    fps = 24 # change it with your input video fps\n    width = 640 # change it with your input video width\n    height = 352 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 200 # roi line position\n    deviation = 3 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_y_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"vehicle counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"vehicle counting case-study\": [vehicle_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/vehicle_counting.py)**\n\n---\n\n### 2.) Usage of \"Real-Time Counting Mode\"\n\n#### 2.1) For detecting, tracking and counting the *targeted object/s* with disabled color prediction\n \n *Usage of \"the targeted object is bicycle\":*\n \n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"bicycle\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n    \n *Result of \"the targeted object is bicycle\":*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411751-1ae1d3f0-820a-11e8-8465-9ec9b44d4fe7.gif\" | width=700>\n</p>\n\n*Usage of \"the targeted object is person\":*\n\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"person\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n \n *Result of \"the targeted object is person\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411749-1a80362c-820a-11e8-864e-acdeed85b1f2.gif\" | width=700>\n</p>\n\n*Usage of \"detecting, counting and tracking all the objects\":*\n\n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n \n *Result of \"detecting, counting and tracking all the objects\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411750-1aae0d72-820a-11e8-8726-4b57480f4cb8.gif\" | width=700>\n</p>\n\n---\n*Usage of \"detecting, counting and tracking **the multiple targeted objects**\":*\n\n    targeted_objects = \"person, bicycle\" # (for counting targeted objects) change it with your targeted objects\n    fps = 25 # change it with your input video fps\n    width = 1280 # change it with your input video width\n    height = 720 # change it with your input video height\n    is_color_recognition_enabled = 0\n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n---\n \n#### 2.2) For detecting, tracking and counting \"all the objects with disabled color prediction\"\n\n*Usage of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n    \n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411748-1a5ab49c-820a-11e8-8648-d78ffa08c28c.gif\" | width=700>\n</p>\n\n\n*Usage of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n    is_color_prediction_enabled = 1 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411747-1a215e4a-820a-11e8-8aef-faa500df6836.gif\" | width=700>\n</p>\n\n### 3.) Usage of \"Object Tracking Mode\"\n\nJust run [object_tracking.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/object_tracking.py)\n\n---\n\n**For sample usages of \"Real-Time Counting Mode\": [real_time_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/real_time_counting.py)**\n\n---\n\n*The minimum object detection threshold can be set [in this line](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/utils/visualization_utils.py#L443) in terms of percentage. The default minimum object detecion threshold is 0.5!*\n\n## General Capabilities of The TensorFlow Object Counting API\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421361-6662c280-e76d-11e8-9680-ec86e245fdac.jpg\" | width = 720>\n</p>\n\nHere are some cool capabilities of TensorFlow Object Counting API:\n\n- Detect just the targeted objects\n- Detect all the objects\n- Count just the targeted objects\n- Count all the objects\n- Predict color of the targeted objects\n- Predict color of all the objects\n- Predict speed of the targeted objects\n- Predict speed of all the objects\n- Print out the detection-counting result in a .csv file as an analysis report\n- Save and store detected objects as new images under [detected_object folder](www)\n- Select, download and use state of the art [models that are trained by Google Brain Team](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n- Use [your own trained models](https://www.tensorflow.org/guide/keras) or [a fine-tuned model](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb) to detect spesific object/s\n- Save detection and counting results as a new video or show detection and counting results in real time\n- Process images or videos depending on your requirements\n\nHere are some cool architectural design features of TensorFlow Object Counting API:\n\n- Lightweigth, runs in real-time\n- Scalable and well-designed framework, easy usage\n- Gets \"Pythonic Approach\" advantages\n- It supports REST Architecture and RESTful Web Services\n\nTODOs:\n\n- Kalman Filter based object tracker util will be developed.\n- Autonomus Training Image Annotation Tool will be developed.\n\n## Theory\n\n### System Architecture\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421362-6662c280-e76d-11e8-9b63-da9698626f75.jpg\" | width=720>\n</p>\n\n- Object detection and classification have been developed on top of TensorFlow Object Detection API, [see](https://github.com/tensorflow/models/tree/master/research/object_detection) for more info.\n\n- Object color prediction has been developed using OpenCV via K-Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features, [see](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/utils/color_recognition_module) for more info.\n\n[TensorFlow\u2122](https://www.tensorflow.org/) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n[OpenCV (Open Source Computer Vision Library)](https://opencv.org/about.html) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n\n### Tracker\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/41812993-a4b5a172-7735-11e8-89f6-083ec0625f21.png\" | width=700>\n</p>\n\nSource video is read frame by frame with OpenCV. Each frames is processed by [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) is developed on TensorFlow. This is a loop that continue working till reaching end of the video. The main pipeline of the tracker is given at the above Figure.\n\n### Models\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48481757-b1d5a900-e81f-11e8-824b-4317115fe5b4.png\">\n</p>\n\nBy default I use an [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) in this project. You can find more information about SSD in [here](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab). \n\nPlease, See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies. You can easily select, download and use state-of-the-art models that are suitable for your requeirements using TensorFlow Object Detection API.\n\nYou can perform transfer learning on trained TensorFlow models to build your custom object counting systems!\n\n## Project Demo\n\nDemo video of the project is available on [My YouTube Channel](https://www.youtube.com/watch?v=bas6c8d1JyU).\n\n## Installation\n\n### Dependencies\n\nTensorflow Object Counting API depends on the following libraries:\n\n- TensorFlow Object Detection API\n- Protobuf 3.0.0\n- Python-tk\n- Pillow 1.0\n- lxml\n- tf Slim (which is included in the \"tensorflow/models/research/\" checkout)\n- Jupyter notebook\n- Matplotlib\n- Tensorflow\n- Cython\n- contextlib2\n- cocoapi\n\nFor detailed steps to install Tensorflow, follow the [Tensorflow installation instructions](https://www.tensorflow.org/install/). \n\nTensorFlow Object Detection API have to be installed to run TensorFlow Object Counting API, for more information, please see [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).\n\n## Citation\nIf you use this code for your publications, please cite it as:\n\n    @ONLINE{tfocapi,\n        author = \"Ahmet \u00d6zl\u00fc\",\n        title  = \"TensorFlow Object Counting API\",\n        year   = \"2018\",\n        url    = \"https://github.com/ahmetozlu/tensorflow_object_counting_api\"\n    }\n\n## Author\nAhmet \u00d6zl\u00fc\n\n## License\nThis system is available under the MIT license. See the LICENSE file for more info.\n\n\n\n\n\n\n"
 },
 {
  "repo": "zhengyima/DeepNude_NoWatermark_withModel",
  "language": "Python",
  "readme_contents": "\n# DeepNude\n\nDeepNude Source Code \n\n[The README.md in English](https://github.com/zhengyima/DeepNude_NoWatermark_withModel/blob/master/README_EN.md)\n\nDeepNude\u6e90\u4ee3\u7801\n\n\u53bb\u6c34\u5370 \n\n\u5e26\u4e09\u4e2a\u6a21\u578b.lib\u6587\u4ef6\u4e0b\u8f7d\u5730\u5740\n\n\u4f9b\u5e7f\u5927\u7a0b\u5e8f\u5458\u6280\u672f\u4ea4\u6d41\u4f7f\u7528\n\n~~[demo\u5730\u5740](http://39.105.149.229/dn): demo\u5f88\u539f\u59cb\u8106\u5f31\u4e0d\u9c81\u68d2\uff0c\u6240\u4ee5\u611f\u5174\u8da3\u7684\u8bdd\u5c3d\u91cf\u8fd8\u662f\u81ea\u5df1\u53bb\u8dd1\u4ee3\u7801\u5427\u3002\u4e0d\u8981\u5bf9demo\u505a\u574f\u4e8b\u54e6\uff0c\u4e0d\u7136\u5c31\u5173\u6389= =~~\n\n# Preinstallation\n\nBefore launch the script install these packages in your **Python3** environment:\n- numpy\n- Pillow\n- setuptools\n- six\n- pytorch \n- torchvision\n- wheel\n```\npip3 install numpy pillow setuptools six pytorch torchvision wheel\n```\n\n\u5efa\u8bae\u4f7f\u7528Conda\u5b89\u88c5 :) \n\n\n```\n conda create -n deepnude python=3.6 numpy Pillow setuptools six pytorch torchvision wheel\n conda activate deepnude\n```\n\n**\u6ce8\uff1a\u5982\u679c\u61d2\u5f97\u6298\u817ePython\u73af\u5883\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\uff0c\u89c1\u4e0b**\n\n\u611f\u8c22\u7f51\u53cb[\u98de\u54e5](https://github.com/fizzday)\u63d0\u4f9bdocker\u4e00\u952e\u8fd0\u884c\u90e8\u5206\u6280\u672f\u652f\u6301\n\n## \u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\n```bash\ncd ~\n\ngit clone https://github.com/zhengyima/DeepNude_NoWatermark_withModel.git --depth 1 deepnude\n\ncd deepnude\n\ndocker run --rm -it -v $PWD:/app:rw ababy/python-deepnude /bin/bash\n\npython main.py\n```\n> \u6ce8\u610f: docker\u8fd0\u884c\u53ea\u80fd\u4f7f\u7528cpu,\u6240\u4ee5,\u9700\u8981\u4fee\u6539gpu\u8fd0\u884c\u4e3acpu, \u4fee\u6539\u65b9\u6cd5\u8bf7\u53c2\u8003 [#GPU](#gpu). \u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u4e5f\u6162\u4e0d\u4e86\u591a\u5c11.  \n\n> \u5bf9\u5e94\u7684\u4e09\u4e2a .lib \u6587\u4ef6\u9700\u8981\u81ea\u5df1\u624b\u52a8\u4e0b\u8f7d\u540e, \u6dfb\u52a0\u5230\u9879\u76ee\u6839\u76ee\u5f55 `checkpoints` \u76ee\u5f55\u4e0b, \u624d\u80fd\u6b63\u5e38\u8fd0\u884c, \u7531\u4e8e\u6587\u4ef6\u592a\u5927, \u5c31\u6ca1\u6709\u653e\u5165docker\u955c\u50cf\n\n# Models\n\n\u5728\u8fd0\u884c\u4e4b\u524d\u9700\u4e0b\u8f7d\u4e09\u4e2a.lib\u6587\u4ef6\uff0c\u4e4b\u540e\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u65b0\u5efacheckpoints\u76ee\u5f55\uff0c\u5c06\u4e0b\u8f7d\u7684\u4e09\u4e2a\u6587\u4ef6\u653e\u81f3checkpoints\u76ee\u5f55\u4e0b\u3002\n\n\u53cb\u60c5\u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u4e0b\u8f7d\u6e20\u9053\uff1a\n\n\n* [Link](http://39.105.149.229/dn.zip)\n\n* [Google Drive](https://drive.google.com/drive/folders/1OKuIp0nxMUucgEScTc2vESvlpKzIWav4?usp=sharing)\n\n\n# Launch the script\n\n\u73af\u5883\u914d\u597d\uff0c\u6a21\u578b\u4e0b\u597d\u4e4b\u540e\u4fbf\u53ef\u4ee5\u8fd0\u884c\u4ee3\u7801\u4e86\uff01\n\n```\n python main.py\n```\n\nThe script will transform *input.png* to *output.png*.\n\n\n\n\n# GPU\n\n\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528id\u4e3a0\u7684GPU\u8fd0\u884c\u3002\n\n\u82e5\u8fd0\u884c\u73af\u5883\u4e0d\u5e26GPU\uff0c\u5219\u62a5\u9519\u3002\u5982\u679c\u6ca1\u6709GPU\u6216\u60f3\u4f7f\u7528CPU\u8fd0\u884c\u7a0b\u5e8f\uff0c\u8bf7\u5c06gan.py\u4e2d\n\n```\nself.gpu_ids = [0] #FIX CPU\n```\n\n\u6539\u4e3a (\n\n```\nself.gpu_ids = [] #FIX CPU\n```\n\n## Links\n- https://pytorch.org/\n\n"
 },
 {
  "repo": "christopher5106/FastAnnotationTool",
  "language": "C++",
  "readme_contents": "# Fast Image Data Annotation Tool (FIAT)\n\nFIAT enables image data annotation, data augmentation, data extraction, and result visualisation/validation.\n\n- annotate images for image classification, optical character reading (digit classification, letter classification), ...\n\n- extract data into different format (Caffe LMDB, OpenCV Cascade Classifiers, Tesseract ... ) with data augmentation (resizing, noise in translation / rotation / scaling, pepper noise , gaussian noise, rectangle merging, line extraction ...)\n\nThe philosophy of this tool is\n\n- to enable fast annotation : annotate data just by selecting the diagonal of the object, for a fixed ratio. Press Enter if the class is always the same. Type the letter of the class otherwise,\n\n- to be re-usable for different scenarios, and leave you free to build any other tool as input of the annotation process, using any pre-existing weaky classifier (depending on your case) or bounding box proposal algorithm such as selective search, to facilitate annotation with a list of rectangles to select or discard, by just typing the letter of the class or ESCAPE KEY,\n\n- to have the extraction tool act like a monad, so that you can apply transformation in any order, at any stage of your process, the format remaining the same : given a directory and a CSV file, extraction tool will produce a new directory and CSV file, in the same format, by default,\n\n- to feed any classification / training process,\n\n- to be usable for visualisation and export : a visual check that the data is correctly annotated, after manual annotation, extraction, or even after your own bounding box prediction algorithm if it uses the Output class to its produce results.\n\nRequires OPENCV 3 and Google Protobuf.\n\n### Build on Ubuntu 18.04\n    sudo apt-get install caffe-cpu caffe-doc caffe-tools-cpu libcaffe-cpu-dev libcaffe-cpu1 python3-caffe-cpu\n    sudo apt-get install libgoogle-glog-dev\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install liblmdb-dev\n    make all\n\n### File format\n\nRectangle extraction tools create annotations CSV files in the RotatedRect file format. [This blog post will give you the reasons motivating this choice](http://christopher5106.github.io/computer/vision/2015/12/26/file-format-for-computer-vision-annotations.html).\n\n\n### Annotation tool\n\n```bash\n./bin/annotateRect [FLAGS] input_dir output_file.csv\n```\n\nA tool used to annotate rectangles or to show the results (rectangles with `--init` option).\n\n![data annotation](http://christopher5106.github.io/img/annotator_erase.png)\n\n\n**Meaning of the colors** :\n\n- blue rectangle : currently active rectangle to annotate\n\n- green rectangles : remaining rectangles to annotate. Use `--init init_file.csv` option to feed rectangles to annotate to the annotation tool.\n\n- yellow rectangles : annotated rectangles\n\n**Add a rectangle or modify the rectangle** with the following keys :\n\n- Click with the mouse to set the center of the active rectangle (blue) or create a new active rectangle at this position.\n\n- Use arrow keys to move the active rectangle (blue)\n\n    - move left (left arrow key)\n    - move right (right arrow key)\n    - move up (up arrow key)\n    - move down (down arrow key)\n\n- Press **FN** while using arrow keys to change orientation/scale of the active rectangle (blue)\n\n    - rotate left (left arrow key)\n    - rotate right (right arrow key)\n    - augment size (up arrow key)\n    - downsize (down arrow key)\n\nFor platforms for which pressing FN with arrow key does not change key value, press **Space bar** to change into \"Rotation/Scale\" mode and use arrow keys.\n\n- **BACKSPACE**: erase the currently active rectangle.\n\n- **ESC**: next init rectangle or next image (without save)\n\n**Annotate the class** with :\n\n- **Any letter**: save the currently active rectangle (blue) with this letter as category / class . For example \"0\", if there is only one category. The blue rectangle will become yellow.\n\n- **ENTER**: save the letter with the same class as previously.\n\n\nFLAGS :\n\n- `--ratio 1.0` is the ratio height/width of the annotation rectangles.\n\n- `--cross` display a cross instead of a rectangle for non-current rectangles, ie previous (yellow) or future (green) rectangles. This is a useful display option when rectangles are too close together.\n\n![](http://christopher5106.github.io/img/annotation_cross.png)\n\n- `--init init_file.csv` to initialize the rectangles instead of selecting them manually (appear in green). The first one of them will be use as the currently active rectangle (blue). You can still add new rectangles when all init rectangles (green or blue) have been annotated.\n\n- `--export=output_dir` will not display annotation interface. Saves the annotated images to directory.\n\nNOTES :\n\n- the annotation tool can be stopped and launched again : it will resume the work from *output_file.csv*, previously annotated rectangles appear in yellow.\n\n- in case the init rectangles are bigger than the image, a white border is added to the image to show the rectangles outside the image.\n\n- although annotation tool can read images in an output_file.csv or init_file.csv outside current exe directory (by adding the CSV dir to the image path), it will save images with the input_dir as base path. So, when annotating, execute the command in the same directory as the output_file.csv.\n\nEXAMPLE :\n\nAnnotate images in the current directory :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nA first image will appear :\n\n![](pic.jpg)\n\nTo annotate fast, just select the diagonal, by clicking first the top left corner of the object, then the bottom right corner, as shown here with the red arrows :\n\n![](tutorial/pic_selection.jpg)\n\nImage is now selected and appear in blue:\n\n![](tutorial/selected.png)\n\nPress the key corresponding to its class, for example 'a'. Now it appears in yellow.\n\n![](tutorial/labeled.png)\n\nSelect the second book.\n\n![](tutorial/second_selected.png)\n\nAnd press the key corresponding to its class, for example 'e'.\n\n![](tutorial/result.png)\n\nThe output annotation file in CSV format *out.csv* will look like :\n\n```\n./pic.jpg,a,709,816,826,1116,-14.6958\n./pic.jpg,e,1510,607,741,1001,6.32224\n```\n\nAt any time, you can view how the annotations are, and potentially add new annotations with the same command :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nThe `--export` option is also very useful : it allows to export the images with the rectangles, without having the burden of the annotation interface and to facilitate results sharing for instance.\n\n### Extraction tool\n\nUse annotation information to extract a version of the images :\n\n```bash\n./bin/extractRect [FLAGS] annotations.csv output_dir\n```\n\nMoreove, the tool will create an output CSV file listing the new rectangle coordinates in the format `path,label,center_x,center_y,width,height,rotation,noise_x,noise_y,noise_rotation,noise_scale`.\n\nExtraction extracts at best quality possible.\n\nImage will be rotated so that annotation window will be parallel to the image borders.\n\nFor example in the previous example,\n\n```bash\n./bin/extractRect out.csv out\n```\n\nwill create an output directory *out* with two subdirectories corresponding to each label, *out/a* and *out/b* and its corresponding extracted objects, and a CSV file *out/results.csv* with image path, labels and new rectangle coordinates after the extraction.\n\n![](tutorial/a.jpg) ![](tutorial/e.jpg)\n\nExtract with a noise in rotation with `./bin/extractRect out3.csv out8 --noise_rotation=30`\n\n![](tutorial/a_with_rotation_noise.jpg) ![](tutorial/e_with_rotation_noise.jpg)\n\nSeveral options such as noise in translation, scale, or pepper/gaussian noise are available.\n\nThe `--full_image` option makes transformation available without extracting the rectangle.\n\nSee below for more options.\n\nINPUT SELECTION FLAGS :\n\n- `--input_class_filter` select entries of the specified class only.\n\n- `--limit` limit the number of annotation rectangles to consider, good for debuging purposes.\n\nEDIT RECTANGLES\n\n- `--skip_rotation` skips rotation information in annotation (all set to 0.0).\n\n- `--factor=1.2` extends the extraction box by a factor of the width and height. `--factor_width` and `--factor_height` do the same on one axis only and are cumulative with `--factor` (multiplication). In the example, `./bin/extractRect out.csv out7 --factor 1.2`\n\n![](tutorial/a_with_factor.jpg) ![](tutorial/e_with_factor.jpg)\n\n\n- `--offset_x` and `--offset_y` add an offset on each axis of the rectangle, in percentage of the width of the rectangle. In the example, select the titles of the book with the command `./bin/extractRect out3.csv out12 --offset_y=0.5 --factor_height=0.3`\n\n![](tutorial/a_subselection.jpg) ![](tutorial/e_subselection.jpg)\n\n- `--merge` : if multiple bounding box per images, will extract the global bounding box containing all rectangles in each image. In the example : `./bin/extractRect out.csv out --merge` will produce :\n\n![](tutorial/merge.jpg)\n\n\n- `--merge_line` : If multiple rectangle per images, merge rectangles that are roughly on the same line.\n\n- `--correct_ratio` : corrects the ratio of the annotated rectangles to the specified `--ratio` by augmenting one of the two dimensions (height or width). Default is false.\n\n- `--add_borders=true` : adds borders to the extracted image to fit the ratio. Default is false. Available only when `--resize_width` not zero.\n\n- `--ratio=1.0` is used in combination to `--correct_ratio` or `--resize-width` options. It defines the ratio (height/width) of the window to extract. The use of `--resize-width` option without `--correct_ratio` will stretch the image to final dimensions.\n\nNOISE FLAGS\n\n- `--noise_translation=0.2` adds a noise in translation of 20% of the width/height. Not taken into consideration for negatives generation. `--noise_translation_offset` can be used to specify a mininum noise (for negative generation for example).\n\n- `--noise_rotation=30` adds a noise in rotation in `[-noise_rotation\u00b0,noise_rotation\u00b0]`.\n\n- `--noise_zoomin=2 --noise_zoomout=3` adds a noise in scale factor uniformly distributed in `[1/3,200%]`. Not taken into consideration for negatives generation and in `--full_image` mode.\n\n- `--pepper_noise=0.1 --gaussian_noise=30` adds a pixel noise\n\n- `--samples` is the number of sample to extract per image. Default is 1. Useful in combination with noise option.\n\n\nOUTPUT FLAGS\n\n- `--full_image` will not extract the rectangle along the given annotation. Always true in `--backend=opencv` output mode.\n\n- `--resize_width=400` resizes the output to a width of `resize_width` and a height of `resize_width*ratio`. `--resize_width=0` will not resize the output. Default value is no resize. Not available in `--full_image` and `--backend=opencv` mode.\n\n- `--gray=true` extracts as a gray image. Default is false. Always true in `--backend=opencv` output mode.\n\n- `--backend=directory` defines the output format for storing the results. Possible values are : directory, lmdb, tesseract, opencv. Default value is directory.\n\n- `--output_class` override the class by the specified class\n\n- `--output_by_label=false` avoids creation of different output directories per label. Available for `--backend=directory` only.\n\n- `--append` append new extracts to an existing directory. Available for `--backend=directory` only.\n\n\nNEGATIVE GENERATION\n\n- `--neg_per_pos` defines the number of negative samples per positives to extract. By default, no negative (0).\n\n- `--neg_width=0.2` defines the width of negative samples to extract, in pourcentage to the largest image dimension (width or height).\n\n\n# License conditions\n\nCopyright (c) 2016 Christopher5106\n\nThis tool has been developped for a work at Axa, and is a contribution to OpenSource by Axa.\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n"
 },
 {
  "repo": "RaiMan/SikuliX1",
  "language": "Java",
  "readme_contents": "[![RaiMan's Stuff](https://raw.github.com/RaiMan/SikuliX-2014-Docs/master/src/main/resources/docs/source/RaiManStuff64.png)](http://sikulix.com) SikuliX\n============\n\n**What is SikuliX**<br>SikuliX automates anything you see on the screen of your desktop computer \nrunning Windows, Mac or some Linux/Unix. It uses image recognition powered by OpenCV to identify \nGUI components and can act on them with mouse and keyboard actions.\nThis is handy in cases when there is no easy access to a GUI's internals or \nthe source code of the application or web page you want to act on. [More details](http://sikulix.com)\n\n<hr>\n\n**You need at least Java 8, but it works on Java 9 up to latest (currently 14)**\n\n**Windows:** Works out of the box ([for exceptions look here](https://github.com/RaiMan/SikuliX1/wiki/Windows:-Problems-with-libraries-OpenCV-or-Tesseract))\n\n**Mac:** you have to make Tesseract OCR available ([for HowTo look here](https://github.com/RaiMan/SikuliX1/wiki/macOS-Linux:-Support-libraries-for-Tess4J-Tesseract-4-OCR)). **Java 14: open problems with Tesseract**\n\n**Linux:** you have to make OpenCV and Tesseract OCR available ([for HowTo look here](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html#version-1-1-4-special-for-linux-people)).\n\n<hr>\n\n**Latest stable version is 2.0.4** (branch `release_2.0.x` - [see what is fixed](https://github.com/RaiMan/SikuliX1/wiki/ZZZ-Bug-Fixes))\n\n**Development version 2.1.0 currently not useable until further notice**<br>\nNew features will only be available in new major versions (currently 2.1.0, branches master and/or dev_...). \n<br>Until release of a stable 2.1.0, there will be nightly builds and snapshots available (see below).\n\n[Here you can read about the changes/enhancements](https://sikulix-2014.readthedocs.io/en/latest/news.html)\n\n**Get SikuliX ready to use**\n- [SikuliX IDE for editing and running scripts](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixide-2.0.4.jar)\n  - [Jython support for the IDE](https://repo1.maven.org/maven2/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar)\n  - [JRuby support for the IDE](https://repo1.maven.org/maven2/org/jruby/jruby-complete/9.2.0.0/jruby-complete-9.2.0.0.jar)\n  - download all needed to one folder and run sikulix-2.0.x.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixapi-2.0.4.jar)\n  - for use in non-Maven projects\n \nFor use in **Java Maven projects** the dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.0.4</version>\n</dependency>\n```\n<hr>\n\n**Current development version is 2.1.0** (branch `master` nightly builds / snapshots):<br>\n[![Build Status](https://travis-ci.org/RaiMan/SikuliX1.svg?branch=master)](https://travis-ci.org/RaiMan/SikuliX1)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1?ref=badge_shield)\n\n[Read about fixes, enhancements and new features](https://github.com/RaiMan/SikuliX1/wiki/About-fixes-and-enhancements-in-2.1.0)\n\n**Get the nightly builds ready to use** \n- [SikuliX IDE for editing and running scripts]()\n  - [Jython support for the IDE]()\n  - [JRuby support for the IDE]()\n  - download all needed to one folder and run sikulix-2.1.0.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages]()\n  - for use in non-Maven projects\n\nFor use in **Java Maven projects** use the SNAPSHOT dependency information:<br><br>\nThe repository URL:\n```\n<repositories>\n  <repository>\n    <id>sonatype-ossrh</id>\n    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n  </repository>\n</repositories>\n```\nThe dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.1.0-SNAPSHOT</version>\n</dependency>\n```\n<hr>\n\n**Development environment**\n\n - Java 11 (current JDK LTS release)\n - Java 8 (Oracle) for comatibility test\n - Source and target level for Java is version 8 as long as supported by Oracle\n - Maven project\n - Windows 10 latest (Pro 64-Bit)\n - Mac 10.15 latest\n - Ubuntu 18.04 in WSL on Windows 10 (basic tests only, headless)\n - Ubuntu 18.04 running in Oracle VM VirtualBox 6.1 on Windows 10\n - Using IntelliJ IDEA CE in all environments\n\n<hr>\n\n#### Contributions are welcome and appreciated\n - for `bugreports and requests for features or enhancements` use the issue tracker here\n - for `bugfixes` related to the latest release version you should create a pull request against the release branch (currently `release_2.0.x`), so your fix will be in the next bug-fix release (see milestones).\n- for `smaller bugfixes and/or feature enhancements` related to the running development (currently branch master as version 2.1.0-SNAPSHOT and dev_... branches) you should create a pull request against the target branch\n- a pull request should target only one branch. It is the resposibility and job of the maintainer to apply the changes to other branches in case \n- for `more complex revisions and/or enhancements` you should ask for a development branch together with a short description of your ideas\n \n **Please respect the following rules and guidelines when contributing**\n  - Start with smaller fixes. E.g. choose an issue from the issue tracker and try to fix it. Or fix issues you encounter while using SikuliX.\n  - Only fix cosmetic stuff if it's related to an issue you want to fix.\n  - Before you change stuff like dependencies / overall code style and so on, talk with the maintainer beforehand.<br>Sometimes there is a a reason that things are as they are (... and sometimes not :-)).\n  - Try to accept the individual coding styles of the acting contributors, even if some of the stuff might be sub-optimal in your eyes.<br>But feel free to talk about your ideas and the reasons behind.\n\n \n"
 },
 {
  "repo": "kylemcdonald/FaceTracker",
  "language": "C++",
  "readme_contents": "# FaceTracker\n\n**This repository is no longer maintained, due to challenges of upgrading to OpenCV 4.**\n\nFaceTracker is a library for deformable face tracking written in C++ using OpenCV 2, authored by [Jason Saragih](http://jsaragih.org/) and maintained by [Kyle McDonald](http://kylemcdonald.net/).\n\nAny publications arising from the use of this software, including but not limited to academic journal and conference publications, technical reports and manuals, should cite the following work: `J. M. Saragih, S. Lucey, and J. F. Cohn. Face Alignment through Subspace Constrained Mean-Shifts. International Conference of Computer Vision (ICCV), September, 2009.`\n\n## FAQ\n\n1. **\"I successfully compiled the code, so why is my app is crashing?\"** Make sure that your model files are in the right location. If you see the error `Assertion failed: s.is_open()` when running your app, that means you forgot to put the model files in the right directory.\n2. **\"Is there an example of using FaceTracker on a mobile device?\"** There is no official example. But there is an example of using ofxFaceTracker on iOS [here](https://github.com/kylemcdonald/ofxFaceTracker-iOS) and a native Android example [here](https://github.com/ajdroid/facetrackerapp).\n3. **\"Why is the tracking is slow, and why is there high CPU usage?\"** The face detection step (finding the general location of the face) can be slow. If this is causing an issue, you might want to put the tracking in a separate thread. If the detection is very slow you might try using a face detector that is native to your platform, and initializing FaceTracker with that rectangle.\n4. **Can I use this for my commercial project/product?** Yes. FaceTracker was re-licensed under the MIT license on April 8, 2020. Previously it was available under a custom non-commercial use license, with a separate license for commercial use available for purchase.\n\nWrappers are available for:\n\n* Android: [facetrackerapp](https://github.com/ajdroid/facetrackerapp)\n* [openFrameworks](http://www.openframeworks.cc/): [ofxFaceTracker](https://github.com/kylemcdonald/ofxFaceTracker)\n* [Cinder](http://libcinder.org/): [ciFaceTracker](https://github.com/Hebali/ciFaceTracker)\n* Python: [pyfacetracker](https://bitbucket.org/amitibo/pyfacetracker)\n\n## Installation\n\nThese instructions are for compiling the code on OS X and Ubuntu, but it should be possible to compile on other platforms.\n\nFirst, install OpenCV3 (if you're using OpenCV2, use the [opencv2](https://github.com/kylemcdonald/FaceTracker/tree/opencv2) branch of this repo). On OSX you can use [homebrew](http://brew.sh/):\n\n```\n$ brew tap homebrew/science\n$ brew install opencv3\n```\n\nAnd on Ubuntu use:\n\n```\n$ sudo apt-get install libcv-dev libopencv-dev\n```\n\nAlternatively, you can download [OpenCV from the GitHub](https://github.com/opencv/opencv) and compile it manually.\n\nAfter installing OpenCV, clone this repository with `git clone git://github.com/kylemcdonald/FaceTracker.git`. This repository contains a few subdirectories within the root directory:\n   - src (contains all source code)\n   - model (contains a pre-trained tracking model)\n   - bin (will contain the executable after building)\n\nNext, make sure that your copy of OpenCV is located in `/usr/local` (this should be the case if you used `apt-get`). If it isn't located there, modify the `OPENCV_PATH` in the `Makefile`. If you installed with Homebrew, it should be set to `/usr/local/opt/opencv3`.\n\nOptionally, you can also add `-fopenmp` to the `CFLAGS` and `-lgomp` to the `LIBRARIES` variable to compile with [OpenMP](http://openmp.org/) support.\n\nFrom the root `FaceTracker` directory, build the library and example by running `make`.\n\nTo test the demo, `cd bin` and `./face_tracker`. Because many webcams are 1280x720, try running `./face_tracker -s .25` to rescale the image before processing for a smoother framerate.\n\n## `face_tracker` Usage\n\n````\nUsage: face_tracker [options]\nOptions:\n-m <string> : Tracker model (default: ../model/face2.tracker)\n-c <string> : Connectivity (default: ../model/face.con)\n-t <string> : Triangulation (default: ../model/face.tri)\n-s <double> : Image scaling (default: 1)\n-d <int>    : Frames/detections (default: -1)\n--check     : Check for failure \n--help      : Print help\n-?          : Print help\n````\n"
 },
 {
  "repo": "jayrambhia/Install-OpenCV",
  "language": "Shell",
  "readme_contents": "Install-OpenCV\n==============\n\nshell scripts to install different version of OpenCV in different distributions of Linux\n\n### Ubuntu\nif your system is Ubuntu, run the commands below.\n```\n$ cd Ubuntu\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### RedHat\nif your system is RedHat, run the commands below.\n```\n$ cd RedHat\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### ArchLinux\nif your system is ArchLinux, run the commands below.\n```\n$ cd ArchLinux\n$ chmod +x * \n$ ./opencv2_4_0.sh\n```\n"
 },
 {
  "repo": "ryfeus/lambda-packs",
  "language": "Python",
  "readme_contents": "# lambda-packs \n\nPrecompiled packages for AWS lambda\n\n## How to start\n\n1. https://aws.amazon.com/lambda/ and create/sign in into account\n2. Lambda > Functions - Create lambda function\n3. Blank function\n4. Configure triggers - Next\n5. Configure function\n  - Runtime - Python 2.7\n6. Lambda function handler and role\n  - Handler - service.handler\n  - Role - Create new role from template(s)\n  - Role name - test\n  - Policy templates - Simple Microservice Permissions\n7. Advanced settings\n  - Memory (MB) 128\n  - Timeout 1 min 0 sec\n6. Code entry type - Upload a .ZIP file - choose Pack.zip from rep\n7. Test -> Save and test\n\n## How to modify\n\n1. Modify service.py file from sources folder\n2. Choose all files in sources folder when compressing, don't put it in one folder\n3. Upload zip file on function page\n\n## Current packs\n\n### Selenium PhantomJS\n\n#### Intro\n\nSelenium on PhantomJS. In fact - a ready-made tool for web scraping. For example, the demo now opens a random page in Wikipedia and sends its header. (PhantomJS at the same time disguises itself as a normal browser, knows how to log in, click and fill out forms) Also added requests, so you can do API requests for different resources to discard / take away the information.\n\nUseful for web testing and scraping.\n\n#### Demo\n\nCurrent demo opens random page from wiki (https://en.wikipedia.org/wiki/Special:Random) and prints title.\n\n#### Serverless start\n\n```\ngit clone https://github.com/ryfeus/lambda-packs.git\ncd lambda-packs/Selenium_PhantomJS/source/\nserverless deploy\nserverless invoke --function main --log\n```\n\nYou can also see the results from the API Gateway endpoint in a web browser.\n\n#### Documentation\n\nhttps://selenium-python.readthedocs.io/\n\n\n\n---\n### Pyresttest + WRK\n\n#### Intro\n\nWhat does the lambda have to do with it? In a nutshell on AWS in one region you can simultaneously run 200 lambdas (more if you write to support). Lambda works in 11 regions. So you can run in parallel more than 2000 lambdas, each of which will conduct load testing of your service. Five minutes of such testing will cost just one dollar.\n\n#### Demo\n\nDemo in this package tries to send requests to github.com for 5 seconds with 1 connection and also conduct pyresttest dummy test.\n\n#### Tools\n\n1. WRK (https://github.com/wg/wrk) - the main tool for load testing. It works with multiple threads, you can specify the number of connections and length of the load. For more fine-tuning, you can use LuaJIT scripts (https://www.lua.org/).\n2. Pyrestest (https://github.com/svanoort/pyresttest) is a handy tool for testing the full pipeline of the API. For example, the user registers, then uses the api key to create tasks / make notes / downloads files, then reads them, then deletes them.\n\n#### Documentation\n\nhttps://github.com/wg/wrk\n\nhttps://github.com/svanoort/pyresttest\n\n---\n### Lxml + requests\n\n#### Intro\n\nPackage for parsing static HTML pages. Difference here is that it works faster and consumes less memory than PhantomJS but is limited in terms websites it can parse and other features.\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/Lxml_requests/source -n lxml-requests\ncd lxml-requests\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\n```\nwget https://github.com/ryfeus/lambda-packs/blob/master/Lxml_requests/buildPack.sh\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\n#### Tools\n\nLxml 3.7.1\n\n#### Documentation\n\nhttp://lxml.de/\n\n---\n### Tensorflow\n\n#### Intro \n\nOpen source library for Machine Intelligence. Basically revolutionized AI and made it more accessible. Using tensorflow on lambda is not as bad as it may sound - for some simple models it is the simplest and the cheapest way to deploy.\n\n#### Demo\n\nAs hello world code I used recognition of images trained on imagenet (https://www.tensorflow.org/tutorials/image_recognition). Given the price tag lambda one run (recognition of one picture) will cost $0.00005. Therefore for a dollar you can recognize 20,000 images. It is much cheaper than almost any alternatives, though completely scalable (200 functions can be run in parallel), and can be easily integrated into cloud infrastructure. Current demo downloads image from link 'imagelink' from event source ( if empty - then downloads https://s3.amazonaws.com/ryfeuslambda/tensorflow/imagenet/cropped_panda.jpg)\n\n#### Tools\n\nTensorflow 1.4.0\n\n#### Documentation\n\nhttps://www.tensorflow.org/tutorials/image_recognition\n\n#### Nightly version\n\nNightly version archive is more than 50 MB in size but it is still eligible for using with AWS Lambda (though you need to upload pack through S3). For more read here:\n\nhttps://hackernoon.com/exploring-the-aws-lambda-deployment-limits-9a8384b0bec3\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/tensorflow/source -n tensorflow\ncd tensorflow\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\nfor Python2:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\nfor Python3:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack_py3.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index_py3.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack_py3.sh\n```\n\n> Note: Remember You should set `python3.6` for AWS Lambda function environment.\n\n#### Layer ARN\n\narn:aws:lambda:us-east-1:339543757547:layer:tensorflow-pack\n\n---\n### Sklearn\n\n#### Intro\n\nPackage for fans of machine learning, building models and the like. I doubt that there is a more convenient way to deploy model to the real world.\n\n#### Tools\n\n1. Scikit-learn 0.17.1\n2. Scipy 0.17.0\n\n#### Documentation\n\nhttp://scikit-learn.org/\n\n---\n### Skimage\n\n#### Intro\n\nPackage of image processing tools, and not only to style image, but also a large set of computer vision algorithms.\n\nThere are currently two zipped packs available, Pack.zip and Pack_nomatplotlib.zip, you probably want to use Pack_nomatplotlib.zip. See https://github.com/ryfeus/lambda-packs/issues/5 for more information.\n\n#### Tools\n\nScikit-image 0.12.3\n\n#### Documentation\n\nhttp://scikit-image.org/\n\n---\n### OpenCV + PIL\n\n#### Intro\n\nAnother package of image processing tools, and not only to style image, but also a large set of Computer vision algorithms.\n\n#### Tools\n\n1. OpenCV 3.1.0\n2. PIL 4.0.0\n\n#### Documentation\n\nhttps://pillow.readthedocs.io/\n\nhttp://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n\n---\n### Pandas\n\n#### Intro\n\nPackage for fans of statistics, data scientists and data engineers. RAM at lambda is 1.5 gigabytes, and the maximum operating time - 5 minutes. I am sure that will be enough for most tasks.\n\n#### Tools\n\nPandas 0.19.0\n\n#### Documentation\n\nhttp://pandas.pydata.org/\n\n---\n### Spacy\n\n#### Intro\n\nOpensource library for Natural Language Processing in python.\n\n#### Tools\n\n1. Spacy 2.0.11\n\n#### Documentation\n\nhttps://spacy.io/\n\n#### Example\n\nExample code loads language model from S3 and uses it to analyze sentence.\n\n---\n### Tesseract\n\n#### Intro\n\nOCR (optical character recognition) library for text recognition from the image.\n\n#### Documentation\n\nhttps://github.com/tesseract-ocr/tesseract\n\n---\n\n### PDF generator + Microsoft office file generator (docx, xlsx, pptx) + image generator (jpg, png) + book generator (epub)\n\n#### Intro\n\n\"Hello world\" code in package creates example of every document. Basically these libs are low memory (less than 128MB) and high speed (less than 0.5 seconds) so it's something like ~1m documents generated per 1$ in terms of AWS Lambda pricing.\n\n#### Tools\n\n- docx (python-docx - https://pypi.python.org/pypi/python-docx)\n- xlsx (XlsxWriter - https://pypi.python.org/pypi/XlsxWriter)\n- pptx (python-pptx - https://pypi.python.org/pypi/python-pptx)\n- pdf (Reportlab - https://pypi.python.org/pypi/reportlab)\n- epub (EbookLib - https://pypi.python.org/pypi/EbookLib)\n- png/jpg/... (Pillow - https://pypi.python.org/pypi/Pillow)\n\n\n---\n\n### Satellite imagery processing (rasterio + OSGEO + pyproj + shapely + PIL)\n\n#### Intro\n\nAWS Lambda pack in Python for processing satellite imagery. Basically it enables to deploy python code in an easy and cheap way for processing satellite imagery or polygons. In \u201chello world\u201d code of the pack I download red, green, blue Landsat 8 bands from AWS, make True Color image out of it and upload it to S3. It takes 35 seconds and 824MB of RAM for it so ~2500 scenes can be processed for 1$.\n\n#### Tools\n\n- Rasterio (https://github.com/mapbox/rasterio 0.36)\n- OSGEO (https://trac.osgeo.org/gdal/wiki/GdalOgrInPython)\n- Pyproj (https://github.com/jswhit/pyproj)\n- Shapely (https://github.com/Toblerity/Shapely)\n- PIL (https://pillow.readthedocs.io/)\n\n---\n\n### PyTorch\n\nPython 3.6 based PyTorch\n\n#### Tools\n\n- PyTorch 1.0.1 (CPU)\n- torchvision 0.2.1\n\n#### Installed Packages (deps)\n\n- numpy-1.16.1 \n- pillow-5.4.1 \n- six-1.12.0 \n- torchvision-0.2.1\n\n#### Build Pack\n\n```bash\n# You need `docker` before run\n./build-with-docker.sh\n```\n"
 },
 {
  "repo": "shanren7/real_time_face_recognition",
  "language": "Python",
  "readme_contents": "# real_time_face_detection and recognition\nThis is a real time face detection and recognition project base  on opencv/tensorflow/mtcnn/facenet. Chinese version of description is [here](https://zhuanlan.zhihu.com/p/25025596) .Face detection is based on [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html).Face embedding is based on [Facenet](https://arxiv.org/abs/1503.03832).\n##Workflow\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/real%20time%20face%20detection%20and%20recognition.jpg)\n\n##Inspiration\nThe code was inspired by several projects as follows:\n\n1.[OpenFace](https://github.com/cmusatyalab/openface). The main idea was inspired by openface. However, I prefer python and tensorflow,so there comes this project.\n\n2.[davidsandberg/facenet](https://github.com/davidsandberg/facenet).\n\n   facenet.py was taken from https://github.com/davidsandberg/facenet/blob/master/facenet/src/facenet.py\n    \n   nn4.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/models/nn4.py\n    \n   detect_face.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/align/detect_face.py\n    \n3.[yobibyte/yobiface](https://github.com/yobibyte/yobiface).\n\n##Dependencies\n1.tensorflow\n2.opencv with python bindings (cv2)\n3.jupyter notebook for running .ipynb examples\n\n##Running\n1.Downloading pre-trained facenet from https://github.com/yobibyte/yobiface/blob/master/model/model-20160506.ckpt-500000 and putting in model_check_point folder.\n\n2.Running [real time face detection and recognition.ipynb](https://github.com/shanren7/real_time_face_recognition/blob/master/real%20time%20face%20detection%20and%20%20recognition.ipynb) with jupyter notebook\n\n##Results\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_20.jpg)\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_2192.jpg)\n"
 },
 {
  "repo": "Image-Py/imagepy",
  "language": "Python",
  "readme_contents": "# Introduction\n\nImagePy is an open source image processing framework written in Python. Its UI interface, image data structure and table data structure are wxpython-based, Numpy-based and pandas-based respectively. Furthermore, it supports any plug-in based on Numpy and pandas, which can talk easily between scipy.ndimage, scikit-image, simpleitk, opencv and other image processing libraries.\n\n![newdoc01](http://idoc.imagepy.org/imgs/newdoc01.png)\n<div align=center>Overview, mouse measurement, geometric transformation, filtering, segmentation, counting, etc.</div><br>\n\n![newdoc02](http://idoc.imagepy.org/imgs/newdoc02.png)\n<div align=center>If you are more a IJ-style user, try `Windows -> Windows Style` to switch</div><br>\n\nImagePy:\n- has a user-friendly  interface\n- can read/save a variety of image data formats\n- supports ROI settings, drawing, measurement and other mouse operations\n- can perform image filtering, morphological operations and other routine operations\n- can do image segmentation, area counting, geometric measurement and density analysis.\n- is able to perform data analysis, filtering, statistical analysis and others related to the parameters extracted from the image.\n\nOur long-term goal of this project is to be used as ImageJ + SPSS (although not achieved yet)\n\n## Installation\n\n__OS support\uff1awindows, linux, mac, with python3.x__\n\n1.  ImagePy is a ui framework based on wxpython, which can not be installed\n    with pip on Linux. You need download\u00a0[the whl according to your\n    Linux system](https://wxpython.org/pages/downloads/).\n2.  On Linux and Mac, there may be permission denied promblem, for\n    ImagePy will write some config information, so please\u00a0start with\n    sudo. If you install with pip, please add \\--user parameter like\n    this:\u00a0pip install --user imagepy\n3.  If you install ImagePy in an Anaconda virtual environment, you may\n    get a error when starting like this:\u00a0This program needs access to the\n    screen. Please run with a Framework build of python, and only when\n    you are logged in on the main display, if so, please start with\n    pythonw -m imagepy.\n\n### - Pre-compiled package\nThis is the simplest option to run ImagePy.  \nA precompiled archive can be downloaded from the [release tab](https://github.com/Image-Py/imagepy/releases) of the repository.  \nSimply unzip the archive and run the ImagePy.bat file.  \nThis will open a command line window and open the GUI of ImagePy.\n\n### - Using pip\nIn a command-prompt type `pip install imagepy`.\n~~On Windows you currently need to first install shapely using conda.~~ This should also work for windows, now that shapely is available via pip.\nOnce installed, ImagePy can be run by typing `python -m imagepy` in a command prompt.\n\n\n\n## Citation\uff1a\n[ImagePy: an open-source, Python-based and platform-independent software package for bioimage analysis](https://academic.oup.com/bioinformatics/article/34/18/3238/4989871)\n\n## Forum\n\nImagePy is a community partner of forum.image.sc, Anything about the usage and development of ImagePy could be discussed in https://forum.image.sc.\n\n\n\n## Contribute\n\n**Contribute Manual:** All markdown file under [doc folder](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc) be parsed as manual. Plugins and manual are paired by plugins's title and manual's file name. We can browse document from the parameter dialog's Help button. We need more manual contributors, just pull request markdown file [here](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc).\n\n**Contribute Plugins:** Here is a [demo plugin](https://github.com/Image-Py/demoplugin) repositories with document to show how to write plugins and publish on ImagePy. You are wellcom and feel free to contact with us if you need help.\n\n**Improve Main Framework:** Just fork ImagePy, then give Pull Request. But if you want to add some new feature, Please have a issue with us firstly.\n\n## Basic operations\uff1a\n\nImagePy has a very rich set of features, and here, we use a specific example to show you a glimpse of the capacity of ImagePy. We choose the official coin split of scikit-image, since this example is simple and comprehensive.\n\n### Open image\n\n`menu: File -> Local Samples -> Coins` to open the sample image within ImagePy.\n_PS: ImagePy supports bmp, jpg, png, gif, tif and other commonly used file formats. By installing ITK plug-in\uff0cdicom\uff0cnii and other medical image formats can also be read/saved. It is also possible to read/write wmv, avi and other video formats by installing OpenCV._\n\n![newdoc03](http://idoc.imagepy.org/imgs/newdoc03.png)\n<div align=center>Coins</div><br>\n\n\n### Filtering & Segmentation\n\n`menu\uff1aProcess -> Hydrology -> Up And Down Watershed` Here, a composite filter is selected to perform sobel gradient extraction on the image, and then the upper and lower thresholds are used as the mark, and finally we watershed on the gradient map.\nFiltering and segmentation are the crucial skills in the image processing toolkit, and are the key to the success or failure of the final measurement.\nSegmentation methods such as adaptive thresholds, watersheds and others are also supported.\n\n![newdoc04](http://idoc.imagepy.org/imgs/newdoc04.png)\n<div align=center>Up And Down Watershed</div><br>\n\n![newdoc05](http://idoc.imagepy.org/imgs/newdoc05.png)\n<div align=center>Mask</div><br>\n\n### Binarization\n\n`menu\uff1aProcess -> Binary -> Binary Fill Holes` After the segmentation, we obtained a relatively clean mask image, but there is still some hollowing out, as well as some impurities, which will interfere with counting and measurement.\n_ImagePy supports binary operations such as erode, dilate, opening and closing, as well as skeletonization, central axis extraction, and distance transformation._\n\n![newdoc06](http://idoc.imagepy.org/imgs/newdoc06.png)\n<div align=center>Fill Holes</div><br>\n\n### Geometry filtering\n\n`menu\uff1aAnalysis -> Region Analysis -> Geometry Filter` ImagePy can perform geometric filtering based on :__the area, the perimeter, the topology, the solidity, the eccentricity__ and other parameters. You can also use multiple conditions for filtering. Each number can be positive|negative, which indicates the kept object will have the corresponding parameter greater|smaller than the value respectively. The kept objects will be set to the front color, the rejected ones will be set to the back color. In this demo, the back color is set to 100 in order to see which ones are filtered out. Once satisfied with the result, set the back color to 0 to reject them. In addition, ImagePy also supports gray density filtering, color filtering, color clustering and other functions.\n\n![newdoc07](http://idoc.imagepy.org/imgs/newdoc07.png)\n<div align=center>Geometry filtering (the area is over-chosen to emphasize the distinction)</div><br>\n\n\n### Geometry Analysis\n\n`menu\uff1aProcess -> Region Analysis -> Geometry Analysis` Count the area and analyze the parameters. By choosing the `cov` option, ImagePy will fit each area with an ellipse calculated via the covariance.\nThe parameters such as area, perimeter, eccentricity, and solidity shown in the previous step are calculated here. In fact, the filtering of the previous step is a downstream analysis of this one.\n\n![newdoc08](http://idoc.imagepy.org/imgs/newdoc08.png)\n<div align=center>Geometry Analysis</div><br>\n\n![newdoc09](http://idoc.imagepy.org/imgs/newdoc09.png)\n<div align=center>Generate the result table (dark to emphasize the ellipse)</div><br>\n\n\n### Sort Table by area\n\n`menu\uff1aTable -> Statistic -> Table Sort By Key` Select the major key as area, and select descend. The table will be sorted in descending order of area. A table is another important piece of data other than an image. In a sense, many times we need to get the required information on the image and then post-process the data in the form of a table. ImagePy supports table I/O (xls, xlsx, csv), filtering, slicing, statistical analysis, sorting and more.  (Right click on the column header to set the text color, decimal precision, line style, etc.)\n\n![newdoc10](http://idoc.imagepy.org/imgs/newdoc10.png)\n<div align=center>Table</div><br>\n\n\n### Charts\n\n`menu\uff1aTable -> Chart -> Hist Chart` From tabular data, we often need to draw a graph. Here, we plot the histograms of the area and the perimeter columns. ImagePy's tables can be used to draw common charts such as line charts, pie charts, histograms, and scatter plots (matplotlib-based). The chart comes with zooming, moving and other functions. The table can also be saved as an image.\n\n![newdoc11](http://idoc.imagepy.org/imgs/newdoc11.png)\n<div align=center>Histograms</div><br>\n\n\n### 3D chart\n\n`menu\uff1aKit3D -> Viewer 3D -> 2D Surface` Surface reconstruction of the image. This image shows the three reconstructed results including, sobel gradient map, high threshold and low threshold. It shows how the Up And Down Watershed works:\n- calculate the gradient.\n- mark the coin and background through the high and low thresholds,\n- simulate the rising water on the dem diagram to form the segmentation.\n\nImagePy can perform 3D filtering of images, 3D skeletons, 3D topological analysis, 2D surface reconstruction, and 3D surface visualization. The 3D view can be freely dragged, rotated, and the image results can be saved as a .stl file.\n\n![newdoc12](http://idoc.imagepy.org/imgs/newdoc12.png)\n<div align=center>3D visualisation</div><br>\n\n\n\n### Macro recording and execution\n\n`menu\uff1aWindow -> Develop Tool Suite` Macro recorder is shown in the develop tool panel. We have manually completed an image segmentation. However, batch processing more than 10 images can be tedious. So, assuming that these steps are highly repeatable and robust for dealing with such problems, we can record a macro to combine several processes into a one-click program. The macro recorder is similar to a radio recorder. When it is turned on, each step of the operation will be recorded. We can click the pause button to stop recording, then click the play button to execute. When the macro is running, the recorded commands will be executed sequentially, therefore achieving simplicity and reproducibility.\n\nMacros are saved into .mc files. drag and drop the file to the status bar at the bottom of ImagePy, the macro will be executed automatically. we can also copy the .mc file to the submenu of the menus under the ImagePy file directory. When ImagePy is started, the macro file will be parsed into a menu item at the corresponding location. By clicking the menu, the macro will also be executed.\n\n![newdoc13](http://idoc.imagepy.org/imgs/newdoc13.png)\n<div align=center>Macro Recording</div><br>\n\n\n### Workflow\n\nA macro is a sequence of predefined commands. By recording a series of fixed operations into macros, you can improve your work efficiency. However, the disadvantage is the lack of flexibility. For example, sometimes the main steps are fixed, but the parameter tuning needs human interaction. In this case, the workflow is what you want. A workflow in ImagePy is a flow chart that can be visualized, divided into two levels: __chapters and sections__.\nThe chapter corresponds to a rectangular area in the flow chart, and the section is a button in the rectangular area, which is also a command and is accompanied by a graphic explanation. The message window on the right will display the corresponding function description, while mousing hovering above. Click on the `Detail Document` in the top right corner to see the documentation of the entire process.\n\n\nThe workflow is actually written in MarkDown (a markup language), but it needs to be written respecting several specifications, as follows:\n\n```markdown\nTitle\n=====\n## Chapter1\n1. Section1\nsome coment for section1 ...\n2. ...\n## Chapter 2\n\t...\n```\n![newdoc14](http://idoc.imagepy.org/imgs/newdoc14.png)\n<div align=center>Workflow</div><br>\n\n### Report Plugin\n\nSometimes we need to make a report to print or generate a PDF document. ImagePy can generate report from a xlsx template. We just need put specific mark in some cells, ImagePy will parse the template and generate a parameter dialog, then we can input some information, or give image/table in, the report will be generated! more about how to make template please see [here](https://github.com/Image-Py/demoplugin/blob/master/doc/report.md).\n\n![newdoc14](http://idoc.imagepy.org/demoplugin/38.png)\n\n<div align=center>generate report</div><br>\n\n### Filter Plugin\n\nWe introduced macros and workflows in the last sections, using macros and workflows to connect existing functions is convenient. But sometimes we need to create new features. In this section, we are trying to add a new feature to ImagePy. ImagePy can easily access any Numpy-based function. Let's take the Canny operator of scikit-image as an example.\n\n```python\nfrom skimage import feature\nfrom imagepy.core.engine import Filter\n\nclass Plugin(Filter):\n    title = 'Canny'\n    note = ['all', 'auto_msk', 'auto_snap', 'preview']\n    para = {'sigma':1.0, 'low_threshold':10, 'high_threshold':20}\n\n    view = [(float, 'sigma', (0,10), 1, 'sigma', 'pix'),\n            ('slide', 'low_threshold', (0,50), 4, 'low_threshold'),\n            ('slide', 'high_threshold', (0,50), 4, 'high_threshold')]\n\ndef run(self, ips, snap, img, para = None):\n    return feature.canny(snap, para['sigma'], para['low_threshold'],\n        para['high_threshold'], mask=ips.get_msk())*255\n```\n![newdoc15](http://idoc.imagepy.org/imgs/newdoc15.png)\n<div align=center>Canny Filter Demo</div><br>\n\n#### Steps to create a your own filter:\n\n1. Import the package(s), often third party.\n2. Inherit the __`Filter`__ class\u3002\n3. The __`title`__ will be used as the name of the menu and the title of the parameter dialog, also as a command for macro recording.\n4. Tell the framework what needs to do for you in __`Note`__, whether to do type checking, to support the selection, to support _UNDO_, etc.\n5. __`Para`__ is the a dictionary of parameters, including needed parameters for the\n   functions.\n6. Define the interaction method for each of the parameters in __`View`__, the framework will automatically generate the dialog for parameter tuning by reading these information.\n7. Write the core function __`run`__. `img` is the current image, `para` is the result entre by user. if `auto_snap` is set in `note`, `snap` will be a duplicate of `img`. We can process the `snap`, store the result in `img`. <span style=\"color:red\">If the function does not support the specified output</span>, we can also return the result, and the framework will help us copy the result to img and display it.\n8. Save the file as `xxx_plg.py` and copy to the `menu` folder, restart ImagePy.\n   It will be loaded as a menu item.\n\n#### What did the framework do for us?\n\nThe framework unifies the complex tasks in a formal manner and helps us to perform:\n- type checking. If the current image type does not meet the requirements in the note, the analysis is terminated.\n- according to the `para`, generate automatically a dialog box to detect the input legality from the `view`.\n- Real-time preview\n- automatic ROI support\n- undo support\n- parallelization support\n- image stack support\n- etc.\n\n### Table\n\nAs mentioned earlier, the table is another very important data type other than the image. Similarly, ImagePy also supports the extension of table. Here we give an example of sorting-by-key used in the previous description.\n\n```python\nfrom imagepy.core.engine import Table\nimport pandas as pd\n\nclass Plugin(Table):\n    title = 'Table Sort By Key'\n    para = {'major':None, 'minor':None, 'descend':False}\n\n    view = [('field', 'major', 'major', 'key'),\n    \t    ('field', 'minor', 'minor', 'key'),\n    \t    (bool, 'descend', 'descend')]\n\ndef run(self, tps, data, snap, para=None):\n    by = [para['major'], para['minor']]\n    data.sort_values(by=[i for i in by if i != 'None'],\n        axis=0, ascending = not para['descend'], inplace=True)\n```\n![newdoc16](http://idoc.imagepy.org/imgs/newdoc16.png)\n<div align=center>Table Sort Demo</div><br>\n\n#### How Table works\n\nSame as `Filter`\uff0c`Table` also has parameters such as `title`\uff0c`note`\uff0c`para`\uff0c`view`.\nWhen the plugin is running, the framework will generate a dialog box according to `para`\nand `view`. After the parameters are chosen, they are passed to the `run` together with the current table and be processed. The table data is a pandas.DataFrame object in the current table, stored in `tps`. Other information, such as `tps.rowmsk`, `tps.colmsk` can also be retrieved from `tps` to get the row and column mask of the current selected table.\n\n### Other type of plugins\n\nThe `Filter` and `Table` described above are the two most important plugins, but ImagePy also supports some other types of plugin extensions. There are currently ten, they are:\n\n1. `Filter`: mainly for image processing\n2. `Simple`: similar to `Filter`, but focus on the overall characteristics of the image, such as the operation of the ROI, the operation of the false color, the area measurement, or the three-dimensional analysis of the entire image stack, visualization, and so on.\n3. `Free`: operate that are independant of image. Used to open image, close software etc.\n4. `Tool`: use the mouse to interact on the diagram and show small icons on the toolbar, such as a brush.\n5. `Table`: operate on the table, such as statistics analysis, sorting, plotting.\n6. `Widget`: widgets that are displayed in panels, such as the navigation bar on the right, the macro recorder, and others.\n7. `Markdown`: markup language, when clicked, a separate window will pop up to display the document.\n8. `Macros`\uff1acommand sequence file for serially fixed operational procedures.\n9. `Workflow`: combination of macro and MarkDown to create an interactive guidance process.\n10. `Report`: a xlsx template with specific mark, rename as `.rpt`, used to auto generate report.\n\n## Motivation & Goal\n\nPython is a simple, elegant, powerful language, and has very rich third-party libraries for scientific computing. Based on the universal matrix structure and the corresponding rules, numpy-based libraries such as scipy, scikit-image, scikit-learn and other scientific computing libraries have brought great convenience to scientific research. On the other hand, more and more problems in biology, material science and other scientific research can be efficiently and accurately solved via scientific computing, image processing.\n\nHowever there are still many researchers that lack programming skills. Thus it is a crucial to make the Numpy-based scientific computing libraries available to more researchers. ImagePy brings the computing capacities closer to the non-programmer researchers, so that they won't need to be concerned about the UI and interaction design, and focus exclusively on the algorithm itself, and finally, accelerate open-source tool building or even commercial products incubation. These tools, meanwhile, can let more researchers, who are not good at programming, gain, promote and popularize scientific knowledge such as image processing and statistics.\n"
 },
 {
  "repo": "joelibaceta/video-to-ascii",
  "language": "Python",
  "readme_contents": "<div align=center>\n\n  ![Logo](./images/logo.svg)\n\n<p>\n\n  It's a simple python package to play videos in a terminal using [ASCII](https://en.wikipedia.org/wiki/ASCII) characters.\n\n  [![Financial Contributors on Open Collective](https://opencollective.com/video-to-ascii/all/badge.svg?label=financial+contributors)](https://opencollective.com/video-to-ascii) [![PyPI version](https://badge.fury.io/py/video-to-ascii.svg)](https://badge.fury.io/py/video-to-ascii)\n  [![Maintainability](https://api.codeclimate.com/v1/badges/a5fcdf2b0cab41654ca3/maintainability)](https://codeclimate.com/github/joelibaceta/video-to-terminal/maintainability)\n  [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/joelibaceta/video-to-ascii)\n  [![HitCount](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii.svg)](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii)\n\n</p>\n\n![Screenshot](./images/Simpsons.apng)\n\n</div>\n\n<details><summary><b>Translations</b></summary>\n<p>\n\n- [\ud83c\uddfa\ud83c\uddf8 English](./README.md)\n- [\ud83c\uddea\ud83c\uddf8 Espa\u00f1ol](./translations/README_es.md)\n- [\ud83c\uddf9\ud83c\uddfc \u7e41\u9ad4\u4e2d\u6587](./translations/README_zh-TW.md)\n\n<p>\n</details>\n\n## Requirements\n\n- Python3\n- PortAudio (_Only required for installation with audio support_)\n- FFmpeg (_Only required for installation with audio support_)\n\n## Installation\n\nStandard installation\n\n```bash\n$ pip3 install video-to-ascii\n```\n\nWith audio support installation\n\n```bash\n$ pip3 install video-to-ascii --install-option=\"--with-audio\"\n```\n\n## How to use\n\nJust run `video-to-ascii` in your terminal\n\n```bash\n$ video-to-ascii -f myvideo.mp4\n```\n\n### Options\n\n**`--strategy`**\nAllow to choose an strategy to render the output.\n\n![Render Strategies](./images/Strategies.png)\n\n**`-o --output`**\nExport the rendering output to a bash file to share with someone.\n\n![Exporting](./images/export.png)\n\n**`-a --with-audio`**\nIf an installation with audio support was made, you can use this option to play the audio track while rendering the video ascii characters.\n\n## How it works\n\nEvery video is composed by a set of frames that are played at a certain frame rate.\n\n![Video Frames](./images/imgVideoFrames.png)\n\nSince a terminal has a specific number of rows and columns, we have to resize our video to adjust to the terminal size limitations.\n\n![Terminal](./images/imgTerminal.png)\n\nTo reach a correct visualization of an entire frame we need to adjust the _frame height_ to match the _terminal rows_, avoiding using more _characters_ than the number of _terminal columns_.\n\n![Resizing](./images/imgResizing.png)\n\nWhen picking a character to represent a pixel we need to measure the relevance of that pixel's color in the frame, based on that we can then select the most appropriate character based on the [relative luminance](https://en.wikipedia.org/wiki/Relative_luminance) in colorimetric spaces, using a simplify version of the luminosity function.\n\n<p align=\"center\">\n  <img src=\"./images/Luminosity.svg\">\n</p>\n\n> Green light contributes the most to the intensity perceived by humans, and blue light the least.\n\nThis function returns an integer in the range from 0 to 255, we assign a character according to density to show more colored surface for areas with more intense color (highest values).\n\n```python\nCHARS_LIGHT \t= [' ', ' ', '.', ':', '!', '+', '*', 'e', '$', '@', '8']\nCHARS_COLOR \t= ['.', '*', 'e', 's', '@']\nCHARS_FILLED    = ['\u2591', '\u2592', '\u2593', '\u2588']\n```\n\nThe reduced range of colors supported by the terminal is a problem we need to account for. Modern terminals support up to 256 colors, so we need to find the closest 8 bit color that matches the original pixel in 16 or 24 bit color, we call this set of 256 colors [ANSI colors](https://stackoverflow.com/questions/4842424/list-of-ansi-color-escape-sequences).\n\n![The Mapping of RGB and ANSI Colors](./images/imgPixelSection.png)\n\n![8 Bits Color Table](./images/8-bit_color_table.png)\n\nFinally, when putting it all together, we will have an appropriate character for each pixel and a new color.\n\n![Frame Image by Characters](../images/imgPixelImage.png)\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](./CONTRIBUTING.md)].\n\n<a href=\"https://github.com/joelibaceta/video-to-ascii/graphs/contributors\"><img src=\"https://opencollective.com/video-to-ascii/contributors.svg?width=890&button=false\" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/video-to-ascii/contribute/)].\n\nOr maybe just [buy me a coffee](https://ko-fi.com/joelibaceta).\n\n#### Individuals\n\n<a href=\"https://opencollective.com/video-to-ascii#backers\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/individuals.svg?width=890\"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/video-to-ascii/contribute)]\n\n<a href=\"https://opencollective.com/video-to-ascii/organization/0/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/1/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/2/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/3/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/4/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/5/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/6/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/7/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/8/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/9/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/9/avatar.svg\"></a>\n"
 },
 {
  "repo": "Paperspace/DataAugmentationForObjectDetection",
  "language": "Jupyter Notebook",
  "readme_contents": "# Data Augmentation For Object Detection\n\nAccompanying code for the [Paperspace tutorial series on adapting data augmentation methods for object detection tasks](https://blog.paperspace.com/data-augmentation-for-bounding-boxes/)\n\n## Dependencies\n1. OpenCV 3.x\n2. Numpy\n3. Matplotlib\n\nWe support a variety of data augmentations, like.\n\n### Horizontal Flipping\n![Horizontal Flip](Images/hflip.png)\n\n### Scaling\n![Scaling](Images/scale_aug.png)\n\n### Translation\n![Translation](Images/transl_aug.png)\n\n### Rotation\n![Rotation](Images/rotate.png)\n\n### Shearing\n![Shearing](Images/shear_box.png)\n\n### Resizing\n![Resizing](Images/resize.png)\n\n\n## Quick Start\nA quick start tutorial can be found in the file `quick-start.ipynb` in this repo.\n\n## Documentation\nA list of all possible transforms and extensive documentation can be found in by opening `docs/build/html/index.html` in your browser or at this [link.](https://augmentationlib.paperspace.com/)\n"
 },
 {
  "repo": "ITCoders/Human-detection-and-Tracking",
  "language": "Python",
  "readme_contents": "# Human detection and Tracking\n\n[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/arpit1997)\n\n## Introduction\n_In this project we have worked on the problem of human detection,face detection, face recognition and tracking an individual. Our project is capable of detecting a human and its face in a given video and storing Local Binary Pattern Histogram (LBPH) features of the detected faces. LBPH features are the key points extracted from an image which is used to recognize and categorize images. Once a human is detected in video, we have tracked that person assigning him a label. We have used the stored LBPH features of individuals to recognize them in any other videos. After scanning through various videos our program gives output like- person labeled as subject1 is seen in video taken by camera1, subject1 is seen in video by camera2. In this way we have tracked an individual by recognizing him/her in the video taken by multiple cameras. Our whole work is based on the application of machine learning and image processing with the help of [openCV](http://opencv.org)._**This code is built on opencv 3.1.1, python 3.4 and C++, other versions of opencv are NOT SUPPORTED.**\n## Requirements\n* **opencv [v3.1.1]**\n\t* **Installation in linux:**\n\t\t\tFor complete installation of opencv in ubuntu you can refer [here](http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/).\n\t* **Installation in windows**\n\t\t\tFor complete installation of opencv in windows you can refer [here](https://putuyuwono.wordpress.com/2015/04/23/building-and-installing-opencv-3-0-on-windows-7-64-bit/)\n* **python3**\n\t* In Ubuntu python 3.4 can be installed via terminal with the command given below:\n\t\t`sudo apt-get install python3`\n* **python libraries:**\n\tHere is a list of all the python dependencies \n\t* Python Image Library (PILLOW)\n\t* Imutils\n\t* numpy\n\n* **C++**\n\n## Approach\n* The code follows the steps given below:\n\t1. First it reads a video and process each frame one by one.\n\t2. For each frame it tries to detect a human. If a human is detected it draws a rectangle around it.\n\t3. after completing step 2 it tries to detect human face.\n\t4. if a human face is detected it tries to recognize it with a pre-trained model file.\n\t5. If human face is recognized it puts the label on that human face else it moves to step 2 again for next frame \n* The repository is structured as follows:\n\t* `main.py` : This is the main python file that detects and recognizes humans.\n\t* `main.cpp` : This is the main C++ file that detects and recognizes humans.\n\t* `create_face_model.py` : This python script is used to create model file using the given data in `data/` folder \n\t* `model.yaml` : This file contains trained model for given data. This trained model contains LBPH features of each and every face for given data.\n\t* `face_cascades/` : This directory contains sample data for testing our codes. This data is prepared by extracting face images of a praticular person from some videos.\n\t* `scripts/` : This directory contains some useful scripts that we used to work on different problems.\n\t* `video/` : This directory contains some of the videos that we used to while testing.\n\n## Installation \n\n## Python\nDon't forget to install the necessary libraries described in the install paragraph above.\n\nFirst you need to run the create_face_model.py file, which uses the images in /data to create a .yaml file\n* In the project folder run \n```sh \npython create_face_model.py\n```\n* To run the python version of the code you have to put all the input videos in one folder and then provide the path of that folder as command line argument:\n```sh\npython3 main.py -v /path/to/input/videos/  \n```\nExample- for our directory structure it is: \n```sh\n python3 main.py -v /video \n```\n\n## C++\n* To compile the C++ version of the code with openCV the command is:\n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename name_of_file.cpp .cpp` name_of_file.cpp `pkg-config --libs opencv` \n```\nExample- for our directory structure it is: \n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename main.cpp .cpp` main.cpp `pkg-config --libs opencv` \n```  \n* To run the C++ version of the code you have to put all the input videos in one folder and then provide the path of that video as command line argument:\n```sh\n./name_of_file /path/to/input/video_file \n```  \nExample- for our directory structure it is: \n```sh\n ./main /video/2.mp4\n```\n* creating your own model file; just follow the steps given below to create your own model file:\n\t* for each individual rename the images as `subjectx.y.jpg` for example for person 1 images should be named as `subject01.0.jpg` , `subject01.1.jpg` and so on.\n\t* put all the images of all the persons in a single folder for example you can see `data\\` folder then run this command given below:\n\t\t`python3 create_face_model.py -i /path/to/persons_images/` \n\n## Performance of code\n* Since this is a computer vision project it requires a lot of computation power and performance of the code is kind of an issue here.\n* The code was tested on two different machines to analyse performace. The input was 30fps 720p video.\n\t* On a machine with AMD A4 dual-core processor we got an output of 4fps which is quite bad.\n\t* on a machine with Intel i5 quad-core processor we got an output of 12fps.\n\n## Results\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/g.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/o.jpg \"Logo Title Text 1\")\n\nYou can find project report [here](https://github.com/ITCoders/Human-detection-and-Tracking/raw/master/results/HUMAN%20DETECTION%20ANDaRECOGNITION.pdf)\n## To do\n* improve the performance of the code\n* improve the accuracy of the code and reducing the false positive rate.\n* improve the face recognition accuracy to over 90 percent\n\n## Special Thanks to:\n* [Jignesh S. Bhatt](http://www.iiitvadodara.ac.in/faculty/jsb001.html) - Thank you for mentoring this project\n* [Kamal Awasthi](http://github.com/KamalAwasthi) - Helped in testing the code\n"
 },
 {
  "repo": "asingh33/CNNGestureRecognizer",
  "language": "Python",
  "readme_contents": "[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/V7V01IK53)\n\nIf you find my work useful, then please do consider supporting me. This will help me keep motivated and do more of such projects. \nThanks !\n\n\n\n\n[![DOI](https://zenodo.org/badge/89872749.svg)](https://zenodo.org/badge/latestdoi/89872749)\n# CNNGestureRecognizer\nGesture recognition via CNN neural network implemented in Keras + Theano + OpenCV\n\n\nKey Requirements:\nPython 3.6.1\nOpenCV 3.4.1\nKeras 2.0.2\nTensorflow 1.2.1\nTheano 0.9.0   (obsolete and not supported any further)\n\nSuggestion: Better to download Anaconda as it will take care of most of the other packages and easier to setup a virtual workspace to work with multiple versions of key packages like python, opencv etc.\n\n# New changes\nI have uploaded few more changes to this repo -\n- Project is Python3 compatible now.\n- Added TensorFlow support, as Theano's development has been stopped.\n- Added a new background subtraction filter, which is by far the best performing filter for this project\n- Added lots of performance improving changes. There is now literally no FPS drop when prediction mode is enabled\n- An in-app graph plotting has been added to observe the probability of the gesture predictions \n \n# Repo contents\n- **trackgesture.py** : The main script launcher. This file contains all the code for UI options and OpenCV code to capture camera contents. This script internally calls interfaces to gestureCNN.py.\n- **gestureCNN.py** : This script file holds all the CNN specific code to create CNN model, load the weight file (if model is pretrained), train the model using image samples present in **./imgfolder_b**, visualize the feature maps at different layers of NN (of pretrained model) for a given input image present in **./imgs** folder.\n- **imgfolder_b** : This folder contains all the 4015 gesture images I took in order to train the model.\n```diff\n- Note: I have replaced ori_4015imgs_weights.hdf5 weight file with these two OS specific weight files. \n```\n- **_pretrained_weights_MacOS.hdf5_** : This is pretrained weight file on MacOS. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1j7K96Dkatz6q6zr5RsQv-t68B3ZOSfh0/view\n- **_pretrained_weights_WinOS.hdf5_** : This is pretrained weight file on Windows. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1PA7rJxHYQsW5IvcZAGeoZ-ExYSttFuGs/view\n- **_imgs_** - This is an optional folder of few sample images that one can use to visualize the feature maps at different layers. These are few sample images from imgfolder_b only.\n- **_ori_4015imgs_acc.png_** : This is just a pic of a plot depicting model accuracy Vs validation data accuracy after I trained it.\n- **_ori_4015imgs_loss.png_** : This is just a pic of a plot depicting model loss Vs validation loss after I training.\n\n# Usage\n**On Mac**\n```bash\neg: With Theano as backend\n$ KERAS_BACKEND=tensorflow python trackgesture.py \n```\n**On Windows**\n```bash\neg: With Tensorflow as backend\n> set \"KERAS_BACKEND=tensorflow\"\n> python trackgesture.py \n```\n\nWe are setting KERAS_BACKEND to change backend to Theano, so in case you have already done it via Keras.json then no need to do that. But if you have Tensorflow set as default then this will be required.\n\n# Features\nThis application comes with CNN model to recognize upto 5 pretrained gestures:\n- OK\n- PEACE\n- STOP\n- PUNCH\n- NOTHING (ie when none of the above gestures are input)\n\nThis application provides following functionalities:\n- Prediction : Which allows the app to guess the user's gesture against pretrained gestures. App can dump the prediction data to the console terminal or to a json file directly which can be used to plot real time prediction bar chart (you can use my other script - https://github.com/asingh33/LivePlot)\n- New Training : Which allows the user to retrain the NN model. User can change the model architecture or add/remove new gestures. This app has inbuilt options to allow the user to create new image samples of user defined gestures if required.\n- Visualization : Which allows the user to see feature maps of different NN layers for a given input gesture image. Interesting to see how NN works and learns things.\n\n\n# Demo \nYoutube link - https://www.youtube.com/watch?v=CMs5cn65YK8\n\n![](https://j.gifs.com/X6zwYm.gif)\n\n# Gesture Input\nI am using OpenCV for capturing the user's hand gestures. In order to simply things I am doing post processing on the captured images to highlight the contours & edges. Like applying binary threshold, blurring, gray scaling.\n\nI have provided two modes of capturing:\n- Binary Mode : In here I first convert the image to grayscale, then apply a gaussian blur effect with adaptive threshold filter. This mode is useful when you have an empty background like a wall, whiteboard etc.\n- SkinMask Mode : In this mode, I first convert the input image to HSV and put range on the H,S,V values based on skin color range. Then apply errosion followed by dilation. Then gaussian blur to smoothen out the noises. Using this output as a mask on original input to mask out everything other than skin colored things. Finally I have grayscaled it. This mode is useful when there is good amount of light and you dont have empty background.\n\n**Binary Mode processing**\n```python\ngray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray,(5,5),2)   \nth3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\nret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n```\n\n![OK gesture in Binary mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiiok160.png)\n\n\n**SkindMask Mode processing**\n```python\nhsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n    \n#Apply skin color range\nmask = cv2.inRange(hsv, low_range, upper_range)\n\nmask = cv2.erode(mask, skinkernel, iterations = 1)\nmask = cv2.dilate(mask, skinkernel, iterations = 1)\n\n#blur\nmask = cv2.GaussianBlur(mask, (15,15), 1)\n#cv2.imshow(\"Blur\", mask)\n\n#bitwise and mask original frame\nres = cv2.bitwise_and(roi, roi, mask = mask)\n# color to grayscale\nres = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n```\n![OK gesture in SkinMask mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiok44.png)\n\n\n# CNN Model used\nThe CNN I have used for this project is pretty common CNN model which can be found across various tutorials on CNN. Mostly I have seen it being used for Digit/Number classfication based on MNIST database.\n\n```python\nmodel = Sequential()\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv),\n                    padding='valid',\n                    input_shape=(img_channels, img_rows, img_cols)))\nconvout1 = Activation('relu')\nmodel.add(convout1)\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\nconvout2 = Activation('relu')\nmodel.add(convout2)\nmodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n```\n\nThis model has following 12 layers -\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 32, 198, 198)      320       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 32, 198, 198)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 32, 196, 196)      9248      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 32, 196, 196)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 32, 98, 98)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32, 98, 98)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 307328)            0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               39338112  \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 5)                 0         \n=================================================================\n```\nTotal params: 39,348,325.0\nTrainable params: 39,348,325.0\n\n# Training\nIn version 1.0 of this project I had used 1204 images only for training. Predictions probability was ok but not satisfying. So in version 2.0 I increased the training image set to 4015 images i.e. 803 image samples per class. Also added an additional class 'Nothing' along with the previous 4 gesture classes.\n\nI have trained the model for 15 epochs.\n\n![Training Accuracy Vs Validation Accuracy](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_acc.png)\n\n![Training Loss Vs Validation Loss](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_loss.png)\n\n\n# Visualization\nCNN is good in detecting edges and thats why its useful for image classificaion kind of problems. In order to understand how the neural net is understanding the different gesture input its possible to visualize the layer feature map contents.\n\nAfter launching the main script choose option 3 for visualizing different or all layer for a given image (currently it takes images from ./imgs, so change it accordingly)\n```\nWhat would you like to do ?\n    1- Use pretrained model for gesture recognition & layer visualization\n    2- Train the model (you will require image samples for training under .\\imgfolder)\n    3- Visualize feature maps of different layers of trained model\n    3\nWill load default weight file\nImage number 7\nEnter which layer to visualize -1\n(4015, 40000)\nPress any key\nsamples_per_class -  803\nTotal layers - 12\nDumping filter data of layer1 - Activation\nDumping filter data of layer2 - Conv2D\nDumping filter data of layer3 - Activation\nDumping filter data of layer4 - MaxPooling2D\nDumping filter data of layer5 - Dropout\nCan't dump data of this layer6- Flatten\nCan't dump data of this layer7- Dense\nCan't dump data of this layer8- Activation\nCan't dump data of this layer9- Dropout\nCan't dump data of this layer10- Dense\nCan't dump data of this layer11- Activation\nPress any key to continue\n```\n\nTo understand how its done in Keras, check visualizeLayer() in gestureCNN.py\n```python\nlayer = model.layers[layerIndex]\n\nget_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\nactivations = get_activations([input_image, 0])[0]\noutput_image = activations\n```\nLayer 4 visualization for PUNCH gesture\n![Layer 4 visualization for PUNCH gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_4_layer4_MaxPooling2D.png)\n\nLayer 2 visualization for STOP gesture\n![Layer 2 visualization for STOP gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_7_layer2_Conv2D.png)\n\n\n\n# Conclusion\nSo where to go from here? Well I thought of testing out the responsiveness of NN predictions and games are good benchmark. On MAC I dont have any games installed but then this Chrome Browser Dino Jump game came handy. So I bound the 'Punch' gesture with jump action of the Dino character. Basically can work with any other gesture but felt Punch gesture is easy. Stop gesture was another candidate.\n\nWell here is how it turned out :)\n\nWatch full video - https://www.youtube.com/watch?v=lnFPvtCSsLA&t=49s\n\n![](https://j.gifs.com/58pxVx.gif)\n\n\n\n# In case you want to cite my work\nAbhishek Singh,\u201dasingh33/CNNGestureRecognizer: CNNGestureRecognizer (Version 1.3.0)\u201d, Zenodo. http://doi.org/10.5281/zenodo.1064825, Nov. 2017.  \nDo tell me how you used this work in your project. Would love to see your work. Good Luck!\n\n\n\nDont forget to check out my other github project where I used this framework and applied SuperVised machine learning technique to train the Chrome Browser's TRex character :)\nhttps://github.com/asingh33/SupervisedChromeTrex\nYoutube link - https://youtu.be/ZZgvklkQrss\n\n![](https://j.gifs.com/DRg4mn.gif)\n\n"
 },
 {
  "repo": "abreheret/PixelAnnotationTool",
  "language": "C++",
  "readme_contents": "PixelAnnotationTool\n============================\n\n-----------------\n| **` Linux/MAC `** | **` Windows `** | **` Donate  `** | \n|-----------------|---------------------|---------------------|\n| [![Build Status](https://api.travis-ci.org/abreheret/PixelAnnotationTool.svg?branch=master)](https://travis-ci.org/abreheret/PixelAnnotationTool) | [![Appveyor Build Status](https://img.shields.io/appveyor/ci/abreheret/pixelannotationtool.svg)](https://ci.appveyor.com/project/abreheret/pixelannotationtool) |  [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) |\n\n\n\nSoftware that allows you to manually and quickly annotate images in directories.\nThe method is pseudo manual because it uses the algorithm [watershed marked](http://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1) of OpenCV. The general idea is to manually provide the marker with brushes and then to launch the algorithm. If at first pass the segmentation needs to be corrected, the user can refine the markers by drawing new ones on the erroneous areas (as shown on video below).\n\n[![gif_file](giphy.gif)](https://youtu.be/wxi2dInWDnI)\n\nExample :\n\n<img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road.jpg\" width=\"300\"/> <img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road_color_mask.png\" width=\"300\"/>\n\nLittle example from an user ([tenjumh](https://github.com/tenjumh/Pixel-Annotation-Tool)) of PixelAnnotationTools : https://www.youtube.com/watch?v=tX-xcg5wY4U\n\n----------\n\n### Building Dependencies :\n* [Qt](https://www.qt.io/download-open-source/)  >= 5.x\n* [CMake](https://cmake.org/download/) >= 2.8.x \n* [OpenCV](http://opencv.org/releases.html) >= 2.4.x \n* For Windows Compiler : Works under Visual Studio >= 2015\n\nHow to build go to [here](scripts_to_build)\n\n### Download binaries :\nGo to release [page](https://github.com/abreheret/PixelAnnotationTool/releases)\n\n### Donate :\nIf you like, donate !\n\n\nDonating is very simple - and secure. Please click [here](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) to make a donation. \n\n**Thank you!**\n\nYour donation will help me to maintain and update PixelAnnotationTool.\n\n### License :\n\nGNU Lesser General Public License v3.0 \n\nPermissions of this copyleft license are conditioned on making available complete source code of licensed works and modifications under the same license or the GNU GPLv3. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. However, a larger work using the licensed work through interfaces provided by the licensed work may be distributed under different terms and without source code for the larger work.\n\n[more](https://github.com/abreheret/PixelAnnotationTool/blob/master/LICENSE)\n\n### Citation :\n\n```bib\n  @MISC{Breheret:2017\n    author = {Amaury Br{\\'e}h{\\'e}ret},\n    title = {{Pixel Annotation Tool}},\n    howpublished = \"\\url{https://github.com/abreheret/PixelAnnotationTool}\",\n    year = {2017},\n  }\n```\n\n\n"
 },
 {
  "repo": "foundry/OpenCVSwiftStitch",
  "language": "Objective-C++",
  "readme_contents": "__OpenCV computer vision with iOS: stitching panoramas__  \n\n<img src = \"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/example.big.jpg\">\n\nVersion 4.0 of [OpenCVStitch](http://github.com/foundry/OpenCVStitch) - updated for Swift 4.2  \nSee appropriate branches and tags for Swift 2.x - 3.x\n\nThis project was created to a answer a couple of Stack Overflow questions:  \n[libraries to CAPTURE panorama in iOS](http://stackoverflow.com/q/14062932/1375695)  \n[Can I mix Swift with C++? Like the Objective - C .mm files](http://stackoverflow.com/q/24042774/1375695)    \n\nv2 demonstrates how to mix Swift, Objective-C and C++ in one project whilst keeping the code clearly separate. \n\nThe project AppDelegate and View Controller are written in Swift. Swift cannot talk directly to C++ (which we need for OpenCV), so we provide an Objective-C++ wrapper class to mediate between Swift and C++. We also provide an Objective-C++ category on UIImage to mediate between UIImage and CV::Mat image formats. The CVWrapper header file is pure Objective-C. For [v1](https://github.com/foundry/OpenCVStitch)(which doesn't use Swift) this separation was a matter of clean style. For v2, it is a requirement: if any C++ headers are included in the wrapper, the app will not compile (Swift won't like it).\n\n__Installation__  \nTo run the project you need to install the OpenCV framework using Cocoapods    \n\nAssuming you have first [installed CocoaPods](https://guides.cocoapods.org/using/getting-started.html), run 'pod install' in this directory to install OpenCV for the project. From then on, always open the project in XCode from the `SwiftStitch.xcworkspace` file that the pod install creates. \n\nv3.1.0: The default podfile will install openCV v3.1.0, with a hotfix for arm64 compataibility\n\nv2.4.9: Recomment the podfile:  \n    pod 'OpenCV', '2.4.9-zp'  \n    #pod 'OpenCV', '~> 3.1.0.1'  \nChange the `#include` line for 2.4.9 compatibility as indicated in `stitching.cpp`  \n\n__Use__  \nOpenCVStitch is a very simple iOS/openCV example showing basic use of the Stitcher class. The c++ code is adapted from a sample included with the openCV distribution.  \n\nThe app has almost no user interface. On launch, the stitching code operates on four sample images, displaying the result in a UIScrollView.\n\n__OpenCVStitch Versions__   \n[Version 4.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/4.0)   \nSwift / Objective-C / C++   \nTested with XCode 10.0 / Swift 4.2 for iOS 8.0+  \n[Version 3.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/3.0)   \nSwift / Objective-C / C++   \nTested with XCode 8.0 / Swift 3.0 for iOS 8.0+  \n[Version 2.1](https://github.com/foundry/OpenCVSwiftStitch)   \nSwift / Objective-C / C++   \nTested with XCode 7.0 / Swift 2.0 for iOS 7.0+  \n[Version 2.0](https://github.com/foundry/OpenCVSwiftStitch/tree/v2.0)   \nSwift / Objective-C / C++   \nTested with XCode 6.4 / Swift 1.2 for iOS 7.0+  \n[Version 1.0](https://github.com/foundry/OpenCVStitch)  \nObjective-C / C++   \nTested with XCode 4.5.2 -> 6.3 for iOS 5.1 upwards   \n\nProvides a partial answer to: [Libraries to capture panoramas in iOS 6](http://stackoverflow.com/questions/14062932/libraries-to-capture-panorama-in-ios-6/14064788#14064788) (Stack Overflow)\n\n__OpenCV Versions__  \n_OpenCV 3.x_   \nThe podfile installs a hotfixed version of 3.1 as the prebuilt binary provided by openCV  [breaks on arm64 devices](https://github.com/kylefleming/opencv/releases/tag/3.1.0-ios-fix).\n\nThe stitching seems to be much more efficient (85% faster on my iphone 5s). However the quality is noticeably inferior with the test images supplied, so v2.4x remains the default for now [_some improvement with openCV 3.1_].\n\n_OpenCV 2.4.x_  \nThe openCV distribution is not as clean as we would like.  \n2.4.10 - the pod spec and the distribution build for iOS [is broken](http://stackoverflow.com/questions/28331397/cocoapods-opencv-2-4-10-linker-error/28820510)  \n2.4.9 - the pod spec [is broken](http://stackoverflow.com/questions/31005022/cant-install-opencv-with-cocoapods-could-not-resolve-host-hivelocity-dl-sourc). This is likely a result of relying on Sourceforge for hosting.  \n\nTherefore we are using a [self-hosted podspec](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV) (_thanks Matteo!_) until official channels are fixed. Specs are available for [2.4.9](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.9-zp) and [2.4.10](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.10-zp), but as the latter won't run we use 2.4.9.\n\n_this version of OpenCVStitch opted to use cocoapods to overcome the [notorious](http://stackoverflow.com/q/13905471/1375695) [installation](http://stackoverflow.com/q/15855894/1375695) [issues](http://stackoverflow.com/a/14186883/1375695) with previous versions of the openCV 2.4.x framework.. it seems our optimism was slightly premature!_\n\n__XCode 10__  \n\nProject is now updated for Swift 4.1 and XCode 10. For backwards compatibility checkout the 2.0 / 2.1 branches, 3.0 release or refer to the Objective-C version v1.0.\n\n__Comparisons__\n\n<table><tr>\n<td>OpenCV 2.4.9</td><td>OpenCV 3.0.0</td><td>OpenCV 3.1.0</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n</tr></table>\n"
 },
 {
  "repo": "movidius/ncappzoo",
  "language": "Python",
  "readme_contents": "# Neural Compute Application Zoo (ncappzoo) \n[![Stable release](https://img.shields.io/badge/For_OpenVINO\u2122_Version-2020.1-green.svg)](https://github.com/opencv/dldt/releases/tag/2020.1)\n[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\nWelcome to the Neural Compute Application Zoo (_ncappzoo_). This repository is a place for any interested developers to share their projects (code and Neural Network content) that make use of the [Intel&reg; Neural Compute Stick 2 (Intel&reg; NCS2)](https://software.intel.com/en-us/neural-compute-stick)  or the original [Intel&reg; Movidius&trade; Neural Compute Stick](https://software.intel.com/en-us/movidius-ncs) and the Deep Learning Deployment Toolkit (DLDT) portion of the [OpenVINO&trade; Toolkit](https://software.intel.com/en-us/openvino-toolkit).\n \nThe _ncappzoo_ is a community repository with many content owners and maintainers. All _ncappzoo_ content is open source and being made available in this central location for others to download, experiment with, modify, build upon, and learn from.\n\n## _ncappzoo_ Quick Start\nIf you have an  Intel&reg; NCS2 (or the first generation Intel&reg; Movidus&trade; NCS) device and want to jump into the _ncappzoo_, follow these steps: \n\nClone the repo with the following command:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\nRun this command inside of any app/network folder to check your system software dependencies for that particular sample:\n```bash\nmake install_reqs\n```\nIf the script returns successfully, you're ready to run the app or network sample!\n\n## _ncappzoo_ Apps and Networks\nExplore apps by opening a terminal window navigating to any directory under **_ncappzoo_/apps** and execute this command:\n```bash\nmake run\n```\nExplore the neural networks by navigating to any network directory under **_ncappzoo_/networks** and execute the same command:\n```bash\nmake run\n```\nThats it! All of the network and app directories have simple consistent makefiles. To see other make targets supported from these directories just execute this command:\n```bash\nmake help\n```\n\n\n## _ncappzoo_ Repository Branches\nThere are main three branches in the repository; their descriptions are below.  **The master branch is the one most developers will want.**  The others are provided for legacy compatibility.\n\n- **master** branch: This is the most current branch, and the content relies on the DLDT from the OpenVINO&trade; Toolkit.  This is the only branch that is compatible with the Intel&reg; NCS2 however, it is also compatible with the original Intel&reg; Movidius&trade; NCS device.\n- **ncsdk2** branch: This branch is a legacy branch and the content relies on the NCSDK 2.x tools and APIs rather than the OpenVINO&trade; toolkit. This branch is only compatible with the original Intel&reg; Movidius&trade; NCS device and is **NOT** compatible with the Intel&reg; NCS2 device.\n- **ncsdk1** branch: This branch is a legacy branch and the content relies on the NCSDK 1.x tools and APIs rather than OpenVINO&trade; toolkit.  This branch is only compatible with the original Intel&reg; Movidius&trade; Neural Compute Stick and is **NOT** compatible with the Intel&reg; NCS2 device.\n\nYou can use the following git command to use the master branch of the repo:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\n## _ncappzoo_ Compatibility Requirements\n\n### Hardware compatibility\nThe projects in the _ncappzoo_ are periodically tested on Intel&reg; x86-64 Systems unless otherwise stated in the project's README.md file.  Although not tested on other harware platforms most projects should also work on any hardware which can run the OpenVINO&trade; toolkit including the Raspberry Pi 3/3B/3B+/4B hardware<br><br>\nThe projects in the _ncappzoo_ work on both the Intel&reg; NCS2 and the original Intel&reg; Movidius NCS devices.\n\n\n### Operating System Compatibility\nThe projects in the _ncappzoo_ are tested and known to work on the **Ubuntu 18.04** OS.  These projects will likely work on other Linux based operating systems as well but they aren't tested on those unless explicitly stated in the project's README.md file and there may be some tweaks required as well.  If any specific issues are found for other OSes please submit a pull request as broad compatibility is desirable.\n\n### OpenVINO and DLDT Compatibility\nThe projects in the **master branch** depend on the Deep Learning Deployment Toolkit (DLDT) portion of the OpenVINO&trade; toolkit.  There are two flavors of the the OpenVINO&trade; toolkit's DLDT:  \n- The [Intel&reg; Distribution of the OpenVINO&trade; toolkit](https://software.intel.com/en-us/openvino-toolkit) is a binary installation available for  supported platforms.  Here are some links regarding the Intel Distribution of the OpenVINO&trade; Toolkit and the Intel&reg; NCS2\n  - Getting started web page: https://software.intel.com/en-us/articles/get-started-with-neural-compute-stick\n  - Getting Started Video for Linux: https://youtu.be/AeEjQKKkPzg?list=PL61cFkSnEEmOF3AJvLtlDTSbwjlCP4iCs\n  - OpenVINO Toolkit documentation: https://docs.openvinotoolkit.org/latest/index.html\n- The [open source distribution of the OpenVINO&trade; toolkit DLDT](https://github.com/opencv/dldt).  This is the means by which the Intel&reg; NCS2 device can be used with most single board computers on the market and is also helpful for other non-Ubuntu development systems.  Here are some links regarding the open source distribution of the OpenVINO&trade; with the Intel&reg;  NCS2: \n  - Applies to all target system: https://software.intel.com/en-us/articles/intel-neural-compute-stick-2-and-open-source-openvino-toolkit\n  - ARMv7: https://software.intel.com/en-us/articles/ARM-sbc-and-NCS2\n  - ARM64: https://software.intel.com/en-us/articles/ARM64-sbc-and-NCS2\n  - Python on all: https://software.intel.com/en-us/articles/python3-sbc-and-ncs2\n\n**Note:** When using the open source distribution of the OpenVINO&trade; toolkit, you may need to modify your shell's path and environment variables to point to the toolkit's directories.\n\nThe projects in the _ncappzoo_ work with both flavors of the OpenVINO&trade; Toolkit and unless otherwise specified in a project's README.md file all projects are targeted for the **OpenVINO&trade; Toolkit 2020.1 release**.\n\n### OpenCV Compatibility\nSome projects also rely on OpenCV. For these projects, OpenCV distributed with the OpenVINO&trade; release is the recommended version.  Other versions may also work but are not tested an may require tweaks to get working.  \n\n### Python Compatibility\nThe Python projects in the _ncappzoo_ rely on Python 3.5, unless otherwise stated in the individual project's README.\n\n### Raspberry Pi Compatibility\nThe _ncappzoo_ is compatible with the Raspberry Pi 3 B+ and the Raspberry Pi 4. Some additional configuration steps are required:\n\n#### Intel&reg; Distribution of OpenVINO&trade; for Raspbian* OS\nThe **Intel&reg; Distribution of OpenVINO&trade; toolkit for Raspbian OS** does not include the Model Optimizer. To use the _ncappzoo_, you must clone the open source version of the OpenVINO&trade; Deep Learning Development Toolkit (DLDT) and use that version of the Model Optimizer. Clone the repository, install dependencies for TensorFlow* and Caffe*, and set up your **PATH** and **PYTHONPATH** variables to point to the Model Optimizer:\n```\ncd ~\ngit clone https://github.com/opencv/dldt.git\ncd dldt/model-optimizer\npip3 install -r requirements_tf.txt\npip3 install -r requirements_caffe.txt\nexport PATH=~/dldt/model-optimizer:$PATH\nexport PYTHONPATH=~/dldt/model-optmizer:$PYTHONPATH\n```\n#### Open Source OpenVINO&trade; Deep Learning Development Toolkit (DLDT)\nTo setup the open source version of OpenVINO&trade; with your Raspberry Pi, add to the PATH, PYTHONPATH, and LD_LIBRARY_PATH environment variables the location of the build Inference Engine libraries and Python API.\n\n## _ncappzoo_ Repository Layout\nThe _ncappzoo_ contains the following top-level directories.  See the README file in each of these directories or just click on the links below to explore the contents of the _ncappzoo_.\n- **[apps](apps/README.md)** : Applications built to use the Intel&reg; Movidius Intel&reg; NCS and Intel&reg; Neural Compute Stick 2.  **This is a great place to start in the _ncappzoo_!**\n- **[networks](networks/README.md)** : Scripts to download models and optimize neural networks based on any framework for use with the Intel&reg; NCS and Intel&reg; NCS2.\n- **[caffe](caffe/README.md)** : Scripts to download caffe models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[tensorflow](tensorflow/README.md)** : Scripts to download TensorFlow&trade; models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[data](data/README.md)** : Data and scripts to download data for use with models and applications that use the Intel&reg; NCS and Intel&reg; NCS2\n\nThe top-level directories above have subdirectories that hold project content. Each of these project subdirectories has one or more owners that assumes responsibility for it. The [OWNERS](OWNERS) file contains the mapping of subdirectory to owner. \n\n## Contributing to the ncappzoo\nThe _ncappzoo_ is meant to explore and teach features available for the Intel&reg; Movidius&trade; Neural Compute Stick and Intel&reg; Neural Compute Stick 2 with the Intel&reg; OpenVINO&trade; toolkit. The more contributions to the ncappzoo, the more successful this community will be! We always encourage everyone with Neural Compute Stick related content to share by contributing their applications and model-related work to the ncappzoo. It's easy to do, and even when contributing new content you will be the owner and maintainer of the content.\n\nIf your inclusion is an opportunity to explore a new idea in computer vision, add as much documentation about the functionality and your process in creating your app or network, including smartly commenting your code. This will give others - and you! - a chance to learn from your addition. Your addition will help grow our community and improve all of our AI and computer vision skills. Most importantly, the insights you get from releasing your app into the wild here will only help you down the line if you ever want to commercialize your idea. As always, your work in the _ncappzoo_ should be properly attributed so that its ownership will always be managed by you and those you grant additional rights to.\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file for instructions and guidelines for contributing.\n\n## Licensing\nAll content in the _ncappzoo_ is licensed via the [MIT license](https://opensource.org/licenses/MIT) unless specifically stated otherwise in lower-level projects. Individual model and code owners maintain the copyrights for their content, but provide it to the community in accordance with the MIT License.\n\nSee the [LICENSE](LICENSE) file in the top-level directory for all licensing details, including reuse and redistribution of content in the _ncappzoo_ repository.\n\nIntel and the Intel logo are trademarks of Intel Corporation or its subsidiaries.\n\nOpenVINO is a trademark of Intel Corporation or its subsidiaries.\n\nRaspberry Pi is a trademark of the Raspberry Pi Foundation.\n\nTensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc.\n\n"
 },
 {
  "repo": "trishume/eyeLike",
  "language": "C++",
  "readme_contents": "## eyeLike\nAn OpenCV based webcam gaze tracker based on a simple image gradient-based eye center algorithm by Fabian Timm.\n\n## DISCLAIMER\n**This does not track gaze yet.** It is basically just a developer reference implementation of Fabian Timm's algorithm that shows some debugging windows with points on your pupils.\n\nIf you want cheap gaze tracking and don't mind hardware check out [The Eye Tribe](https://theeyetribe.com/).\nIf you want webcam-based eye tracking contact [Xlabs](http://xlabsgaze.com/) or use their chrome plugin and SDK.\nIf you're looking for open source your only real bet is [Pupil](http://pupil-labs.com/) but that requires an expensive hardware headset.\n\n## Status\nThe eye center tracking works well but I don't have a reference point like eye corner yet so it can't actually track\nwhere the user is looking.\n\nIf anyone with more experience than me has ideas on how to effectively track a reference point or head pose\nso that the gaze point on the screen can be calculated contact me.\n\n## Building\n\nCMake is required to build eyeLike.\n\n### OSX or Linux with Make\n```bash\n# do things in the build directory so that we don't clog up the main directory\nmkdir build\ncd build\ncmake ../\nmake\n./bin/eyeLike # the executable file\n```\n\n### On OSX with XCode\n```bash\nmkdir build\n./cmakeBuild.sh\n```\nthen open the XCode project in the build folder and run from there.\n\n### On Windows\nThere is some way to use CMake on Windows but I am not familiar with it.\n\n## Blog Article:\n- [Using Fabian Timm's Algorithm](http://thume.ca/projects/2012/11/04/simple-accurate-eye-center-tracking-in-opencv/)\n\n## Paper:\nTimm and Barth. Accurate eye centre localisation by means of gradients.\nIn Proceedings of the Int. Conference on Computer Theory and\nApplications (VISAPP), volume 1, pages 125-130, Algarve, Portugal,\n2011. INSTICC.\n\n(also see youtube video at http://www.youtube.com/watch?feature=player_embedded&v=aGmGyFLQAFM)\n"
 },
 {
  "repo": "jhansireddy/AndroidScannerDemo",
  "language": "C++",
  "readme_contents": "# ScanLibrary\nScanLibrary is an android document scanning library built on top of OpenCV, using the app you will be able to select the exact edges and crop the document accordingly from the selected 4 edges and change the perspective transformation of the cropped image.\n\n# Screenshots\n\n<div align=\"center\">\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" alt=\"Scan Input\" title=\"Scan Input\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" alt=\"Scan Points\" title=\"Scan Points\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" alt=\"After Scan\" title=\"After Scan\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" alt=\"Scanned Result\" title=\"Scanned Result\"></img>\n\n</div>\n\n# Videos\n\n<div align=\"center\" >\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Kl7rRZ79m6k\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" \nalt=\"Scan Video\" width=\"40%\" border=\"10\" /></a>\n</div>\n\n# Using it in your project\n\n* `git clone https://github.com/jhansireddy/AndroidScannerDemo.git` into a standalone dir;\n* Create or using your project: `File -> New -> Import module...`;\n* As source directory point to: `~/_dirWhereYouClonedAndroidScannerDemo_/AndroidScannerDemo/ScanDemoExample/scanlibrary` and confirm;\n\n- Add the dependency to your main app build.gradle this way: \n```\t    \ncompile project(':scanlibrary')\n```\n- In your activity or fragment when you want to give an option of document scanning to user then:\nStart the scanlibrary ScanActivity, with this the app will go to library, below is the sample code snippet:\nNote: preference can be one of OPEN_CAMERA or OPEN_MEDIA or left empty, based on the passed preference the scan library decides to open camera or media or open the scan home page.\n```java\n       int REQUEST_CODE = 99;\n       int preference = ScanConstants.OPEN_CAMERA;\n       Intent intent = new Intent(this, ScanActivity.class);\n       intent.putExtra(ScanConstants.OPEN_INTENT_PREFERENCE, preference);\n       startActivityForResult(intent, REQUEST_CODE);\n```\n\n- Once the scanning is done, the application is returned from scan library to main app, to retrieve the scanned image, add onActivityResult in your activity or fragment from where you have started startActivityForResult, below is the sample code snippet:\n```java\n    @Override\n    protected void onActivityResult(int requestCode, int resultCode, Intent data) {\n        super.onActivityResult(requestCode, resultCode, data);\n        if (requestCode == REQUEST_CODE && resultCode == Activity.RESULT_OK) {\n            Uri uri = data.getExtras().getParcelable(ScanConstants.SCANNED_RESULT);\n            Bitmap bitmap = null;\n            try {\n                bitmap = MediaStore.Images.Media.getBitmap(getContentResolver(), uri);\n                getContentResolver().delete(uri, null, null);\n                scannedImageView.setImageBitmap(bitmap);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n```\n# License\n\n\tCopyright (c) 2016 Jhansi Karee\n\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\n\n- **IMPORTANT:** This project uses the OPENCV Framework. Download the newest version here 'http://opencv.org/.\n"
 },
 {
  "repo": "airob0t/idcardgenerator",
  "language": "Python",
  "readme_contents": "# idcardgenerator\n\u3010\u4ec5\u505a\u7814\u7a76\u4f7f\u7528\uff0c\u8bf7\u9075\u5b88\u5f53\u5730\u6cd5\u5f8b\u6cd5\u89c4\uff0c\u6cd5\u5f8b\u540e\u679c\u81ea\u8d1f\u3011\n\n\u8eab\u4efd\u8bc1\u56fe\u7247\u751f\u6210\u5de5\u5177,\u586b\u5165\u4fe1\u606f\uff0c\u9009\u62e9\u4e00\u5f20\u5934\u50cf\u56fe\u7247,\u5373\u53ef\u751f\u6210\u9ed1\u767d\u548c\u5f69\u8272\u8eab\u4efd\u8bc1\u56fe\u7247\u3002\n\n\u53ef\u4ee5\u9009\u62e9\u662f\u5426\u81ea\u52a8\u62a0\u56fe\uff0c\u81ea\u52a8\u62a0\u56fe\u76ee\u524d\u4ec5\u652f\u6301\u7eaf\u8272\u80cc\u666f\uff0c\u5bf9\u81ea\u52a8\u62a0\u56fe\u6548\u679c\u4e0d\u6ee1\u610f\u53ef\u4ee5\u624b\u52a8\u62a0\u56fe\u3002\n\n\u5728\u7ebf\u62a0\u56fe\u5730\u5740:(https://burner.bonanza.com/)\n\n(https://www.gaoding.com/koutu)\n\n## \u66f4\u65b0:\n- \u81ea\u52a8\u6539\u53d8\u5934\u50cf\u5927\u5c0f\n- \u81ea\u52a8\u4ece\u7eaf\u8272\u80cc\u666f\u4e2d\u62a0\u56fe\n- \u652f\u6301pip\u5b89\u88c5\n\n## ToDo\n- \u81ea\u52a8\u4ece\u590d\u6742\u80cc\u666f\u4e0b\u62a0\u56fe\n\n## \u73af\u5883\n- numpy\n- pillow\n- opencv\n\n## \u4e0b\u8f7d\n## pip\u5b89\u88c5\n`pip install idcardgenerator`\n\n```\nfrom idcardgenerator import gui\ngui.run()\n```\n\u6587\u4ef6\u4f1a\u751f\u6210\u5728\u8fd0\u884c\u76ee\u5f55\n\n### Windows\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/win_v1.3/idcardgenerator.exe)\n### Mac\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/v1.1/idcardgenerator)\n\n## \u6253\u5305\u7a0b\u5e8f\n\n\u5b89\u88c5pyinstaller\n\n`pip install pyinstaller`\n\nMac\u6253\u5305(\u6253\u5305\u6210Mac app\u5c1a\u6709\u95ee\u9898\u672a\u89e3\u51b3)\n\n    pyinstaller -i usedres/ico.icns --windowed --clean --noconfirm --onefile --add-data ./usedres:./usedres idcardgenerator.py\n\nWindows\u6253\u5305\n\n    pyinstaller -i usedres/ico.ico --windowed --clean --noconfirm --onefile --add-data \"usedres;usedres\" idcardgenerator.py\n\n## \u53c2\u7167\u6807\u51c6\uff1a\n\n### \u6b63\u9762\n\u3000\u3000\u5de6\u4e0a\u89d2\u4e3a\u56fd\u5fbd\uff0c\u7528\u7ea2\u8272\u6cb9\u58a8\u5370\u5237;\u5176\u53f3\u4fa7\u4e3a\u8bc1\u4ef6\u540d\u79f0\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\uff0c\u5206\u4e0a\u4e0b\u4e24\u6392\u6392\u5217\uff0c\u5176\u4e2d\u4e0a\u6392\u7684\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u201d\u4e3a4\u53f7\u5b8b\u4f53\u5b57\uff0c\u4e0b\u6392\u7684\u201c\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\u4e3a2\u53f7\u5b8b\u4f53\u5b57;\u201c\u7b7e\u53d1\u673a\u5173\u201d\u3001\u201c\u6709\u6548\u671f\u9650\u201d\u4e3a6\u53f7\u52a0\u7c97\u9ed1\u4f53\u5b57;\u7b7e\u53d1\u673a\u5173\u767b\u8bb0\u9879\u91c7\u7528\uff0c\u201cxx\u5e02\u516c\u5b89\u5c40\u201d;\u6709\u6548\u671f\u9650\u91c7\u7528\u201cxxxx.xx-xxxx.xx.xx\u201d\u683c\u5f0f\uff0c\u4f7f\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff0c\u5168\u90e8\u7528\u9ed1\u8272\u6cb9\u58a8\u5370\u5237\u3002\n\n### \u80cc\u9762\n\u3000\u3000\u201c\u59d3\u540d\u201d\u3001\u201c\u6027\u522b\u201d\u3001\u201c\u6c11\u65cf\u201d\u3001\u201c\u51fa\u751f\u5e74\u6708\u65e5\u201d\u3001\u201c\u4f4f\u5740\u201d\u3001\u201c\u516c\u6c11\u8eab\u4efd\u53f7\u7801\u201d\u4e3a6\u53f7\u9ed1\u4f53\u5b57\uff0c\u7528\u84dd\u8272\u6cb9\u58a8\u5370\u5237\uff1b\u767b\u8bb0\u9879\u76ee\u4e2d\u7684\u59d3\u540d\u9879\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u5176\u4ed6\u9879\u76ee\u5219\u7528\u5c0f5\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u51fa\u751f\u5e74\u6708\u65e5 \u65b9\u6b63\u9ed1\u4f53\u7b80\u4f53\u5b57\u7b26\u5927\u5c0f\uff1a\u59d3\u540d\uff0b\u53f7\u7801\uff0811\u70b9\uff09\u5176\u4ed6\uff089\u70b9\uff09\u5b57\u7b26\u95f4\u8ddd\uff08AV\uff09\uff1a\u53f7\u7801\uff0850\uff09\u5b57\u7b26\u884c\u8ddd\uff1a\u4f4f\u5740\uff0812\u70b9\uff09\uff1b\u8eab\u4efd\u8bc1\u53f7\u7801\u5b57\u4f53   OCR-B 10 BT   \u6587\u5b57 \u534e\u6587\u7ec6\u9ed1\u3002\n"
 },
 {
  "repo": "xiangjiana/Android-MS",
  "language": null,
  "readme_contents": "[**\u7248\u6743\u58f0\u660e**](#\u7248\u6743\u58f0\u660e)\n\n# Android \u9ad8\u7ea7\u9762\u8bd5\n\n### ![\u9762\u8bd5](img/2020Android\u6700\u65b0\u6280\u672f\u8be6\u89e3.png)\n#### \u6700\u65b0\u66f4\u6587\uff1a\n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n\n#### \u5199\u7ed9Android\u7684\u4e00\u5c01\u4fe1\n### \u5bf9\u4e8e\u8eab\u8fb9\u6b63\u5728\u9762\u8bd5\u548c\u9762\u8bd5\u4e2d\u7684\u4eba\uff0c\u52a0\u4e0a\u6211\u4ee5\u5f80\u7684\u9762\u8bd5\u7ecf\u5386\u6574\u7406\u4e86\u5982\u4e0b\u9762\u8bd5\u5907\u8003\u8def\u7ebf\uff0c\u548cPDF\u7248\uff08\u6709\u76f8\u5e94\u7684\u89c6\u9891\u6559\u7a0b\u5728\u540e\u9762\uff09\n### ![MS](img/MS.png)\n### ![PDF](img/PDF.png)\n\n\u6700\u8fd1\u534a\u5e74\uff0c\u5e38\u5e38\u6709\u4eba\u95ee\u6211 \u201cAndroid\u5c31\u4e1a\u5e02\u573a\u7a76\u7adf\u600e\u4e48\u6837\uff0c\u6211\u8fd8\u80fd\u4e0d\u80fd\u575a\u6301\u4e0b\u53bb ?\u201d\n\n\u73b0\u5728\u60f3\u60f3\uff0c\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u53d1\u5c55\u4e0d\u77e5\u4e0d\u89c9\u5df2\u7ecf\u5341\u591a\u5e74\u4e86\uff0cMobile First \u4e5f\u5df2\u7ecf\u53d8\u6210\u4e86 AI First\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u518d\u662f\u201c\u98ce\u53e3\u4e0a\u7684\u732a\u201d\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u5149\u73af\u548c\u6ea2\u4ef7\u5f00\u59cb\u6162\u6162\u6d88\u5931\uff0c\u5e76\u4e14\u6b63\u5728\u5411 AI\u3001\u533a\u5757\u94fe\u7b49\u65b0\u7684\u9886\u57df\u8f6c\u79fb\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u65b0\u9c9c\u8840\u6db2\u4e5f\u5df2\u7ecf\u53d8\u5c11\uff0c\u6700\u660e\u663e\u7684\u662f\u56fd\u5185\u5e94\u5c4a\u751f\u90fd\u7eb7\u7eb7\u6d8c\u5411\u4e86 AI \u65b9\u5411\u3002\n\n\u200b       \u53ef\u4ee5\u8bf4\uff0c\u56fd\u5185\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u7ea2\u5229\u671f\u5df2\u7ecf\u8fc7\u53bb\u4e86\uff0c\u73b0\u5728\u662f\u589e\u91cf\u4e0b\u964d\u3001\u5b58\u91cf\u53ae\u6740\uff0c\u4ece\u4e89\u593a\u7528\u6237\u5230\u4e89\u593a\u65f6\u957f\u3002\u6bd4\u8f83\u660e\u663e\u7684\u662f\u624b\u673a\u5382\u5546\u7eb7\u7eb7\u4e92\u8054\u7f51\u5316\uff0c\u4e0e\u4f20\u7edf\u4e92\u8054\u7f51\u4f01\u4e1a\u76f4\u63a5\u7ade\u4e89\u3002\u53e6\u5916\u4e00\u65b9\u9762\uff0c\u8fc7\u53bb\u6e20\u9053\u7684\u6253\u6cd5\u5931\u7075\uff0c\u5c0f\u7a0b\u5e8f\u3001\u5feb\u5e94\u7528\u7b49\u65b0\u5174\u6e20\u9053\u5d1b\u8d77\uff0c\u65e0\u8bba\u662f\u624b\u673a\u5382\u5546\uff0c\u8fd8\u662f\u5404\u5927 App \u90fd\u628a\u51fa\u6d77\u6446\u5230\u4e86\u6218\u7565\u7684\u4f4d\u7f6e\u3002\n\n\u5404\u5927\u57f9\u8bad\u5e02\u573a\u4e5f\u4e0d\u518d\u57f9\u8badAndroid\uff0c**\u4f5c\u4e3a\u5f00\u53d1Android\u7684\u6211\u4eec\u8be5\u4f55\u53bb\u4f55\u4ece\uff1f**\n\n\u200b        \u5176\u5b9e\u5982\u679c\u4f60\u6280\u672f\u6df1\u5ea6\u8db3\u591f\uff0c\u5927\u5fc5\u4e0d\u7528\u4e3a\u5c31\u4e1a\u800c\u5fe7\u6101\u3002\u6bcf\u4e2a\u884c\u4e1a\u4f55\u5c1d\u4e0d\u662f\u8fd9\u6837\uff0c\u6700\u5f00\u59cb\u7684\u98ce\u53e3\uff0c\u5230\u6162\u6162\u7684\u6210\u719f\u3002Android\u521d\u7ea7\u57282019\u5e74\u7684\u65e5\u5b50\u91cc\u98ce\u5149\u4e0d\u518d\uff0c \u9760\u4f1a\u56db\u5927\u7ec4\u4ef6\u5c31\u80fd\u591f\u83b7\u53d6\u5230\u6ee1\u610f\u85aa\u8d44\u7684\u65f6\u4ee3\u4e00\u53bb\u4e0d\u590d\u8fd4\u3002**\u7ecf\u8fc7\u4e00\u6ce2\u4e00\u6ce2\u7684\u6dd8\u6c70\u4e0e\u6d17\u724c\uff0c\u5269\u4e0b\u7684\u90fd\u662f\u6280\u672f\u7684\u91d1\u5b50\u3002\u5c31\u50cf\u5927\u6d6a\u892a\u53bb\uff0c\u88f8\u6cf3\u7684\u4f1a\u6162\u6162\u4e0a\u5cb8\u3002**\u800c\u771f\u6b63\u575a\u6301\u4e0b\u6765\u7684\u4e00\u5b9a\u4f1a\u53d6\u5f97\u4e0d\u9519\u6210\u7ee9\u3002\u6bd5\u7adfAndroid\u5e02\u573a\u662f\u5982\u6b64\u4e4b\u5927\u3002\u4eceAndroid\u9ad8\u7ea7\u7684\u84ec\u52c3\u7684\u5c31\u4e1a\u5c97\u4f4d\u9700\u6c42\u6765\u770b\uff0c\u80fd\u575a\u4fe1\u6211\u4eec\u6bcf\u4e00\u4f4dAndroid\u5f00\u53d1\u8005\u7684\u68a6\u60f3 \u3002\n\n### ![2020\u9762\u8bd5\u4e13\u9898](img/2020\u9762\u8bd5\u4e13\u9898.png)\n### ![2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55](img/2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55.png)\n### ![23\u79cd\u8bbe\u8ba1\u6a21\u5f0f](img/23\u79cd\u8bbe\u8ba1\u6a21\u5f0f.png)\n\n \u63a5\u4e0b\u6765\u6211\u4eec\u9488\u5bf9Android\u9ad8\u7ea7\u5c55\u5f00\u7684\u5b8c\u6574\u9762\u8bd5\u9898 \n ### \u4e00\u4e36kotlin(\u89c6\u9891\uff09\n - [kotlin\u5927\u51681-10\u89c6\u9891\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/Kotlin_%E5%89%AF%E6%9C%AC.png)\n ### \u4e8c\u4e36flutter\uff08\u89c6\u9891\uff09\n - [flutter\u89c6\u9891\u5305](https://github.com/xiangjiana/Android-MS/blob/master/img/flutter.png)\n ### \u4e09\u4e36\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u4ece0\u5f00\u59cb\u8ba4\u8bc6\uff08\u89c6\u9891\uff09\n - [\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u7cfb\u5217\u89c6\u98911-10](https://github.com/xiangjiana/Android-MS/blob/master/img/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%911-10%E8%A7%86%E9%A2%91.png)\n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08\u89c6\u9891\u89e3\u7801\u57fa\u7840-\u5c01\u88dd\u683c\u5f0f\uff09](https://mp.weixin.qq.com/s/vNKkl7xXsZhgALu7VigcDg) \n \n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08H264\u7f16\u7801-\u5e27\u5185\u9884\u6d4b\uff09](https://mp.weixin.qq.com/s/OA6DV_hnCoWnn_0lcheVbg)\n \n \n \n ### \u56db\u4e36\u6700\u65b0\u89c6\u9891\u66f4\u65b0\uff1a\n - [1.90\u5206\u949f\u641e\u5b9a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide\uff0c\u9762\u8bd5\u5b9e\u6218\u4e00\u6761\u9f99](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n - [2.\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [3.\u963f\u91cc\u9762\u8bd5\u9898\uff1a\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [4.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u5f00\u53d1\u6280\u80fd-\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [5.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u8fdb\u9636\uff0c\u5b9e\u73b0\u8d44\u6e90\u66f4\u65b0\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [6.Binder\u673a\u5236\u8be6\u89e3\uff0c\u7528Binder\u4e2dmmap\u601d\u60f3\u89e3\u51b3\u4f60\u7684APP\u5361\u987f\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [7.\u5317\u4e0a\u5e7f\u6df110\u5e74\u9762\u8bd5\u7ecf\u9a8c\u8001\u53f8\u673a\u503e\u56ca\u76f8\u6388\uff0c\u8ba9\u4f60\u5c11\u8d705-10\u5e74\u5f2f\u8def\u7684\u9762\u8bd5\u79c1\u623f\u8bfe\uff08\u804c\u4e1a\u8def\u5f84.\u7b80\u5386\u89c4\u5212.\u9762\u8bd5\u5b98\u5fc3\u7406\u5206\u6790.\u6280\u672f\u9762\u8bd5\u5b9e\u6218\uff0cGlide\uff0cOkhttp\uff0c\u4f18\u5316\u9762\u8bd5\u9898\u52a9\u4f60\u6253\u901a\u4efb\u7763\u4e8c\u8109\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [8.\u4e0d\u4f1aNDK\u600e\u4e48\u73a9\u70ed\u4fee\u590d\uff1f\u4eca\u665a\u6559\u4f60\u4eceJava\u5c42\u5b9e\u73b0\u817e\u8bafTinker\u70ed\u4fee\u590d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [9.\u8fc8\u5411\u67b6\u6784\u5e08\u7684\u7b2c\u4e00\u6b65-\u4ece\u6253\u9020\u81ea\u5df1\u7684\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u5f00\u59cb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [10.Android\u9879\u76ee\u7684\u6700\u7ec8\u8fdb\u5316,\u63d2\u4ef6\u5316\u5f00\u53d1\u8ba9\u4f60\u7684\u5e94\u7528\u52a0\u8f7d\u6d77\u91cf\u63d2\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [11.\u544a\u522b\u201c\u642c\u8fd0\u5de5\u201d\u624b\u5199\u5fae\u4fe1\uff0cQQ\u90fd\u5728\u7528\u7684\u6570\u636e\u6846\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [12.JVM\u865a\u62df\u673a\u5c42\u770bKlass \u5bf9\u8c61\u751f\u6210\u673a\u5236\uff0c\u63ed\u79d8\u4ece\u672a\u770b\u8fc7\u7684\u7ec6\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [13.Android\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u6218\uff0c\u6253\u9020\u5168\u81ea\u52a8\u6ce8\u5165\u6846\u67b6ButterKnife](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [14.\u6027\u80fd\u4f18\u4ece\u53ea\u4f1a\u5f00\u53d1\u5230\u5168\u76d8\u638c\u63a7App\u6027\u80fd\uff0c\u53ea\u9700\u8981\u4ece\u8fd9\u8282\u8bfe\u5f00\u59cb\u5316](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [15.\u8001\u53f8\u673a\u6253\u7834Bitmap\u5e38\u89c4\u601d\u7ef4\uff0c\u4eceSkia\u5f15\u64ce\u770bBitmap\u52a0\u8f7d\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [16.\u4f60\u7684\u5e94\u7528\u505a\u4e86\u57cb\u70b9\u4e0a\u4f20\u5417\uff1f\u624b\u5199\u7f16\u8bd1\u65f6\u4eca\u65e5\u5934\u6761\u7684\u57cb\u70b9\u67b6\u6784](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [17.NDK\u5927\u725b\u5e26\u4f60\u4e00\u5802\u8bfe\u641e\u5b9a\u4e00\u7ebf\u5927\u5382\u97f3\u89c6\u9891\u9762\u8bd5\u96c6\u5408](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [18.\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\u5417\uff1f\uff0c\u5982\u679c\u8fde\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u90fd\u62e7\u4e0d\u6e05\u600e\u4e48\u884c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [19.\u9762\u8bd5\u4e13\u9898-Okhttp\u76f8\u5173\u9762\u8bd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [20.\u9762\u8bd5\u4e13\u9898-Okhttp\u9762\u8bd5\u4e13\u9898\u5b8c\u7ed3\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [21.\u9762\u8bd5\u4e13\u9898\u4e4b\u6253\u901aGlide\u6e90\u7801\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [22.UI\u4f18\u5316\u662f\u4e0d\u662f\u53ea\u4f1a\u8bf4\u5e03\u5c40\u5c42\u7ea7\u4e0d\u80fd\u592a\u6df1\uff1f\u6765\uff0c\u8fd9\u91cc\u6709\u5168\u76d8\u6df1\u5ea6\u5206\u6790\uff01](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [23.\u6ce8\u89e3\u53cd\u5c04\u7684\u9ad8\u7ea7\u6280\u5de7\uff0c\u8ba9\u4f60\u5f7b\u5e95\u4e86\u89e3EventBus\u662f\u5982\u4f55\u8fdb\u884c\u7ec4\u4ef6\u901a\u4fe1\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [24.\u542c\u8bf4\u4f60\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\uff1f\u90a3\u4f60\u6709\u67b6\u6784style\u6ca1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [25.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u9ad8\u7ea7\u8fdb\u9636\uff0c\u624b\u5199\u5b9e\u73b0\u7f51\u6613\u4e91\u4e3b\u9898\u6362\u80a4\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [26.Binder\u4e13\u9898\uff08\u4e00\uff09\u76f4\u6363Binder\u56db\u5c42\u6846\u67b6\uff0c\u8da3\u8bb2Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [27.Binder\u4e13\u9898\uff08\u4e8c\uff09\u8fdb\u7a0b\u901a\u4fe1\u7684\u6838\u5fc3\u5185\u5b58\u7ba1\u7406\u4e0e\u8c03\u5ea6\uff0c\u6df1\u5165\u7406\u89e3Binder\u4e2d\u5185\u5b58\u64cd\u4f5c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [28.Binder\u4e13\u9898\uff08\u4e09\uff09\u57fa\u4e8eBinder\u7684\u5e95\u5c42\u5b9e\u73b0\uff0c\u624b\u5199Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [29.Jetpack\u4e4b\u540e\u4f60\u8fd8\u6ca1\u6709\u89e3\u9501LiveData\uff1f\u6765\uff0c\u4e00\u8282\u8bfe\u5e26\u4f60\u89e3\u9501\u5e76\u4e14\u8fd0\u7528\u5230\u9879\u76ee\u4e2d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [30.\u4e3a\u4ec0\u4e48\u9009\u62e9Glide\u4f5c\u4e3a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u4e0eFresco\uff0cPascco\u4f18\u52bf\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [31.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u6280\u80fd\uff0c\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0(\u54c8\u592b\u66fc\u7b97\u6cd5\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [32.\u9ad8\u7ea7UI\u8981\u4e0d\u8981\u4e86\u89e3\u4e0b\uff1f\u8001\u53f8\u673a\u5e26\u4f60\u6765\u4e00\u5802\u81ea\u5b9a\u4e49ViewGroup\u5b9e\u6218\u8bfe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [33.\u9762\u8bd5\u65f6\u603b\u88ab\u5185\u5b58\u95ee\u9898\u8650\u5343\u904d\uff1f\u7406\u8bba\u77e5\u8bc6\u53c8\u770b\u4e0d\u61c2\uff1f\u6765\uff0c\u8fd9\u8282\u8bfe\u5f7b\u5e95\u641e\u5b9a\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [34.Android\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u5230\u5e95\u6709\u591a\u7b80\u5355\uff0c\u56db\u4e2a\u6838\u5fc3\u7c7b\u9610\u8ff0\u5176\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Android\u5e38\u7528\u7ec4\u4ef6\u901a\u4fe1\u65b9\u5f0f\u9610\u8ff0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ce8\u89e3\u4e0e\u53cd\u5c04\u5728\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u4e2d\u7684\u4f7f\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u4ece\u5b9e\u6218\u4e2d\u4e86\u89e3\u89e3\u8026\u7684\u6838\u5fc3\u601d\u60f3\u4e0e\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [35.\u865a\u62df\u673a\u8be6\u89e3\u5185\u5b58\u7ed3\u6784\u539f\u7406\uff0c\u7528\u4ee3\u7801\u7684\u89d2\u5ea6\u5206\u6790\u5185\u5b58\u5206\u5e03](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u65b9\u6cd5\u533a\uff0c\u5806\u533a\uff0c\u6808\u533a\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5b57\u8282\u7801\u6307\u4ee4\u4e0eDex\u6307\u4ee4\u96c6\u6267\u884c\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5bc4\u5b58\u5668\u4e0e\u865a\u62df\u6808\u6307\u4ee4\u6d41\u7a0b\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [36.\u5927\u5382\u67b6\u6784\u5e08\u5e26\u4f60\u624b\u5199Glide\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u8ba9\u4f60\u79bb\u67b6\u6784\u5e08\u7684\u8ddd\u79bb\u66f4\u8fd1\u4e00\u6b65](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Glide\u6846\u67b6\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u8981\u600e\u6837\u5c01\u88c5?](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u600e\u4e48\u5904\u7406\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u7684\u9ad8\u5e76\u53d1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.Glide\u7684\u4e09\u7ea7\u7f13\u5b58\u673a\u5236\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [37.\u5e26\u4f60\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u89e3\u8bfbHandler\u6838\u5fc3\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [1.Handler\u6e90\u7801\u5206\u6790\u7684\u4e09\u6761\u4e3b\u7ebf](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6e90\u7801\u4e2d\u9690\u85cf\u7684\u90a3\u4e9b\u4e0d\u80fd\u5ffd\u89c6\u7684\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.Handler\u76f8\u5173\u9762\u8bd5\u9898\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [38.FFmpeg\u548cMediaCodec\u7684API\u770b\u4e0d\u61c2\uff1f\u5e94\u8be5\u4ece\u97f3\u89c6\u9891H264\u7f16\u7801\u539f\u7406\u5165\u624b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.H264\u539f\u7406\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4fe1\u6e90\u7f16\u7801\u5668\u662f\u5982\u4f55\u5bf9\u89c6\u9891\u5e27\u8fdb\u884c\u7f16\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3. slice(\u5207\u7247) Nal(\u5b8f\u5757) \uff08\u50cf\u7d20\u9884\u6d4b\uff09\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [39.\u5982\u679c\u4f60\u662f\u67b6\u6784\u5e08\uff1f\u600e\u6837\u80fd\u8ba9\u4f60\u7684\u5e94\u7528\u53ea\u670910M\u7684\u4f53\u79ef\u786e\u62e5\u67091000M\u7684\u529f\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5982\u4f55\u8ba9\u4f60\u7684\u9879\u76ee\u4f53\u79ef\u5c0f\u529f\u80fd\u591a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ca1\u6709\u5b89\u88c5\u7684APK\u5305\u6211\u4eec\u600e\u4e48\u8ba9\u5b83\u201c\u52a8\u8d77\u6765\u201d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u67b6\u6784\u5e08\u8be5\u5e72\u7684\u6d3b\u5c31\u662f\u628a\u4e0d\u53ef\u80fd\u53d8\u6210\u53ef\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u624b\u5199\u5b9e\u73b0\u5927\u5382\u90fd\u5728\u73a9\u7684\u63d2\u4ef6\u5316\u5f00\u53d1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [40.\u6296\u97f3\u89c6\u9891\u526a\u8f91\u539f\u7406\uff0c\u624b\u5199\u89c6\u9891\u526a\u8f91\u4e0e\u80cc\u666f\u97f3\u4e50\u5408\u6210](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  - [1.MeidiaCodec\u7f16\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4e0d\u66ff\u6362\u89c6\u9891\u97f3\u4e50\u524d\u63d0\u4e0b\uff0c\u5c06\u6b4c\u66f2\u5408\u6210\u5230\u89c6\u9891\u58f0\u97f3  ](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u767b\u9876IT\u754c\u7684\u7687\u51a0-\u624b\u5199\u89c6\u9891\u526a\u8f91\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [41.Android\u9ad8\u7ea7\u6280\u80fd-\u5927\u56fe\u52a0\u8f7d\uff0c\u800c\u4f60\u5728\u672c\u8282\u8bfe\u5b66\u5230\u7684\u53ef\u4e0d\u6b62\u8fd9\u4e00\u4e2a\u70b9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5927\u56fe\u52a0\u8f7d\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5185\u5b58\u590d\u7528\uff0c\u56fe\u7247\u5360\u7528\u5185\u5b58\u5206\u6790\u4ee5\u53ca\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5982\u4f55\u521b\u5efa\u5927\u56fe\u800c\u4e0dOOM\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u989d\u5916\u5206\u4eab\uff1a\u9762\u8bd5\u9047\u5230\u5b8c\u5168\u4e0d\u4f1a\u7684\u95ee\u9898\u600e\u4e48\u529e\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [42. jepg\u56fe\u50cf\u5f15\u64ce\u5b9e\u73b0\u8d85\u8fc7\u539f\u751f\u7684\u56fe\u7247\u538b\u7f29\u6027\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u54c8\u592b\u66fc\u538b\u7f29\u7b97\u6cd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.Bitmap\u6e90\u7801\u89e3\u8bfb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.native\u5c42\u8bfb\u53d6\u56fe\u7247](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [43.\u7834\u89e3\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7aa5\u63a2\u963f\u91ccARouter\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u8def\u7531\u6846\u67b6\u7a76\u7adf\u662f\u4ec0\u4e48\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u963f\u91ccARouter\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.APT\u6280\u672f\u5b9e\u73b0\u624b\u5199Arouter\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [44.\u963f\u91ccP7\u5c97\u9762\u8bd5\u5173\u4e8eRecyclerView\u7684\u8fde\u73af\u70ae\uff0c\u4e00\u5c0f\u65f6\u89e3\u51b3RecyclerView\u6240\u6709\u5e95\u5c42\u7591\u60d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.RecyclerView\u7684\u590d\u7528\u673a\u5236\uff0c\u7b80\u5355\u8bf4\u8bf4View\u56de\u6536\u4e0e\u590d\u7528\u7684\u8fc7\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.RecyclerView\u652f\u6301\u591a\u4e2a\u4e0d\u540c\u7c7b\u578b\u5e03\u5c40\uff0c\u4ed6\u4eec\u600e\u4e48\u7f13\u5b58\uff0c\u5e76\u4e14\u67e5\u627e\u7684\u5462](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u8bf4\u4e00\u8bf4RecyclerView\u9002\u914d\u5668\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u7406\u6e05RecyclerView\u67b6\u6784\u601d\u60f3\uff0c\u624b\u5199RecyclerView\u81ea\u5b9a\u4e49\u63a7\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [45.\u5343\u4e07\u7ea7\u5e94\u7528\u7f8e\u56e2Robust\u4fee\u590d\u539f\u7406\uff0c\u624b\u5199\u5b57\u8282\u7801\u63d2\u4ef6\u6280\u672f\u6280\u672f\u70b9\uff1a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u7f8e\u56e2robust\u4fee\u590d\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.robust\u4f9d\u8d56\u7684\u63d2\u4ef6\u5b9e\u73b0\u65b9\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.apk\u7f16\u8bd1\u539f\u7406\uff0cgroovy\u5b9e\u73b0\u52a8\u6001\u63d2\u5165\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u81ea\u5df1\u5b9e\u73b0robust\u63d2\u4ef6\uff0c\u52a8\u6001\u6539\u52a8\u4e3b\u5de5\u7a0b\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [46.\u67b6\u6784\u5e08\u4fee\u70bc\u4e4b\u8def-\u7ad9\u5728\u67b6\u6784\u5e08\u7684\u89d2\u5ea6\u5982\u4f55\u5999\u7528\u81ea\u5b9a\u4e49\u6ce8\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [1.\u4e3a\u4ec0\u4e48\u4e0d\u7528EventBus\u4e86](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [2. \u4e8b\u4ef6\u81a8\u80c0\u5982\u4f55\u89e3\u51b3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [3. \u7ec4\u4ef6\u901a\u4fe1\u8fd8\u80fd\u600e\u4e48\u73a9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  \n  \n \n ### \u4e94\u4e36\u4e13\u9898\u7bc7\n #### 2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898\n  - [2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n - [1.\u4ec0\u4e48\u662f\u6ca1\u6709\u65b9\u5411\u5c31\u505c\u4e0b\u6765\uff0c\u4e86\u89e3Android\u53d1\u5c55\u624d\u80fd\u7a33\u6b65\u524d\u884c\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u7528\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1Android\u5f00\u53d1\u8005\u7684\u524d\u666f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u8ba9offer\u8ffd\u7740\u4f60\u7684\u79d8\u8bc0\u662f\u4ec0\u4e48\uff1f\u7b80\u5386\u5168\u76d8\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5236\u5b9a\u804c\u4e1a\u89c4\u5212\uff0c\u4e3a\u672a\u6765\u94fa\u5e73\u9053\u8def](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [2.\u5de5\u6b32\u5584\u5176\u4e8b\u5fc5\u5148\u5229\u5176\u5668-OkHttp\u6e90\u7801\u89e3\u8bfb](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4e3a\u4ec0\u4e48OkHttp\u4f7f\u7528Socket\u800c\u4e0d\u662fHttpUrlConnection](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Okhttp\u6d41\u7a0b\uff0c\u6838\u5fc3\u7c7b\u5168\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4ece\u6e90\u7801\u89d2\u5ea6\u4e86\u89e3Okhttp\u4e2d\u6784\u5efa\u8005\u4e0e\u8d23\u4efb\u94fe\u6a21\u5f0f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5982\u4f55\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [3.\u6280\u80fd\u6301\u7eed\u8fdb\u9636-Glide\u8be6\u89e3\uff0c\u8ba9\u4f60\u5bf9\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u77e5\u6839\u77e5\u5e95](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u6e90\u7801\u4e09\u6761\u4e3b\u7ebf\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u751f\u547d\u5468\u671f\u7ba1\u7406\u7b56\u7565\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n\n\n #### \u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab\n  - [\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n - [1.\u544a\u522b\u4f20\u7edf\u5355\u4e00\u6a21\u5757\u5f00\u53d1\uff0c\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\u6a21\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff0c\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u600e\u6837\u5bf9\u9879\u76ee\u4e2d\u6240\u6709\u7684\u4e1a\u52a1\u6a21\u5757\u8fdb\u884c\u79d1\u5b66\u7ba1\u7406\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u600e\u6837\u8fdb\u884c\u7a97\u4f53\u8df3\u8f6c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7ec4\u4ef6\u5316\u7406\u7531\u6846\u67b6\u624b\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b8c\u7f8e\u5229\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7c7b\u751f\u6210\u795e\u5668Java poet\u795e\u52a9\u653b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u8def\u7531\u8868\u7684\u7a7a\u865a\u5bc2\u5bde\u51b7\u8ba9\u6211\u4eec\u5982\u4f55\u6ee1\u8db3\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u624b\u5199\u5b9e\u73b0\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u8ba9\u7ec4\u4ef6\u5316\u9879\u76ee\u5982\u864e\u6dfb\u7ffc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u8981\u5982\u4f55\u901a\u4fe1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u9762\u5bf9Android\u5e02\u573a\u7684N\u4e2d\u901a\u4fe1\u65b9\u5f0f\u6211\u4eec\u8be5\u5982\u4f55\u6289\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Jetpack\u4e2d\u7684liveData\u6253\u9020\u7ec4\u4ef6\u5316\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n #### Android\u7cfb\u7edf\u6e90\u7801FrameWork\u5b9e\u6218\u4e13\u9898\n - [1.Android10.0 9.0 8.0\u6df1\u5165\u865a\u62df\u673a\u5e95\u5c42\u4e2d\u8bb2\u89e3\u5185\u5b58\u5206\u5e03\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [30\u5206\u949f\u7406\u6e05\u7a0b\u5e8f\u5458\u5bf9\u5185\u5b58\u7684\u6050\u60e7\uff0c\u5e73\u65f6\u5199\u7684\u53d8\u91cf\uff0c\u5bf9\u8c61\u5728\u5c4b\u91cc\u5185\u5b58\u7684\u5206\u90e8](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u65b9\u6cd5\u533a\uff0c\u6808\u533a\uff0c\u5806\u533a\u8fd0\u884c\u673a\u5236\u548c\u6d41\u7a0b\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u57fa\u4e8e\u5bc4\u5b58\u5668\u4e0eJVM\u57fa\u4e8eJava\u6808\u7684\u533a\u522b\u548c\u539f\u7406-\u5bc4\u5b58\u5668\u7684\u4f5c\u7528\u4e8e\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u53cd\u7f16\u8bd1dex\u6587\u4ef6\u4e4barm\u6307\u4ee4\u96c6\u5206\u6790\uff0c\u770b\u770bnew\u4e00\u4e2a\u5bf9\u8c61\u6700\u7ec8\u53d8\u6210\u4e86\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u67e5\u770bAndroid\u7cfb\u7edf\u6e90\u7801\uff0cdex\u4e2dARM\u516c\u53f8\u7684\u6307\u4ee4\u96c6\u52a0\u8f7d\u4e0e\u6267\u884c\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6df1\u5165\u7406\u89e3Android\u6838\u5fc3\u5173\u952e\u5b57 new synchronized volatile final\u5e95\u5c42\u5b9e\u73b0\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Java\u5bf9\u8c61\u5728\u5806\u533a\u5927\u5c0f\u4e3a8\u4e2a\u5b57\u8282\u53ca\u6bcf\u4e00\u4e2a\u5206\u90e8\u8be6\u89e3-hashcode\u51fd\u6570\u6267\u884c\u673a\u5236\uff0chashcode\u5728\u5185\u5b58\u4e2d\u5b58\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [class\u7c7b\u5728\u65b9\u6cd5\u533a\u4e3a\u4f55\u662f426\u4e2a\u5b57\u8282\uff0c\u65b9\u6cd5\u4e3a\u4f55\u4f1a\u589e\u52a04\u4e2a\u5b57\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4eceJNIEnv\u5165\u624b\u627e\u5230\u7c7b\u52a0\u8f7d\u4e0eclass\u7c7b\u6784\u9020\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3. sophix\u70ed\u4fee\u590d\u524d\u8eab--Arm\u6307\u4ee4\u96c6\u66ff\u6362\u5b9e\u73b0java\u65b9\u6cd5\u52a8\u6001\u4fee\u590d\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [artMethod\u7ed3\u6784\u4f53 \u8be6\u7ec6\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6267\u884c\u5f15\u64ce\u4e2dclass\u5982\u4f55\u88ab\u52a0\u8f7d\uff0c\u4e09\u90e8\u66f2\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Findclass\u51fd\u6570\uff0c\u7c7b\u5982\u4f55\u5728\u865a\u62df\u673a\u5c42\u53ea\u52a0\u8f7d\u4e00\u6b21](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Defineclass\u51fd\u6570\uff0c\u5b9a\u4e49\u4e00\u4e2a\u7a7a\u7684class](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Loadclass\u51fd\u6570\uff0cdex\u6587\u4ef6\u4e2d\u5c06\u7c7b\u6709\u78c1\u76d8\u7f13\u5b58\u5230\u5185\u5b58](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [\u865a\u62df\u673a\u5c42\u66ff\u6362\u6267\u884c\u5f15\u64ce\u4e2darm\u6307\u4ee4](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u865a\u62df\u673a\u5c42Java\u65b9\u6cd5\u66ff\u6362\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7cfb\u7edf\u6e90\u7801\u79fb\u690d\u5230AS\u7f16\u8bd1\u4e0e\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n### \u516d\u4e362020\u6700\u65b0Android\u6587\u7ae0\u7cfb\u5217\uff1a\n - [\u9762\u8bd5\u5b98\uff1a\u4f60\u6709\u7528\u8fc7Flutter\u5417? Flutte\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c\u67b6\u6784\u662f\u600e\u4e48\u6837\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u6bd4\u5176\u4ed6\u5982ReactNative\u597d](https://www.jianshu.com/p/3e2d9b23cfd6)\n \n - [\u5982\u4f55\u52a0\u8f7d100M\u7684\u56fe\u7247\u5374\u4e0d\u6491\u7206\u5185\u5b58,\u4e00\u5f20 100M \u7684\u5927\u56fe\uff0c\u5982\u4f55\u9884\u9632 OOM\uff1f](https://www.jianshu.com/p/878e4ddaa51b)\n  \n - [\u5b9d\u5b9d\u5df4\u58eb\uff1aKotlin\u4e3a\u4ec0\u4e48\u4f1a\u706b\u8d77\u6765\uff0c\u6709\u4ec0\u4e48\u7279\u70b9\uff0c\u8ddfJava\u533a\u522b](https://www.jianshu.com/p/dd9c0b9af2a1)\n  \n - [IGG\uff1aAndroid\u5185\u5b58\u56de\u6536\u673a\u5236\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/2b2642ce379f)\n  \n - [\u6012\u5237Android\u9762\u8bd5100\u9898\uff0c\u518d\u4e5f\u4e0d\u62c5\u5fc3\u4e0d\u80fd\u540a\u6253\u9762\u8bd5\u5b98\u4e86](https://www.jianshu.com/p/c01c3d0b1ee9)\n \n - [\u5b57\u8282\u8df3\u52a8:Android R\u5982\u4f55\u8bbf\u95ee\u6587\u4ef6\uff0c\u4fee\u6539\u6587\u4ef6\uff0c\u4f60\u4eec\u5bf9R\u9002\u914d\u4e86\u5417](https://www.jianshu.com/p/994b72f06af9)\n  \n  - [\u5b57\u8282\u8df3\u52a8\uff1aIO\u4f18\u5316\u662f\u600e\u4e48\u505a\u7684\uff0c\u4f7f\u7528 SharedPreferences\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u5361\uff0cmmkv\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/12428890ae1e)\n   \n  - [2020\u5e74Android\u6700\u5148\u9762\u8bd5\u4e13\u9898\u52a9\u4f60\u65a9\u83b7offer\uff0c\u4ee5\u53ca\u6559\u4f60\u5982\u4f55\u4e00\u6b65\u6b65\u7b80\u5386](https://www.jianshu.com/p/af7938c116bb)\n   \n - [\u9762\u8bd5\u5b98\u8bf4\uff1a\u5927\u5bb6\u90fd\u8bf4 Java \u53cd\u5c04\u6548\u7387\u4f4e\uff0c\u4f60\u77e5\u9053\u539f\u56e0\u5728\u54ea\u91cc\u4e48](https://www.jianshu.com/p/4a32b9b71115)\n \n - [Android \u5f00\u53d1\u4e2d\u7684\u67b6\u6784\u6a21\u5f0f -- MVC / MVP / MVVM](https://www.jianshu.com/p/218f9432ee52)\n \n - [\u5173\u4e8e\u963f\u91cc\u63a8\u51fa\u7684\u8def\u7531\u6846\u67b6ARouter\u6e90\u7801\u89e3\u6790](https://www.jianshu.com/p/7b4d085e23a9)\n  \n - [\u8131\u4e86\u9a6c\u7532\u6211\u4e5f\u8ba4\u8bc6\u4f60: \u804a\u804a Android \u4e2d\u7c7b\u7684\u771f\u5b9e\u5f62\u6001](https://www.jianshu.com/p/1094f6e4444f)\n\n- [Android\u9762\u8bd5\u5206\u6790\u4e00\uff1a\u5173\u4e8eOKhttp\u8be6\u89e3\uff08\u9644\u5e26\u89c6\u9891\u6559\u7a0b\uff09](https://www.jianshu.com/p/f4e353336b86)\n \n - [\u9762\u8bd5\u5b98\uff1a\u5185\u5b58\u6cc4\u6f0f\u8fde\u73af\u95ee\u3002\u88ab\u95ee\u61f5\u4e86\uff1f\u6765\u770b\u770b\u8fd9\u4e2a](https://www.bilibili.com/video/BV1ck4y1r7PK)\n \n  - [\u963f\u91cc\u9762\u8bd5\u5b98\uff1a\u5173\u4e8eRecyclerView\u505a\u4e0b\u5206\u4eab](https://www.jianshu.com/p/e5b2963706c7)\n  \n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n \n#### \u4e03\u4e36Android\u6027\u80fd\u4f18\u5316\u7bc7\n\n- [Android\u6027\u80fd\u4f18\u5316(1)- \u542f\u52a8\u4f18\u5316](https://mp.weixin.qq.com/s/gZdL4rNuw0cHTXL7RkkJ1A)\n  \n - [Android\u6027\u80fd\u4f18\u5316(2)-UI\u6e32\u67d3\u4f18\u5316](https://mp.weixin.qq.com/s/YeSkBcMB2tf0pVmddxA2Mg)\n   \n - [Android\u6027\u80fd\u4f18\u5316(3)-\u5954\u6e83\u4f18\u5316 ](https://mp.weixin.qq.com/s/J8qPFv9Fbt9_UGKqGxS5YQ)\n    \n - [Android\u6027\u80fd\u4f18\u5316(4)-\u5185\u5b58\u4f18\u5316 ](https://mp.weixin.qq.com/s/Rte_e7R7grfrYUE6RXJYKg)\n     \n - [Android\u6027\u80fd\u4f18\u5316(5)-\u5361\u987f\u4f18\u5316](https://pan.baidu.com/s/15rJ_qEWSJdlODA6ifjHTAw)\n      \n - [Android\u6027\u80fd\u4f18\u5316(6)-\u5b58\u50a8\u4f18\u5316](https://mp.weixin.qq.com/s/6A4jU8spcLJa2BKdMS10yA)\n     \n - [Android\u6027\u80fd\u4f18\u5316(7)-\u7f51\u7edc\u4f18\u5316](https://mp.weixin.qq.com/s/rJChehNdyPFbB_LcKSV_jA)\n\n - [Android\u6027\u80fd\u4f18\u5316(8)-\u8017\u7535\u4f18\u5316](https://mp.weixin.qq.com/s/wJdzAM5a9a6rFHDdJMBfAA)\n         \n - [Android\u6027\u80fd\u4f18\u5316(9)-\u591a\u7ebf\u7a0b\u5e76\u53d1\u4f18\u5316](https://mp.weixin.qq.com/s/sZ1MgTlDlusgGDWGZUZZzw)\n\n - [Android\u6027\u80fd\u4f18\u5316(10)-\u5b89\u88c5\u5305\u4f18\u5316](https://mp.weixin.qq.com/s/Qo0y6xbZ8LFdYvcWdqmKjQ)\n \n #### \u516b\u4e36Android Framework\u5c42\u9762\u8bd5\n \n- [Android Framework\u5c42\u9762\u8bd5\uff081\uff09-\u4e4bActivity\u542f\u52a8\u9762\u8bd5\u8fde\u73af\u70ae](https://mp.weixin.qq.com/s/LYQXe93evbHrleUPs62Jvw)\n  \n - [Android Framework\u5c42\u9762\u8bd5\uff082\uff09-\u4e4bBinder\u901a\u4fe1\u673a\u5236](https://mp.weixin.qq.com/s/Qnf79D54UF3o9k_VmuAIWQ)\n   \n - [Android Framework\u5c42\u9762\u8bd5\uff083\uff09-\u4e4bHandler\u9762\u8bd5\u96c6\u5408](https://mp.weixin.qq.com/s/MAAQLTgMYD3FxVS6ZFPDww)\n    \n - [Android Framework\u5c42\u9762\u8bd5\uff084\uff09-\u4e4b\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u5168\u9762\u6574\u7406 ](https://mp.weixin.qq.com/s/UEL_jxU8nugGGBerYFuN_g)\n     \n - [Android Framework\u6e90\u7801\u9762\u8bd5\uff085\uff09-\u4e4bonMeasure\u6d4b\u91cf\u539f\u7406](https://mp.weixin.qq.com/s/jydw_NT3AMIyLCoiynFFnA)\n      \n - [AndroidFramework\u5c42\u9762\u8bd5\uff086\uff09-\u4e4bAndroid \u5c4f\u5e55\u5237\u65b0\u673a\u5236(\u8bf4\u8bf4\u5361\u987f\u539f\u56e0)](https://mp.weixin.qq.com/s/xqoJRfXHUawQjfbioCcBfw)\n\n  \n\n\n### \u4e5d\u4e36\u4e92\u8054\u7f51\u7684\u5bd2\u51ac\u4e0b\uff0c\u5982\u4f55\u624b\u63e1\u5b89\u535370\u4e07\u5e74\u85aa\u3002\u4e00\u5802\u8bfe\u5e26\u4f60\u8d70\u8fdb\u8c61\u7259\u5854\n\n[\u4e3a\u4ec0\u4e48\u4f1a\u53d1\u751f\u4e92\u8054\u7f51\u7684\u5bd2\u51ac](android/videowhy.md)\n\n[\u97f3\u9891\u964d\u566a\u539f\u7406\uff0c\u97f3\u9891\u539f\u7406](android/voice.md)\n\n[\u97f3\u89c6\u9891\u662f\u4ec0\u4e48\uff0c\u89c6\u9891\u4e3a\u4ec0\u4e48\u9700\u8981\u538b\u7f29](android/videowhy.md)\n\n[\u89c6\u9891\u538b\u7f29\u538b\u7f29\u7684\u662f\u4ec0\u4e48\u4fe1\u606f? \u5e27\u5185\u538b\u7f29\u4e0e\u5e27\u95f4\u538b\u7f29\u539f\u7406](android/videoencode.md)\n\n[\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u4f4e\u5ef6\u65f6\u600e\u4e48\u5b9e\u73b0](android/live-optimitor.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n[\u5fae\u4fe1---\u97f3\u89c6\u9891\u901a\u8bdd\u5982\u4f55\u5b9e\u73b0\uff0c\u89c6\u9891\u4f1a\u8bae\u539f\u7406](android/mediacodec.md)\n\n[5G\u65f6\u4ee3\u5982\u4f55\u91cd\u751f\u79fb\u52a8\u4e92\u8054\u7f51,\u5e26\u4f60\u770b\u770b\u4ec0\u4e48\u662f5G\u5e94\u7528]()\n\n[\u5c0f\u7c73--\u4e07\u7269\u4e92\u8054\u5b9e\u73b0\u539f\u7406](android/net.md)\n\n### \u5341\u4e362019Android\u5e74\u9ad8\u7ea7\u9762\u8bd5\n\n * [2019\u5e74Bat\u9762\u8bd5\u96c6\u5408](#2019\u5e74Bat\u9762\u8bd5\u96c6\u5408)\n * [\u67b6\u6784\u76f8\u5173\u9762\u8bd5](#\u67b6\u6784\u76f8\u5173\u9762\u8bd5)\n * [NDK\u76f8\u5173\u9762\u8bd5](#NDK\u76f8\u5173\u9762\u8bd5)\n * [\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5](#\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5)\n * [\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5](#\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5)\n * [\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5](#\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5)\n * [\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5](#\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5)\n * [\u5176\u4ed6](#\u5176\u4ed6)\n\n### \u5341\u4e00\u4e362019\u5e74Bat\u9762\u8bd5\u96c6\u5408\n\n\n\n> \u963f\u91cc\u5df4\u5df4\u9762\u8bd5\u96c6\u5408\n\n- [Android P\u7981\u6b62\u4e86http\u5bf9\u4f60\u4eec\u6709\u5f71\u54cd\u5417\uff1fhttps\u539f\u7406\u4f60\u77e5\u9053\u5417\uff1f](android/https.md)\n\n- [\u4ec0\u4e48\u662f\u5bf9\u79f0\u52a0\u5bc6\uff0c\u4ec0\u4e48\u662f\u975e\u5bf9\u79f0\u52a0\u5bc6\uff0c\u516c\u94a5\u4e0e\u79c1\u94a5\u5c5e\u4e8e\u5bf9\u79f0\u52a0\u5bc6\u5417](android/cert.md)\n\n- [https\u8bf7\u6c42\u4f1a\u4e0d\u4f1a\u5b58\u5728\u88ab\u62e6\u622a\u7684\u53ef\u80fd\uff1f\u4f60\u5bf9\u8fd9\u65b9\u9762\u6709\u8fc7\u7814\u7a76\u5417](android/cert.md)\n\n- [\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u5982\u4f55\u4f7f\u753b\u9762\u4fdd\u8bc1\u6d41\u7545\u7684\u6548\u679c](android/live-optimitor.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n- [Flutter\u4e3a\u4ec0\u4e48\u4f1a\u505a\u5230\u4e00\u5904\u5199 \u5904\u5904\u8fd0\u884c \u4e0eRN\u7684\u533a\u522b](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [Flutter\u7684\u56fe\u5f62\u5f15\u64ce\u4e0eAndroid\u7684\u6e32\u67d3\u5f15\u64ce\u539f\u7406](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u5bf9\u4e8eTersorflow\u4f60\u600e\u4e48\u7406\u89e3\u7684\uff0c\u6709\u505a\u8fc7\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u5417](android/tersorflow.md)\n\n- [\u4e3a\u4ec0\u4e48Android\u4f1a\u51fa\u73b0\u5361\u987f](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u7ed9\u4f60\u4e00\u4e2aDemo \u4f60\u5982\u4f55\u5feb\u901f\u5b9a\u4f4dANR](android/anr.md)\n\n- [Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n- [Glide\u5bf9Bitmap\u7684\u7f13\u5b58\u4e0e\u89e3\u7801\u590d\u7528\u5982\u4f55\u505a\u5230\u7684](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u8bf4\u8bf4\u4f60\u5bf9Dalvik\u865a\u62df\u673a\u7684\u8ba4\u8bc6 ](android/dalvik.md)\n\n- [\u63a5\u4e0b\u6765\u8bf4\u8bf4 Android \u865a\u62df\u673aDalvik\u4e0eART\u533a\u522b\u5728\u54ea\u91cc\uff1f](android/artordalvik.md)\n\n- [Handler\u7684\u539f\u7406\u662f\u4ec0\u4e48?\u80fd\u6df1\u5165\u5206\u6790\u4e0b Handler\u7684\u5b9e\u73b0\u673a\u5236\u5417\uff1f](./study/framework/Handler\u673a\u5236\u6e90\u7801.md)\n\n- [ Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n  \n\n> \u817e\u8baf\u9762\u8bd5\u96c6\u5408\n\n- [\u8be6\u7ec6\u8bf4\u8bf4Binder\u901a\u4fe1\u539f\u7406\u4e0e\u673a\u5236](android/binder.md)\n\n- [Linux\u81ea\u5e26\u591a\u79cd\u8fdb\u7a0b\u901a\u4fe1\u65b9\u5f0f\uff0c\u4e3a\u4ec0\u4e48Android\u90fd\u6ca1\u91c7\u7528\u4e8c\u504f\u504f\u4f7f\u7528Binder\u901a\u4fe1](android/binder1.md)\n\n- [\u8c08\u4e00\u8c08Binder\u7684\u539f\u7406\u548c\u5b9e\u73b0\u4e00\u6b21\u62f7\u8d1d\u7684\u6d41\u7a0b](android/binder2.md)\n\n- [\u8fdb\u7a0b\u4fdd\u6d3b\u5982\u4f55\u505a\u5230\uff0c\u4f60\u4eec\u4fdd\u6d3b\u7387\u6709\u591a\u9ad8\uff1f](android/process.md)\n\n- [ButterKnife\u4e3a\u4ec0\u4e48\u6267\u884c\u6548\u7387\u4e3a\u4ec0\u4e48\u6bd4\u5176\u4ed6\u6ce8\u5165\u6846\u67b6\u9ad8\uff1f\u5b83\u7684\u539f\u7406\u662f\u4ec0\u4e48](android/butterknife.md)\n\n- [\u7ec4\u4ef6\u5316\u5982\u4f55\u5b9e\u73b0\uff0c\u7ec4\u4ef6\u5316\u4e0e\u63d2\u4ef6\u5316\u7684\u5dee\u522b\u5728\u54ea\u91cc\uff0c\u8be5\u600e\u4e48\u9009\u578b](android/commpont.md)\n\n- [\u8bf4\u4e0b\u7ec4\u4ef6\u4e4b\u95f4\u7684\u8df3\u8f6c\u548c\u7ec4\u4ef6\u901a\u4fe1\u539f\u7406\u673a\u5236](android/commpontrounter.md)\n\n- [\u6709\u6ca1\u6709\u4f7f\u7528\u8fc7\u7ec4\u4ef6\u5316\uff0c\u7ec4\u4ef6\u5316\u901a\u4fe1\u5982\u4f55\u505a\u5230\u7684\uff0cARouter\u6709\u7528\u8fc7\u5417](android/router.md)\n\n- [\u6709\u7528\u8fc7\u63d2\u4ef6\u5316\u5417\uff1f\u8c08\u8c08\u63d2\u4ef6\u5316\u539f\u7406\uff1f](android/plugin.md)\n\n- [\u70ed\u4fee\u590d\u8fde\u73af\u70ae(\u70ed\u4fee\u590d\u662f\u4ec0\u4e48  \u6709\u63a5\u89e6\u8fc7tinker\u5417\uff0ctinker\u539f\u7406\u662f\u4ec0\u4e48)](tencent/tinker.md)\n\n- [\u589e\u91cf\u5347\u7ea7\u4e3a\u4ec0\u4e48\u51cf\u5c11\u5347\u7ea7\u4ee3\u4ef7\uff0c\u589e\u91cf\u5347\u7ea7\u539f\u7406](tencent/update.md)\n\n- [ PMS\u4e4b\u524d\u4e86\u89e3\u8fc7\u5417?\u4f60\u5bf9PMS\u600e\u4e48\u770b\u7684\uff0c\u80fd\u804a\u804aPMS\u7684\u8be6\u7ec6\u5b9e\u73b0\u6d41\u7a0b\u5417](android/pms.md)\n\n- [ AMS\u5728Android\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff0cActivtiy\u542f\u52a8\u8ddfAMS\u6709\u4ec0\u4e48\u5173\u7cfb](android/ams.md)\n\n- [\u4f60\u77e5\u9053\u4ec0\u4e48\u662fAOP\u5417\uff1fAOP\u4e0eOOP\u6709\u4ec0\u4e48\u533a\u522b\uff0c\u8c08\u8c08AOP\u7684\u539f\u7406](android/aop.md)\n\n- [\u7f51\u6613\u4e91--\u624b\u673aQQ\u7684\u6362\u80a4\u662f\u600e\u4e48\u505a\u5230\u7684\uff0c\u4f60\u5bf9\u6362\u80a4\u6709\u4e86\u89e3\u5417\uff1f\u770b\u8fc7\u6362\u80a4\u7684\u539f\u7406\u6ca1\uff1f](android/load.md)\n\n- [\u5783\u573e\u56de\u6536\u673a\u5236\u662f\u5982\u4f55\u5b9e\u73b0\u7684](android/traked.md)\n\n- [\u6570\u636e\u5e93\u7248\u672c\u5982\u4f55\u5355\u72ec\u5347\u7ea7\uff0c\u5e76\u4e14\u5c06\u539f\u6709\u6570\u636e\u8fc1\u79fb\u8fc7\u53bb](tencent/sqlite.md)\n\n- [\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u591a\u7528\u6237\uff0c\u591a\u89d2\u8272\u7684App\u67b6\u6784](android/thread.md)\n\n- [\u8c08\u8c08volatile\u5173\u952e\u5b57\u4e0esynchronized\u5173\u952e\u5b57\u5728\u5185\u5b58\u7684\u533a\u522b](android/volatile.md)\n\n- [synchronize\u5173\u952e\u5b57\u5728\u865a\u62df\u673a\u6267\u884c\u539f\u7406\u662f\u4ec0\u4e48\uff0c\u80fd\u8c08\u4e00\u8c08\u4ec0\u4e48\u662f\u5185\u5b58\u53ef\u89c1\u6027\uff0c\u9501\u5347\u7ea7\u5417](android/synchronize.md)\n\n- [\u7c7b\u6bd4\u4e8e\u5fae\u4fe1\uff0c\u5982\u4f55\u5bf9Apk\u8fdb\u884c\u6781\u9650\u538b\u7f29,\u8c08\u4e0bAndroid\u538b\u7f298\u5927\u6b65 ](android/AndResGuard.md)\n\n- [\u5982\u4f55\u5f7b\u5e95\u9632\u6b62\u53cd\u7f16\u8bd1\uff0cdex\u52a0\u5bc6\u600e\u4e48\u505a ](android/dex.md)\n\n- [\u5e8f\u5217\u5316\u4e0e\u53cd\u5e8f\u5217\u5316\u7684\u539f\u7406\uff0cAndroid\u7684Parcelable\u4e0eSerializable\u533a\u522b\u662f\u4ec0\u4e48](android/herms.md)\n\n- [\u4f60\u66fe\u7ecf\u6709\u6ca1\u6709\u5bf9SqliteDatabase\u505a\u8fc7\u5c01\u88c5\uff0c\u4f60\u81ea\u5df1\u6709\u8bbe\u8ba1\u8fc7\u6570\u636e\u5e93\u6846\u67b6\u5417?\u6216\u8005\u53ea\u662f\u505c\u7559\u5728\u4f7f\u7528ormlite  greenDao\u8fd9\u7c7b\u6846\u67b6](android/sqlite.md)\n\n  \n### \u5341\u4e8c\u4e36\u89c6\u9891\u533a\u57df\n- [\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u963f\u91cc\u9762\u8bd5\u9898\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u624b\u5199\u7ec4\u4ef6Lifecycle](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6Navigation\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6DataBinding-Ex\uff1a\u53cc\u5411\u7ed1\u5b9a\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u6700\u65b0Jetpack\u67b6\u6784\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Foundation\uff08\u6846\u67b6\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Jetpack\u6e90\u7801\u5206\u6790\u3002\u5de8\u4eba\u662f\u5982\u4f55\u70bc\u6210\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [MVVM+Jetpack\u5b9e\u73b0\u7684GitHub\u5ba2\u6237\u7aef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u8df5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [ButterKnife\u8be6\u89e3\u4e0e\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Dagger2\u6838\u5fc3\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u997f\u4e86\u4e48\u8fdb\u7a0b\u901a\u4fe1\u6838\u5fc3\u6280\u672fherms\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199handler\uff0c\u5f15\u53d1\u5185\u5b58\u6cc4\u6f0f\u7684\u6839\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7ec4\u4ef6\u5316\u67b6\u6784\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u63d2\u4ef6\u5316\u6846\u67b6\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199\u70ed\u4fee\u590d\u6846\u67b6Tinker](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u6613\u4e91\u6362\u80a4\u6280\u672f\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u7edc\u52a0\u8f7d\u6846\u67b6OKHTTP\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Rxjava2\u67b6\u6784\u5206\u6790\u4e0e\u6e90\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [C/C++\u5165\u95e8\u8bed\u6cd5\u4ee5\u53ca\u57fa\u7840\u77e5\u8bc6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-JNI\u7f16\u7a0b\u5b9e\u6218](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-\u6784\u5efa\u811a\u672c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n### \u5341\u4e09\u4e36OPCV\u5b66\u4e60\u8d44\u6599\n#### \u7b2c\u4e00\u7ae0 \u9884\u5907\u77e5\u8bc6\n- [1.1.\u7f16\u7a0b\u7684\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.2.\u4ec0\u4e48\u53eb\u7f16\u8f91](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.3.\u4ec0\u4e48\u53eb\u7f16\u8bd1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.4.\u4ec0\u4e48\u53eb\u8fde\u63a5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.5.\u4ec0\u4e48\u53eb\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.6.Visual C++\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.7.\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.8.\u5e93\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.9.OpenCV\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.10.\u4ec0\u4e48\u662f\u547d\u4ee4\u884c\u53c2\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.11.\u5e38\u89c1\u7f16\u8bd1\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.1\u627e\u4e0d\u5230\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.2\u62fc\u5199\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [1.12.\u5e38\u89c1\u94fe\u63a5\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.13.\u8fd0\u884c\u65f6\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e8c\u7ae0 OpenCV\u4ecb\u7ecd\n\n- [2.1.OpenCV\u7684\u6765\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [2.2.OpenCV\u7684\u534f\u8bae](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e09\u7ae0 \u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c\n\n- [3.1.\u56fe\u50cf\u7684\u8868\u793a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.2.Mat\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.3.\u521b\u5efaMat\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.1\u6784\u9020\u51fd\u6570\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.2create\uff08\uff09\u51fd\u6570\u521b\u5efa\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.3Matlab\u98ce\u683c\u7684\u521b\u5efa\u5bf9\u8c61\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.4.\u77e9\u9635\u7684\u57fa\u672c\u5143\u7d20\u8868\u8fbe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.5.\u50cf\u7d20\u503c\u7684\u8bfb\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.1 at()\u51fd\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.2 \u4f7f\u7528\u8fed\u4ee3\u5668](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.3 \u901a\u8fc7\u6570\u636e\u6307\u9488](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.6.\u9009\u53d6\u56fe\u50cf\u5c40\u90e8\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.1 \u5355\u884c\u6216\u5355\u5217\u9009\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.2 \u7528Range\u9009\u62e9\u591a\u884c\u6216\u591a\u5217](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.3 \u611f\u5174\u8da3\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n  - [3.6.4 \u53d6\u5bf9\u89d2\u7ebf\u5143\u7d20](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.7.Mat\u8868\u8fbe\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.8.Mat_\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.9.Mat\u7c7b\u7684\u5185\u5b58\u7ba1\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.10.\u8f93\u51fa](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.11.Mat\u4e0elpllmage\u548cCvMat\u7684\u8f6c\u6362](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n   - [3.11.1 Mat\u8f6c\u4e3alpllmage\u548cCvMat\u7684\u683c\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)   \n   - [3.11.2 lpllmage\u548cCvMat\u683c\u5f0f\u8f6c\u4e3aMat](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n   \n#### \u7b2c\u56db\u7ae0 \u6570\u636e\u83b7\u53d6\u4e0e\u5b58\u50a8\n\n - [4.1.\u8bfb\u53d6\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.1\u8bfb\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   \n - [4.2\u8bfb\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.2.1\u8bfb\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n\n   \n  ## \u66f4\u591a\u76f8\u5173\u9762\u8bd5\u5185\u5bb9\uff0c\u89c6\u9891\u6587\u6863\uff0c2020\u9762\u6700\u65b0\u9762\u8bd5\u4e13\u9898PPT\uff0c\u9ad8\u7ea7\u8fdb\u9636\n  ## (\u5907\u6ce8GitHub\uff09 VX\uff1amm14525201314\n  \n  # \u7248\u6743\u58f0\u660e\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"\u77e5\u8bc6\u5171\u4eab\u8bb8\u53ef\u534f\u8bae\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />\u672c\u4f5c\u54c1\u91c7\u7528<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">\u77e5\u8bc6\u5171\u4eab\u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u7981\u6b62\u6f14\u7ece 4.0 \u56fd\u9645\u8bb8\u53ef\u534f\u8bae</a>\u8fdb\u884c\u8bb8\u53ef\u3002\n"
 },
 {
  "repo": "youyuge34/Anime-InPainting",
  "language": "Python",
  "readme_contents": "Anime-InPainting: An application Tool based on [Edge-Connect](https://github.com/knazeri/edge-connect)\n------------------------------------------------------------------------------------------------------\n<p align=\"left\">\n\t\t<img src=\"https://img.shields.io/badge/version-0.2-brightgreen.svg?style=flat-square\"\n\t\t\t alt=\"Version\">\n\t\t<img src=\"https://img.shields.io/badge/status-Release-gold.svg?style=flat-square\"\n\t\t\t alt=\"Status\">\n\t\t<img src=\"https://img.shields.io/badge/platform-win | linux-lightgrey.svg?style=flat-square\"\n\t\t\t alt=\"Platform\">\n\t\t<img src=\"https://img.shields.io/badge/PyTorch version-1.0-blue.svg?style=flat-square\"\n\t\t\t alt=\"PyTorch\">\n\t\t<img src=\"https://img.shields.io/badge/License-CC BY\u00b7NC 4.0-green.svg?style=flat-square\"\n\t\t\t alt=\"License\">\n</p>\n\nEnglish | [\u4e2d\u6587\u7248\u4ecb\u7ecd](#jump_zh)\n\n<hr>\n\n### Important\n**2019.3.27 Update:**     \nOur **latest** drawing method [PI-REC](https://github.com/youyuge34/PI-REC) is more powerful.      \n Take a look on it, and I'm sure it won't disappoint you.\n<hr>\n\n<p align=\"center\">\n<img src=\"files/banner.png\" width=\"720\" height=\"240\">\n</p>\n\n## Tool show time \ud83c\udff3\ufe0f\u200d\ud83c\udf08\n#### Outputs\n<p align=\"center\">\n<img src=\"files/show1.jpg\" width=\"720\" height=\"400\">\n</p>\n\n#### Tool operation\n<p align=\"center\">\n<img src=\"files/cut2.gif\" width=\"425\" height=\"425\">\n<img src=\"files/cut3.gif\" width=\"406\" height=\"222\">\n</p>\n\nIntroduction:\n-----\nThis is an optimized application tool which has a frontend based on `Opencv`, whose backend used [Edge-Connect](https://github.com/knazeri/edge-connect).\nMake sure you have read their awesome work and license thoroughly.\nCompared with the original work, this project has such <span id=\"improve\">improvements</span> :\n- Add tool application modes\n- Optimize the training phase\n  - Auto-save and auto-load latest weights files\n  - Add a fast training phase combined with origin phase 2 and 3\n- bugs fixed (most of them are merged into the original work)\n- Add utility files\n- Add configs in `config.yml`\n  - PRINT_FREQUENCY\n  - DEVICE : cpu or gpu\n- ... ...\n\n**You can do the amazing Anime inpainting conveniently here.**\n\n**And detailed [training manual](training_manual.md) is released. You may train your own dataset smoothly now.**\n\n## <span id='pre'>Prerequisites</span>\n- Python 3\n- PyTorch `1.0` (`0.4` is not supported)\n- NVIDIA GPU + CUDA cuDNN\n\n## <span id='ins'>Installation</span>\n- Clone this repo\n- Install PyTorch and dependencies from http://pytorch.org\n- Install python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## Run the Tool\nI want to run the tool! Calm down and follow such step:\n\n\n**Info: The following weights files are trained on anime face dataset which performs not good on a large whole anime character.**\n1. Download the well trained model weights file --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. Unzip the `.7z` and put it under your root directory.\nSo make sure your path now is: `./model/getchu/<xxxxx.pth>`\n3. Complete the above [Prerequisites](#pre) and [Installation](#ins)\n4. (Optional) Check and edit the `./model/getchu/config.yml` config file as you wish\n5. Run the cooool tool:\n\n#### Default Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### Tool with edge window:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### Args help\n```bash\npython tool_patch.py -h\n```\n\n> PS. You can run any well trained model, not only above one. You can download more model weights files\nfrom the original work [Edge-Connect](https://github.com/knazeri/edge-connect). Then you can run the Tool as above.\nOnly one thing to be careful: The `config.yml` in this project has some additional options than the config from the [Edge-Connect](https://github.com/knazeri/edge-connect).\n\n\n## Tool operation\nFor detailed manual, refer to your `terminal` prints or the `__doc__` in `tool_patch.py`.\n\nBelow is the simplified tool operation manual:\n\n\n\nKey | description\n-----|-----\nMouse `Left` | To draw out the defective area in window `input` and to draw the edge in window `edge`\nMouse `Right` | To erase edge in window `edge`\nKey `[` | To make the brush thickness smaller\nKey `]` | To make the brush thickness larger\nKey `0` | Todo\nKey `1` | Todo\nKey `n` | To patch the black part of image, just use input image\nKey `e` | To patch the black part of image, use the input image and edit edge (only work under edge window opened)\nKey `r` | To reset the setup\nKey `s` | To save the output\nKey `q` | To quit\n\n## Training manual\nClick here --> [Training manual by yourself](training_manual.md)\n\n\n\n<span id=\"jump_zh\">\u4e2d\u6587\u7248\u4ecb\u7ecd\ud83c\udde8\ud83c\uddf3 </span>\n-----\n\n<hr>\n\n### \u91cd\u8981\n**2019.3.27 \u66f4\u65b0:**     \n\u6211\u4eec\u7684\u6700\u65b0\u6a21\u578b [PI-REC](https://github.com/youyuge34/PI-REC) \u66f4\u5f3a\u5927.            \n\u5982\u679c\u4f60\u60f3\u7528\u6700\u65b0\u7684AI\u7ed8\u753b\u9ed1\u79d1\u6280\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u4fee\u8865\u56fe\u50cf\uff0c\u8bf7\u70b9\u51fb\u4e0a\u9762\u7684\u94fe\u63a5\ud83d\udc46\n<hr>\n\n\n## \u7b80\u4ecb\nTool\u6548\u679c\u770b\u4e0a\u9762\ud83d\udc46 | Bilibili\u89c6\u9891\u6559\u7a0b\uff1aTO DO\n\n\u8fd9\u662f\u56fe\u50cf\u4fee\u8865\u65b9\u5411\u6700\u65b0\u7814\u7a76\u6210\u679c[Edge-Connect](https://github.com/knazeri/edge-connect)\u7684~~\u963f\u59c6\u65af\u7279\u6717\u6c2e\u6c14\u52a0\u901f\u9b54\u6539~~\uff08\u4f18\u5316\uff09\u7248\u3002\n\u7528`Opencv`\u5199\u4e86\u4e2a\u524d\u7aef\u90e8\u5206\uff0c\u540e\u7aef\u662f[Edge-Connect](https://github.com/knazeri/edge-connect)\uff0c\u65b9\u4fbf\u5f53\u4f5c\u5de5\u5177\u4f7f\u7528\u3002\n\u6b64\u5de5\u5177\u53ef\u4ee5\u7528\u6765\u81ea\u52a8\u56fe\u50cf\u4fee\u8865\uff0c\u53bb\u9a6c\u8d5b\u514b\u2026\u2026\u540c\u6837\u4f18\u5316\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u8fc7\u7a0b\u3002\u5177\u4f53\u4f18\u5316\u5185\u5bb9\u8bf7\u770b[\u82f1\u6587\u7248Improvements](#improve)\u3002\n\n\u66f4\u65b0\uff1a[\u8bad\u7ec3\u624b\u518c](training_manual.md#jump_zh)\u5df2\u7ecf\u586b\u5751\u5b8c\u53d1\u5e03\u4e86\uff01\u4f60\u53ef\u4ee5\u7167\u7740\u6307\u5357\u8bad\u7ec3\u81ea\u5df1\u6570\u636e\u96c6\u4e86~\n\n## \u57fa\u7840\u73af\u5883\n- Python 3\n- PyTorch `1.0` (`0.4` \u4f1a\u62a5\u9519)\n- NVIDIA GPU + CUDA cuDNN \uff08\u5f53\u524d\u7248\u672c\u5df2\u53ef\u9009cpu\uff0c\u8bf7\u4fee\u6539`config.yml`\u4e2d\u7684`DEVICE`\uff09\n\n## \u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\n- Clone this repo\n- \u5b89\u88c5PyTorch\u548ctorchvision --> http://pytorch.org\n- \u5b89\u88c5 python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## \u8fd0\u884cTool\n\u6559\u7ec3\uff01\u6211\u6709\u4e2a\u5927\u80c6\u7684\u60f3\u6cd5\ud83c\ude32\u2026\u2026\u522b\u6025\uff0c\u4e00\u6b65\u6b65\u6765\uff1a\n\n\n**\u6ce8\u610f\uff1a\u4ee5\u4e0b\u6a21\u578b\u662f\u5728\u52a8\u6f2b\u5934\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\uff0c\u6240\u4ee5\u5bf9\u52a8\u6f2b\u5168\u8eab\u5927\u56fe\u4fee\u8865\u6548\u679c\u4e00\u822c\uff0c\u60f3\u81ea\u5df1\u518d\u8bad\u7ec3\u7684\u53c2\u8003\u4e0b\u9762\u7684\u8bad\u7ec3\u6307\u5357**\n1. \u4e0b\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6587\u4ef6 --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. \u89e3\u538b `.7z` \u653e\u5230\u4f60\u7684\u6839\u76ee\u5f55\u4e0b.\n\u786e\u4fdd\u4f60\u7684\u76ee\u5f55\u73b0\u5728\u662f\u8fd9\u6837: `./model/getchu/<xxxxx.pth>`\n3. \u5b8c\u6210\u4e0a\u9762\u7684\u57fa\u7840\u73af\u5883\u548c\u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\u6b65\u9aa4\n4. (\u53ef\u9009) \u68c0\u67e5\u5e76\u7f16\u8f91 `./model/getchu/config.yml` \u914d\u7f6e\u6587\u4ef6\n5. \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\uff1a\n\n#### \u9ed8\u8ba4Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### \u5e26Edge\u7f16\u8f91\u7a97\u53e3\u7684Tool:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### \u547d\u4ee4\u884c\u53c2\u6570\u5e2e\u52a9\n```bash\npython tool_patch.py -h\n```\n\n> PS. \u4f60\u4e5f\u80fd\u7528tool\u8dd1\u522b\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5728\u8fd9\u91cc\u4e0b\u8f7d\u539f\u4f5c\u66f4\u591a\u6a21\u578b[Edge-Connect](https://github.com/knazeri/edge-connect).\n\u6587\u4ef6\u7ec4\u7ec7\u65b9\u5f0f\u53c2\u8003\u4e0a\u9762\uff0c\u5176\u4f59\u8fd0\u884c\u547d\u4ee4\u90fd\u4e00\u6837\u3002\u552f\u4e00\u6ce8\u610f\u7684\u662f\u8fd9\u4e2a\u9879\u76ee\u7684 `config.yml` \u6bd4\u539f\u4f5c\u7684\u591a\u4e86\u51e0\u4e2a\u9009\u9879\uff0c\u62a5\u9519\u4e86\u7684\u8bdd\u6ce8\u610f\u4fee\u6539\u3002\n\n## Tool\u64cd\u4f5c\u6307\u5357\n\u8be6\u7ec6\u5185\u5bb9\u8bf7\u7ffb\u770b\u63a7\u5236\u53f0\u7684\u6253\u5370\u5185\u5bb9\uff0c\u6216\u67e5\u770b`tool_patch.py`\u91cc\u7684`__doc__`      \n\u7b80\u7565\u7248tool\u4f7f\u7528\u6307\u5357\uff1a\n\n\u6309\u952e | \u8bf4\u660e\n-----|------\n\u9f20\u6807\u5de6\u952e | Input\u7a97\u53e3\uff1a\u753b\u51fa\u7455\u75b5\u533a\u57df\u7684\u906e\u76d6\uff0cEdge\u7a97\u53e3\uff1a\u624b\u52a8\u753b\u8fb9\u7f18\n\u9f20\u6807\u53f3\u952e | Edge\u7a97\u53e3\uff1a\u6a61\u76ae\u64e6\n\u6309\u952e `[` | \u7b14\u5237\u53d8\u7ec6 \uff08\u63a7\u5236\u53f0\u6253\u5370\u7c97\u7ec6\u5927\u5c0f\uff09\n\u6309\u952e `]` | \u7b14\u5237\u53d8\u7c97\n\u6309\u952e `0` | Todo\n\u6309\u952e `1` | Todo\n\u6309\u952e `n` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u53ea\u4f7f\u7528\u4e00\u5f20\u8f93\u5165\u56fe\u7247\n\u6309\u952e `e` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u4f7f\u7528\u8f93\u5165\u56fe\u7247\u548c\u8fb9\u7f18\u56fe\u7247\uff08\u4ec5\u5f53edge\u7a97\u53e3\u542f\u52a8\u65f6\u6709\u6548\uff09\n\u6309\u952e `r` | \u5168\u90e8\u91cd\u7f6e\n\u6309\u952e `s` | \u4fdd\u5b58\u8f93\u51fa\u56fe\u7247\n\u6309\u952e `q` | \u9000\u51fa\n\n\n## \u8bad\u7ec3\u6307\u5357\n\u8bad\u7ec3\u6307\u5357 --> [\u9605\u8bfb](training_manual.md#jump_zh)\n\n\n## License\nLicensed under a [Creative Commons Attribution-NonCommercial 4.0 International](https://creativecommons.org/licenses/by-nc/4.0/).\n\nExcept where otherwise noted, this content is published under a [CC BY-NC](https://creativecommons.org/licenses/by-nc/4.0/) license, which means that you can copy, remix, transform and build upon the content as long as you do not use the material for commercial purposes and give appropriate credit and provide a link to the license.\n\n\n## Citation\nIf you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/1901.00212\">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a>:\n\n```\n@inproceedings{nazeri2019edgeconnect,\n  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},\n  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},\n  journal={arXiv preprint},\n  year={2019},\n}\n```\n\n"
 },
 {
  "repo": "JimmyHHua/opencv_tutorials",
  "language": "Python",
  "readme_contents": "# OpenCV 4.0 Tutorial\n[![](https://img.shields.io/badge/opencv-v4.0.0-orange.svg)](https://opencv.org/)       [![](https://img.shields.io/badge/opencv-tutorial-brightgreen.svg)](https://docs.opencv.org/4.0.0/d9/df8/tutorial_root.html)\n\n\u2712\ufe0f [\u4e2d\u6587\u7248\u672c](./README_CN.md)\n## Introduction\n\nThis repository contains source code of OpenCV Tutorial application, the environment is python3.0 and opencv4.0.\n\n## Sample\n- **Image load**\n```python\nimport cv2\n\nsrc = cv2.imread(\"test.png\")\ncv2.namedWindow(\"input\", cv2.WINDOW_AUTOSIZE)\ncv2.imshow(\"input\", src)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n<div align=center><img src=\"https://i.loli.net/2019/05/22/5ce4b40258c9155103.jpg\" width=200></div>\n\n- **Gray Image**\n```python\ngray = cv2.cvtColor(src, cv.COLOR_BGR2GRAY)\n```\n\n<div align=center><img src=https://i.loli.net/2019/05/22/5ce4b2ae1e7ce86434.png width=120>       <img src=https://i.loli.net/2019/05/22/5ce4b2ae220a248459.png width=120></div>\n\n\n\n***More opencv4.0 tutorials plese follow the learning road as below*** \ud83d\udc47\ud83d\udc47\ud83d\udc47\n\n## Learning Road \u26f3\ufe0f\n\n***Annotation:***\n- \u2714\ufe0f  **: Basic**\n- \u270f\ufe0f  **: Attention**\n- \u2763\ufe0f  **: Important**\n\nNo    | Description   | Annotation\n:--------: | :--------: | :--------:\ncode_001 | [Load Image](python/code_001/opencv_001.py)   | \u2714\ufe0f\ncode_002 | [Gray Image](python/code_002/opencv_002.py)   | \u2714\ufe0f\ncode_003 | [Image Create](python/code_003/opencv_003.py)   | \u2714\ufe0f\ncode_004 | [Pixel Read and Write](python/code_004/opencv_004.py)   | \u2714\ufe0f\ncode_005 | [Image Pixel Arithmetic Operations](python/code_005/opencv_005.py)   | \u2714\ufe0f\ncode_006 | [Image Pseudo-Color Enhancement](python/code_006/opencv_006.py)   | \u2714\ufe0f\ncode_007 | [Image Pixel Operation (Logical Operation)](python/code_007/opencv_007.py)   | \u2714\ufe0f\ncode_008 | [Image Channel Separation and Merging](python/code_008/opencv_008.py)   | \u2714\ufe0f\ncode_009 | [Color Space Conversion](python/code_009/opencv_009.py)   | \u270f\ufe0f\ncode_010 | [Image Pixel Value Statistics](python/code_010/opencv_010.py)   | \u2714\ufe0f\ncode_011 | [Image Pixel Normalization](python/code_011/opencv_011.py)   | \u2714\ufe0f\ncode_012 | [Video Read and Write](python/code_012/opencv_012.py)   | \u2714\ufe0f\ncode_013 | [Image Flip](python/code_013/opencv_013.py)   | \u2714\ufe0f\ncode_014 | [Image Interpolation](python/code_014/opencv_014.py)   | \u2714\ufe0f\ncode_015 | [Draw Geometry](python/code_015/opencv_015.py)   | \u2714\ufe0f\ncode_016 | [ROI of Image](python/code_016/opencv_016.py)   | \u2714\ufe0f\ncode_017 | [Image Histogram](python/code_017/opencv_017.py)   | \u2714\ufe0f\ncode_018 | [Histogram Dqualization](python/code_018/opencv_018.py)   | \u270f\ufe0f\ncode_019 | [Histogram Comparison](python/code_019/opencv_019.py)   | \u2714\ufe0f\ncode_020 | [Histogram Backprojection](python/code_020/opencv_020.py)   | \u2714\ufe0f\ncode_021 | [Image Convolution](python/code_021/opencv_021.py)   | \u2714\ufe0f\ncode_022 | [Averaging and Gaussian Blur](python/code_022/opencv_022.py)   | \u2763\ufe0f\ncode_023 | [Median Blur](python/code_023/opencv_023.py)   | \u2714\ufe0f\ncode_024 | [Image Noise](python/code_024/opencv_024.py)   | \u2714\ufe0f\ncode_025 | [Smoothing Images](python/code_025/opencv_025.py)   | \u2714\ufe0f\ncode_026 | [Gaussian Bilateral Blur](python/code_026/opencv_026.py)   | \u2714\ufe0f\ncode_027 | [Mean-shift Blur)](python/code_027/opencv_027.py)   | \u2714\ufe0f\ncode_028 | [Image Integral Algorithm](python/code_028/opencv_028.py)   | \u2714\ufe0f\ncode_029 | [Fast Image Edge Filtering Algorithm](python/code_029/opencv_029.py)   | \u2714\ufe0f\ncode_030 | [Custom Filter](python/code_030/opencv_030.py)   | \u2714\ufe0f\ncode_031 | [Sobel Operator](python/code_031/opencv_031.py)   | \u2714\ufe0f\ncode_032 | [More Gradient Operators](python/code_032/opencv_032.py)   | \u2714\ufe0f\ncode_033 | [Laplace Operator](python/code_033/opencv_033.py)   | \u2714\ufe0f\ncode_034 | [Image Sharpening ](python/code_034/opencv_034.py)   | \u2714\ufe0f\ncode_035 | [USM Sharpen Algorithm](python/code_035/opencv_035.py)   | \u2714\ufe0f\ncode_036 | [Canny Edge Detection](python/code_036/opencv_036.py)   | \u2763\ufe0f\ncode_037 | [Image Pyramid](python/code_037/opencv_037.py)   | \u2714\ufe0f\ncode_038 | [Laplace Pyramid](python/code_038/opencv_038.py)   | \u2714\ufe0f\ncode_039 | [Image Template Matching](python/code_039/opencv_039.py)   | \u2714\ufe0f\ncode_040 | [Binary introduction](python/code_040/opencv_040.py)   | \u2714\ufe0f\ncode_041 | [Basic Thresholding](python/code_041/opencv_041.py)   | \u2714\ufe0f\ncode_042 | [OTSU Thresholding](python/code_042/opencv_042.py)   | \u270f\ufe0f\ncode_043 | [TRIANGLE Thresholding](python/code_043/opencv_043.py)   | \u2714\ufe0f\ncode_044 | [Adaptive Thresholding](python/code_044/opencv_044.py)   | \u270f\ufe0f\ncode_045 | [Binary and Smoothing](python/code_045/opencv_045.py)   | \u270f\ufe0f\ncode_046 | [Image Connectivity component](python/code_046/opencv_046.py)   | \u2714\ufe0f\ncode_047 | [Image Connected component state statistics](python/code_047/opencv_047.py)   | \u2714\ufe0f\ncode_048 | [Image Contours](python/code_048/opencv_048.py)   | \u2763\ufe0f\ncode_049 | [Bounding Rectangle](python/code_049/opencv_049.py)   | \u2763\ufe0f\ncode_050 | [Contour Area and Perimeter](python/code_050/opencv_050.py)   | \u270f\ufe0f\ncode_051 | [Contour Approximation](python/code_051/opencv_051.py)   | \u2714\ufe0f\ncode_052 | [Contour Centroid Calculate](python/code_052/opencv_052.py)   | \u2714\ufe0f\ncode_053 | [HuMoment for Contour Matching](python/code_053/opencv_053.py)   | \u2714\ufe0f\ncode_054 | [Contour Cricle and Ellipse fitting](python/code_054/opencv_054.py)   | \u2714\ufe0f\ncode_055 | [Convex Hull](python/code_055/opencv_055.py)   | \u270f\ufe0f\ncode_056 | [Fitting a Line](python/code_056/opencv_056.py)   | \u2714\ufe0f\ncode_057 | [Point Polygon Test](python/code_057/opencv_057.py)   | \u2714\ufe0f\ncode_058 | [The Largest Inner Circle](python/code_058/opencv_058.py)   | \u2714\ufe0f\ncode_059 | [Hoffman Line Detection](python/code_059/opencv_059.py)   | \u2714\ufe0f\ncode_060 | [Probability Hoffman Line Detection](python/code_060/opencv_060.py)   | \u2763\ufe0f\ncode_061 | [Hoffman Cricle Detection](python/code_061/opencv_061.py)   | \u2763\ufe0f\ncode_062 | [Dilation and Erosion](python/code_062/opencv_062.py)   | \u2763\ufe0f\ncode_063 | [Structuring Element](python/code_063/opencv_063.py)   | \u2714\ufe0f\ncode_064 | [Opening Transformation](python/code_064/opencv_064.py)   | \u270f\ufe0f\ncode_065 | [Closing Transformation](python/code_065/opencv_065.py)   | \u270f\ufe0f\ncode_066 | [Application of Opening and Closing Operations](python/code_066/opencv_066.py)   | \u270f\ufe0f\ncode_067 | [Top Hat](python/code_067/opencv_067.py)   | \u2714\ufe0f\ncode_068 | [Black Hat](python/code_068/opencv_068.py)   | \u2714\ufe0f\ncode_069 | [Morph Gradient](python/code_069/opencv_069.py)   | \u2714\ufe0f\ncode_070 | [Contour based on Morph Gradient](python/code_070/opencv_070.py)   | \u270f\ufe0f\ncode_071 | [Hit and Miss](python/code_071/opencv_071.py)   | \u2714\ufe0f\ncode_072 | [Defect Detecting-1](python/code_072)   | \u2714\ufe0f\ncode_073 | [Defect Detecting-2](python/code_073/opencv_073.py)   | \u2714\ufe0f\ncode_074 | [Extract the Maximum Contour and Coding Key Points](python/code_074)   | \u2714\ufe0f\ncode_075 | [Image Inpainting](python/code_075/opencv_075.py)   | \u2714\ufe0f\ncode_076 | [Perspective Transformation](python/code_076/opencv_076.py)   | \u270f\ufe0f\ncode_077 | [Video Read, Write and Process](python/code_077/opencv_077.py)   | \u270f\ufe0f\ncode_078 | [Identify and Track Specific Color Objects in Video](python/code_078)   | \u2714\ufe0f\ncode_079 | [Video Analysis-Background/Foreground Extraction](python/code_079/opencv_079.py)   | \u2714\ufe0f\ncode_080 | [Video Analysis\u2013Background Subtraction and ROI Extraction of the Foreground](python/code_080)   | \u2714\ufe0f\ncode_081 | [Corner Detection-Harris](python/code_081)   | \u2714\ufe0f\ncode_082 | [Corner Detection-Shi-Tomas](python/code_082)   | \u270f\ufe0f\ncode_083 | [Corner Detection-Sub-Pixel ](python/code_083)   | \u2714\ufe0f\ncode_084 | [Video Analysis-KLT Optical Flow-1](python/code_084)   | \u270f\ufe0f\ncode_085 | [Video Analysis-KLT Optical Flow-2](python/code_085)   | \u270f\ufe0f\ncode_086 | [Video Analysis-Dense Optical Flow](python/code_086)   | \u270f\ufe0f\ncode_087 | [Video Analysis-Frame Difference Moving Object Analysis](python/code_087/opencv_087.py)   | \u2714\ufe0f\ncode_088 | [Video Analysis-Meanshift](python/code_088)   | \u270f\ufe0f\ncode_089 | [Video Analysis-CamShift](python/code_089)   | \u270f\ufe0f\ncode_090 | [Video Analysis-Object Movement Trajectory Drawing](python/code_090)   | \u2714\ufe0f\ncode_091 | [Object Detection-HAAR Cascade Classification ](python/code_091)   | \u2714\ufe0f\ncode_092 | [Object Detection-HAAR Feature Analysis](python/code_092)   | \u2714\ufe0f\ncode_093 | [Object Detection-LBP Feature Analysis](python/code_093/opencv_093.py)   | \u2714\ufe0f\ncode_094 | [ORB Feature Critical Point Detection](python/code_094)   | \u270f\ufe0f\ncode_095 | [ORB Feature Descriptor Matching](python/code_095)   | \u2714\ufe0f\ncode_096 | [Multiple  Descriptor Matching Methods](python/code_096)   | \u270f\ufe0f\ncode_097 | [Location of Known Objects Based on Descriptor Matches](python/code_097)   | \u270f\ufe0f\ncode_098 | [SIFT Feature Critical Point Detection](python/code_097)   | \u2714\ufe0f\ncode_099 | [SIFT Feature Descriptor Matching](python/code_097)   | \u2714\ufe0f\ncode_100 | [HOG Pedestrian Detection](python/code_100/opencv_100.py)   | \u2714\ufe0f\ncode_101 | [HOG Multiscale Detection](python/code_101/opencv_101.py)   | \u270f\ufe0f\ncode_102 | [HOG Extract Descriptor](python/code_102/opencv_102.py)   | \u2714\ufe0f\ncode_103 | [HOG Use Descriptors to Generate Sample Data](python/code_103/opencv_103.py)   | \u2714\ufe0f\ncode_104 | [(Detection Case)-HOG+SVM Train](python/code_104/opencv_104.py)   | \u2714\ufe0f\ncode_105 | [(Detection Case)-HOG+SVM Predict](python/code_105/opencv_105.py)   | \u2714\ufe0f\ncode_106 | [AKAZE Features and Descriptors](python/code_106)   | \u2714\ufe0f\ncode_107 | [Brisk Features and Descriptors](python/code_107)   | \u2714\ufe0f\ncode_108 | [GFTT Detector](python/code_108)   | \u2714\ufe0f\ncode_109 | [BLOB Feature Analysis](python/code_109)   | \u2714\ufe0f\ncode_110 | [KMeans Data Classification](python/code_110)   | \u2714\ufe0f\ncode_111 | [KMeans Image Segmentation](python/code_111)   | \u2714\ufe0f\ncode_112 | [KMeans Background Change](python/code_112)   | \u2714\ufe0f\ncode_113 | [KMeans Extract Image Color Card](python/code_113)   | \u2714\ufe0f\ncode_114 | [KNN Classification](python/code_114)   | \u2714\ufe0f\ncode_115 | [KNN-Train Data Save and Load](python/code_115)   | \u2714\ufe0f\ncode_116 | [Decision Tree Algorithm](python/code_116)   | \u2714\ufe0f\ncode_117 | [Image Mean-shift Segmentation](python/code_117)   | \u2714\ufe0f\ncode_118 | [Grabcut-Image Segmentation](python/code_118)   | \u2714\ufe0f\ncode_119 | [Grabcut-Background Change](python/code_119)   | \u270f\ufe0f\ncode_120 | [Qrcode detect and decode](python/code_120)   | \u270f\ufe0f\ncode_121 | [DNN- Read the information of each layer of the model](python/code_121)   | \u2714\ufe0f\ncode_122 | [DNN- Realize image classification](python/code_122)   | \u2714\ufe0f\ncode_123 | [DNN- Model runs to set the target device and compute the background](python/code_123)   | \u2714\ufe0f\ncode_124 | [DNN- SSD Single Image Detection](python/code_124)   | \u2714\ufe0f\ncode_125 | [DNN- SSD Real-time Video Detection](python/code_125)   | \u2714\ufe0f\ncode_126 | [DNN- Face Detection based on Residual Network](python/code_126)   | \u2714\ufe0f\ncode_127 | [DNN- Video Face Detection based on Residual Network](python/code_127)   | \u2714\ufe0f\ncode_128 | [DNN- Call the Detection Model of Tensorflow](python/code_128)   | \u2714\ufe0f\ncode_129 | [DNN- Call the Openpose Implementation Attitude Assessment](python/code_129)   | \u2714\ufe0f\ncode_130 | [DNN- Call YOLO Object Detection Network](python/code_130)   | \u2714\ufe0f"
 },
 {
  "repo": "alyssaq/face_morpher",
  "language": "Python",
  "readme_contents": "Face Morpher\n============\n\n| Warp, average and morph human faces!\n| Scripts will automatically detect frontal faces and skip images if\n  none is detected.\n\nBuilt with Python, `dlib`_, Numpy, Scipy, dlib.\n\n| Supported on Python 2.7, Python 3.6+\n| Tested on macOS Mojave and 64bit Linux (dockerized).\n\nRequirements\n--------------\n-  ``pip install -r requirements.txt``\n- Download `http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2` and extract file.\n- Export environment variable ``DLIB_DATA_DIR`` to the folder where ``shape_predictor_68_face_landmarks.dat`` is located. Default ``data``. E.g ``export DLIB_DATA_DIR=/Downloads/data``\n\nEither:\n\n-  `Use as local command-line utility`_\n-  `Use as pip library`_\n-  `Try out in a docker container`_\n\n.. _`Use as local command-line utility`:\n\nUse as local command-line utility\n---------------------------------\n::\n\n    $ git clone https://github.com/alyssaq/face_morpher\n\nMorphing Faces\n--------------\n\nMorph from a source to destination image:\n\n::\n\n    python facemorpher/morpher.py --src=<src_imgpath> --dest=<dest_imgpath> --plot\n\nMorph through a series of images in a folder:\n\n::\n\n    python facemorpher/morpher.py --images=<folder> --out_video=out.avi\n\nAll options listed in ``morpher.py`` (pasted below):\n\n::\n\n    Morph from source to destination face or\n    Morph through all images in a folder\n\n    Usage:\n        morpher.py (--src=<src_path> --dest=<dest_path> | --images=<folder>)\n                [--width=<width>] [--height=<height>]\n                [--num=<num_frames>] [--fps=<frames_per_second>]\n                [--out_frames=<folder>] [--out_video=<filename>]\n                [--plot] [--background=(black|transparent|average)]\n\n    Options:\n        -h, --help              Show this screen.\n        --src=<src_imgpath>     Filepath to source image (.jpg, .jpeg, .png)\n        --dest=<dest_imgpath>   Filepath to destination image (.jpg, .jpeg, .png)\n        --images=<folder>       Folderpath to images\n        --width=<width>         Custom width of the images/video [default: 500]\n        --height=<height>       Custom height of the images/video [default: 600]\n        --num=<num_frames>      Number of morph frames [default: 20]\n        --fps=<fps>             Number frames per second for the video [default: 10]\n        --out_frames=<folder>   Folder path to save all image frames\n        --out_video=<filename>  Filename to save a video\n        --plot                  Flag to plot images to result.png [default: False]\n        --background=<bg>       Background of images to be one of (black|transparent|average) [default: black]\n        --version               Show version.\n\nAveraging Faces\n---------------\n\nAverage faces from all images in a folder:\n\n::\n\n    python facemorpher/averager.py --images=<images_folder> --out=average.png\n\nAll options listed in ``averager.py`` (pasted below):\n\n::\n\n    Face averager\n\n    Usage:\n        averager.py --images=<images_folder> [--blur] [--plot]\n                [--background=(black|transparent|average)]\n                [--width=<width>] [--height=<height>]\n                [--out=<filename>] [--destimg=<filename>]\n\n    Options:\n        -h, --help             Show this screen.\n        --images=<folder>      Folder to images (.jpg, .jpeg, .png)\n        --blur                 Flag to blur edges of image [default: False]\n        --width=<width>        Custom width of the images/video [default: 500]\n        --height=<height>      Custom height of the images/video [default: 600]\n        --out=<filename>       Filename to save the average face [default: result.png]\n        --destimg=<filename>   Destination face image to overlay average face\n        --plot                 Flag to display the average face [default: False]\n        --background=<bg>      Background of image to be one of (black|transparent|average) [default: black]\n        --version              Show version.\n\nSteps (facemorpher folder)\n--------------------------\n\n1. Locator\n^^^^^^^^^^\n\n-  Locates face points\n-  For a different locator, return an array of (x, y) control face\n   points\n\n2. Aligner\n^^^^^^^^^^\n\n-  Align faces by resizing, centering and cropping to given size\n\n3. Warper\n^^^^^^^^^\n\n-  Given 2 images and its face points, warp one image to the other\n-  Triangulates face points\n-  Affine transforms each triangle with bilinear interpolation\n\n4a. Morpher\n^^^^^^^^^^^\n\n-  Morph between 2 or more images\n\n4b. Averager\n^^^^^^^^^^^^\n\n-  Average faces from 2 or more images\n\nBlender\n^^^^^^^\n\nOptional blending of warped image:\n\n-  Weighted average\n-  Alpha feathering\n-  Poisson blend\n\nExamples - `Being John Malkovich`_\n----------------------------------\n\nCreate a morphing video between the 2 images:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_video=out.avi``\n\n(out.avi played and recorded as gif)\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/being_john_malvokich.gif\n   :alt: gif\n\nSave the frames to a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_frames=out_folder --num=30``\n\nPlot the frames:\n^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--num=12 --plot``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/plot.png\n   :alt: plot\n\nAverage all face images in a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n85 images used\n\n| ``> python facemorpher/averager.py --images=images --blur --background=transparent``\n| ``--width=220 --height=250``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/average_faces.png\n   :alt: average\\_faces\n\n.. _`Use as pip library`:\n\nUse as pip library\n---------------------------------\n::\n\n    $ pip install facemorpher\n\nExamples\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAdditional options are exactly the same as the command line\n\n::\n\n    import facemorpher\n\n    # Get a list of image paths in a folder\n    imgpaths = facemorpher.list_imgpaths('imagefolder')\n\n    # To morph, supply an array of face images:\n    facemorpher.morpher(imgpaths, plot=True)\n\n    # To average, supply an array of face images:\n    facemorpher.averager(['image1.png', 'image2.png'], plot=True)\n\n\nOnce pip installed, 2 binaries are also available as a command line utility:\n\n::\n\n    $ facemorpher --src=<src_imgpath> --dest=<dest_imgpath> --plot\n    $ faceaverager --images=<images_folder> --plot\n\nTry out in a docker container\n---------------------------------\nMount local folder to `/images` in docker container, run it and enter a bash session.\n--rm removes the container when you close it.\n::\n\n    $ docker run -v  /Users/alyssa/Desktop/images:/images --name py3 --rm -it jjanzic/docker-python3-opencv bash\n\nOnce you're in the container, install ``facemorpher`` and try the examples listed above\n::\n\n    root@0dad0912ebbe:/# pip install facemorpher\n    root@0dad0912ebbe:/# facemorpher --src=<img1> --dest=<img2> --plot\n\nDocumentation\n-------------\n\nhttp://alyssaq.github.io/face_morpher\n\nBuild & publish Docs\n^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    ./scripts/publish_ghpages.sh\n\nLicense\n-------\n`MIT`_\n\n.. _Being John Malkovich: http://www.rottentomatoes.com/m/being_john_malkovich\n.. _Mac installation steps: https://gist.github.com/alyssaq/f60393545173379e0f3f#file-4-opencv3-with-python3-md\n.. _MIT: http://alyssaq.github.io/mit-license\n.. _OpenCV: http://opencv.org\n.. _Homebrew: https://brew.sh\n.. _source: https://github.com/opencv/opencv\n.. _dlib: http://dlib.net\n"
 },
 {
  "repo": "vipul-sharma20/document-scanner",
  "language": "Python",
  "readme_contents": "document-scanner\n================\n\nA document scanner built using OpenCV + Python.\nI highly recommend to see my blog post for better understanding: [http://vipulsharma20.blogspot.on](http://vipulsharma20.blogspot.in)\n\nMy sincere thanks to the article and the author here: [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/) which has some really good set of articles on OpenCV and way more informative.\n\n* Original Image\n![Alt](http://1.bp.blogspot.com/-gbFBHQKFU7w/VpGzzVfxmLI/AAAAAAAAEks/jtX2fikHs5o/s1600/Original.jpg \"original\")\n\n* Grayscaled Image\n![Alt](http://2.bp.blogspot.com/--LzOU44-dhM/VpG2B6DJbxI/AAAAAAAAEk4/4BGXnfhvsk4/s1600/Original%2BGray.jpg \"gray\")\n\n* Gaussian Blur\n![Alt](http://2.bp.blogspot.com/-KfEWWzIXxBg/VpG2RY0upjI/AAAAAAAAElA/psuYvv1rnm0/s1600/Original%2BBlurred.jpg \"blurred\")\n\n* Edge Detection (Canny Edge Detection)\n![Alt](http://3.bp.blogspot.com/-5TVP2UFeGXk/VpG5-bIYNqI/AAAAAAAAElM/zmyNrbvnh8Q/s1600/Original%2BEdged.jpg \"edge\")\n\n* Contour Detection\n![Alt](http://1.bp.blogspot.com/-Es0PkMvJJxU/VpHcQEXzXaI/AAAAAAAAElc/NuCZmuW1K6o/s1600/Outline_all.jpg \"contour\")\n\n* Approximated Contour\n![Alt](http://4.bp.blogspot.com/-DL7XWsLvWg8/VpHeN6bA3gI/AAAAAAAAElo/1TMug5_tCeQ/s1600/Outline.jpg \"approx\")\n\n* Perspective Transform\n![Alt](http://4.bp.blogspot.com/-1dhSo9PrD6o/VpHjhgH0viI/AAAAAAAAEl4/AzYqjzLiNbI/s1600/dst.jpg \"transform\")\n\n"
 },
 {
  "repo": "opencv/opencv_extra",
  "language": null,
  "readme_contents": "### OpenCV: Open Source Computer Vision Library\n\nThis repository contains extra data for the OpenCV library.\n\n#### Resources\n* Homepage: http://opencv.org\n* Docs: http://docs.opencv.org\n* Q&A forum: http://answers.opencv.org\n* Issue tracking: https://github.com/opencv/opencv/issues\n\n#### Contributing\n\nPlease read before starting work on a pull request: https://github.com/opencv/opencv/wiki/How_to_contribute\n\nSummary of guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the coding style guide.\n"
 },
 {
  "repo": "mrnugget/opencv-haar-classifier-training",
  "language": "Perl",
  "readme_contents": "# Train your own OpenCV Haar classifier\n\n**Important**: This guide assumes you work with OpenCV 2.4.x. Since I no longer work with OpenCV, and don't have the time to keep up with changes and fixes, this guide is **unmaintained**. Pull requests will be merged of course, and if someone else wants commit access, feel free to ask!\n\nThis repository aims to provide tools and information on training your own\nOpenCV Haar classifier.  Use it in conjunction with this blog post: [Train your own OpenCV Haar\nclassifier](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n\n\n## Instructions\n\n1. Install OpenCV & get OpenCV source\n\n        brew tap homebrew/science\n        brew install --with-tbb opencv\n        wget http://downloads.sourceforge.net/project/opencvlibrary/opencv-unix/2.4.9/opencv-2.4.9.zip\n        unzip opencv-2.4.9.zip\n\n2. Clone this repository\n\n        git clone https://github.com/mrnugget/opencv-haar-classifier-training\n\n3. Put your positive images in the `./positive_images` folder and create a list\nof them:\n\n        find ./positive_images -iname \"*.jpg\" > positives.txt\n\n4. Put the negative images in the `./negative_images` folder and create a list of them:\n\n        find ./negative_images -iname \"*.jpg\" > negatives.txt\n\n5. Create positive samples with the `bin/createsamples.pl` script and save them\nto the `./samples` folder:\n\n        perl bin/createsamples.pl positives.txt negatives.txt samples 1500\\\n          \"opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\\\n          -maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40\"\n\n6. Use `tools/mergevec.py` to merge the samples in `./samples` into one file:\n\n        python ./tools/mergevec.py -v samples/ -o samples.vec\n\n   Note: If you get the error `struct.error: unpack requires a string argument of length 12`\n   then go into your **samples** directory and delete all files of length 0.\n\n7. Start training the classifier with `opencv_traincascade`, which comes with\nOpenCV, and save the results to `./classifier`:\n\n        opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024\n          \n    If you want to train it faster, configure feature type option with LBP:\n\n         opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024 -featureType LBP\n\n    After starting the training program it will print back its parameters and then start training. Each stage will print out some analysis as it is trained:\n\n      ```\n      ===== TRAINING 0-stage =====\n      <BEGIN\n      POS count : consumed   1000 : 1000\n      NEG count : acceptanceRatio    600 : 1\n      Precalculation time: 11\n      +----+---------+---------+\n      |  N |    HR   |    FA   |\n      +----+---------+---------+\n      |   1|        1|        1|\n      +----+---------+---------+\n      |   2|        1|        1|\n      +----+---------+---------+\n      |   3|        1|        1|\n      +----+---------+---------+\n      |   4|        1|        1|\n      +----+---------+---------+\n      |   5|        1|        1|\n      +----+---------+---------+\n      |   6|        1|        1|\n      +----+---------+---------+\n      |   7|        1| 0.711667|\n      +----+---------+---------+\n      |   8|        1|     0.54|\n      +----+---------+---------+\n      |   9|        1|    0.305|\n      +----+---------+---------+\n      END>\n      Training until now has taken 0 days 3 hours 19 minutes 16 seconds.\n      ```\n\n    Each row represents a feature that is being trained and contains some output about its HitRatio and FalseAlarm ratio. If a training stage only selects a few features (e.g. N = 2) then its possible something is wrong with your training data.\n\n    At the end of each stage the classifier is saved to a file and the process can be stopped and restarted. This is useful if you are tweaking a machine/settings to optimize training speed.\n\n8. Wait until the process is finished (which takes a long time \u2014 a couple of days probably, depending on the computer you have and how big your images are).\n\n9. Use your finished classifier!\n\n        cd ~/opencv-2.4.9/samples/c\n        chmod +x build_all.sh\n        ./build_all.sh\n        ./facedetect --cascade=\"~/finished_classifier.xml\"\n\n\n## Acknowledgements\n\nA huge thanks goes to Naotoshi Seo, who wrote the `mergevec.cpp` and\n`createsamples.cpp` tools and released them under the MIT licencse. His notes\non OpenCV Haar training were a huge help. Thank you, Naotoshi!\n\n## References & Links:\n\n- [Naotoshi Seo - Tutorial: OpenCV haartraining (Rapid Object Detection With A Cascade of Boosted Classifiers Based on Haar-like Features)](http://note.sonots.com/SciSoftware/haartraining.html)\n- [Material for Naotoshi Seo's tutorial](https://code.google.com/p/tutorial-haartraining/)\n- [OpenCV Documentation - Cascade Classifier Training](http://docs.opencv.org/doc/user_guide/ug_traincascade.html)\n"
 },
 {
  "repo": "RaftLib/RaftLib",
  "language": "C++",
  "readme_contents": "[RaftLib](http://raftlib.io) is a C++ Library for enabling stream/data-flow parallel computation. Using simple right shift operators (just like the C++ streams that you would use for string manipulation), you can link parallel compute kernels together. With RaftLib, we do away with explicit use of pthreads, std::thread, OpenMP, or any other parallel \"threading\" library. These are often mis-used, creating non-deterministic behavior. RaftLib's model allows lock-free FIFO-like access to the communications channels connecting each compute kernel. The full system has many auto-parallelization, optimization, and convenience features that enable relatively simple authoring of performant applications. Feel free to give it a shot, if you have any issues, please create an issue request. Minor issues, \nthe Slack group is the best way to resolve. We take pull requests!! For benchmarking, feel free to send the \nauthors an email. We've started a benchmark collection, however, it's far from complete. We'd love to add your \ncode!! \n\nUser Group / Mailing List: [slack channel](https://join.slack.com/t/raftlib/shared_invite/zt-3sk6ms6f-eEBd23dz98JnoRiXLaRmNw)\n\n=============\n\n### Build status\n\n![CI](https://github.com/RaftLib/RaftLib/workflows/CI/badge.svg?event=push)\n\n### Pre-requisites\n\n#### OS X & Linux\nCompiler: c++14 capable -> Clang, GNU GCC 5.0+, or Intel icc\n\n#### Windows\n* Latest merge from pull request to main should enable compilation on VS on Win10.\n\n### Install\nMake a build directory (for the instructions below, we'll \nwrite [build]). If you want to build the OpenCV example, then\nyou'll need to add to your cmake invocation:\n```bash\n-DBUILD_WOPENCV=true \n```\n\nTo use the [QThreads User space HPC threading library](http://www.cs.sandia.gov/qthreads/) \nyou will need to use the version with the RaftLib org and follow the RaftLib specific readme. \nThis QThreads version has patches for hwloc2.x applied and fixes for test cases. To compile\nRaftLib with QThreads linked, add the following (assumes the QThreads library is in your path):\n```bash\n-DUSEQTHREAD=1\n```\n\nBuilding the examples, benchmarks and tests can be disabled using:\n```bash\n-DBUILD_EXAMPLES=false\n-DBUILD_BENCHMARKS=false\n-DBUILD_TESTS=false\n```\n\nTo build:\n\n```bash\nmkdir [build]\ncd [build]\ncmake ..\nmake && make test\nsudo make install\n```\nNOTE: The default prefix in the makefile is: \n```\nPREFIX ?= /usr/local\n```\n\n## Using\n* When building applications with RaftLib, on Linux it is best to \nuse the **pkg-config** file, as an example, using the _poc.cpp_ example,\n```bash\ng++ `pkg-config --cflags raftlib` poc.cpp -o poc `pkg-config --libs raftlib`\n```\n\nFeel free to substitute your favorite build tool. I use Ninja and make depending on which machine I'm on. To change out, use cmake to generate the appropriate build files with the -Gxxx flag.\n\n### Citation\nIf you use this framework for something that gets published, please cite it as:\n```bibtex\n@article{blc16,\n  author = {Beard, Jonathan C and Li, Peng and Chamberlain, Roger D},\n  title = {RaftLib: A C++ Template Library for High Performance Stream Parallel Processing},\n  year = {2016},\n  doi = {http://dx.doi.org/10.1177/1094342016672542},\n  eprint = {http://hpc.sagepub.com/content/early/2016/10/18/1094342016672542.full.pdf+html},\n  journal = {International Journal of High Performance Computing Applications}\n}\n```\n### Other Info Sources\n* [OpenCV wrappers for RaftLib](https://github.com/RaftLib/RaftOCV)\n* [Project web page](http://raftlib.io)\n* [Project wiki page](https://github.com/jonathan-beard/RaftLib/wiki)\n* [Blog post intro](https://goo.gl/4VDlbr)\n* [Jonathan Beard's thesis](http://goo.gl/obkWUh)\n* [Views on parallel computing, general philosphy](https://goo.gl/R5fQAl)\n* Feel free to e-mail one of the authors of the repo\n"
 },
 {
  "repo": "bethesirius/ChosunTruck",
  "language": "Python",
  "readme_contents": "# <img src=\"https://github.com/bethesirius/ChosunTruck/blob/master/README/Logo.png\" width=\"64\">ChosunTruck\n\n## Introduction\nChosunTruck is an autonomous driving solution for [Euro Truck Simulator 2](https://eurotrucksimulator2.com/).\nRecently, autonomous driving technology has become a big issue and as a result we have been studying technology that incorporates this.\nIt is being developed in a simulated environment called Euro Truck Simulator 2 to allow us to study it using vehicles.\nWe chose Euro Truck Simulator 2 because this simulator provides a good test environment that is similar to the real road.\n\n## Features\n* You can drive a vehicle without handling it yourself.\n* You can understand the principles of autonomous driving.\n* (Experimental/Linux only) You can detect where other vehicles are.\n\n## How To Run It\n### Windows\n\n#### Dependencies\n- OS: Windows 7, 10 (64bit)\n\n- IDE: Visual Studio 2013, 2015\n\n- OpenCV version: >= 3.1\n\n- [Cuda Toolkit 7.5](https://developer.nvidia.com/cuda-75-downloads-archive) (Note: Do an ADVANCED INSTALLATION. ONLY install the Toolkit + Integration to Visual Studio. Do NOT install the drivers + other stuff it would normally give you. Once installed, your project properties should look like this: https://i.imgur.com/e7IRtjy.png)\n\n- If you have a problem during installation, look at our [Windows Installation wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Windows-Installation)\n\n#### Required to allow input to work in Windows:\n- **Go to C:\\Users\\YOURUSERNAME\\Documents\\Euro Truck Simulator 2\\profiles and edit controls.sii from** \n```\nconfig_lines[0]: \"device keyboard `di8.keyboard`\"\nconfig_lines[1]: \"device mouse `fusion.mouse`\"\n```\nto \n```\nconfig_lines[0]: \"device keyboard `sys.keyboard`\"\nconfig_lines[1]: \"device mouse `sys.mouse`\"\n```\n(thanks Komat!)\n- **While you are in controls.sii, make sure your sensitivity is set to:**\n```\n config_lines[33]: \"constant c_rsteersens 0.775000\"\n config_lines[34]: \"constant c_asteersens 4.650000\"\n```\n#### Then:\n- Set controls.sii to read-only\n- Open the visual studio project and build it. \n- Run ETS2 in windowed mode and set resolution to 1024 * 768.(It will work properly with 1920 * 1080 screen resolution and 1024 * 768 window mode ETS2.)\n\n### Linux\n#### Dependencies\n- OS: Ubuntu 16.04 LTS\n\n- [OpenCV version: >= 3.1](http://embedonix.com/articles/image-processing/installing-opencv-3-1-0-on-ubuntu/)\n\n- (Optional) Tensorflow version: >= 0.12.1\n\n### Build the source code with the following command (inside the linux directory).\n```\nmake\n```\n### If you want the car detection function then:\n````\nmake Drive\n````\n#### Then:\n- Run ETS2 in windowed mode and set its resolution to 1024 * 768. (It will work properly with 1920 * 1080 screen resolution and 1024 * 768 windowed mode ETS2)\n- It cannot find the ETS2 window automatically. Move the ETS2 window to the right-down corner to fix this.\n- In ETS2 Options, set controls to 'Keyboard + Mouse Steering', 'left click' to acclerate, and 'right click' to brake.\n- Go to a highway and set the truck's speed to 40~60km/h. (I recommend you turn on cruise mode to set the speed easily)\n- Run this program!\n\n#### To enable car detection mode, add -D or --Car_Detection.\n```\n./ChosunTruck [-D|--Car_Detection]\n```\n## Troubleshooting\nSee [Our wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Troubleshooting).\n\nIf you have some problems running this project, reference the demo video below. Or, [open a issue to contact our team](https://github.com/bethesirius/ChosunTruck/issues).\n\n## Demo Video\nLane Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/vF7J_uC045Q/0.jpg)](http://www.youtube.com/watch?v=vF7J_uC045Q)\n[![youtube link](http://img.youtube.com/vi/qb99czlIklA/0.jpg)](http://www.youtube.com/watch?v=qb99czlIklA)\n\nLane Detection + Vehicle Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/w6H2eGEvzvw/0.jpg)](http://www.youtube.com/watch?v=w6H2eGEvzvw)\n\n## Todo\n* For better detection performance, Change the Tensorbox to YOLO2.\n* The information from in-game screen have Restrictions. Read ETS2 process memory to collect more driving environment data.\n\n## Founders\n- Chiwan Song, chi3236@gmail.com\n\n- JaeCheol Sim, simjaecheol@naver.com\n\n- Seongjoon Chu, hs4393@gmail.com\n\n## Contributors\n- [zappybiby](https://github.com/zappybiby)\n\n## How To Contribute\nAnyone who is interested in this project is welcome! Just fork it and pull requests!\n\n## License\nChosunTruck, Euro Truck Simulator 2 auto driving solution\nCopyright (C) 2017 chi3236, bethesirius, uoyssim\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n"
 },
 {
  "repo": "uvipen/QuickDraw",
  "language": "Python",
  "readme_contents": "# [PYTHON] QuickDraw\n\n## Introduction\n\nHere is my python source code for QuickDraw - an online game developed by google. with my code, you could: \n* **Run an app which you could draw in front of a camera (If you use laptop, your webcam will be used by default)**\n* **Run an app which you could draw on a canvas**\n\n## Camera app\nIn order to use this app, you need a pen (or any object) with blue, red or green color. When the pen (object) appears in front of camera, it will be catched and highlighted by an yellow circle. When you are ready for drawing, you need to press **space** button. When you want to stop drawing, press **space** again\nBelow is the demo by running the sript **camera_app.py**:\n<p align=\"center\">\n  <img src=\"demo/quickdraw.gif\" width=600><br/>\n  <i>Camera app demo</i>\n</p>\n\n## Drawing app\nThe script and demo will be released soon\n\n## Dataset\nThe dataset used for training my model could be found at [Quick Draw dataset] https://console.cloud.google.com/storage/browser/quickdraw_dataset/sketchrnn. Here I only picked up 20 files for 20 categories\n\n## Categories:\nThe table below shows 20 categories my model used:\n\n|           |           |           |           |\n|-----------|:-----------:|:-----------:|:-----------:|\n|   apple   |   book    |   bowtie  |   candle  |\n|   cloud   |    cup    |   door    | envelope  |\n|eyeglasses |  guitar   |   hammer  |    hat    |\n| ice cream |   leaf    | scissors  |   star    |\n|  t-shirt  |   pants   | lightning |    tree   |\n\n## Trained models\n\nYou could find my trained model at **trained_models/whole_model_quickdraw**\n\n## Training\n\nYou need to download npz files corresponding to 20 classes my model used and store them in folder **data**. If you want to train your model with different list of categories, you only need to change the constant **CLASSES** at **src/config.py** and download necessary npz files. Then you could simply run **python3 train.py**\n\n## Experiments:\n\nFor each class, I take the first 10000 images, and then split them to training and test sets with ratio 8:2. The training/test loss/accuracy curves for the experiment are shown below:\n\n<img src=\"demo/loss_accuracy_curves.png\" width=\"800\"> \n\n## Requirements\n\n* **python 3.6**\n* **cv2**\n* **pytorch** \n* **numpy**\n"
 },
 {
  "repo": "mukyasa/MMCamScanner",
  "language": "Objective-C",
  "readme_contents": "# MMCamScanner\n#### Simulation to CamScanner app With Custom Camera and Crop Rect Validation \n\n\n------------------\n\n![MMCamScanner](https://github.com/mukyasa/MMCamScanner/blob/master/camscan.gif)\n\n#### Video preview [Here](https://www.youtube.com/watch?v=vO1kA6fjKQ4)\n \n#### ChangeLog(31/7/2015)\n * Crop Feature Tweaked for more control in horizontal and vertical dragging.\n * NOW , Landscape and Potrait both images can be used in scanning the objects in images.\n * Addded Image Filters (Gray Scale, Magic Color ,Black and White)\n * Left Rotate and Right Rotate\n \n\n**Framework**\nAdd the Frameworks to see the Demo\n\nOpenCV:http://opencv.org/<br />\nTesseract OCR:https://github.com/gali8/Tesseract-OCR-iOS\n\n**Thanks to Stackoverflow for solving queries related to OpenCV**<br />\n\n**Credits**<br />\n\nExcellent Square Detection Code Ref:[Here](http://stackoverflow.com/questions/8667818/opencv-c-obj-c-detecting-a-sheet-of-paper-square-detection) and [Here](https://github.com/Itseez/opencv/blob/master/samples/cpp/squares.cpp)<br />\n\nOpenCV Tutorials:[Here](http://opencv.org/\">http://opencv.org)<br />\n\nCustom Camera Project Ref:[Here](https://github.com/LoganWright/SimpleCam)<br />  (LoganWright)\n\nCrop Rect Validation Vector Ref:[Here](https://github.com/mysterioustrousers/MTGeometry) (Mysterious Trousers)\n\n\n**Also Thanks to this Project from where it is Inspired From**\n\nMaximilian Mackh :[Here](https://github.com/mmackh/MAImagePickerController-of-InstaPDF)\n\n\n**This Project Contains**\n\n1.Custom Camera(Support Potrait Only Now) with Zoom Slider,Pinch Zoom,Tap to Focus features also.<br />\n2.Custom Ripple View Controller Animation.<br />\n3.OpenCV square Detection in Image for detecting objects.<br />\n4.OpenCV perpespective and warptransform for CROP feature.<br />\n5.Crop Validation for checking INVALID RECT.<br />\n6.Memory Efficient Camera(Thanks to SimpleCam Project).<br />\n7.Tesseract OCR.<br />\n\n\n**My Other Repositories**\n\n**MMPaper:**<br />\nhttps://github.com/mukyasa/MMPaper<br />\n\n**MMTextFieldEffects:**<br />\nhttps://github.com/mukyasa/MMTextFieldEffects<br />\n\n**MMGooglePlayNewsStand:**<br />\nhttps://github.com/mukyasa/MMGooglePlayNewsStand\n\n**MMPaperPanFlip:**<br /> \nhttps://github.com/mukyasa/MMPaperPanFlip<br />\n\n**MMTransitionEffect:**<br />\nhttps://github.com/mukyasa/MMTransitionEffect<br />\n\n\nContact Me\n==========\nMukesh Mandora\n\nContact: mandoramuku07@gmail.com\n\nTwitter: http://twitter.com/mandymuku\n\nLinkedIn: https://in.linkedin.com/in/mukeshmandora\n\nGithub:https://github.com/mukyasa\n\n\n## License\nMMCamScanner is available under the Apache license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "bytefish/opencv",
  "language": "C++",
  "readme_contents": "# bytefish/opencv #\n\nThis repository contains OpenCV code and documents.\n\nMore (maybe) here: [https://www.bytefish.de](https://www.bytefish.de).\n\n## colormaps ##\n\nAn implementation of various colormaps for OpenCV2 C++ in order to enhance visualizations. Feel free to fork and add your own colormaps.\n\n### Related posts ###\n\n* https://bytefish.de/blog/colormaps_in_opencv\n  \n## misc ##\n\nSample code that doesn't belong to a specific project. \n\n* Skin Color detection\n* PCA\n* TanTriggs Preprocessing\n\n## machinelearning ##\n\nDocument and sourcecode about OpenCV C++ machine learning API including:\n\n* Support Vector Machines\n* Multi Layer Perceptron\n* Normal Bayes\n* k-Nearest-Neighbor\n* Decision Tree\n\n### Related posts ###\n  \n* https://www.bytefish.de/blog/machine_learning_opencv\n\n## eigenfaces ##\n\nEigenfaces implementation using the OpenCV2 C++ API. There's a very basic function for loading the dataset, you probably want to make this a bit more sophisticated. The dataset is available at [http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html).\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/pca_in_opencv\n* https://www.bytefish.de/blog/eigenfaces\n* https://www.bytefish.de/blog/fisherfaces\n  \n## lbp ##\n\nImplements various Local Binary Patterns with the OpenCV2 C++ API:\n  \n* Original LBP\n* Circular LBP (also known as Extended LBP)\n* Variance-based LBP\n\nBasic code for spatial histograms and histogram matching with a chi-square distance is included, but it's not finished right now. There's a tiny demo application you can experiment with.\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/local_binary_patterns\n* https://www.bytefish.de/blog/numpy_performance/\n  \n## lda ##\n\nFisherfaces implementation with the OpenCV2 C++ API. \n\n### Related posts ###\n\n* https://www.bytefish.de/blog/fisherfaces\n* https://www.bytefish.de/blog/lda_in_opencv\n* https://www.bytefish.de/blog/fisherfaces_in_opencv\n"
 },
 {
  "repo": "opencv/opencv_for_ios_book_samples",
  "language": "Objective-C",
  "readme_contents": "OpenCV for iOS (samples for the book)\n=====================================\n\n - Authors: Alexander Shishkov and Kirill Kornyakov\n - Book: <http://bit.ly/OpenCV_for_iOS_book>\n - Copyright: Packt Publishing 2013\n - License: see the `LICENSE.txt` file\n\nBuild & run sample projects\n---------------------------\n\nAlmost every chapter of the book describes a separate project. There are 15\nXcode projects for 17 chapters in the book. Chapter 16 extends the project of\nChapter 14, and for Chapter 17 you need only OpenCV source code.\n\n- First of all you will need a computer with Mac OSX and Xcode. An iOS device\n  is also helpful, since not all samples can be executed on Simulator.\n- Install OpenCV v2.4.6 or newer. You can go to <http://opencv.org>, click on\n  _Downloads_, and download the latest OpenCV framework for iOS. Copy it to the\n  folder with this code.\n- Now you can import sample projects to Xcode and run them on Simulator or a\n  real device.\n\nFor detailed instructions and explanations please refer to the book.\n\nChapters\n--------\n\n  1. _Getting Started with iOS_ helps you to setup your development environment\n     and run your first \"Hello World\" iOS application.\n  2. _Displaying Image from Resources_ introduces you to basic GUI concepts on\n     iOS, and covers loading of an image from resources and displaying it on the\n     display.\n  3. _Linking OpenCV to iOS Project_ explains how to link OpenCV library and\n     call any function from it.\n  4. _Detecting Faces with Cascade Classifier_ shows how to detect faces using\n     OpenCV.\n  5. _Printing Postcard_ demonstrates how a simple photo effect can be\n     implemented.\n  6. _Working with Images in Gallery_ explains how to load and save images\n     from/to Gallery.\n  7. _Applying Retro Effect_ demonstrates another interesting photo effect,\n     which makes photos look old.\n  8. _Taking Photos From Camera_ shows how to capture static images with camera.\n  9. _Creating Static Library_ explains how to create a Static Library project\n     in Xcode.\n  10. _Capturing Video from Camera_ shows how to capture video stream from\n      camera.\n  11. _Control Advanced Camera Settings_ explains how to control advanced camera\n      settings, like exposure, focus and white balance.\n  12. _Applying Effects to Live Video_ shows how to process captured video\n      frames on the fly.\n  13. _Saving Video from Camera_ explains how to save video stream to the device\n      with hardware encoding.\n  14. _Optimizing Performance with ARM NEON_ explains how to use SIMD\n      instructions to vectorize you code and improve performance.\n  15. _Detecting Facial Features_ presents a simple facial feature detection\n      demo.\n  16. _Using Accelerate Framework_ explains how to link the framework, and how\n      to use it for performance optimization.\n  17. _Building OpenCV for iOS from sources_ explains where to get and how to\n      build the latest OpenCV sources.\n\nScreenshots\n-----------\n\n### Chapter 5. \"Printing Postcard\"\n![](./Chapter05_PrintingPostcard/screenshot.png)\n\n### Chapter 6. \"Working with Images in Gallery\"\n![](./Chapter06_WorkingWithGallery/screenshot.png)\n\n### Chapter 7. \"Applying Retro Effect\"\n![](./Chapter07_ApplyingRetroEffect/screenshot.png)\n\n### Chapter 12. \"Applying Effects to Live Video\"\n![](./Chapter12_ProcessingVideo/screenshot.png)\n\n### Chapter 15. \"Detecting Facial Features\"\n![](./Chapter15_DetectingFacialFeatures/screenshot.png)"
 },
 {
  "repo": "DingProg/Makeup",
  "language": "Java",
  "readme_contents": "# \u9879\u76ee\u4ecb\u7ecd  \n\n\u672c\u9879\u76ee\u662f\u4e00\u4e2aAndroid Project\uff0c\u7528Canvas\u7ed9\u4eba\u8138\u5316\u5986(\u753b\u5986)\u7684APP\u6f14\u793a\u9879\u76ee  \n\n\u4e3b\u8981\u5185\u5bb9\u5305\u62ec\uff1a\n- \u5507\u5f69\uff0c\u7f8e\u77b3\uff0c\u7c89\u5e95\uff0c\u773c\u5f71\uff0c\u816e\u7ea2\uff0c\u773c\u7ebf\uff0c\u53cc\u773c\u76ae\uff0c\u7709\u6bdb\u7b49\uff0c\u80fd\u753b\u7684\u5986\uff0c\u90fd\u753b\u4e86\n- \u5229\u7528\u56fe\u5f62\u5c40\u90e8\u53d8\u5f62\u7b97\u6cd5\u8fdb\u884c \u5927\u773c\uff0c\u7626\u8138\uff0c\u4e30\u80f8\uff0c\u5927\u957f\u817f\u7b49\n- \u78e8\u5e73/\u7f8e\u767d\n\n# \u90e8\u5206\u6548\u679c\u5c55\u793a\n\u7f8e\u5986  \n![](https://github.com/DingProg/Makeup/blob/master/doc/3.png)\n![](https://github.com/DingProg/Makeup/blob/master/doc/5.png)      \n\u5927\u773c  \n![](https://github.com/DingProg/Makeup/blob/master/doc/1.png)  \n\u7626\u8138  \n![](https://github.com/DingProg/Makeup/blob/master/doc/2.png)  \n\u5927\u957f\u817f  \n![](https://github.com/DingProg/Makeup/blob/master/doc/4.png)   \n\n\n![](https://github.com/DingProg/Makeup/blob/master/doc/smallface.gif)\n\n\u66f4\u591a\u6f14\u793a\u6548\u679c\u8bf7\u76f4\u63a5\u67e5\u770b\u4e0b\u65b9\u539f\u7406\u6587\u7ae0\uff0c\u6216\u8005\u76f4\u63a5\u4e0b\u8f7d [\u6f14\u793aAPP Release V1.0.0\u7248\u672c](https://github.com/DingProg/Makeup/releases)   \n\n\u5982\u679c\u4f60\u8981\u770bOpenCV\u76f8\u5173\u7684(\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d)\uff0c\u53ef\u4ee5\u5207\u6362\u5230\u5206\u652f[with-photo-changecolor](https://github.com/DingProg/Makeup/tree/with-photo-changecolor)   \n\u76f8\u5173\u7684\u6f14\u793aAPP\u4e3a [\u5e26\u66ff\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d\u7248\u672c](https://github.com/DingProg/Makeup/releases)\n\n# \u6f14\u793aAPP \u4e3b\u8981\u5b9e\u73b0\u4e86\u7684\u90e8\u5206\u4e3a\n```java\npublic enum Region {\n\n    FOUNDATION(\"\u7c89\u5e95\"),\n    BLUSH(\"\u816e\u7ea2\"),\n    LIP(\"\u5507\u5f69\"),\n    BROW(\"\u7709\u6bdb\"),\n\n    EYE_LASH(\"\u776b\u6bdb\"),\n    EYE_CONTACT(\"\u7f8e\u77b3\"),\n    EYE_DOUBLE(\"\u53cc\u773c\u76ae\"),\n    EYE_LINE(\"\u773c\u7ebf\"),\n    EYE_SHADOW(\"\u773c\u5f71\");\n\n    private String name;\n    Region(String name) {\n        this.name = name;\n    }\n}\n\npublic enum BeautyType {\n\n    SMALLFACE(2,\"\u7626\u8138\"),\n    LONGLEG(3,\"\u5927\u957f\u817f\u589e\u9ad8\"),\n    EYE(4,\"\u773c\u775b\u653e\u5927\"),\n    BREST(5,\"\u4e30\u80f8\"),\n    WHITE(7,\"\u7f8e\u767d\"),\n    SMALLBODY(9,\"\u7626\u8138\u7626\u8eab\");\n\n    private int type;\n    private String name;\n\n    BeautyType(int type, String name) {\n        this.type = type;\n        this.name = name;\n    }\n}\n```\n\n# \u539f\u7406\n\n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986\uff08\u753b\u5986)](https://github.com/DingProg/Makeup/blob/master/doc/doc1.md)  \n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986 2\uff08\u5927\u773c\uff0c\u7626\u8138\uff0c\u5927\u957f\u817f\uff09](https://github.com/DingProg/Makeup/blob/master/doc/doc2.md)\n\n# \u58f0\u660e  \n\u672c\u9879\u76ee\u662f\u6f14\u793a\u6027\u53ca\u5b66\u4e60\u6027\u9879\u76ee\uff0c\u9879\u76ee\u4e2d\u6240\u7528\u7d20\u6750\u5bf9\u4e8e\u76f4\u63a5\u62ff\u53bb\u5546\u7528\u6240\u9020\u6210\u7684\u4fb5\u6743\uff0c\u6982\u4e0d\u8d1f\u8d23."
 },
 {
  "repo": "mbeyeler/opencv-machine-learning",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine Learning for OpenCV\n\n[![Google group](https://img.shields.io/badge/Google-Discussion%20group-lightgrey.svg)](https://groups.google.com/d/forum/machine-learning-for-opencv)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.833523.svg)](https://doi.org/10.5281/zenodo.833523)\n\nThis is the Jupyter notebook version of the following book:\n\n<img src=\"https://images-na.ssl-images-amazon.com/images/I/41CKBKW8y4L.jpg\" width=\"200\" align=\"left\" style=\"padding: 1px; border: 1px solid black; margin-right: 5px\"/> <br/>\nMichael Beyeler <br/>\n<a href=\"https://www.amazon.com/Machine-Learning-OpenCV-Michael-Beyeler/dp/1783980281\" target=\"_blank\"><b>Machine Learning for OpenCV</b></a> <br/>\nIntelligent Image Processing with Python\n<br/><br/>\n14 July 2017 <br/>\nPackt Publishing Ltd., London, England <br/>\nPaperback: 382 pages <br/>\nISBN 978-178398028-4\n<br clear=\"both\"/><br/>\n\nThe content is available on [GitHub](https://github.com/mbeyeler/opencv-machine-learning).\nThe code is released under the [MIT license](https://opensource.org/licenses/MIT).\n\nThe book is also available as a two-part video course:\n- [Part I: Supervised Learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-supervised-learning-video) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789347357/59952/59953/the-course-overview))\n- [Part II: Advanced methods and deep learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-%E2%80%93-advanced-methods-and-deep-learning-vide) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789340525/62127/62128/the-course-overview))\n\nFor questions, discussions, and more detailed help please refer to the [Google group](https://groups.google.com/d/forum/machine-learning-for-opencv).\n\nIf you use either book or code in a scholarly publication, please cite as:\n\n> M. Beyeler, (2017). Machine Learning for OpenCV. Packt Publishing Ltd., London, England, 380 pages, ISBN 978-178398028-4.\n\nOr use the following bibtex:\n\n```\n@book{MachineLearningOpenCV,\n\ttitle = {{Machine Learning for OpenCV}},\n\tsubtitle = {{Intelligent image processing with Python}},\n\tauthor = {Michael Beyeler},\n\tyear = {2017},\n\tpages = {380},\n\tpublisher = {Packt Publishing Ltd.},\n\tisbn = {978-178398028-4}\n}\n```\n\nScholarly work referencing this book:\n- S Lynch (2018). Image Processing with Python. *Dynamical Systems with Applications using Python*, Springer.\n- MQG Quiroz (2018). Inductive Machine Learning with Image Processing for Objects Detection of a Robotic Arm with Raspberry PI. *International Conference on Technology Trends*.\n- A Konate (2018). Un aper\u00e7u sur quelques m\u00e9thodes en apprentissage automatique supervis\u00e9. *HAL* 01946237.\n\n\n## Table of Contents\n\n[Preface](notebooks/00.00-Preface.ipynb)\n\n[Foreword by Ariel Rokem](notebooks/00.01-Foreword-by-Ariel-Rokem.ipynb)\n\n1. [A Taste of Machine Learning](notebooks/01.00-A-Taste-of-Machine-Learning.ipynb)\n\n2. [Working with Data in OpenCV](notebooks/02.00-Working-with-Data-in-OpenCV.ipynb)\n   - [Dealing with Data Using Python's NumPy Package](notebooks/02.01-Dealing-with-Data-Using-Python-NumPy.ipynb)\n   - [Loading External Datasets in Python](notebooks/02.02-Loading-External-Datasets-in-Python.ipynb)\n   - [Visualizing Data Using Matplotlib](notebooks/02.03-Visualizing-Data-Using-Matplotlib.ipynb)\n   - [Dealing with Data Using OpenCV's TrainData container](notebooks/02.05-Dealing-with-Data-Using-the-OpenCV-TrainData-Container-in-C%2B%2B.ipynb)\n\n3. [First Steps in Supervised Learning](notebooks/03.00-First-Steps-in-Supervised-Learning.ipynb)\n   - [Measuring Model Performance with Scoring Functions](notebooks/03.01-Measuring-Model-Performance-with-Scoring-Functions.ipynb)\n   - [Understanding the k-NN Algorithm](notebooks/03.02-Understanding-the-k-NN-Algorithm.ipynb)\n   - [Using Regression Models to Predict Continuous Outcomes](notebooks/03.03-Using-Regression-Models-to-Predict-Continuous-Outcomes.ipynb)\n   - [Applying Lasso and Ridge Regression](notebooks/03.04-Applying-Lasso-and-Ridge-Regression.ipynb)\n   - [Classifying Iris Species Using Logistic Regression](notebooks/03.05-Classifying-Iris-Species-Using-Logistic-Regression.ipynb)\n\n4. [Representing Data and Engineering Features](notebooks/04.00-Representing-Data-and-Engineering-Features.ipynb)\n   - [Preprocessing Data](notebooks/04.01-Preprocessing-Data.ipynb)\n   - [Reducing the Dimensionality of the Data](notebooks/04.02-Reducing-the-Dimensionality-of-the-Data.ipynb)\n   - [Representing Categorical Variables](notebooks/04.03-Representing-Categorical-Variables.ipynb)\n   - [Representing Text Features](notebooks/04.04-Represening-Text-Features.ipynb)\n   - [Representing Images](notebooks/04.05-Representing-Images.ipynb)\n\n5. [Using Decision Trees to Make a Medical Diagnosis](notebooks/05.00-Using-Decision-Trees-to-Make-a-Medical-Diagnosis.ipynb)\n   - [Building Our First Decision Tree](notebooks/05.01-Building-Our-First-Decision-Tree.ipynb)\n   - [Using Decision Trees to Diagnose Breast Cancer](notebooks/05.02-Using-Decision-Trees-to-Diagnose-Breast-Cancer.ipynb)\n   - [Using Decision Trees for Regression](notebooks/05.03-Using-Decision-Trees-for-Regression.ipynb)\n\n6. [Detecting Pedestrians with Support Vector Machines](notebooks/06.00-Detecting-Pedestrians-with-Support-Vector-Machines.ipynb)\n   - [Implementing Your First Support Vector Machine](notebooks/06.01-Implementing-Your-First-Support-Vector-Machine.ipynb)\n   - [Detecting Pedestrians in the Wild](notebooks/06.02-Detecting-Pedestrians-in-the-Wild.ipynb)\n   - [Additional SVM Exercises](notebooks/06.03-Additional-SVM-Exercises.ipynb)\n\n7. [Implementing a Spam Filter with Bayesian Learning](notebooks/07.00-Implementing-a-Spam-Filter-with-Bayesian-Learning.ipynb)\n   - [Implementing Our First Bayesian Classifier](notebooks/07.01-Implementing-Our-First-Bayesian-Classifier.ipynb)\n   - [Classifying E-Mails Using Naive Bayes](notebooks/07.02-Classifying-Emails-Using-Naive-Bayes.ipynb)\n\n8. [Discovering Hidden Structures with Unsupervised Learning](notebooks/08.00-Discovering-Hidden-Structures-with-Unsupervised-Learning.ipynb)\n   - [Understanding k-Means Clustering](notebooks/08.01-Understanding-k-Means-Clustering.ipynb)\n   - [Compressing Color Images Using k-Means](notebooks/08.02-Compressing-Color-Images-Using-k-Means.ipynb)\n   - [Classifying Handwritten Digits Using k-Means](notebooks/08.03-Classifying-Handwritten-Digits-Using-k-Means.ipynb)\n   - [Implementing Agglomerative Hierarchical Clustering](notebooks/08.04-Implementing-Agglomerative-Hierarchical-Clustering.ipynb)\n\n9. [Using Deep Learning to Classify Handwritten Digits](notebooks/09.00-Using-Deep-Learning-to-Classify-Handwritten-Digits.ipynb)\n   - [Understanding Perceptrons](notebooks/09.01-Understanding-Perceptrons.ipynb)\n   - [Implementing a Multi-Layer Perceptron in OpenCV](notebooks/09.02-Implementing-a-Multi-Layer-Perceptron-in-OpenCV.ipynb)\n   - [Getting Acquainted with Deep Learning](notebooks/09.03-Getting-Acquainted-with-Deep-Learning.ipynb)\n   - [Training an MLP in OpenCV to Classify Handwritten Digits](notebooks/09.04-Training-an-MLP-in-OpenCV-to-Classify-Handwritten-Digits.ipynb)\n   - [Training a Deep Neural Net to Classify Handwritten Digits Using Keras](notebooks/09.05-Training-a-Deep-Neural-Net-to-Classify-Handwritten-Digits-Using-Keras.ipynb)\n\n10. [Combining Different Algorithms Into an Ensemble](notebooks/10.00-Combining-Different-Algorithms-Into-an-Ensemble.ipynb)\n    - [Understanding Ensemble Methods](notebooks/10.01-Understanding-Ensemble-Methods.ipynb)\n    - [Combining Decision Trees Into a Random Forest](notebooks/10.02-Combining-Decision-Trees-Into-a-Random-Forest.ipynb)\n    - [Using Random Forests for Face Recognition](notebooks/10.03-Using-Random-Forests-for-Face-Recognition.ipynb)\n    - [Implementing AdaBoost](notebooks/10.04-Implementing-AdaBoost.ipynb)\n    - [Combining Different Models Into a Voting Classifier](notebooks/10.05-Combining-Different-Models-Into-a-Voting-Classifier.ipynb)\n\n11. [Selecting the Right Model with Hyper-Parameter Tuning](notebooks/11.00-Selecting-the-Right-Model-with-Hyper-Parameter-Tuning.ipynb)\n    - [Evaluating a Model](notebooks/11.01-Evaluating-a-Model.ipynb)\n    - [Understanding Cross-Validation, Bootstrapping, and McNemar's Test](notebooks/11.02-Understanding-Cross-Validation-Bootstrapping-and-McNemar's-Test.ipynb)\n    - [Tuning Hyperparameters with Grid Search](notebooks/11.03-Tuning-Hyperparameters-with-Grid-Search.ipynb)\n    - [Chaining Algorithms Together to Form a Pipeline](notebooks/11.04-Chaining-Algorithms-Together-to-Form-a-Pipeline.ipynb)\n\n12. [Wrapping Up](notebooks/12.00-Wrapping-Up.ipynb)\n\n\n\n## Running the Code\n\nThere are at least two ways you can run the code:\n- Using [Binder](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master) (no installation required).\n- Using Jupyter Notebook on your local machine.\n\nThe code in this book was tested with Python 3.5, although Python 3.6 and 2.7 should work as well. \n\n\n### Using Binder\n\n[Binder](http://www.mybinder.org) allows you to run Jupyter notebooks in an interactive Docker container.\nNo installation required!\n\nLaunch the project: [mbeyeler/opencv-machine-learning](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n\n\n\n### Using Jupyter Notebook\n\nYou basically want to follow the installation instructions in Chapter 1 of the book.\n\nIn short:\n\n1. Download and install [Python Anaconda](https://www.continuum.io/downloads).\n   On Unix, when asked if the Anaconda path should be added to your `PATH` variable, choose yes. Then either open a new terminal or run `$ source ~/.bashrc`.\n\n2. Fork and clone the GitHub repo:\n   - Click the\n     [`Fork`](https://github.com/mbeyeler/opencv-machine-learning#fork-destination-box)\n     button in the top-right corner of this page.\n   - Clone the repo, where `YourUsername` is your actual GitHub user name:\n\n   ```\n   $ git clone https://github.com/YourUsername/opencv-machine-learning\n   $ cd opencv-machine-learning\n   ```\n   \n   - Add the following to your remotes:\n   ```\n   $ git remote add upstream https://github.com/mbeyeler/opencv-machine-learning\n   ```\n   \n3. Add Conda-Forge to your trusted channels (to simplify installation of OpenCV on Windows platforms):\n\n   ```\n   $ conda config --add channels conda-forge\n   ```\n\n4. Create a conda environment for Python 3 with all required packages:\n\n   ```\n   $ conda create -n Python3 python=3.6 --file requirements.txt\n   ```\n\n5. Activate the conda environment.\n   On Linux / Mac OS X:\n\n   ```\n   $ source activate Python3\n   ```\n\n   On Windows:\n\n   ```\n   $ activate Python3\n   ```\n\n   You can learn more about conda environments in the\n   [Managing Environments](http://conda.pydata.org/docs/using/envs.html)\n   section of the conda documentation.\n\n6. Launch Jupyter notebook:\n\n   ```\n   $ jupyter notebook\n   ```\n\n   This will open up a browser window in your current directory.\n   Navigate to the folder `opencv-machine-learning`.\n   The README file has a table of contents.\n   Else navigate to the `notebooks` folder, click on the notebook of your choice,\n   and select `Kernel > Restart & Run All` from the top menu.\n   \n   \n## Getting the latest code\n\nIf you followed the instructions above and:\n- forked the repo,\n- cloned the repo,\n- added the `upstream` remote repository,\n\nthen you can always grab the latest changes by running a git pull:\n\n```\n$ cd opencv-machine-learning\n$ git pull upstream master\n```\n\n## Errata\n\nThe following errata have been reported that apply to the print version of the book. Some of these are typos, others are bugs in the code. Please note that all known bugs have been fixed in the code of this repository.\n- p.32: `Out[15]` should read '3' instead of 'int_arr[3]'.\n- p.32: `Out[22]` should read `array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])` instead of `array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])`.\n- p.33: In the sentence: \"Here, the first dimension defines the color channel...\", the order of color channels should read \"blue, green, and red in OpenCV\" instead of \"red, green, blue, green, and red\".\n- p.36: The range of x values should read \"0 <= x <= 10\" instead of \"0 <= x < 10\", since `np.linspace` by default includes the endpoint.\n- p.51: `In [15]` shoud read `precision = true_positive / (true_positive + false_positive)` instead of `precision = true_positive / (true_positive + true_negative)`.\n- p.51: `Out[15]` should read 0.2 instead of 1.0.\n- p.72: `In [6]` should read `ridgereg = linear_model.Ridge()` instead of `ridgereg = linear_model.RidgeRegression()`.\n- p.85: The first line of `In [8]` should read `min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-10,10))` instead of `min_max_scaler = preprocessing.MinMaxScaler(feature_range (-10,10))`.\n- p.91:  The last paragraph should read `We also specify an empty array, np.array([]), for the mean argument, which tells OpenCV to  compute the mean from the data:` instead of `We also specify an empty array, np.array([]), for the mask argument, which tells OpenCV to use all data points in the feature matrix:`.\n- p.112: `In [3]` should read `vec.get_feature_names()[:5]` instead of `function:vec.get_feature_names()[:5]`.\n- p.120: `In [16]` should read `dtree = cv2.ml.DTrees_create()` instead of `dtree = cv2.ml.dtree_create()`.\n- p.122: `In [26]` should read `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(dtc, out_file=f, feature_names=vec.get_feature_names(), class_names=['A', 'B', 'C', 'D'])` instead of `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(clf, out_file=f)`. Also, the second line should be indented.\n- p.147: The first occurrences of `X_hypo = np.c_[xx.ravel().astype(np.float32), yy.ravel().astype(np.float32)]` and `_, zz = svm.predict(X_hypo)` should be removed, as they mistakenly appear twice.\n- p.193: `In [28]` is missing `from sklearn import metrics`.\n- p.197: The sentence right below `In [3]` should read \"Then we can pass the preceding data matrix (`X`) to `cv2.kmeans`\", not `cv2.means`.\n- p.201: Indentation in bullet points 2-4 are wrong. Please refer to the Jupyter notebook for the correct indentation.\n- p.228: The last sentence in the middle paragraph should read \"[...] thus hopefully classifying the sample as y_{hat}=+1\" instead of \"[...] thus hopefully classifying the sample as y_{hat}=-1\".\n- p.230: `In [2]` has wrong indentation: `class Perceptron(object)` correctly has indentation level 1, but `def __init__` should have indentation level 2, and the two commands `self.lr = lr; self.n_iter = n_iter` should have indentation level 3.\n- p.260: `In [5]` should read `from keras.models import Sequential` instead of `from keras.model import Sequential`.\n- p.260: `In [6]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1]), padding='valid', input_shape=input_shape))` instead of `model.add(Convolution2D(n_filters, kernel_size[0], kernel_size[1], border_mode='valid', input_shape=input_shape))`.\n- p.260: `In [8]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1])))` instead of `model.add(Convolution2D(n_filters, (kernel_size[0], kernel_size[1])))`.\n- p.261: `In [12]` should read `model.fit(X_train, Y_train, batch_size=128, epochs=12, verbose=1, validation_data=(X_test, Y_test))` instead of `model.fit(X_train, Y_train, batch_size=128, nb_epoch=12, verbose=1, validation_data=(X_test, Y_test))`.\n- p.275, in bullet point 2 it should say `ret = classifier.predict(X_hypo)` instead of `zz = classifier.predict(X_hypo); zz = zz.reshape(xx.shape)`.\n- p.285: `plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')` should be indented so that it is aligned with the previous line.\n- p.288: `In [14]` should read `_, y_hat = rtree.predict(X_test)` instead of `_, y_hat = tree.predict(X_test)`.\n- p.305: The first paragraph should read \"...and the remaining folds (1, 2, and 4) for training\" instead of \"...and the remaining folds (1, 2, and 4) for testing\".\n- p.306: `In [2]` should read `from sklearn.model_selection import train_test_split` instead of `from sklearn.model_selection import model_selection`.\n- p.310: `In [18]` should read `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_train, cv2.ml.ROW_SAMPLE, y_boot)`.\n- p.311: `In [20]` should have a line `model.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)`, as well as `_, y_hat = model.predict(X_oob)` instead of `_, y_hat = knn.predict(X_oob)`.\n- p.328: `In [5]` is missing the statement `from sklearn.preprocessing import MinMaxScaler`.\n- p.328: `In [5]` should have a line `pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])` instead of `pipe = Pipeline([\"scaler\", MinMaxScaler(), (\"svm\", SVC())])`.\n\n\n## Acknowledgment\n\nThis book was inspired in many ways by the following authors and their corresponding publications:\n- Jake VanderPlas, Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly, ISBN 978-149191205-8, 2016, https://github.com/jakevdp/PythonDataScienceHandbook\n- Andreas Muller and Sarah Guido, Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly, ISBN\n978-144936941-5, 2016, https://github.com/amueller/introduction_to_ml_with_python\n- Sebastian Raschka, Python Machine Learning. Packt, ISBN 978-178355513-0, 2015, https://github.com/rasbt/python-machine-learning-book\n\nThese books all come with their own open-source code - check them out when you get a chance!\n"
 },
 {
  "repo": "nomacs/nomacs",
  "language": "C++",
  "readme_contents": "# nomacs - Image Lounge \ud83c\udf78\n\nnomacs is a free, open source image viewer, which supports multiple platforms. You can use it for viewing all common image formats including RAW and psd images. nomacs is licensed under the GNU General Public License v3 and available for Windows, Linux, FreeBSD, Mac, and OS/2.\n\n[![Build Status](https://travis-ci.org/nomacs/nomacs.svg?branch=master)](https://travis-ci.org/nomacs/nomacs)\n[![Build status](https://ci.appveyor.com/api/projects/status/0lw27jchw3ymaqd4?svg=true)](https://ci.appveyor.com/project/diemmarkus/nomacs)\n[![Downloads](https://img.shields.io/github/downloads/nomacs/nomacs/total.svg)](https://github.com/nomacs/nomacs/releases/latest)\n[![Crowdin](https://badges.crowdin.net/nomacs/localized.svg)](http://translate.nomacs.org/project/nomacs)\n\n## Build nomacs (Windows)\n\nWe assume you have an IDE (i.e. Visual Studio), python, git, and [Qt](https://www.qt.io/download-open-source) installed.  \n\nGet all dependencies:\n```bash\ngit submodule init\ngit submodule update\n```\nProject folders in ``3rd-party`` will not be empty anymore. Now call:\n```bash\npython scripts/make.py \"qtpath/bin\"\n```\n\nThis will build nomacs into `build/nomacs`. If you are using Visual Studio, you can then double-click `build/nomacs/nomacs.sln`. Right-click the nomacs project and choose `Set as StartUp Project`.\n\nBuild individual projects using:\n```bash\npython scripts/make.py \"qt/bin\" --project quazip,libraw --force\n```\n\n### Developer Build\nI like having a separate developer build (without submodules) that uses 3rd party libs already compiled. To do so you need to: \n```bash\ngit submodule update --init --remote scripts \n\n# python scripts/make.py \"C:\\Qt\\Qt-5.14.1-installer\\5.14.2\\msvc2017_64\\bin\" --lib-path C:\\coding\\nomacs\\nomacs\\3rd-party\\build\npython scripts/make.py \"qt/bin\" --lib-path \"nomacs/3rd-party/build\"\n```\n\n### If anything did not work\n\n- check if you have setup opencv (otherwise uncheck ENABLE_OPENCV)\n- check if your Qt is set correctly (otherwise set the path to `qt_install_dir/qtbase/bin/qmake.exe`)\n- check if your builds proceeded correctly\n\n## Build nomacs (Ubuntu)\n\nGet the required packages:\n\n``` console\nsudo apt-get install debhelper cdbs qt5-qmake qttools5-dev-tools qt5-default qttools5-dev libqt5svg5-dev qt5-image-formats-plugins libexiv2-dev libraw-dev libopencv-dev cmake libtiff-dev libquazip5-dev libwebp-dev git build-essential lcov libzip-dev\n```\n\nGet the nomacs sources from github:\n``` console\ngit clone https://github.com/nomacs/nomacs.git\n```\n\nThis will by default place the source into ~/nomacs\nGo to the nomacs/ImageLounge directory and run `cmake` to get the Makefiles:\n``` console\nmkdir build\ncd build\ncmake ../ImageLounge/.\n```\n\nCompile nomacs:\n``` console\nmake\n```\n\nYou will now have a binary (~/nomacs/build/nomacs), which you can test (or use directly). To install it to /usr/local/bin, use:\n``` console\nsudo make install\n```\n\nnote that you have to execute\n``` console\nsudo ldconfig\n```\nafter a successful install.\n\nInstall the [heif plugin](https://github.com/jakar/qt-heif-image-plugin) for HEIF support.\n\n### For Package Maintainers\n\n- Set `ENABLE_TRANSLATIONS` to `true` (default: `false`)\n- Build all officially supported [plugins](https://github.com/nomacs/nomacs-plugins/)\n\n## Build nomacs (MacOS)\n\nInstall [Homebrew](http://brew.sh/) for easier installation of dependencies.\nInstall required dependencies:\n\n``` console\n$ brew install qt5 exiv2 opencv libraw quazip cmake pkg-config\n```\n\nGo to the `nomacs` directory and run cmake to get the Makefiles:\n\n``` console\n$ mkdir build\n$ cd build\n$ Qt5_DIR=/usr/local/opt/qt5/ cmake -DQT_QMAKE_EXECUTABLE=/usr/local/opt/qt5/bin ../ImageLounge/.\n```\n\nRun make:\n\n```console\n$ make\n```\n\nYou will now have a binary (`nomacs.app`), which you can test (or use directly). To install it to `/usr/local/bin`, use\n\n```console\n$ sudo make install\n```\n\n## Build in Docker\nWe have created a docker image that best simulates the travis system (currently it's ubuntu xenial 16.04). To build nomacs in a docker, you have to create the image:\n````bash\ndocker build --rm -f \"Dockerfile\" -t nomacs:latest empty-docker-dir\n`````\nTo deploy nomacs in a docker on your system, you can mount this directory using:\n````bash\ndocker run --rm -it -v C:\\\\coding\\\\nomacs:/usr/nomacs nomacs:latest\n````\nIf needed, you can upload the image:\n````bash\ndocker login\ndocker tag nomacs diemmarkus/nomacs\ndocker push diemmarkus/nomacs:latest\n````\n\n## Links\n\n- [nomacs.org](https://nomacs.org)\n- [GitHub](https://github.com/nomacs)\n\n[![nomacs-icon](https://nomacs.org/startpage/nomacs.svg)](https://nomacs.org)\n"
 },
 {
  "repo": "nladuo/captcha-break",
  "language": "C++",
  "readme_contents": "# captcha-break\ncaptcha break based on opencv2, tesseract-ocr and some machine learning algorithm.\n\n## Types\n### Basic[[cpp](./basic/cpp)][[python](./basic/python)]\n![](./basic/basic.jpg)  \nThe simplest captcha breaking.\n\n### CSDN[[cpp](./csdn/cpp)][[python](./csdn/python)]\n![](./csdn/csdn.png)  \nCAPTCHA from http://download.csdn.net/\n\n### SubMail[[cpp](./submail/cpp)]\n![](./submail/submail.png)   \nCAPTCHA from http://submail.cn/sms\n\n### Weibo.cn[[cpp](./weibo.cn/cpp)][[python](./weibo.cn/python)]\n![](./weibo.cn/weibo.cn.png)  \nCAPTCHA from http://login.weibo.cn/login/.  \n(Note: This website has changed now, and the captcha is not available!)\n\n### JiKeXueYuan[[python](./jikexueyuan/python)]\n![](./jikexueyuan/jikexueyuan.png)   \nCAPTCHA of http://passport.jikexueyuan.com/sso/verify\n\n### Weibo.com[[python3](./weibo.com)]\n![](./weibo.com/weibo.com.png)  \nCAPTCHA of [http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0](http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0)\n\n\n## License\nMIT\n"
 },
 {
  "repo": "Mjrovai/OpenCV-Face-Recognition",
  "language": "Python",
  "readme_contents": "# OpenCV-Face-Recognition\nReal-time face recognition project with OpenCV and Python\n<br><br>\nLinks for complete Tutorial:\n<br>\nhttps://www.hackster.io/mjrobot/real-time-face-recognition-an-end-to-end-project-a10826\nhttps://www.instructables.com/id/Real-time-Face-Recognition-an-End-to-end-Project/\n<br>\n<p><img src=\"https://github.com/Mjrovai/OpenCV-Face-Recognition/blob/master/FaceRecogBlock.png?raw=true\"></p>\n"
 },
 {
  "repo": "Cartucho/OpenLabeling",
  "language": "Python",
  "readme_contents": "# OpenLabeling: open-source image and video labeler\n\n[![GitHub stars](https://img.shields.io/github/stars/Cartucho/OpenLabeling.svg?style=social&label=Stars)](https://github.com/Cartucho/OpenLabeling)\n\nImage labeling in multiple annotation formats:\n- PASCAL VOC (= [darkflow](https://github.com/thtrieu/darkflow))\n- [YOLO darknet](https://github.com/pjreddie/darknet)\n- ask for more (create a new issue)...\n\n<img src=\"https://media.giphy.com/media/l49JDgDSygJN369vW/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc1csRs9PoDgCeuk/giphy.gif\" width=\"40%\">\n<img src=\"https://media.giphy.com/media/3o752fXKwTJJkhXP32/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc11t9auzSo6fwLS/giphy.gif\" width=\"40%\">\n\n## Citation\n\nThis project was developed for the following paper, please consider citing it:\n\n```bibtex\n@INPROCEEDINGS{8594067,\n  author={J. {Cartucho} and R. {Ventura} and M. {Veloso}},\n  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, \n  title={Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots}, \n  year={2018},\n  pages={2336-2341},\n}\n```\n\n## Latest Features\n\n- Jun 2019: Deep Learning Object Detection Model\n- May 2019: [ECCV2018] Distractor-aware Siamese Networks for Visual Object Tracking\n- Jan 2019: easy and quick bounding-boxe's resizing!\n- Jan 2019: video object tracking with OpenCV trackers!\n- TODO: Label photos via Google drive to allow \"team online labeling\".\n[New Features Discussion](https://github.com/Cartucho/OpenLabeling/issues/3)\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Prerequisites](#prerequisites)\n- [Run project](#run-project)\n- [GUI usage](#gui-usage)\n- [Authors](#authors)\n\n## Quick start\n\nTo start using the YOLO Bounding Box Tool you need to [download the latest release](https://github.com/Cartucho/OpenLabeling/archive/v1.3.zip) or clone the repo:\n\n```\ngit clone --recurse-submodules git@github.com:Cartucho/OpenLabeling.git\n```\n\n### Prerequisites\n\nYou need to install:\n\n- [Python](https://www.python.org/downloads/)\n- [OpenCV](https://opencv.org/) version >= 3.0\n    1. `python -mpip install -U pip`\n    1. `python -mpip install -U opencv-python`\n    1. `python -mpip install -U opencv-contrib-python`\n- numpy, tqdm and lxml:\n    1. `python -mpip install -U numpy`\n    1. `python -mpip install -U tqdm`\n    1. `python -mpip install -U lxml`\n\nAlternatively, you can install everything at once by simply running:\n\n```\npython -mpip install -U pip\npython -mpip install -U -r requirements.txt\n```\n- [PyTorch](https://pytorch.org/get-started/locally/) \n    Visit the link for a configurator for your setup.\n    \n### Run project\n\nStep by step:\n\n  1. Open the `main/` directory\n  2. Insert the input images and videos in the folder **input/**\n  3. Insert the classes in the file **class_list.txt** (one class name per line)\n  4. Run the code:\n  5. You can find the annotations in the folder **output/**\n\n         python main.py [-h] [-i] [-o] [-t] [--tracker TRACKER_TYPE] [-n N_FRAMES]\n\n         optional arguments:\n          -h, --help                Show this help message and exit\n          -i, --input               Path to images and videos input folder | Default: input/\n          -o, --output              Path to output folder (if using the PASCAL VOC format it's important to set this path correctly) | Default: output/\n          -t, --thickness           Bounding box and cross line thickness (int) | Default: -t 1\n          --tracker tracker_type    tracker_type being used: ['CSRT', 'KCF','MOSSE', 'MIL', 'BOOSTING', 'MEDIANFLOW', 'TLD', 'GOTURN', 'DASIAMRPN']\n          -n N_FRAMES               number of frames to track object for\n  To use DASIAMRPN Tracker:\n  1. Install the [DaSiamRPN](https://github.com/foolwood/DaSiamRPN) submodule and download the model (VOT) from [google drive](https://drive.google.com/drive/folders/1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H)\n  2. copy it into 'DaSiamRPN/code/'\n  3. set default tracker in main.py or run it with --tracker DASIAMRPN\n\n\n#### How to use the deep learning feature\n\n- Download one or some deep learning models from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n  and put it into `object_detection/models` directory (you need to create the `models` folder by yourself). The outline of `object_detection` looks like that:\n  + `tf_object_detection.py`\n  + `utils.py`\n  + `models/ssdlite_mobilenet_v2_coco_2018_05_09`\n\nDownload the pre-trained model by clicking this link http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz and put it into `object_detection/models`. Create the `models` folder if necessary. Make sure to extract the model.\n\n  **Note**: Default model used in `main_auto.py` is `ssdlite_mobilenet_v2_coco_2018_05_09`. We can\n  set `graph_model_path` in file `main_auto.py` to change the pretrain model\n- Using `main_auto.py` to automatically label data first\n\n  TODO: explain how the user can \n\n### GUI usage\n\nKeyboard, press: \n\n<img src=\"https://github.com/Cartucho/OpenLabeling/blob/master/keyboard_usage.jpg\">\n\n| Key | Description |\n| --- | --- |\n| a/d | previous/next image |\n| s/w | previous/next class |\n| e | edges |\n| h | help |\n| q | quit |\n\nVideo:\n\n| Key | Description |\n| --- | --- |\n| p | predict the next frames' labels |\n\nMouse:\n  - Use two separate left clicks to do each bounding box\n  - **Right-click** -> **quick delete**!\n  - Use the middle mouse to zoom in and out\n  - Use double click to select a bounding box\n\n## Authors\n\n* **Jo\u00e3o Cartucho**\n\n    Feel free to contribute\n\n    [![GitHub contributors](https://img.shields.io/github/contributors/Cartucho/OpenLabeling.svg)](https://github.com/Cartucho/OpenLabeling/graphs/contributors)\n"
 },
 {
  "repo": "kylemcdonald/ofxCv",
  "language": "C++",
  "readme_contents": "# Introduction\n\nofxCv represents an alternative approach to wrapping OpenCV for openFrameworks.\n\n# Installation\n\nFirst, pick the branch that matches your version of openFrameworks:\n\n* OF [stable](https://github.com/openframeworks/openFrameworks/tree/stable) (0.9.8): use [ofxCv/stable](https://github.com/kylemcdonald/ofxCv/tree/stable)\n* OF [master](https://github.com/openframeworks/openFrameworks) (0.10.0): use [ofxCv/master](https://github.com/kylemcdonald/ofxCv/)\n\nEither clone out the source code using git:\n\n\t> cd openFrameworks/addons/\n\t> git clone https://github.com/kylemcdonald/ofxCv.git\n\nOr download the source from GitHub [here](https://github.com/kylemcdonald/ofxCv/archive/master.zip), unzip the folder, rename it from `ofxCv-master` to `ofxCv` and place it in your `openFrameworks/addons` folder.\n\nTo run the examples, import them into the project generator, create a new project, and open the project file in your IDE.\n\n# Goals\n\nofxCv has a few goals driving its development.\n\n### Wrap complex things in a helpful way\n\nSometimes this means: providing wrapper functions that require fewer arguments than the real CV functions, providing a smart interface that handles dynamic memory allocation to make things faster for you, or providing in place and out of place alternatives.\n\n### Present the power of OpenCv clearly\n\nThis means naming things in an intuitive way, and, more importantly, providing classes that have methods that transform the data represented by that class. It also means providing demos of CV functions, and generally being more useful than ofxOpenCv.\n\n### Interoperability of openFrameworks and OpenCv\n\nMaking it easy to work directly with CV by providing lightweight conversion functions, and providing wrappers for CV functions that do the conversions for you.\n\n### Elegant internal OpenCv code\n\nProvide clean implementations of all functions in order to provide a stepping stone to direct OpenCV use. This means using function names and variable names that follow the OpenCV documentation, and spending the time to learn proper CV usage so I can explain it clearly to others through code. Sometimes there will be heavy templating in order to make OF interoperable with OpenCV, but this should be avoided in favor of using straight OpenCV as often as possible.\n\n# Usage\n\nSometimes this readme will fall out of date. Please refer to the examples as the primary reference in that case.\n\n## Project setup\n\nUsing ofxCv requires:\n\n* ofxCv/libs/ofxCv/include/ Which contains all the ofxCv headers.\n* ofxCv/libs/ofxCv/src/ Which contains all the ofxCv source.\n* ofxCv/src/ Which ties together all of ofxCv into a single include.\n* opencv/include/ The OpenCv headers, located in addons/ofxOpenCv/\n* opencv/lib/ The precompiled static OpenCv libraries, located in addons/ofxOpenCv/\n\nYour linker will also need to know where the OpenCv headers are. In XCode this means modifying one line in Project.xconfig:\n\n\tHEADER_SEARCH_PATHS = $(OF_CORE_HEADERS) \"../../../addons/ofxOpenCv/libs/opencv/include/\" \"../../../addons/ofxCv/libs/ofxCv/include/\"\n\nAlternatively, I recommend using [OFXCodeMenu](https://github.com/openframeworks/OFXcodeMenu) to add ofxCv to your project.\n\n## Including ofxCv\n\nInside your ofApp.h you will need one include:\n\n\t#include \"ofxCv.h\"\n\nOpenCv uses the `cv` namespace, and ofxCv uses the `ofxCv` namespace. You can automatically import them by writing this in your `.cpp` files:\n\n\tusing namespace cv;\n\tusing namespace ofxCv;\n\nIf you look inside the ofxCv source, you'll find lots of cases of `ofxCv::` and `cv::`. In some rare cases, you'll need to write `cv::` in your code. For example, on OSX `Rect` and `Point` are defined by OpenCv, but also `MacTypes.h`. So if you're using an OpenCv `Rect` or `Point` you'll need to say so explicitly with `cv::Rect` or `cv::Point` to disambiguate.\n\nofxCv takes advantage of namespaces by using overloaded function names. This means that the ofxCv wrapper for `cv::Canny()` is also called `ofxCv::Canny()`. If you write simply `Canny()`, the correct function will be chosen based on the arguments you pass.\n\n## Working with ofxCv\n\nUnlike ofxOpenCv, ofxCv encourages you to use either native openFrameworks types or native OpenCv types, rather than introducing a third type like `ofxCvImage`. To work with OF and OpenCv types in a fluid way, ofxCv includes the `toCv()` and `toOf()` functions. They provide the ability to convert openFrameworks data to OpenCv data and vice versa. For large data, like images, this is done by wrapping the data rather than copying it. For small data, like vectors, this is done by copying the data.\n\nThe rest of ofxCv is mostly helper functions (for example, `threshold()`) and wrapper classes (for example, `Calibration`).\n\n### toCv() and copy()\n\n`toCv()` is used to convert openFrameworks data to OpenCv data. For example:\n\n\tofImage img;\n\timg.load(\"image.png\");\n\tMat imgMat = toCv(img);\n\nThis creates a wrapper for `img` called `imgMat`. To create a deep copy, use `clone()`:\n\n\tMat imgMatClone = toCv(img).clone();\n\nOr `copy()`, which works with any type supported by `toCv()`:\n\n\tMat imgCopy;\n\tcopy(img, imgCopy);\n\n`toCv()` is similar to ofxOpenCv's `ofxCvImage::getCvImage()` method, which returns an `IplImage*`. The biggest difference is that you can't always use `toCv()` \"in place\" when calling OpenCv code directly. In other words, you can always write this:\n\n\tMat imgMat = toCv(img);\n\tcv::someFunction(imgMat, ...);\n\nBut you should avoid using `toCv()` like this:\n\n\tcv::someFunction(toCv(img), ...);\n\nBecause there are cases where in place usage will cause a compile error. More specifically, calling `toCv()` in place will fail if the function requires a non-const reference for that parameter.\n\n### imitate()\n\n`imitate()` is primarily used internally by ofxCv. When doing CV, you regularly want to allocate multiple buffers of similar dimensions and channels. `imitate()` follows a kind of prototype pattern, where you pass a prototype image `original` and the image to be allocated `mirror` to `imitate(mirror, original)`. `imitate()` has two big advantages:\n\n* It works with `Mat`, `ofImage`, `ofPixels`, `ofVideoGrabber`, and anything else that extends `ofBaseHasPixels`.\n* It will only reallocate memory if necessary. This means it can be used liberally.\n\nIf you are writing a function that returns data, the ofxCv style is to call `imitate()` on the data to be returned from inside the function, allocating it as necessary.\n\n### drawMat() vs. toOf()\n\nSometimes you want to draw a `Mat` to the screen directly, as quickly and easily as possible, and `drawMat()` will do this for you. `drawMat()` is not the most optimal way of drawing images to the screen, because it creates a texture every time it draws. If you want to draw things efficiently, you should allocate a texture using `ofImage img;` *once* and draw it using `img.draw()`.\n\n1. Either use `Mat mat = toCv(img);` to treat the `ofImage` as a `Mat`, modify the `mat`, then `img.update()` to upload the modified pixels to the GPU.\n2. Alternatively; call `toOf(mat, img)` each time after modifying the `Mat`. This will only reallocate the texture if necessary, e.g. when the size has changed.\n\n\n# Working with OpenCv 2\n\nOpenCv 2 is an incredibly well designed API, and ofxCv encourages you to use it directly. Here are some hints on using OpenCv.\n\n### OpenCv Types\n\nOpenCv 2 uses the `Mat` class in place of the old `IplImage`. Memory allocation, copying, and deallocation are all handled automatically. `operator=` is a shallow, reference-counted copy. A `Mat` contains a collection of `Scalar` objects. A `Scalar` contains a collection of basic types (unsigned char, bool, double, etc.). `Scalar` is a short vector for representing color or other multidimensional information. The hierarchy is: `Mat` contains `Scalar`, `Scalar` contains basic types.\n\nDifferent functions accept `Mat` in different ways:\n\n* `Mat` will create a lightweight copy of the underlying data. It's easy to write, and it allows you to use `toCv()` \"in-place\" when passing arguments to the function.\n* `Mat&` allows the function to modify the header passed in. This means the function can allocate if necessary.\n* `const Mat&` means that the function isn't going to modify the underlying data. This should be used instead of `Mat` when possible. It also allows \"in-place\" `toCv()` usage.\n\n### Mat creation\n\nIf you're working with `Mat` directly, it's important to remember that OpenCv talks about `rows` and `cols` rather than `width` and `height`. This means that the arguments are \"backwards\" when they appear in the `Mat` constructor. Here's an example of creating a `Mat` wrapper for some grayscale `unsigned char* pixels` for which we know the `width` and `height`:\n\n\tMat mat = Mat(height, width, CV_8UC1, pixels, 0);\n\n### Mat operations\n\nBasic mathematical operations on `Mat` objects of the same size and type can be accomplished with matrix expressions. Matrix expressions are a collection of overloaded operators that accept `Mat`, `Scalar`, and basic types. A normal mathematical operation might look like:\n\n\tfloat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nA matrix operation looks similar:\n\n\tMat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nThis will add every element of `a` and `b`, then multiply the results by 10, and finally assign the result to `x`.\n\nAvailable matrix expressions include mathematical operators `+`, `-`, `/` (per element division), `*` (matrix multiplication), `.mul()` (per-element multiplication). As well as comparison operators `!=`, `==`, `<`, `>`, `>=`, `<=` (useful for thresholding). Binary operators `&`, `|`, `^`, `~`. And a few others like `abs()`, `min()`, and `max()`. For the complete listing see the OpenCv documention or `mat.hpp`.\n\n# Code Style\n\nofxCv tries to have a consistent code style. It's most similar to the K&R variant used for Java, and the indentation is primarily determined by XCode's auto-indent feature.\n\nMultiline comments are used for anything beyond two lines.\n\nCase statements have a `default:` fall-through with the last case.\n\nWhen two or three similar variables are initialized, commas are used instead of multiple lines. For example `Mat srcMat = toCv(src), dstMat = toCv(dst);`. This style was inherited from reading Jason Saragih's FaceTracker.\n\n- - --\n\n*ofxCv was developed with support from [Yamaguchi Center for Arts and Media](http://ycam.jp/).*\n"
 },
 {
  "repo": "yinguobing/head-pose-estimation",
  "language": "Python",
  "readme_contents": "# Head pose estimation\n\nThis repo shows how to estimate human head pose from videos using TensorFlow and OpenCV.\n\n![demo](doc/demo.gif)\n![demo](doc/demo1.gif)\n\n## Getting Started\n\n:information_source:  *Checkout branch `tf2` if you are using TensorFlow 2*\n\nThe following packages are required:\n\n- TensorFlow 1.14.\n- OpenCV 3.3 or higher.\n- Python 3.5\n\nThe code is tested on Ubuntu 16.04.\n\n## Installing\n\nThis repository comes with a pre-trained model for facial landmark detection. Just git clone then you are good to go.\n\n```bash\n# From your favorite development directory:\ngit clone https://github.com/yinguobing/head-pose-estimation.git\n```\n\n## Running\n\nA video file or a webcam index should be assigned through arguments. If no source provided, the default webcam will be used.\n\n### For video file\n\nFor any video format that OpenCV supported (`mp4`, `avi` etc.):\n\n```bash\npython3 estimate_head_pose.py --video /path/to/video.mp4\n```\n\n### For webcam\n\nThe webcam index should be assigned:\n\n```bash\npython3 estimate_head_pose.py --cam 0\n``` \n\n## How it works\n\nThere are three major steps:\n\n1. Face detection. A face detector is adopted to provide a face box containing a human face. Then the face box is expanded and transformed to a square to suit the needs of later steps.\n\n2. Facial landmark detection. A custom trained facial landmark detector based on TensorFlow is responsible for output 68 facial landmarks.\n\n3. Pose estimation. Once we got the 68 facial landmarks, a mutual PnP algorithms is adopted to calculate the pose.\n\nThe marks is detected frame by frame, which result in small variance between adjacent frames. This makes the pose unstable. A Kalman filter is used to solve this problem, you can draw the original pose to observe the difference.\n\n## Retrain the model\n\nTo reproduce the facial landmark detection model, you can refer to this [series](https://yinguobing.com/deeplearning/) of posts(in Chinese only). And the training code is also open sourced: https://github.com/yinguobing/cnn-facial-landmark\n\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Authors\nYin Guobing (\u5c39\u56fd\u51b0) - [yinguobing](https://yinguobing.com)\n\n![](doc/wechat_logo.png)\n\n## Acknowledgments\nThe pre-trained TensorFlow model file is trained with various public data sets which have their own licenses. Please refer to them before using this code.\n\n- 300-W: https://ibug.doc.ic.ac.uk/resources/300-W/\n- 300-VW: https://ibug.doc.ic.ac.uk/resources/300-VW/\n- LFPW: https://neerajkumar.org/databases/lfpw/\n- HELEN: http://www.ifp.illinois.edu/~vuongle2/helen/\n- AFW: https://www.ics.uci.edu/~xzhu/face/\n- IBUG: https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/\n\nThe 3D model of face comes from OpenFace, you can find the original file [here](https://github.com/TadasBaltrusaitis/OpenFace/blob/master/lib/local/LandmarkDetector/model/pdms/In-the-wild_aligned_PDM_68.txt).\n\nThe build in face detector comes from OpenCV. \nhttps://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n"
 },
 {
  "repo": "twistedfall/opencv-rust",
  "language": "Rust",
  "readme_contents": "# Rust OpenCV bindings\n\n[![Github Actions](https://github.com/twistedfall/opencv-rust/workflows/opencv-rust/badge.svg)](https://github.com/twistedfall/opencv-rust/actions?query=workflow%3Aopencv-rust)\n[![Documentation](https://docs.rs/opencv/badge.svg)](https://docs.rs/opencv)\n[![Package](https://img.shields.io/crates/v/opencv.svg)](https://crates.io/crates/opencv)\n\nExperimental Rust bindings for OpenCV 3 and 4.\n\nThe API is usable, but unstable and not very battle-tested; use at your own risk.\n\n## Quickstart\n\nMake sure the supported OpenCV version (3.2, 3.4 or 4.x) is installed in your system.\n\nUpdate your Cargo.toml\n```toml\nopencv = \"0.46\"\n```\n\nSelect OpenCV version if different from default (opencv-4) in Cargo.toml:\n```toml\nopencv = {version = \"0.46\", default-features = false, features = [\"opencv-34\", \"buildtime-bindgen\"]}\n```\n\nOr enable usage of `contrib` modules:\n```toml\nopencv = {version = \"0.46\", features = [\"contrib\"]}\n```\n\nImport prelude\n```rust\nuse opencv::prelude::*;\n```\n\n## Getting OpenCV\n\n### Linux\n\nYou have several options of getting the OpenCV library:\n\n* install it from the repository, make sure to install `-dev` packages because they contain headers necessary\n  for the crate build (also check that your package contains `pkg_config` or `cmake` files).\n\n* build OpenCV manually and set up the following environment variables prior to building the project with\n  `opencv` crate:\n  * `PKG_CONFIG_PATH` for the location of `*.pc` files or `OpenCV_DIR` for the location of `*.cmake` files\n  * `LD_LIBRARY_PATH` for where to look for the installed `*.so` files during runtime\n\n### Windows package\n\nInstalling OpenCV is easy through the following sources:\n\n* from [chocolatey](https://chocolatey.org), also install `llvm` package, it's required for building:\n  ```shell script\n  choco install llvm opencv\n  ```\n  also set `OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS` environment variables (see below\n  for details).\n  \n  Also, check the user guides [here](https://github.com/twistedfall/opencv-rust/issues/118#issuecomment-619608278)\n  and [here](https://github.com/twistedfall/opencv-rust/issues/113#issue-596076777).\n\n* from [vcpkg](https://docs.microsoft.com/en-us/cpp/build/vcpkg), also install `llvm` package,\n  necessary for building:\n  ```shell script\n  vcpkg install llvm opencv4[contrib,nonfree]\n  ```\n  You most probably want to set environment variable `VCPKGRS_DYNAMIC` to \"1\" unless you're specifically\n  targeting a static build.\n\n### macOS package\n\nGet OpenCV from homebrew:\n\n* [homebrew](https://brew.sh):\n  ```shell script\n  brew install opencv\n  ```\n  You will also need a working C++ compiler and libclang, you can use the ones from XCode or install `llvm`\n  from brew. You most probably need to also check the item 6 of the troubleshooting below.\n\n### Manual build\n\nYou can of course always compile OpenCV of the version you prefer manually. This is also supported, but it\nrequires some additional configuration.\n\nYou need to set up the following environment variables to point to the installed files of your OpenCV build: \n`OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS` (see below for details).\n\n## Troubleshooting\n\n1. One of the common problems is link errors in the end of the build.\n\n   Make sure you're building with `buildtime-bindgen` feature enabled (requires installed clang/llvm), it will\n   recreate rust and cpp files to match the version you have installed. Please be sure to also set up the\n   relevant environment variables that will allow the linker to find the libraries it needs (see below).\n\n2. You're getting runtime errors like:\n   ```\n   thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { code: -215, message: \"OpenCV(4.2.0) /build/opencv/src/opencv-4.2.0/modules/highgui/src/window.cpp:384: error: (-215:Assertion failed) size.width>0 && size.height>0 in function \\'imshow\\'\\n\" }', src/libcore/result.rs:1188:5\n   ```\n   ```\n   thread 'extraction::tests::test_contour_matching' panicked at 'called `Result::unwrap()` on an `Err` value: Error { code: -215, message: \"OpenCV(4.1.1) /tmp/opencv-20190908-41416-17rm3ol/opencv-4.1.1/modules/core/src/matrix_wrap.cpp:79: error: (-215:Assertion failed) 0 <= i && i < (int)vv.size() in function \\'getMat_\\'\\n\" }', src/libcore/result.rs:1165:5\n   ```\n\n   These errors (note the .cpp source file and `Error` return value) are coming from OpenCV itself, not from\n   the crate. It means that you're using the OpenCV API incorrectly, e.g. passing incompatible or unexpected\n   arguments. Please refer to the OpenCV documentation for details.\n\n3. You're getting errors that methods don't exist or not implemented for specific `struct`s, but you can see\n   them in the documentation and in the crate source.\n\n   Be sure to import ```use opencv::prelude::*;```. The crate contains a lot of traits that need to be imported\n   first.\n\n   Also check that if you're using a contrib module that the `contrib` feature is enabled for the crate. \n\n4. On Windows, you're getting the `(exit code: 0xc0000135, STATUS_DLL_NOT_FOUND)` error when running the\n   compiled binary.\n\n   That often means that Windows can't find the OpenCV library dll. Be sure to set up `PATH` environment\n   variable correctly or copy the dll next to the binary you're trying to run. Check\n   [that](https://github.com/twistedfall/opencv-rust/issues/118#issuecomment-619608278) guide too.\n\n5. On Windows with VCPKG you're getting a lot of linking errors in multiple files like in\n   [this issue](https://github.com/twistedfall/opencv-rust/issues/161).\n   \n   Unless you're doing a very specific build, you want to have environment variable `VCPKGRS_DYNAMIC` set to\n   \"1\".\n\n6. On macOS you're getting the `dyld: Library not loaded: @rpath/libclang.dylib` error during the build process.\n\n   OS can't find `libclang.dylib` dynamic library because it resides in a non-standard path, set up \n   the `DYLD_FALLBACK_LIBRARY_PATH` environment variable to point to the path where libclang.dylib can be\n   found, e.g. for Xcode:\n   \n   ```  \n   export DYLD_FALLBACK_LIBRARY_PATH=\"$(xcode-select --print-path)/Toolchains/XcodeDefault.xctoolchain/usr/lib/\"\n   ```\n\n## Reporting issues\n\nIf you still have trouble using the crate after going through the Troubleshooting steps please fill free to\nreport it to the [bugtracker](https://github.com/twistedfall/opencv-rust/issues).\n\nWhen reporting an issue please state:\n1. Operating system\n2. The way you installed OpenCV: package, official binary distribution, manual compilation, etc.\n3. OpenCV version\n4. Attach the full output of the following command from your project directory:\n   ```shell script\n   RUST_BACKTRACE=full cargo build -vv \n   ```\n\n## Environment variables\n\nThe following variables must be set when building without `pkg_config`, `cmake` or `vcpkg`. You can set them\non any platform, the specified values will override those automatically discovered.\n\n* `OPENCV_LINK_LIBS`\n  Comma separated list of library names to link to. `.lib`, `.so` or `.dylib` extension is optional. If you\n  specify the \".framework\" extension then build script will link a macOS framework instead of plain shared\n  library.\n  E.g. \"opencv_world411\".\n  \n  If this list starts with '+' (plus sign) then the specified items will be appended to whatever the system\n  probe returned. E.g. a value of \"+dc1394\" will do a system discovery of the OpenCV library and its linked\n  libraries and then will additionally link `dc1394` library at the end. Can be useful if the system probe\n  produces a mostly working setup, but has incomplete link list, or the order is wrong (especially important\n  during static linking).\n\n* `OPENCV_LINK_PATHS`\n  Comma separated list of paths to search for libraries to link. E.g. \"C:\\tools\\opencv\\build\\x64\\vc15\\lib\".\n  The path list can start with '+', see `OPENCV_LINK_LIBS` for a detailed explanation (e.g.\n  \"+/usr/local/lib\").\n\n* `OPENCV_INCLUDE_PATHS`\n  Comma separated list of paths to search for system include files during compilation. E.g.\n  \"C:\\tools\\opencv\\build\\include\". One of the directories specified therein must contain\n  \"opencv2/core/version.hpp\" or \"core/version.hpp\" file, it's used to detect the version of the headers.\n  The path list can start with '+', see `OPENCV_LINK_LIBS` for a detailed explanation (e.g.\n  \"+/opt/cuda/targets/x86_64-linux/include/\").\n\nThe following variables are rarely used, but you might need them under some circumstances:\n\n* `OPENCV_HEADER_DIR`\n  During crate build it uses OpenCV headers bundled with the crate. If you want to use your own (system)\n  headers supply `OPENCV_HEADER_DIR` environment variable.\n  The directory in that environment variable should contain `opencv2` dir, e.g. set it `/usr/include` for\n  OpenCV-3.4.x or `/usr/include/opencv4` for OpenCV-4.x.\n\n* `OPENCV_PACKAGE_NAME`\n  In some cases you might want to override the pkg-config, cmake or vcpkg package name, you can use this\n  environment variable for that. If you set it pkg-config will expect to find the file with that name and `.pc`\n  extension in the package directory. Cmake will look for that file with `.cmake` extension. And vcpkg will use\n  that name to try to find package in `packages` directory under `VCPKG_ROOT`. You can also use separate\n  environment variables to set different package names for different package systems:\n    * `OPENCV_PKGCONFIG_NAME`\n    * `OPENCV_CMAKE_NAME`\n    * `OPENCV_VCPKG_NAME`\n\n* `OPENCV_CMAKE_BIN`\n  Path to cmake binary (used in OpenCV discovery process using cmake). If not set then just \"cmake\" will be\n  used. For example, you can set something like \"/usr/local/bin/cmake\" here.\n\n* `OPENCV_DISABLE_PROBES`\n  Comma separated list of OpenCV package auto-discovery systems to exclude from running. Might be useful if\n  one of the higher priority systems is producing incorrect results. Can contain the following values:\n    * environment - reads data only from the `OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS`\n      environment variables\n    * pkg_config\n    * cmake\n    * vcpkg_cmake - like vcpkg, but only uses vcpkg for path discovery, the actual OpenCV probe is done using\n      cmake (cmake related environment variables are applicable with this probe)\n    * vcpkg\n\n* `OPENCV_CLANG_STDLIB_PATH`\n  Path that contains the stdlib headers for parsing with libclang. Should be used only as a workaround for\n  the rare cases where it doesn't get picked up automatically. Should help with issues like\n  [this](https://github.com/twistedfall/opencv-rust/issues/125).\n\n* `OPENCV_MODULE_WHITELIST` and `OPENCV_MODULE_BLACKLIST`\n  Comma separated lists that affect modules that get their bindings generated. Setting whitelist will only\n  generate the specified modules, setting blacklist will exclude the specified modules from generation. If the\n  same module is specified in both list it will be excluded (i.e. blacklist has precedence). E.g.\n  \"core,dnn,features2d\" .\n\nThe following variables affect the building the of the `opencv` crate, but belong to external components:\n\n* `PKG_CONFIG_PATH`\n  Where to look for `*.pc` files see the [man pkg-config](https://linux.die.net/man/1/pkg-config)\n  Path specified here must contain `opencv.pc` (pre OpenCV 4) or `opencv4.pc` (OpenCV 4 and later).\n\n* `VCPKG_ROOT` and `VCPKGRS_DYNAMIC`\n  The root of `vcpkg` installation and flag allowing use of `*.dll` libraries, see the\n  [documentation for `vcpkg` crate](https://docs.rs/vcpkg)\n\n* `OpenCV_DIR`\n  The directory that contains OpenCV package cmake files. Usually there are `OpenCVConfig.cmake`,\n  `OpenCVConfig-version.cmake` and `OpenCVModules.cmake` in it.\n\n* `LD_LIBRARY_PATH`\n  On Linux it sets the list of directories to look for the installed `*.so` files during runtime.\n  [Linux documentation](https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html) has more info.\n  Path specified here must contain `libopencv_*.so` files.\n\n* `DYLD_LIBRARY_PATH` and `DYLD_FALLBACK_LIBRARY_PATH`\n  Similar to `LD_LIBRARY_PATH`, but for loading `*.dylib` files on macOS, see [man dyld](https://man.cx/dyld(1))\n  and [this SO answer](https://stackoverflow.com/a/3172515) for more info. Path specified here must contain\n  `*.dylib` files.\n\n* `PATH`\n  Windows searches for `*.dll`s in `PATH` among other places, be sure to set it up, or copy required OpenCV\n  `*.dll`s next to your binary. Be sure to specify paths in UNIX style (/C/Program Files/Dir) because colon\n   in `PATH` might be interpreted as the entry separator. Summary [here](https://stackoverflow.com/a/6546427).\n\n* clang crate environment variables\n  See crate's [README](https://github.com/KyleMayes/clang-sys/blob/master/README.md#environment-variables)\n\n## Cargo features\n* `opencv-32` - build against OpenCV 3.2.0, this feature is aimed primarily on stable Debian and\n  Ubuntu users who can install OpenCV from the repository without having to compile it from the\n  source\n* `opencv-34` - build against OpenCV 3.4.x\n* `opencv-4` (default) - build against OpenCV 4.x\n* `contrib` - enable the usage of OpenCV contrib modules for corresponding OpenCV version\n* `buildtime-bindgen` (default) - regenerate all bindings, requires installed clang/llvm (minimum supported\n  version is 6.0), with this feature enabled the bundled headers are no longer used for the code generation,\n  the ones from the installed OpenCV are used instead\n* `clang-runtime` - only useful with the combination with `buildtime-bindgen`, enables the runtime detection\n  of libclang (`runtime` feature of `clang-sys`). This makes the build slower because it impairs the parallel\n  generation of OpenCV modules. Useful as a workaround for when your dependencies (like `bindgen`) pull in\n  `clang-sys` with hard `runtime` feature. See also this [issue](https://github.com/twistedfall/opencv-rust/issues/129).\n* `docs-only` - internal usage, for building docs on [docs.rs](https://docs.rs/opencv)\n\n## API details\n\n[API Documentation](https://docs.rs/opencv) is automatically translated from OpenCV's doxygen docs. Most\nlikely you'll still want to refer to the official [OpenCV C++ documentation](https://docs.opencv.org/master)\nas well.\n\n### OpenCV version support\n\nThe following OpenCV versions are supported at the moment:\n* 3.2 - enabled by `opencv-32` feature\n* 3.4 - enabled by `opencv-34` feature\n* 4.3 - enabled by the default `opencv-4` feature\n\nIf you need support for `contrib` modules, also enable `contrib` feature.\n\n### Minimum rustc version\n\nGenerally you should use the latest stable rustc to compile this crate.\n\n### Platform support\n\nCurrently, the main development and testing of the crate is performed on Linux, but other major platforms are\nalso supported: macOS and Windows.\n\nFor some more details please refer to the CI build scripts:\n[Linux OpenCV install](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-bionic.sh),\n[macOS OpenCV install as framework](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-macos-framework.sh),\n[macOS OpenCV install via brew](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-macos-brew.sh),\n[Windows OpenCV install via Chocolatey](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-windows-chocolatey.sh),\n[Windows OpenCV install via vcpkg](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-windows-vcpkg.sh),\n[Test runner script](https://github.com/twistedfall/opencv-rust/blob/master/ci/script.sh).\n\n### Functionality\n\nGenerally the crate tries to only wrap OpenCV API and provide some convenience functions\nto be able to use it in Rust easier. We try to avoid adding any functionality besides\nthat.\n\n### Errors\n\nMost functions return a `Result` to expose a potential C++ exception. Although some methods like property reads\nor functions that are marked CV_NOEXCEPT in the OpenCV headers are infallible and return a naked value.\n\n### Properties\n\nProperties of OpenCV classes are accessible through setters and getters. Those functions are infallible, they\nreturn the value directly instead of `Result`.\n\n### Infallible functions\n\nFor infallible functions (like setters) that accept `&str` values the following logic applies: if a Rust\nstring passed as argument contains null byte then this string will be truncated up to that null byte. So if\nfor example you pass \"123\\0456\" to the setter, the property will be set to \"123\". \n\n### Callbacks\n\nSome API functions accept callbacks, e.g. `set_mouse_callback`. While currently it's possible to successfully\nuse those functions there are some limitations to keep in mind. Current implementation of callback handling\nleaks the passed callback argument. That means that the closure used as a callback will never be freed during\nthe lifetime of a program and moreover Drop will not be called for it. There is a plan to implement possibility\nto be able to free at least some of the closures.\n\n### Unsafety\n\nAlthough the crate tries to provide an ergonomic Rust interface for OpenCV, don't expect\nRust safety guarantees at this stage. It's especially true for the borrow-checking and the\nshared mutable ownership. Notable example would be `Mat` which is a reference counted\nobject in its essence. You can own a seemingly separate `Mat` in Rust terms, but\nit's going to be a mutable reference to the other `Mat` under the hood. Treat safety\nof the crate's API as you would treat one of C++, use `clone()` when needed.\n\n## Contrib modules\n\nTo be able to use some modules you need to have [`opencv_contrib`](https://github.com/opencv/opencv_contrib)\ninstalled. You can find the full list of contrib modules [here](https://github.com/opencv/opencv_contrib/tree/master/modules) with the exception that `dnn` module is also considered contrib for OpenCV 3.2.\n\n## Missing modules and functions\n\nWhile most of the API is covered, for various reasons (that might no longer hold) there are modules and\nfunctions that are not yet implemented. If a missing module/function is near and dear to you, please file an\nissue (or better, open a pull request!).\n\n## The binding strategy\n\nThis crate works similar to the model of python and java's OpenCV wrappers - it uses libclang to parse the\nOpenCV C++ headers, generates a C interface to the C++ API, and wraps the C interface in Rust.\n\nAll the major modules in the C++ API are merged together in a huge `cv::` namespace. We instead made one rust\nmodule for each major OpenCV module. So, for example, C++ `cv::Mat` is `opencv::core::Mat` in this crate.\n\nThe methods and field names have been snake_cased. Methods arguments with default value lose these default\nvalues, but they are reported in the API documentation.\n\nOverloaded methods have been mostly manually given different names or automatically renamed to *_1, *_2, etc.\n\n## OpenCV 2 support\n\nIf you can't use OpenCV 3.x or higher, the (no longer maintained) `0.2.4` version of this crate is known to\nwork with OpenCV `2.4.7.13` (and probably other 2.4 versions). Please refer to the README.md file for that\nversion because the crate has gone through the considerable rewrite since.\n\n## Contributor's Guide\n\nThe binding generator code lives in a separate crate under [binding-generator](binding-generator). During the\nbuild phase (with `buildtime-bindgen` feature enabled) it creates bindings from the header files and puts them\ninto [bindings](bindings) directory. Those are then transferred to [src](src) for the consumption by the\ncrate users. \n\nThe crate itself, as imported by users, consists of generated rust code in [src](src) committed to the repo.\nThis way, users don't have to handle the code generation overhead in their builds. When developing this crate,\nyou can test changes to the binding generation using `cargo build -vv`. When changing the `binding-generator`,\nbe sure to push changes to the generated code!\n\nIf you're looking for things to improve be sure to search for `todo` and `fixme` labels in the project\nsource, those usually carry the comment of what exactly needs to be fixed.\n\nThe license for the original work is [MIT](https://opensource.org/licenses/MIT).\n\nSpecial thanks to [ttacon](https://github.com/ttacon) for yielding the crate name.\n"
 },
 {
  "repo": "fendouai/OpenCVTutorials",
  "language": "Python",
  "readme_contents": "# \u5199\u5728\u524d\u9762\u7684\u8bdd\n\n**OpenCV**\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7ecf\u5178\u7684\u4e13\u7528\u5e93\uff0c\u5176\u652f\u6301\u591a\u8bed\u8a00\u3001\u8de8\u5e73\u53f0\uff0c\u529f\u80fd\u5f3a\u5927\u3002**OpenCV-Python**\u4e3aOpenCV\u63d0\u4f9b\u4e86Python\u63a5\u53e3\uff0c\u4f7f\u5f97\u4f7f\u7528\u8005\u5728Python\u4e2d\u80fd\u591f\u8c03\u7528C/C++\uff0c\u5728\u4fdd\u8bc1\u6613\u8bfb\u6027\u548c\u8fd0\u884c\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u6240\u9700\u7684\u529f\u80fd\u3002\n\n**OpenCV-Python Tutorials**\u662f\u5b98\u65b9\u63d0\u4f9b\u7684\u6587\u6863\uff0c\u5176\u5185\u5bb9\u5168\u9762\u3001\u7b80\u5355\u6613\u61c2\uff0c\u4f7f\u5f97\u521d\u5b66\u8005\u80fd\u591f\u5feb\u901f\u4e0a\u624b\u4f7f\u7528\u30022014\u5e74\u6bb5\u529b\u8f89\u5728\u5f53\u65f6\u5df2\u7ffb\u8bd1\u8fc7OpenCV3.0\uff0c\u4f46\u65f6\u9694\u4e94\u5e74\uff0c\u5982\u4eca\u7684**OpenCV4.1**\u4e2d\u8bb8\u591a\u51fd\u6570\u548c\u5185\u5bb9\u5df2\u7ecf\u6709\u6240\u66f4\u65b0\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u8be5\u5b98\u65b9\u6587\u6863\u518d\u8fdb\u884c\u4e00\u6b21\u7ffb\u8bd1\u3002\n\n\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u96be\u514d\u6709\u6240\u758f\u6f0f\uff0c\u5982\u53d1\u73b0\u9519\u8bef\uff0c\u5e0c\u671b\u5927\u5bb6\u6307\u51fa\uff0c\u8c22\u8c22\u652f\u6301\u3002\n\nOpenCV-Python Tutorials\u5b98\u65b9\u6587\u6863\uff1ahttps://docs.opencv.org/4.1.2/d6/d00/tutorial_py_root.html\n\n# \u76ee\u5f55\n\n### [OpenCV\u4e2d\u6587\u5b98\u65b9\u6587\u6863](http://www.woshicver.com/.)\n\n*   <span class=\"caption-text\">OpenCV\u7b80\u4ecb</span>\n\n    * [0_OpenCV-Python Tutorials](http://www.woshicver.com/FirstSection/0_OpenCV-Python%20Tutorials/)\n    \n*   <span class=\"caption-text\">OpenCV\u5b89\u88c5</span>\n\n    *   [1_1_OpenCV-Python\u6559\u7a0b\u7b80\u4ecb](http://www.woshicver.com/SecondSection/1_1_OpenCV-Python\u6559\u7a0b\u7b80\u4ecb/)\n    *   [1_2_\u5728Windows\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_2_\u5728Windows\u4e2d\u5b89\u88c5OpenCV-Python/)\n    *   [1_3_\u5728Fedora\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_3_\u5728Fedora\u4e2d\u5b89\u88c5OpenCV-Python/)\n    *   [1_4_\u5728Ubuntu\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_4_\u5728Ubuntu\u4e2d\u5b89\u88c5OpenCV-Python/)\n    \n*   <span class=\"caption-text\">OpenCV\u4e2d\u7684GUI\u7279\u6027</span>\n\n    *   [2_1_\u56fe\u50cf\u5165\u95e8](http://www.woshicver.com/ThirdSection/2_1_\u56fe\u50cf\u5165\u95e8/)\n    *   [2_2_\u89c6\u9891\u5165\u95e8](http://www.woshicver.com/ThirdSection/2_2_\u89c6\u9891\u5165\u95e8/)\n    *   [2_3_OpenCV\u4e2d\u7684\u7ed8\u56fe\u529f\u80fd](http://www.woshicver.com/ThirdSection/2_3_OpenCV\u4e2d\u7684\u7ed8\u56fe\u529f\u80fd/)\n    *   [2_4_\u9f20\u6807\u4f5c\u4e3a\u753b\u7b14](http://www.woshicver.com/ThirdSection/2_4_\u9f20\u6807\u4f5c\u4e3a\u753b\u7b14/)\n    *   [2_5_\u8f68\u8ff9\u680f\u4f5c\u4e3a\u8c03\u8272\u677f](http://www.woshicver.com/ThirdSection/2_5_\u8f68\u8ff9\u680f\u4f5c\u4e3a\u8c03\u8272\u677f/)\n    \n*   <span class=\"caption-text\">\u6838\u5fc3\u64cd\u4f5c</span>\n\n    *   [3_1_\u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c](http://www.woshicver.com/FourthSection/3_1_\u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c/)\n    *   [3_2_\u56fe\u50cf\u4e0a\u7684\u7b97\u6cd5\u8fd0\u7b97](http://www.woshicver.com/FourthSection/3_2_\u56fe\u50cf\u4e0a\u7684\u7b97\u6cd5\u8fd0\u7b97/)\n    *   [3_3_\u6027\u80fd\u8861\u91cf\u548c\u63d0\u5347\u6280\u672f](http://www.woshicver.com/FourthSection/3_3_\u6027\u80fd\u8861\u91cf\u548c\u63d0\u5347\u6280\u672f/)\n    \n*   <span class=\"caption-text\">OpenCV\u4e2d\u7684\u56fe\u50cf\u5904\u7406</span>\n\n    *   [4_1_\u6539\u53d8\u989c\u8272\u7a7a\u95f4](http://www.woshicver.com/FifthSection/4_1_\u6539\u53d8\u989c\u8272\u7a7a\u95f4/)\n    *   [4_2_\u56fe\u50cf\u51e0\u4f55\u53d8\u6362](http://www.woshicver.com/FifthSection/4_2_\u56fe\u50cf\u51e0\u4f55\u53d8\u6362/)\n    *   [4_3_\u56fe\u50cf\u9608\u503c](http://www.woshicver.com/FifthSection/4_3_\u56fe\u50cf\u9608\u503c/)\n    *   [4_4_\u56fe\u50cf\u5e73\u6ed1](http://www.woshicver.com/FifthSection/4_4_\u56fe\u50cf\u5e73\u6ed1/)\n    *   [4_5_\u5f62\u6001\u8f6c\u6362](http://www.woshicver.com/FifthSection/4_5_\u5f62\u6001\u8f6c\u6362/)\n    *   [4_6_\u56fe\u50cf\u68af\u5ea6](http://www.woshicver.com/FifthSection/4_6_\u56fe\u50cf\u68af\u5ea6/)\n    *   [4_7_Canny\u8fb9\u7f18\u68c0\u6d4b](http://www.woshicver.com/FifthSection/4_7_Canny\u8fb9\u7f18\u68c0\u6d4b/)\n    *   [4_8_\u56fe\u50cf\u91d1\u5b57\u5854](http://www.woshicver.com/FifthSection/4_8_\u56fe\u50cf\u91d1\u5b57\u5854/)\n    *   [4_9_1_OpenCV\u4e2d\u7684\u8f6e\u5ed3](http://www.woshicver.com/FifthSection/4_9_1_OpenCV\u4e2d\u7684\u8f6e\u5ed3/)\n    *   [4_9_2_\u8f6e\u5ed3\u7279\u5f81](http://www.woshicver.com/FifthSection/4_9_2_\u8f6e\u5ed3\u7279\u5f81/)\n    *   [4_9_3_\u8f6e\u5ed3\u5c5e\u6027](http://www.woshicver.com/FifthSection/4_9_3_\u8f6e\u5ed3\u5c5e\u6027/)\n    *   [4_9_4_\u8f6e\u5ed3\uff1a\u66f4\u591a\u5c5e\u6027](http://www.woshicver.com/FifthSection/4_9_4_\u8f6e\u5ed3\uff1a\u66f4\u591a\u5c5e\u6027/)\n    *   [4_9_5_\u8f6e\u5ed3\u5206\u5c42](http://www.woshicver.com/FifthSection/4_9_5_\u8f6e\u5ed3\u5206\u5c42/)\n    *   [4_10_1_\u76f4\u65b9\u56fe-1\uff1a\u67e5\u627e\uff0c\u7ed8\u5236\uff0c\u5206\u6790](http://www.woshicver.com/FifthSection/4_10_1_\u76f4\u65b9\u56fe-1\uff1a\u67e5\u627e\uff0c\u7ed8\u5236\uff0c\u5206\u6790/)\n    *   [4_10_2_\u76f4\u65b9\u56fe-2\uff1a\u76f4\u65b9\u56fe\u5747\u8861](http://www.woshicver.com/FifthSection/4_10_2_\u76f4\u65b9\u56fe-2\uff1a\u76f4\u65b9\u56fe\u5747\u8861/)\n    *   [4_10_3_\u76f4\u65b9\u56fe3\uff1a\u4e8c\u7ef4\u76f4\u65b9\u56fe](http://www.woshicver.com/FifthSection/4_10_3_\u76f4\u65b9\u56fe3\uff1a\u4e8c\u7ef4\u76f4\u65b9\u56fe/)\n    *   [4_10_4_\u76f4\u65b9\u56fe-4\uff1a\u76f4\u65b9\u56fe\u53cd\u6295\u5f71](http://www.woshicver.com/FifthSection/4_10_4_\u76f4\u65b9\u56fe-4\uff1a\u76f4\u65b9\u56fe\u53cd\u6295\u5f71/)\n    *   [4_11_\u5085\u91cc\u53f6\u53d8\u6362](http://www.woshicver.com/FifthSection/4_11_\u5085\u91cc\u53f6\u53d8\u6362/)\n    *   [4_12_\u6a21\u677f\u5339\u914d](http://www.woshicver.com/FifthSection/4_12_\u6a21\u677f\u5339\u914d/)\n    *   [4_13_\u970d\u592b\u7ebf\u53d8\u6362](http://www.woshicver.com/FifthSection/4_13_\u970d\u592b\u7ebf\u53d8\u6362/)\n    *   [4_14_\u970d\u592b\u5708\u53d8\u6362](http://www.woshicver.com/FifthSection/4_14_\u970d\u592b\u5708\u53d8\u6362/)\n    *   [4_15_\u56fe\u50cf\u5206\u5272\u4e0e\u5206\u6c34\u5cad\u7b97\u6cd5](http://www.woshicver.com/FifthSection/4_15_\u56fe\u50cf\u5206\u5272\u4e0e\u5206\u6c34\u5cad\u7b97\u6cd5/)\n    *   [4_16_\u4ea4\u4e92\u5f0f\u524d\u666f\u63d0\u53d6\u4f7f\u7528GrabCut\u7b97\u6cd5](http://www.woshicver.com/FifthSection/4_16_\u4ea4\u4e92\u5f0f\u524d\u666f\u63d0\u53d6\u4f7f\u7528GrabCut\u7b97\u6cd5/)\n    \n*   <span class=\"caption-text\">\u7279\u5f81\u68c0\u6d4b\u4e0e\u63cf\u8ff0</span>\n\n    *   [5_1_\u7406\u89e3\u7279\u5f81](http://www.woshicver.com/Sixth/5_1_\u7406\u89e3\u7279\u5f81/)\n    *   [5_2_\u54c8\u91cc\u65af\u89d2\u68c0\u6d4b](http://www.woshicver.com/Sixth/5_2_\u54c8\u91cc\u65af\u89d2\u68c0\u6d4b/)\n    *   [5_3_Shi-Tomasi\u62d0\u89d2\u63a2\u6d4b\u5668\u548c\u826f\u597d\u7684\u8ddf\u8e2a\u529f\u80fd](http://www.woshicver.com/Sixth/5_3_Shi-Tomasi\u62d0\u89d2\u63a2\u6d4b\u5668\u548c\u826f\u597d\u7684\u8ddf\u8e2a\u529f\u80fd/)\n    *   [5_4_SIFT\uff08\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362\uff09\u7b80\u4ecb](http://www.woshicver.com/Sixth/5_4_SIFT\uff08\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362\uff09\u7b80\u4ecb/)\n    *   [5_5_SURF\u7b80\u4ecb\uff08\u52a0\u901f\u7684\u5f3a\u5927\u529f\u80fd\uff09](http://www.woshicver.com/Sixth/5_5_SURF\u7b80\u4ecb\uff08\u52a0\u901f\u7684\u5f3a\u5927\u529f\u80fd\uff09/)\n    *   [5_6_\u7528\u4e8e\u89d2\u70b9\u68c0\u6d4b\u7684FAST\u7b97\u6cd5](http://www.woshicver.com/Sixth/5_6_\u7528\u4e8e\u89d2\u70b9\u68c0\u6d4b\u7684FAST\u7b97\u6cd5/)\n    *   [5_7_BRIEF\uff08\u4e8c\u8fdb\u5236\u7684\u9c81\u68d2\u72ec\u7acb\u57fa\u672c\u7279\u5f81\uff09](http://www.woshicver.com/Sixth/5_7_BRIEF\uff08\u4e8c\u8fdb\u5236\u7684\u9c81\u68d2\u72ec\u7acb\u57fa\u672c\u7279\u5f81\uff09/)\n    *   [5_8_ORB\uff08\u5b9a\u5411\u5feb\u901f\u548c\u65cb\u8f6c\u7b80\u8981\uff09](http://www.woshicver.com/Sixth/5_8_ORB\uff08\u5b9a\u5411\u5feb\u901f\u548c\u65cb\u8f6c\u7b80\u8981\uff09/)\n    *   [5_9_\u7279\u5f81\u5339\u914d](http://www.woshicver.com/Sixth/5_9_\u7279\u5f81\u5339\u914d/)\n    *   [5_10_\u7279\u5f81\u5339\u914d+\u5355\u5e94\u6027\u67e5\u627e\u5bf9\u8c61](http://www.woshicver.com/Sixth/5_10_\u7279\u5f81\u5339\u914d+\u5355\u5e94\u6027\u67e5\u627e\u5bf9\u8c61/)\n    \n*   <span class=\"caption-text\">\u89c6\u9891\u5206\u6790</span>\n\n    *   [6_1_\u5982\u4f55\u4f7f\u7528\u80cc\u666f\u5206\u79bb\u65b9\u6cd5](http://www.woshicver.com/Seventh/6_1_\u5982\u4f55\u4f7f\u7528\u80cc\u666f\u5206\u79bb\u65b9\u6cd5/)\n    *   [6_2_Meanshift\u548cCamshift](http://www.woshicver.com/Seventh/6_2_Meanshift\u548cCamshift/)\n    *   [6_3_\u5149\u6d41](http://www.woshicver.com/Seventh/6_3_\u5149\u6d41/)\n    \n*   <span class=\"caption-text\">\u76f8\u673a\u6821\u51c6\u548c3D\u91cd\u5efa</span>\n\n    *   [7_1_\u76f8\u673a\u6821\u51c6](http://www.woshicver.com/Eighth/7_1_\u76f8\u673a\u6821\u51c6/)\n    *   [7_2_\u59ff\u6001\u4f30\u8ba1](http://www.woshicver.com/Eighth/7_2_\u59ff\u6001\u4f30\u8ba1/)\n    *   [7_3_\u5bf9\u6781\u51e0\u4f55](http://www.woshicver.com/Eighth/7_3_\u5bf9\u6781\u51e0\u4f55/)\n    *   [7_4_\u7acb\u4f53\u56fe\u50cf\u7684\u6df1\u5ea6\u56fe](http://www.woshicver.com/Eighth/7_4_\u7acb\u4f53\u56fe\u50cf\u7684\u6df1\u5ea6\u56fe/)\n    \n*   <span class=\"caption-text\">\u673a\u5668\u5b66\u4e60</span>\n\n    *   [8_1_\u7406\u89e3KNN](http://www.woshicver.com/Ninth/8_1_\u7406\u89e3KNN/)\n    *   [8_2_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cKNN](http://www.woshicver.com/Ninth/8_2_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cKNN/)\n    *   [8_3_\u7406\u89e3SVM](http://www.woshicver.com/Ninth/8_3_\u7406\u89e3SVM/)\n    *   [8_4_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cSVM](http://www.woshicver.com/Ninth/8_4_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cSVM/)\n    *   [8_5_\u7406\u89e3K\u5747\u503c\u805a\u7c7b](http://www.woshicver.com/Ninth/8_5_\u7406\u89e3K\u5747\u503c\u805a\u7c7b/)\n    *   [8_6_OpenCV\u4e2d\u7684K\u5747\u503c](http://www.woshicver.com/Ninth/8_6_OpenCV\u4e2d\u7684K\u5747\u503c/)\n    \n*   <span class=\"caption-text\">\u8ba1\u7b97\u6444\u5f71\u5b66</span>\n\n    *   [9_1_\u56fe\u50cf\u53bb\u566a](http://www.woshicver.com/Tenth/9_1_\u56fe\u50cf\u53bb\u566a/)\n    *   [9_2_\u56fe\u50cf\u4fee\u8865](http://www.woshicver.com/Tenth/9_2_\u56fe\u50cf\u4fee\u8865/)\n    *   [9_3_\u9ad8\u52a8\u6001\u8303\u56f4](http://www.woshicver.com/Tenth/9_3_\u9ad8\u52a8\u6001\u8303\u56f4/)\n    \n*   <span class=\"caption-text\">\u76ee\u6807\u68c0\u6d4b</span>\n\n    *   [10_1_\u7ea7\u8054\u5206\u7c7b\u5668](http://www.woshicver.com/Eleventh/10_1_\u7ea7\u8054\u5206\u7c7b\u5668/)\n    *   [10_2_\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3](http://www.woshicver.com/Eleventh/10_2_\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3/)\n    \n*   <span class=\"caption-text\">OpenCV-Python Binding</span>\n\n    *   [11_1_OpenCV-Python Bindings](http://www.woshicver.com/Twelfth/11_1_OpenCV-Python%20Bindings/)\n\n# \u5173\u4e8e\n\nOpenCV \u4e2d\u6587\u5b98\u65b9\u6587\u6863\n\n[http://woshicver.com/](http://woshicver.com/)\n"
 },
 {
  "repo": "antoinelame/GazeTracking",
  "language": "Python",
  "readme_contents": "# Gaze Tracking\n\n![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)\n![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![GitHub stars](https://img.shields.io/github/stars/antoinelame/GazeTracking.svg?style=social)](https://github.com/antoinelame/GazeTracking/stargazers)\n\nThis is a Python (2 and 3) library that provides a **webcam-based eye tracking system**. It gives you the exact position of the pupils and the gaze direction, in real time.\n\n[![Demo](https://i.imgur.com/WNqgQkO.gif)](https://youtu.be/YEZMk1P0-yw)\n\n_\ud83d\ude80 Quick note: I'm looking for job opportunities as a software developer, for exciting projects in ambitious companies. Anywhere in the world. Send me an email!_\n\n## Installation\n\nClone this project:\n\n```\ngit clone https://github.com/antoinelame/GazeTracking.git\n```\n\nInstall these dependencies (NumPy, OpenCV, Dlib):\n\n```\npip install -r requirements.txt\n```\n\n> The Dlib library has four primary prerequisites: Boost, Boost.Python, CMake and X11/XQuartx. If you doesn't have them, you can [read this article](https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/) to know how to easily install them.\n\nRun the demo:\n\n```\npython example.py\n```\n\n## Simple Demo\n\n```python\nimport cv2\nfrom gaze_tracking import GazeTracking\n\ngaze = GazeTracking()\nwebcam = cv2.VideoCapture(0)\n\nwhile True:\n    _, frame = webcam.read()\n    gaze.refresh(frame)\n\n    new_frame = gaze.annotated_frame()\n    text = \"\"\n\n    if gaze.is_right():\n        text = \"Looking right\"\n    elif gaze.is_left():\n        text = \"Looking left\"\n    elif gaze.is_center():\n        text = \"Looking center\"\n\n    cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n    cv2.imshow(\"Demo\", new_frame)\n\n    if cv2.waitKey(1) == 27:\n        break\n```\n\n## Documentation\n\nIn the following examples, `gaze` refers to an instance of the `GazeTracking` class.\n\n### Refresh the frame\n\n```python\ngaze.refresh(frame)\n```\n\nPass the frame to analyze (numpy.ndarray). If you want to work with a video stream, you need to put this instruction in a loop, like the example above.\n\n### Position of the left pupil\n\n```python\ngaze.pupil_left_coords()\n```\n\nReturns the coordinates (x,y) of the left pupil.\n\n### Position of the right pupil\n\n```python\ngaze.pupil_right_coords()\n```\n\nReturns the coordinates (x,y) of the right pupil.\n\n### Looking to the left\n\n```python\ngaze.is_left()\n```\n\nReturns `True` if the user is looking to the left.\n\n### Looking to the right\n\n```python\ngaze.is_right()\n```\n\nReturns `True` if the user is looking to the right.\n\n### Looking at the center\n\n```python\ngaze.is_center()\n```\n\nReturns `True` if the user is looking at the center.\n\n### Horizontal direction of the gaze\n\n```python\nratio = gaze.horizontal_ratio()\n```\n\nReturns a number between 0.0 and 1.0 that indicates the horizontal direction of the gaze. The extreme right is 0.0, the center is 0.5 and the extreme left is 1.0.\n\n### Vertical direction of the gaze\n\n```python\nratio = gaze.vertical_ratio()\n```\n\nReturns a number between 0.0 and 1.0 that indicates the vertical direction of the gaze. The extreme top is 0.0, the center is 0.5 and the extreme bottom is 1.0.\n\n### Blinking\n\n```python\ngaze.is_blinking()\n```\n\nReturns `True` if the user's eyes are closed.\n\n### Webcam frame\n\n```python\nframe = gaze.annotated_frame()\n```\n\nReturns the main frame with pupils highlighted.\n\n## You want to help?\n\nYour suggestions, bugs reports and pull requests are welcome and appreciated. You can also starring \u2b50\ufe0f the project!\n\nIf the detection of your pupils is not completely optimal, you can send me a video sample of you looking in different directions. I would use it to improve the algorithm.\n\n## Licensing\n\nThis project is released by Antoine Lam\u00e9 under the terms of the MIT Open Source License. View LICENSE for more information.\n"
 },
 {
  "repo": "kyamagu/mexopencv",
  "language": "Matlab",
  "readme_contents": "mexopencv\n=========\n[![Travis](https://img.shields.io/travis/kyamagu/mexopencv/master.svg)][1]\n[![AppVeyor](https://img.shields.io/appveyor/ci/kyamagu/mexopencv/master.svg)][2]\n[![License](https://img.shields.io/badge/license-BSD%203--Clause-blue.svg)](LICENSE)\n\nCollection and development kit of MATLAB MEX functions for OpenCV library.\n\nThe package provides MATLAB MEX functions that interface with hundreds of\nOpenCV APIs. Also the package contains a C++ class that converts between\nMATLAB's native data type and OpenCV data types. The package is suitable for\nfast prototyping of OpenCV application in MATLAB, use of OpenCV as an external\ntoolbox in MATLAB, and development of custom MEX functions.\n\nThe current version of mexopencv is compatible with OpenCV 3.4.1.\n\nFor previous OpenCV 3.x versions, checkout the corresponding tags:\n\n- [v3.4.0][24]\n- [v3.3.1][23]\n- [v3.3.0][22]\n- [v3.2.0][21]\n- [v3.1.0][20]\n- [v3.0.0][19]\n\nFor OpenCV 2.x, checkout these older branches:\n\n- [v2.4][18] (last tested with OpenCV v2.4.11)\n- [v2.3][17]\n- [v2.1][16]\n\nConsult the [wiki][3] for help.\n\nTable of Contents\n=================\n\n- [Structure](#structure)\n- [Build](#build)\n    - [Linux](#linux)\n    - [OS X](#os-x)\n    - [Windows](#windows)\n- [Usage](#usage)\n    - [Documentation](#documentation)\n    - [Unit Testing](#unit-testing)\n- [License](#license)\n\nStructure\n=========\n\nThe project tree is organized as follows:\n\n    +cv/             OpenCV or custom API directory\n    +mexopencv/      mexopencv utility API directory\n    doc/             directory for documentation\n    include/         header files\n    lib/             directory for compiled C++ library files\n    samples/         directory for sample application codes\n    src/             directory for C++ source files\n    src/+cv/         directory for MEX source files\n    src/+cv/private/ directory for private MEX source files\n    test/            directory for test scripts and resources\n    opencv_contrib/  directory for sources/samples/tests of additional modules\n    utils/           directory for utilities\n    Doxyfile         config file for doxygen\n    Makefile         make script\n    README.markdown  this file\n\nBuild\n=====\n\nPrerequisite\n\n- [MATLAB][4] or [Octave][5] (>= 4.0.0)\n- [OpenCV][6] (3.4.1)\n\nDepending on your platform, you also need the required build tools:\n\n- Linux: g++, make, pkg-config\n- OS X: Xcode Command Line Tools, pkg-config\n- Windows: Visual Studio\n\nRefer to the `Makefile` and `make.m` scripts for a complete list of\noptions accepted for building mexopencv across supported platforms.\n\nRefer to the [wiki][3] for detailed build instructions.\n\nOpenCV\n------\n\nCurrently, mexopencv targets the final **3.4.1** stable version of OpenCV. You\nmust build it against this exact version, rather than using the bleeding-edge\ndev-version of `opencv` and `opencv_contrib`. UNIX users should consider using\na package manager to install OpenCV if available.\n\n- [OpenCV][7]\n- [OpenCV contributed modules][8]\n\n**DO NOT use the \"master\" branch of `opencv` and `opencv_contrib`!**\n**Only the 3.4.1 release is supported by mexopencv.**\n\nLinux\n-----\n\nFirst make sure you have OpenCV 3.4.1 installed in the system:\n\n- if applicable, install OpenCV 3 package available in your package manager\n  (e.g., `libopencv-dev` in Debian/Ubuntu, `opencv-devel` in Fedora).\n  Note that these packages are not always up-to-date, so you might need to use\n  older mexopencv versions to match their [opencv package][9] version.\n- otherwise, you can always build and install OpenCV from [source][7]:\n\n        $ cd <opencv_build_dir>\n        $ cmake <options> <opencv_src_dir>\n        $ make\n        $ sudo make install\n\nAt this point, you should make sure that the [`pkg-config`][10] command can\nidentify and locate OpenCV libraries (if needed, set the `PKG_CONFIG_PATH`\nenvironment variable to help it find the `opencv.pc` file):\n\n    $ pkg-config --cflags --libs opencv\n\nIf you have all the prerequisites, go to the mexopencv directory and type:\n\n    $ make\n\nThis will build and place all MEX functions inside `+cv/`. Specify your MATLAB\ndirectory if you installed MATLAB to a non-default location:\n\n    $ make MATLABDIR=/opt/local/MATLAB/R2017a\n\nYou can also work with [Octave][5] instead of MATLAB by specifying:\n\n    $ make WITH_OCTAVE=true\n\nTo enable support for contributed modules, you must build OpenCV from both\n[`opencv`][7] and [`opencv_contrib`][8] sources. You can then compile\nmexopencv as:\n\n    $ make all contrib\n\nFinally you can test mexopencv functionality:\n\n    $ make test\n\nDeveloper documentation can be generated with Doxygen if installed:\n\n    $ make doc\n\nThis will create HTML files under `doc/`.\n\nRefer to the wiki for detailed instructions on how to compile OpenCV for both\n[MATLAB][14] and [Octave][15].\n\nOS X\n----\n\nCurrently, the recommended approach to install OpenCV in OS X is\n[Homebrew][11]. Install Homebrew first, and do the following to install\nOpenCV 3:\n\n    $ brew install pkg-config homebrew/science/opencv3\n    $ brew link opencv3\n\nOtherwise, you can build OpenCV from [source][7], similar to the Linux case.\n\nIf you have all the prerequisites, go to the mexopencv directory and run\n(modifying the options as needed):\n\n    $ make MATLABDIR=/Applications/MATLAB_R2016a.app PKG_CONFIG_MATLAB=opencv3 LDFLAGS=-L/usr/local/share/OpenCV/3rdparty/lib -j2\n\nReplace the path to MATLAB with your version. This will build and place all\nMEX functions inside `+cv/`.\n\nWindows\n-------\n\nRefer to [the wiki][13] for detailed instructions on how to compile OpenCV\non Windows, and build mexopencv against it.\n\nIn a nutshell, execute the following in MATLAB to compile mexopencv:\n\n    >> addpath('C:\\path\\to\\mexopencv')\n    >> mexopencv.make('opencv_path','C:\\OpenCV\\build')\n\nReplace the path above with the location where OpenCV binaries are installed\n(i.e location specified in `CMAKE_INSTALL_PREFIX` while building OpenCV).\n\nContrib modules are enabled as:\n\n    >> addpath('C:\\path\\to\\mexopencv')\n    >> addpath('C:\\path\\to\\mexopencv\\opencv_contrib')\n    >> mexopencv.make('opencv_path','C:\\OpenCV\\build', 'opencv_contrib',true)\n\nIf you have previously compiled mexopencv with a different configuration,\ndon't forget to clean old artifacts before building:\n\n    >> mexopencv.make('clean',true, 'opencv_contrib',true)\n\nUsage\n=====\n\nOnce MEX functions are compiled, you can add path to the project directory and\ncall MEX functions within MATLAB using package name `cv`.\n\n``` matlab\naddpath('/path/to/mexopencv');\naddpath('/path/to/mexopencv/opencv_contrib');\n\n% recommended\nout = cv.filter2D(img, kern);  % with package name 'cv'\n\n% not recommended\nimport cv.*;\nout = filter2D(img, kern);     % no need to specify 'cv' after imported\n```\n\nNote that some functions such as `cv.imread` will overload MATLAB's built-in\n`imread` function when imported. Use the scoped name when you need to avoid\nname collision. It is also possible to import individual functions. Check\n`help import` in MATLAB.\n\nCheck a list of functions available by `help` command in MATLAB.\n\n``` matlab\n>> help cv;              % shows list of functions in package 'cv'\n\n>> help cv.VideoCapture; % shows documentation of VideoCapture\n```\n\nLook at the `samples/` directory for examples.\n\nDocumentation\n-------------\n\nmexopencv includes a simple documentation utility that generates HTML help\nfiles for MATLAB. The following command creates HTML user documentation\nunder `doc/matlab/` directory.\n\n``` matlab\naddpath('/path/to/mexopencv/utils');\nMDoc;\n```\n\nOn-line documentation is [available][12].\n\nUnit Testing\n------------\n\nYou can test the functionality of compiled files by `UnitTest` class located\ninside `test` directory.\n\n``` matlab\naddpath('/path/to/mexopencv/test');\nUnitTest;\n```\n\nLook at the `test/unit_tests/` directory for all unit-tests.\n\nLicense\n=======\n\nThe code may be redistributed under the [BSD 3-Clause license](LICENSE).\n\n\n[1]: https://travis-ci.org/kyamagu/mexopencv\n[2]: https://ci.appveyor.com/project/kyamagu/mexopencv\n[3]: https://github.com/kyamagu/mexopencv/wiki\n[4]: https://www.mathworks.com/products/matlab.html\n[5]: https://www.gnu.org/software/octave/\n[6]: https://opencv.org/\n[7]: https://github.com/opencv/opencv/releases/tag/3.4.1\n[8]: https://github.com/opencv/opencv_contrib/releases/tag/3.4.1\n[9]: https://packages.ubuntu.com/bionic/libopencv-dev\n[10]: https://people.freedesktop.org/~dbn/pkg-config-guide.html\n[11]: https://brew.sh/\n[12]: http://kyamagu.github.io/mexopencv/matlab\n[13]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Windows%2C-MATLAB%2C-OpenCV-3%29\n[14]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Linux%2C-MATLAB%2C-OpenCV-3%29\n[15]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Linux%2C-Octave%2C-OpenCV-3%29\n[16]: https://github.com/kyamagu/mexopencv/tree/v2.1\n[17]: https://github.com/kyamagu/mexopencv/tree/v2.3\n[18]: https://github.com/kyamagu/mexopencv/tree/v2.4\n[19]: https://github.com/kyamagu/mexopencv/tree/v3.0.0\n[20]: https://github.com/kyamagu/mexopencv/tree/v3.1.0.1\n[21]: https://github.com/kyamagu/mexopencv/tree/v3.2.0\n[22]: https://github.com/kyamagu/mexopencv/tree/v3.3.0\n[23]: https://github.com/kyamagu/mexopencv/tree/v3.3.1\n[24]: https://github.com/kyamagu/mexopencv/tree/v3.4.0\n"
 },
 {
  "repo": "jeffbass/imagezmq",
  "language": "Python",
  "readme_contents": "====================================\nimageZMQ: Transporting OpenCV images\n====================================\n\nIntroduction\n============\n\n**imageZMQ** is a set of Python classes that transport OpenCV images from one\ncomputer to another using PyZMQ messaging. For example, here is a screen on a\nMac computer showing simultaneous video streams from 8 Raspberry Pi cameras:\n\n.. image:: docs/images/screenshottest.png\n\nUsing **imageZMQ**, this is possible with 11 lines of Python on each Raspberry\nPi and with 8 lines of Python on the Mac.\n\nFirst, run this code on the Mac (or other display computer):\n\n.. code-block:: python\n  :number-lines:\n\n    # run this program on the Mac to display image streams from multiple RPis\n    import cv2\n    import imagezmq\n    image_hub = imagezmq.ImageHub()\n    while True:  # show streamed images until Ctrl-C\n        rpi_name, image = image_hub.recv_image()\n        cv2.imshow(rpi_name, image) # 1 window for each RPi\n        cv2.waitKey(1)\n        image_hub.send_reply(b'OK')\n\n\nThen, on each Raspberry Pi, run:\n\n.. code-block:: python\n  :number-lines:\n\n    # run this program on each RPi to send a labelled image stream\n    import socket\n    import time\n    from imutils.video import VideoStream\n    import imagezmq\n\n    sender = imagezmq.ImageSender(connect_to='tcp://jeff-macbook:5555')\n\n    rpi_name = socket.gethostname() # send RPi hostname with each image\n    picam = VideoStream(usePiCamera=True).start()\n    time.sleep(2.0)  # allow camera sensor to warm up\n    while True:  # send images as stream until Ctrl-C\n        image = picam.read()\n        sender.send_image(rpi_name, image)\n\n\nWow! A video surveillance system with 8 (or more!) Raspberry Pi cameras in\n19 lines of Python.\n\nSee `About the multiple RPi video streaming examples <docs/more-details.rst>`_\nfor more details about this example.\n\n.. contents::\n\nWhy use imageZMQ?\n=================\n\n**imageZMQ** is an easy to use image transport mechanism for a distributed image\nprocessing network. For example, a network of a dozen Raspberry Pis with cameras\ncan send images to a more powerful central computer. The Raspberry Pis perform\nimage capture and simple image processing like flipping, blurring and motion\ndetection. Then the images are passed via **imageZMQ** to the central computer for\nmore complex image processing like image tagging, text extraction, feature\nrecognition, etc. An example of using **imageZMQ** can be found\nat `Using imageZMQ in distributed computer vision projects. <docs/imagezmq-uses.rst>`_\n\nFeatures\n========\n\n- Sends OpenCV images from one computer to another using ZMQ.\n- Can send jpeg compressed OpenCV images, to lighten network loads.\n- Uses the powerful ZMQ messaging library through PyZMQ bindings.\n- Allows a choice of 2 different ZMQ messaging patterns (REQ/REP or PUB/SUB).\n- Enables the image hub to receive and process images from multiple image senders\n  simultaneously.\n\nWhy ZMQ? Why not some other messaging protocol?\n===============================================\n\nThere are a number of high quality and well maintained messaging protocols for\npassing messages between computers. I looked at MQTT, RabbitMQ, AMQP and ROS as\nalternatives. I chose ZMQ and its Python PyZMQ bindings for several reasons:\n\n- ZMQ does not require a message broker. It is a peer to peer protocol that does\n  not need to pass an image first to a message broker and then to the imagehub.\n  This means fewer running processes and less \u201cdouble handling\u201d of images.\n  OpenCV images are large compared to simple text messages, so the absence of a\n  message broker is important.\n- ZMQ is very fast for passing OpenCV images. It enables high throughput between\n  image senders and image hubs.\n- ZMQ and its PyZMQ bindings are easy to install.\n\n**imageZMQ** has been transporting images from a dozen Raspberry Pi computers\nscattered around my farm to 2 linux image hub servers for over 2\nyears. The RPi's capture and send dozens to thousands of frames frames a day.\n**imageZMQ** has worked very reliably and is very fast. You can learn more about\nmy \"science experiment urban permaculture farm\" project at\n`Yin Yang Ranch project overview. <https://github.com/jeffbass/yin-yang-ranch>`_\n\n\nMessaging Patterns: REQ/REP versus PUB/SUB\n==========================================\n\nZMQ allows many different messaging patterns. Two are implemented in **imageZMQ**:\n\n- REQ/REP: Each RPi sends an image and waits for a REPLY from the central image\n  hub. The RPi sends a new image only when the REPLY is received. In the REQ/REP\n  messaging pattern, each image sender must await a REPLY before continuing. It is a\n  \"blocking\" pattern for the sender.\n- PUB/SUB: Each RPi sends an image, but does not expect a REPLY from the central\n  image hub. It can continue sending images without awaiting any acknowledgement\n  from the image hub. The image hub provides no REPLY. It is a \"non-blocking\"\n  pattern for the sender.\n\nThere are advantages and disadvantages for each pattern. For further details,\nsee: `REQ/REP versus PUB/SUB Messaging Patterns <docs/req-vs-pub.rst>`_.\n**REQ/REP is the default.**\n\n\nDependencies and Installation\n=============================\n\n+--------------+--------+---------------+-----------+-------+\n| |pyversions| | |pypi| | |releasedate| | |license| | |doi| |\n+--------------+--------+---------------+-----------+-------+\n\n.. |pyversions| image:: /docs/images/python_versions.svg\n   :target: https://pypi.org/project/imagezmq/\n\n.. |pypi| image:: /docs/images/pypi_version.svg\n   :target: https://pypi.org/project/imagezmq/\n\n.. |releasedate| image:: /docs/images/release_date.svg\n   :target: https://github.com/jeffbass/imagezmq/releases/tag/v1.1.1\n\n.. |license| image::  /docs/images/license.svg\n   :target: LICENSE.txt\n\n.. |doi| image::  /docs/images/doi.svg\n   :target: https://doi.org/10.5281/zenodo.3840855\n\n**imageZMQ** has been tested with:\n\n- Python 3.5, 3.6, 3.7 and 3.8\n- PyZMQ 16.0, 17.1 and 19.0\n- Numpy 1.13, 1.16 and 1.18\n- OpenCV 3.3, 4.0 and 4.1\n- Raspbian Buster, Raspbian Stretch and Raspbian Jessie\n- picamera 1.13 (used to capture images for the tests)\n- imutils 0.4.6, 0.5.2 and 0.5.3 (used to capture images from PiCamera)\n\nOpenCV is challenging to install. I recommend using the installation\ninstructions at `PyImageSearch <https://www.pyimagesearch.com/>`_.\nAdrian Rosebrock's PyImageSearch blog and books are great resources for\nlearning about and installing OpenCV on Raspberry Pi's, Macs and\nLinux computers.\n\n- `Raspbian Stretch: Install OpenCV 3 + Python on your Raspberry Pi\n  <https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/>`_\n- `macOS: Install OpenCV 3 and Python 3.5\n  <https://www.pyimagesearch.com/2016/12/05/macos-install-opencv-3-and-python-3-5/>`_\n- `Ubuntu 16.04: How to install OpenCV\n  <https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/>`_\n\nBe sure to install OpenCV, including Numpy, into a Python Virtual Environment,\nas shown in the above tutorials. Be sure to install **imageZMQ**\ninto the **same** virtual environment. For example, my virtual\nenvironment is named **py3cv3**.\n\nInstall **imageZMQ** using pip:\n\n.. code-block:: bash\n\n    workon py3cv3  # use your virtual environment name\n    pip install imagezmq\n\n**imageZMQ** has a directory of tests organized into sender and receiver pairs.\nYou will get the \"tests\" directory containing all the test programs by\ncloning the GitHub repository:\n\n.. code-block:: bash\n\n    git clone https://github.com/jeffbass/imagezmq.git\n\nOnce you have cloned the imagezmq directory to a directory on your local machine,\nyou can run the tests per the instructions below. You can use imageZMQ in your\nown code by importing it (``import imagezmq``).\n\n**imageZMQ** and all of the software dependencies must be installed on the\ndisplay computer that will be receiving the images AND it must all be installed\non every Raspberry Pi that will be sending images. If you will be using multiple\nRaspberry Pis to capture and send images it is best to install the software on a\nsingle Raspberry Pi and test it using the tests below. Once all the tests\nhave run successfully, clone the SD card as needed to use the software on\nmultiple Raspberry Pis.\n\nRunning the Tests\n=================\n\nWhen running the tests, use multiple terminal windows on the computer that will\nbe displaying the images (I used a Mac for these examples; in my descriptions\nI use the term \"Mac\" to refer to any Mac or Linux computer, including a\nRaspbery Pi). One terminal window is used to launch the programs that run on the\nMac to receive the images. Another terminal window on the Mac is used to ssh\ninto the Raspberry Pi and run the image sending program. If sending from multiple\nRaspberry Pis, ssh to each Raspberry Pi in a new terminal window. **imageZMQ**\nand its dependencies must be installed on the Mac and on each Raspberry Pi that\nwill be sending images.\n\nThere are 3 tests. Each of the tests uses 2 programs in matched pairs. One\nprogram sends images and the other program displays images. Because of the\nREQ/REP pattern that is being used, it is important that the receiving program\nbe started before the sending program.\n\n**imageZMQ** is in early development as part of a larger system. There are\ncurrently separate methods for sending and receiving images vs. jpg compressed\nimages. Further development will refactor these into single methods for sending\nand receiving. ::\n\nTest 1: Simple generated images sent and displayed on Mac\n---------------------------------------------------------\n**The first test** runs both the sending program and the receiving program on\nthe Mac. This confirms that all the software is installed correctly and that\n``cv2.imshow()`` works on the Mac. No Raspberry Pi or camera is involved. The\nsending program generates test images and sends them to the receiving program.\nFirst, in one terminal window, activate your virtual environment, then change to\nthe tests directory and run the receiving program, which will receive and\ndisplay images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_1_receive_images.py\n\nThen, in a second terminal window on the same display computer (Mac), change to\nthe tests directory and run the sending program, which will generate and send\nimages::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_1_send_images.py\n\nAfter a few seconds, a ``cv2.imshow()`` window should open and display a green\nsquare on a black background. There will be a yellow number in the green square\nthat will increase (1, 2, ...) once per second until you stop both\nprograms by pressing Ctrl-C. It is normal to get a cascade of error messages\nwhen stopping the program with Ctrl-C. This simple test program has no\ntry / except error trapping.\n\nTest 2: Sending stream of OpenCV images from RPi(s) to Mac\n----------------------------------------------------------\n**The second test** runs the sending program on a Raspberry Pi, capturing\nimages from the PiCamera at up to 32 frames a second and sending them via\n**imageZMQ** to the Mac. The receiving program on the Mac displays a continuous\nvideo stream of the images captured by the Raspberry Pi. First, in one terminal\nwindow, activate your virtual environment, change to the tests directory and\nrun the receiving program which will display the images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_2_receive_images.py\n\nThen, in a second terminal window on the Mac, ssh into the Raspberry Pi that\nwill be sending images. Activate your Python virtual environment, change to the\ntests directory and **edit the test_2_send_images.py program to specify the tcp\naddress of your display computer.** There are 2 lines in the program that show\ndifferent ways of specifying the tcp address: by hostname or by tcp numeric address.\nPick one method, change the tcp address to that of your display computer and\ncomment out the method you are not using. Finally, run the program, which will\ncapture and send images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_2_send_images.py\n\nIn about 5 seconds, a ``cv2.imshow()`` window will appear on the Mac and display\nthe video stream being sent by the Raspberry Pi.  You can repeat this step in\nadditional terminal windows, to ssh into additional Raspberry Pi computers and\nstart additional video streams. The receiving program can receive and display\nimages from multiple Raspberry Pis, with each Raspberry Pi's image stream\nshowing in a separate window. For this to work, each Raspberry Pi must have a\nunique hostname because the images are sorted into different\n``cv2.imshow()`` windows based on the hostname. The ``cv2.imshow()`` windows\nfor multiple Raspberry Pi streams will be stacked on top of each other until you\ndrag them and arrange them on your desktop. The example picture at the start of\nthis documentation shows 8 simultaneous video streams for 8 Raspberry Pi\ncomputers with different hostnames. Each program must be stopped by pressing\nCtrl-C in its terminal window. It is normal to get a cascade of error messages\nwhen stopping these programs with Ctrl-C. This simple test program has no try /\nexcept error trapping.\n\nTest 3: Sending stream of jpgs from RPi(s) to Mac\n-------------------------------------------------\n**The third test** runs a different pair of sending / receiving programs. The\nprogram on the Raspberry Pi captures images from the PiCamera at up to 32\nframes a second and **compresses them to jpeg form** before sending them via\n**imageZMQ** to the Mac. The receiving program on the Mac converts the jpg\ncompressed frames back to OpenCV images and displays them as a continuous video\nstream. This jpeg compression can greatly reduce the network load of sending many\nimages from multiple sources.\n\nThe programs that send and receive the images using jpg compression are run in\nthe same way as the above pair of programs that send uncompressed images. Use\nthe instructions above as a guide. The programs for Test 3 are::\n\n    test_3_receive_jpg.py  # run on the Mac to receive & decompress images\n    test_3_send_jpg.py     # ron on each Raspberry Pi to compress & send images\n\nAs with the previous Test 2 program pair, you will need to edit the \"connect_to\"\naddress in the sending program to the tcp address of your Mac (or other display\ncomputer).  You will also need to remember to start the *receive* program on the\nMac before you start the sending program on the Raspberry Pi. As before, each\nprogram must be stopped by pressing Ctrl-C in its terminal window. It is normal\nto get a cascade of error messages when stopping these programs with Ctrl-C.\nThis simple test program has no try / except error trapping. Be sure to activate\nyour virtual environment as you did for Test 2 (see above) before running these\ntests.\n\nTest 4: Using PUB/SUB to send simple generated images and display them on Mac\n-----------------------------------------------------------------------------\n**The fourth test** is a repeat of Test 1, but uses the PUB/SUB messaging\npattern instead of the REQ/REP messaging pattern. It shows the differences\nin running PUB/SUB versus REQ/REP in the simplest possible test program.\n\nTest 4 runs both the sending program and the receiving program on\nthe Mac. No Raspberry Pi or camera is involved. This test shows the start / stop\nflexibility of the PUB/SUB pattern. All 3 of the above REQ/REP tests require\nthat the receiving program be started first, then the sending program. And they\nrequire that the sending program be restarted if the receiving program is\nrestarted. This is standard behavior for the REQ/REP messaging pattern. But\nthis test shows that either PUB/SUB program can be started first and that\nmessage sending will resume if either program is restarted. That is a feature\nof the PUB/SUB messaging pattern. See other documentation listed below for\nfurther differences, advantages and disadvantages of the REQ/REP versus PUB/SUB\nmessaging patterns.\n\nThe sending program generates test images and sends them to the receiving program.\nFirst, in one terminal window, activate your virtual environment, then change to\nthe tests directory and run the receiving program, which will receive and\ndisplay images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_4_pub.py\n\nThen, in a second terminal window on the same display computer (Mac), change to\nthe tests directory and run the sending program, which will generate and send\nimages::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_4_sub.py\n\nAfter a few seconds, a ``cv2.imshow()`` window should open and display a green\nsquare on a black background. There will be a yellow number in the green square\nthat will increase (1, 2, ...) once per second. Now you can stop either\nprogram and restart it and see that the sending of numbers continues and picks\nup where it left off (though some transmitted images may have been skipped\nduring restart). It is normal to get a cascade of error messages\nwhen starting and stopping the program with Ctrl-C. These simple test program\nhave no try / except error trapping, since their only purpose is this simple\ndemonstration.\n\nTiming tests: Complete imageZMQ usage examples\n==============================================\nThe test programs above are short and simple. They test that the software and\ndependencies are installed correctly and that images transfer successfully between\na Raspberry Pi computer and a display computer such as a Mac.  The tests\ndirectory contains 2 more send / receive program pairs that provide a more\ncomplete example of imageZMQ usage. Each of these programs includes\ntry / except blocks that enable ending the programs by typing Ctrl-C\nwithout starting a cascade of error messages. They also perform frames per\nsecond (FPS) timing tests that measure the speeds of image transfer using the\ncompressed versus the non-compressed transfer methods. They also show how to\ncapture the hub response in the sending program, which wasn't needed in the\nsimple tests.\n\nOne pair of programs transmits and receives **OpenCV images** and measures FPS::\n\n    timing_receive_images.py  # run on Mac to display images\n    timing_send_images.py     # run on Raspberry Pi to send images\n\nAnother pair of programs transmits and receives **jpg compressed images** and\nmeasures FPS::\n\n    timing_send_jpg_buf.py     # run on Raspberry Pi to send images\n    timing_receive_jpg_buf.py  # run on Mac to display images\n\nAs with the other test program pairs, you will need to edit the \"connect_to\"\naddress in the sending program to the tcp address of your Mac (or other display\ncomputer).  You will also need to remember to start the *receive* program on the\nMac before you start the sending program on the Raspberry Pi. With these programs,\nthe try / except blocks will end the programs cleanly with no errors when you\npress Ctrl-C. Be sure to activate your virtual environment before running these\ntests.\n\nAdditional Documentation and Examples\n=====================================\n- `API and Two Simple Example Programs <docs/api-examples.rst>`_\n- `More details about the multiple RPi video streaming example <docs/more-details.rst>`_\n- `REQ/REP versus PUB/SUB Messaging Patterns <docs/req-vs-pub.rst>`_\n- `Examples showing different techniques for using imageZMQ <docs/examples.rst>`_\n- `Using imageZMQ in distributed computer vision projects <docs/imagezmq-uses.rst>`_\n- `FAQ: Frequently Asked Questions <docs/FAQ.rst>`_\n- How **imageZMQ** is used in my own projects connecting multiple\n  Raspberry Pi **imagenodes** to an **imagehub**:\n\n  - My Yin Yang Ranch project to manage a small urban permaculture farm:\n    `Yin Yang Ranch project overview <https://github.com/jeffbass/yin-yang-ranch>`_\n  - `imagenode: Capture and Send Images and Sensor Data <https://github.com/jeffbass/imagenode>`_\n  - `imagehub: Receive and Store Images and Event Logs <https://github.com/jeffbass/imagehub>`_\n\n\nI gave a talk about imageZMQ and its use in my Yin Yang Ranch project at\nPyCon 2020:\n**Jeff Bass - Yin Yang Ranch: Building a Distributed Computer\nVision Pipeline using Python, OpenCV and ZMQ**\n\n`PyCon 2020 Talk Video about the project  <https://youtu.be/76GGZGneJZ4?t=2>`_\n\n`PyCon 2020 Talk Presentation slides  <https://speakerdeck.com/jeffbass/yin-yang-ranch-building-a-distributed-computer-vision-pipeline-using-python-opencv-and-zmq-17024000-4389-4bae-9e4d-16302d20a5b6>`_\n\nContributing\n============\n**imageZMQ** is still in active development. I welcome open issues and\npull requests, but because the programs are still evolving, it is best to\nopen an issue for some discussion before submitting pull requests. We can\nexchange ideas about your potential pull request and open a development branch\nwhere you can develop your code and get feedback and testing help from myself\nand others. **imageZMQ** is used in my own long running projects and the\nprojects of others, so backwards compatibility with the existing API is\nimportant.\n\nContributors\n============\nThanks for all contributions big and small. Some significant ones:\n\n+------------------------+-----------------+----------------------------------------------------------+\n| **Contribution**       | **Name**        | **GitHub**                                               |\n+------------------------+-----------------+----------------------------------------------------------+\n| Initial code & docs    | Jeff Bass       | `@jeffbass <https://github.com/jeffbass>`_               |\n+------------------------+-----------------+----------------------------------------------------------+\n| Added PUB / SUB option | Maksym          | `@bigdaddymax <https://github.com/bigdaddymax>`_         |\n+------------------------+-----------------+----------------------------------------------------------+\n| HTTP Streaming example | Maksym          | `@bigdaddymax <https://github.com/bigdaddymax>`_         |\n+------------------------+-----------------+----------------------------------------------------------+\n| Fast PUB / SUB example | Philipp Schmidt | `@philipp-schmidt <https://github.com/philipp-schmidt>`_ |\n+------------------------+-----------------+----------------------------------------------------------+\n\nHelpful Forks of imageZMQ\n=========================\nSome users have come up with Forks of **imageZMQ** that I think will be helpful\nto others, either by using their code or reading their changed code. If\nyou have developed a fork of **imageZMQ** that demonstrates a concept that\nwould be helpful to others, please open an issue describing your fork so we\ncan have a discussion first rather than opening a pull request. Thanks!\n\n+----------------------------+------------+----------------------------------------------------------------------+\n| **Helpful Fork**           | **Name**   | **GitHub repository of fork**                                        |\n+----------------------------+------------+----------------------------------------------------------------------+\n| Add timeouts to image      | Pat Ryan   | `@youngsoul <https://github.com/youngsoul/imagezmq>`_ See CHANGES.md |\n| sender to fix restarts or  |            |                                                                      |\n| non-response of ImageHub   |            |                                                                      |\n+----------------------------+------------+----------------------------------------------------------------------+\n\nAcknowledgements and Thank Yous\n===============================\n- **ZeroMQ** is a great messaging library with great documentation\n  at `ZeroMQ.org <http://zeromq.org/>`_.\n- **PyZMQ** serialization examples provided a starting point for **imageZMQ**. See the\n  `PyZMQ documentation <https://pyzmq.readthedocs.io/en/latest/index.html>`_.\n- **OpenCV** and its Python bindings provide great scaffolding for computer\n  vision projects large or small: `OpenCV.org <https://opencv.org/>`_.\n- **PyImageSearch.com** is the best resource for installing OpenCV and its Python\n  bindings. Adrian Rosebrock provides many practical OpenCV techniques with\n  tutorials, code examples, blogs\n  and books at `PyImageSearch.com <https://www.pyimagesearch.com/>`_. Installing\n  OpenCV on my Raspberry Pi computers, Macs and Linux boxes went from\n  frustrating to easy thanks to his tutorials. I also learned a **LOT** about\n  computer vision methods and techniques by taking his PyImageSearch Gurus\n  course. Highly recommended.\n- **imutils** is a collection of Python classes and methods that allows computer\n  vision programs using OpenCV to be cleaner and more compact. It has a very\n  helpful threaded image reader for Raspberry PiCamera modules or webcams. It\n  allowed me to shorten my camera reading programs on the Raspberry Pi by half:\n  `imutils on GitHub <https://github.com/jrosebr1/imutils>`_. **imutils** is an\n  open source project authored by Adrian Rosebrock.\n"
 },
 {
  "repo": "techfort/pycv",
  "language": "Python",
  "readme_contents": "# pycv\n\n---\n\n*&#9758; **This repository is no longer actively maintained.** Please consider switching to the [next edition's repository](https://github.com/PacktPublishing/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition), which officially supports OpenCV 4.x and should generally work with OpenCV 3.4 as well. Over there, you will find many bugfixes, improvements, and updates.*\n\n---\n\n## Learning OpenCV 3 with Python - Second Edition\n\nThis is the repository and reference website for [Learning OpenCV 3 Computer Vision with Python](https://www.packtpub.com/application-development/learning-opencv-3-computer-vision-python-second-edition), a book authored by [Joe Minichino](https://github.com/techfort) and [Joe Howse](https://github.com/JoeHowse), and published by Packt Publishing.\n\nThe book also has a Simplified Chinese edition, [OpenCV 3\u8ba1\u7b97\u673a\u89c6\u89c9:Python\u8bed\u8a00\u5b9e\u73b0](http://hzbook.com/Books/9290.html), translated by Liu Bo, Miao Beibei, and Shi Bin, and published by HZ Books / China Machine Press. This repository might also help readers of OpenCV 3\u8ba1\u7b97\u673a\u89c6\u89c9:Python\u8bed\u8a00\u5b9e\u73b0, but for Chinese-language support please contact HZ Books.\n\n### Code\nCode is divided in chapters reflecting the samples contained in the book. Feel free to report errors either by:\n* opening an issue on github; or\n* contacting the authors directly (if you don't have a GitHub account).\n\nGithub is the preferred way as it grants visibility to all issues reported.\n\n### Virtual Machine\nI (Joe Minichino) created a VM for VirtualBox which allows you to skip installation steps and jump straight into action. However, I am yet to find a free storage service where to upload the 12GB file to make it freely available. This is currently being addressed so check to this page constantly.\n"
 },
 {
  "repo": "hrastnik/FaceSwap",
  "language": "C++",
  "readme_contents": "# [Youtube video](https://youtu.be/32i1ca8pcTg)\n\n# How to run?\n\nDownload [OpenCV](http://opencv.org/downloads.html) and [dlib](http://dlib.net/)\n\n- Setup OpenCV\n    - Run the downloaded OpenCV executable file and select a directory to extract the OpenCV library (for example D:\\opencv\\)\n- Setup dlib\n    - Extract the downloaded zip file to a directory (for example D:\\dlib)\n- Download and install Visual Studio 2015 or later versions\n- Run Visual Studio\n- Create new empty project and name it something (for example MyProject)\n- Make sure the \"Debug\" solution configuration is selected\n- In Visual Studio open the Solution Explorer window\n- Right click MyProject and choose Properties\n- Click the \"Configuration Manager...\" button in the top left corner\n- Setup the configuration for the debug\n  - In the active solution platform select x64\n  - Close the Configuration Manager window\n  - In the property window make sure the selected Configuration in the top left is \"Debug\" and Platform is \"x64\"\n  - In the panel on the left choose C/C++\n  - In the Additional Include Directories field add two directories:\n      - \"D:\\opencv\\opencv\\build\\include\"\n      - \"D:\\dlib\\dlib-19.2\"\n      * Note the path might be different if you have different dlib version\n  - In the panel on the left choose Linker>General\n  - In the Additional Library Directories add \"D:\\opencv\\opencv\\build\\x64\\vc14\\lib\"\n      * Note the path might be different if you have different architecture or VS version\n  - In the panel on the left choose Linker>Input\n  - In the Additional Dependencies add \"opencv_world320d.lib\"\n  - Click Apply\n  \n- Change the Configuration in the top left to \"Release\" and repeat \n- Setup the configuration for the release\n  - In the panel on the left choose C/C++\n  - In the Additional Include Directories field add two directories:\n      - \"D:\\opencv\\opencv\\build\\include\"\n      - \"D:\\dlib\\dlib-19.2\"\n      * Note the path might be different if you have different dlib version\n  - In the panel on the left choose Linker>General\n  - In the Additional Library Directories add \"D:\\opencv\\opencv\\build\\x64\\vc14\\lib\"\n      * Note the path might be different if you have different architecture or VS version\n  - In the panel on the left choose Linker>Input\n  - In the Additional Dependencies add \"opencv_world320.lib\"\n\n- Close the property window\n- Right click Source Files in the Solution Explorer\n- Select \"Add Existing Item...\" and add the .cpp files from this project\n- Right click Header Files in the Solution Explorer\n- Select \"Add Existing Item...\" and add the .h files from this project\n- Copy haarcascade_frontalface_default.xml from OpenCV sources/data/haarcascades directory to project directory\n- Download shape_predictor_68_face_landmarks.dat from http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2 and place in project directory \n\nAfter that FaceSwap should work. \n\n# Building on GNU/Linux\n\nIf you want to run this on Ubuntu 16.04 run this set of commands:\n\n    sudo apt install libopencv-dev liblapack-dev libdlib-dev\n    wget http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2\n    bunzip2 *.bz2\n    ln -s /usr/share/opencv/haarcascades/haarcascade_frontalface_default.xml .\n\n    g++ -std=c++1y *.cpp $(pkg-config --libs opencv lapack) -ldlib \n    ./a.out\n    \nSpecial thanks to https://github.com/nqzero for providing the build commands.\n\n# Building on MacOS\n\nSpecial thanks to https://github.com/shaunharker for providing the build commands.\n\n    brew install lapack\n    brew install openblas\n    brew install opencv\n    brew install dlib --with-openblas\n    git clone https://github.com/hrastnik/FaceSwap.git\n    cd FaceSwap\n    wget http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2\n    bunzip2 *.bz2\n    ln -s /usr/local/share/opencv/haarcascades/haarcascade_frontalface_default.xml .\n    export PKG_CONFIG_PATH=/usr/local/opt/lapack/lib/pkgconfig:/usr/local/opt/openblas/lib/pkgconfig:$PKG_CONFIG_PATH\n    g++ -std=c++1y *.cpp $(pkg-config --libs opencv lapack openblas) -ldlib\n    mkdir bin\n    mv a.out bin\n    cd bin\n    ./a.out\n\n# How does it work?\n\nThe algorithm searches until it finds two faces in the frame. Then it estimates facial landmarks using dlib face landmarks. Facial landmarks are used to \"cut\" the faces out of the frame and to estimate the transformation matrix used to move one face over the other.\n\nThe faces are then color corrected using histogram matching and in the end the edges of the faces are feathered and blended in the original frame.\n\n# Result\nBefore...\n\n[![Before](./images/before.jpg)](https://youtu.be/32i1ca8pcTg)\n\nAfter...\n\n[![After](./images/after.jpg)](https://youtu.be/32i1ca8pcTg)\n"
 },
 {
  "repo": "ahmetozlu/vehicle_counting_tensorflow",
  "language": "Python",
  "readme_contents": "# VEHICLE DETECTION, TRACKING AND COUNTING\nThis sample project focuses on \"Vechicle Detection, Tracking and Counting\" using [**TensorFlow Object Counting API**](https://github.com/ahmetozlu/tensorflow_object_counting_api). ***Please contact if you need professional vehicle detection & tracking & counting project with the super high accuracy.***\n\n---\n\n***The [TensorFlow Object Counting API](https://github.com/ahmetozlu/tensorflow_object_counting_api) is used as a base for object counting on this project, more info can be found on this [repo](https://github.com/ahmetozlu/tensorflow_object_counting_api).***\n\n---\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/36344830-095cc4ec-1431-11e8-8e57-976c40d87cf9.gif\">\n</p>\n\n---\n\n***The developing is on progress! This sample project will be updated soon, the more talented traffic analyzer app will be available in this repo!***\n\n---\n\n## General Capabilities of This Sample Project\n\nThis sample project has more than just counting vehicles, here are the additional capabilities of it:\n\n- Detection and classification of the vehicles (car, truck, bicycle, motorcycle, bus)\n- Recognition of approximate vehicle color\n- Detection of vehicle direction of travel\n- Prediction the speed of the vehicle\n- Prediction of approximate vehicle size\n- **The images of detected vehicles are cropped from video frame and they are saved as new images under \"[detected_vehicles](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/detected_vehicles)\" folder path**\n- **The program gives a .csv file as an output ([traffic_measurement.csv](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/traffic_measurement.csv)) which includes \"Vehicle Type/Size\", \" Vehicle Color\", \" Vehicle Movement Direction\", \" Vehicle Speed (km/h)\" rows, after the end of the process for the source video file.**\n\nToDos:\n\n- More powerful detection models will be shared.\n- Sample codes will be developed to process different types of input videos (for different types of road traffics such as two way lane road).\n- Code cleanup will be performed.\n- UI will be developed. \n\nThe input video can be accessible by this [link](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/sub-1504614469486.mp4).\n\n## Theory\n\n### System Architecture\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/35445395-8dba4406-02c2-11e8-84bf-b480edbe9472.jpg\">\n</p>\n\n- Vehicle detection and classification have been developed using TensorFlow Object Detection API, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/vehicle_detection_main.py) for more info.\n- Vehicle speed prediction has been developed using OpenCV via image pixel manipulation and calculation, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/utils/speed_and_direction_prediction_module) for more info.\n- Vehicle color prediction has been developed using OpenCV via K-Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/utils/color_recognition_module) for more info.\n\n[TensorFlow\u2122](https://www.tensorflow.org/) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n[OpenCV (Open Source Computer Vision Library)](https://opencv.org/about.html) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n\n### Tracker\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/41812993-a4b5a172-7735-11e8-89f6-083ec0625f21.png\" | width=700>\n</p>\n\nSource video is read frame by frame with OpenCV. Each frames is processed by [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) is developed on TensorFlow. This is a loop that continue working till reaching end of the video. The main pipeline of the tracker is given at the above Figure.\n\n### Model\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48481757-b1d5a900-e81f-11e8-824b-4317115fe5b4.png\">\n</p>\n\nBy default I use an [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) in this project. You can find more information about SSD in [here](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab). See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n\n*The minimum vehicle detection threshold can be set [in this line](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/utils/visualization_utils.py#L443) in terms of percentage. The default minimum vehicle detecion threshold is 0.5!*\n\n## Project Demo\n\nDemo video of the project is available on [My YouTube Channel](https://www.youtube.com/watch?v=PrqnhHf6fhM).\n\n## Installation\n\n**Docker setup with Nvidia GPU:** Run the demo in the GPU without installing anything, just nvidia-docker. The command to set up this docker:\n\n    docker-compose up\n    \nAlternative for nvidia-docker, you can follow the installation steps are given below!\n\n**1.) Python and pip**\n\nPython is automatically installed on Ubuntu. Take a moment to confirm (by issuing a python -V command) that one of the following Python versions is already installed on your system:\n\n- Python 3.3+\n\nThe pip or pip3 package manager is usually installed on Ubuntu. Take a moment to confirm (by issuing a *pip -V* or *pip3 -V* command) that pip or pip3 is installed. We strongly recommend version 8.1 or higher of pip or pip3. If Version 8.1 or later is not installed, issue the following command, which will either install or upgrade to the latest pip version:\n\n    $ sudo apt-get install python3-pip python3-dev # for Python 3.n\n    \n**2.) OpenCV**\n\nSee required commands to install OpenCV on Ubuntu in [here](https://gist.github.com/dynamicguy/3d1fce8dae65e765f7c4).\n\n**3.) TensorFlow**\n\nInstall TensorFlow by invoking one of the following commands:\n\n    $ pip3 install tensorflow     # Python 3.n; CPU support (no GPU support)\n    $ pip3 install tensorflow-gpu # Python 3.n; GPU support\n\n**4.) TensorFlow Object Detection API**\n\nSee required commands to install TensorFlow Object Detection API on Ubuntu in [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).\n  \nIf you are still getting problem about installation after completed the installation of the packet that are given above, please check that [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) out to get detailed info about installation.\n\n---\n- After completing these 4 installation steps that are given at above, you can test the project by this command:\n\n      python3 vehicle_detection_main.py\n---\n\n## Citation\nIf you use this code for your publications, please cite it as:\n\n    @ONLINE{vdtct,\n        author = \"Ahmet \u00d6zl\u00fc\",\n        title  = \"Vehicle Detection, Tracking and Counting by TensorFlow\",\n        year   = \"2018\",\n        url    = \"https://github.com/ahmetozlu/vehicle_counting_tensorflow\"\n    }\n\n## Author\nAhmet \u00d6zl\u00fc\n\n## License\nThis system is available under the MIT license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "Dovyski/cvui",
  "language": "C++",
  "readme_contents": "cvui\n=====\nA (very) simple UI lib built on top of OpenCV drawing primitives. Other UI libs, such as [imgui](https://github.com/ocornut/imgui), require a graphical backend (e.g. OpenGL) to work, so if you want to use imgui in a OpenCV app, you must make it OpenGL enabled, for instance. It is not the case with cvui, which uses *only* OpenCV drawing primitives to do all the rendering (no OpenGL or Qt required).\n\n![image](https://raw.githubusercontent.com/Dovyski/depository/master/cvui.png?20180627)\n\nFeatures\n--------\n- Lightweight and simple to use user interface;\n- Header-only with no external dependencies (except OpenCV);\n- Based on OpenCV drawing primitives only (OpenGL or Qt are not required);\n- Friendly and C-like API (no classes/objects, etc);\n- Easily render components without worrying about their position (using rows/columns);\n- Simple (yet powerful) mouse API;\n- Modest number of UI components (11 in total);\n- Available in C++ and Python (pure implementation, no bindings).\n\nBuild\n-----\ncvui is a header-only lib that does not require a build. Just add `cvui.h` (or `cvui.py`) to your project and you are ready to go. The only dependency is OpenCV (version `2.x` or `3.x`), which you are probably using already.\n\nUsage\n-----\nCheck the [online documentation](https://dovyski.github.io/cvui) or the [examples](https://github.com/Dovyski/cvui/tree/master/example) folder to learn how to use cvui. The general usage in C++ and Python is shown below.\n\nUsage in C++:\n```cpp\n#include <opencv2/opencv.hpp>\n\n// One (and only one) of your C++ files must define CVUI_IMPLEMENTATION\n// before the inclusion of cvui.h to ensure its implementaiton is compiled.\n#define CVUI_IMPLEMENTATION\n#include \"cvui.h\"\n\n#define WINDOW_NAME \"CVUI Hello World!\"\n\nint main(int argc, const char *argv[])\n{\n\t// Create a frame where components will be rendered to.\n\tcv::Mat frame = cv::Mat(200, 500, CV_8UC3);\n\n\t// Init cvui and tell it to create a OpenCV window, i.e. cv::namedWindow(WINDOW_NAME).\n\tcvui::init(WINDOW_NAME);\n\n\twhile (true) {\n\t\t// Fill the frame with a nice color\n\t\tframe = cv::Scalar(49, 52, 49);\n\n\t\t// Render UI components to the frame\n\t\tcvui::text(frame, 110, 80, \"Hello, world!\");\n\t\tcvui::text(frame, 110, 120, \"cvui is awesome!\");\n\n\t\t// Update cvui stuff and show everything on the screen\n\t\tcvui::imshow(WINDOW_NAME, frame);\n\n\t\tif (cv::waitKey(20) == 27) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n```\n\nUsage in Python:\n```python\nimport numpy as np\nimport cv2\nimport cvui\n\nWINDOW_NAME = 'CVUI Hello World!'\n\n# Create a frame where components will be rendered to.\nframe = np.zeros((200, 500, 3), np.uint8)\n\n# Init cvui and tell it to create a OpenCV window, i.e. cv2.namedWindow(WINDOW_NAME).\ncvui.init(WINDOW_NAME)\n\nwhile True:\n\t# Fill the frame with a nice color\n\tframe[:] = (49, 52, 49)\n\n\t# Render UI components to the frame\n\tcvui.text(frame, 110, 80, 'Hello, world!')\n\tcvui.text(frame, 110, 120, 'cvui is awesome!')\n\n\t# Update cvui stuff and show everything on the screen\n\tcvui.imshow(WINDOW_NAME, frame)\n\n\tif cv2.waitKey(20) == 27:\n\t\tbreak\n```\n\nLicense\n-----\nCopyright (c) 2016 Fernando Bevilacqua. Licensed under the [MIT license](LICENSE.md).\n\nChange log\n-----\nSee all changes in the [CHANGELOG](CHANGELOG.md) file.\n"
 },
 {
  "repo": "avisingh599/mono-vo",
  "language": "C++",
  "readme_contents": "This is an OpenCV 3.0 based implementation of a monocular visual odometry algorithm.\n\n## Algorithm\nUses Nister's Five Point Algorithm for Essential Matrix estimation, and FAST features, with a KLT tracker.\nMore details are available [here as a report](http://avisingh599.github.io/assets/ugp2-report.pdf), and\n[here as a blog post](http://avisingh599.github.io/vision/monocular-vo/). \n\nNote that this project is not yet capable of doing reliable relative scale estimation, \nso the scale informaion is extracted from the KITTI dataset ground truth files.\n\n## Demo Video\n\n[![Demo video](http://share.gifyoutube.com/Ke1ope.gif)](http://www.youtube.com/watch?v=homos4vd_Zs)\n\n\n## Requirements\nOpenCV 3.0\n\n## How to compile?\nProvided with this repo is a CMakeLists.txt file, which you can use to directly compile the code as follows:\n```bash\nmkdir build\ncd build\ncmake ..\nmake\n```\n\n## How to run? \nAfter compilation, in the build directly, type the following:\n```bash\n./vo\n```\n## Before you run\nIn order to run this algorithm, you need to have either your own data, \nor else the sequences from [KITTI's Visual Odometry Dataset](http://www.cvlibs.net/datasets/kitti/eval_odometry.php).\nIn order to run this algorithm on your own data, you must modify the intrinsic calibration parameters in the code.\n\n## Performance\n![Results on the KITTI VO Benchmark](http://avisingh599.github.io/images/visodo/2K.png)\n\n## Contact\nFor any queries, contact: avisingh599@gmail.com\n\n## License\nMIT"
 },
 {
  "repo": "sunglok/3dv_tutorial",
  "language": "C++",
  "readme_contents": "## An Invitation to 3D Vision: A Tutorial for Everyone\r\n_An Invitation to 3D Vision_ is an introductory tutorial on 3D vision (a.k.a. geometric vision or visual geometry or multi-view geometry).\r\nIt aims to make beginners understand basic theory of 3D vision and implement their own applications using [OpenCV][].\r\nIn addition to tutorial slides, example codes are provided in the purpose of education. They include simple but interesting and practical applications. The example codes are written as short as possible (mostly __less than 100 lines__) to be clear and easy to understand.\r\n\r\n* Download [tutorial slides](https://github.com/sunglok/3dv_tutorial/releases/download/misc/3dv_slides.pdf)\r\n* Download [example codes in a ZIP file](https://github.com/sunglok/3dv_tutorial/archive/master.zip)\r\n* Read [how to run example codes](https://github.com/sunglok/3dv_tutorial/blob/master/HOWTO_RUN.md)\r\n\r\n### What does its name come from?\r\n* The main title, _An Invitation to 3D Vision_, came from [a legendary book by Yi Ma, Stefano Soatto, Jana Kosecka, and Shankar S. Sastry](http://vision.ucla.edu/MASKS/). We wish that our tutorial will be the first gentle invitation card for beginners to 3D vision and its applications.\r\n* The subtitle, _for everyone_, was inspired from [Prof. Kim's online lecture](https://hunkim.github.io/ml/) (in Korean). Our tutorial is also intended not only for students and researchers in academia, but also for hobbyists and developers in industries. We tried to describe important and typical problems and their solutions in [OpenCV][]. We hope readers understand it easily without serious mathematical background.\r\n\r\n### Examples\r\n* __Single-view Geometry__\r\n  * Camera Projection Model\r\n    * Object Localization and Measurement: [object_localization.cpp][] (result: [image](https://drive.google.com/open?id=10Lche-1HHazDeohXEQK443ruDTAmIO4E))\r\n    * Image Formation: [image_formation.cpp][] (result: [image0](https://drive.google.com/file/d/0B_iOV9kV0whLY2luc05jZGlkZ2s/view), [image1](https://drive.google.com/file/d/0B_iOV9kV0whLS3M4S09ZZHpjTkU/view), [image2](https://drive.google.com/file/d/0B_iOV9kV0whLV2dLZHd0MmVkd28/view), [image3](https://drive.google.com/file/d/0B_iOV9kV0whLS1ZBR25WekpMYjA/view), [image4](https://drive.google.com/file/d/0B_iOV9kV0whLYVB0dm9Fc0dvRzQ/view))\r\n    * Geometric Distortion Correction: [distortion_correction.cpp][] (result: [video](https://www.youtube.com/watch?v=HKetupWh4V8))\r\n  * General 2D-3D Geometry\r\n    * Camera Calibration: [camera_calibration.cpp][] (result: [text](https://drive.google.com/file/d/0B_iOV9kV0whLZ0pDbWdXNWRrZ00/view))\r\n    * Camera Pose Estimation (Chessboard): [pose_estimation_chessboard.cpp][] (result: [video](https://www.youtube.com/watch?v=4nA1OQGL-ig))\r\n    * Camera Pose Estimation (Book): [pose_estimation_book1.cpp][]\r\n    * Camera Pose Estimation and Calibration: [pose_estimation_book2.cpp][]\r\n    * Camera Pose Estimation and Calibration w/o Initially Given Camera Parameters: [pose_estimation_book3.cpp][] (result: [video](https://www.youtube.com/watch?v=GYp4h0yyB3Y))\r\n* __Two-view Geometry__\r\n  * Planar 2D-2D Geometry (Projective Geometry)\r\n    * Perspective Distortion Correction: [perspective_correction.cpp][] (result: [original](https://drive.google.com/file/d/0B_iOV9kV0whLVlFpeFBzYWVadlk/view), [rectified](https://drive.google.com/file/d/0B_iOV9kV0whLMi1UTjN5QXhnWFk/view))\r\n    * Planar Image Stitching: [image_stitching.cpp][] (result: [image](https://drive.google.com/file/d/0B_iOV9kV0whLOEQzVmhGUGVEaW8/view))\r\n    * 2D Video Stabilization: [video_stabilization.cpp][] (result: [video](https://www.youtube.com/watch?v=be_dzYicEzI))\r\n  * General 2D-2D Geometry (Epipolar Geometry)\r\n    * Visual Odometry (Monocular, Epipolar Version): [vo_epipolar.cpp][]\r\n    * Triangulation (Two-view Reconstruction): [triangulation.cpp][]\r\n* __Multi-view Geometry__\r\n  * Bundle Adjustment\r\n    * Global Version: [bundle_adjustment_global.cpp][]\r\n    * Incremental Version: [bundle_adjustment_inc.cpp][]\r\n  * Structure-from-Motion\r\n    * Global SfM: [sfm_global.cpp][]\r\n    * Incremental SfM: [sfm_inc.cpp][]\r\n  * Feature-based Visual Odometry and SLAM\r\n    * Visual Odometry (Monocular, Epipolar Version): [vo_epipolar.cpp][]\r\n    * Visual Odometry (Stereo Version)\r\n    * Visual Odometry (Monocular, PnP and BA Version)\r\n    * Visual SLAM (Monocular Version)\r\n  * Direct Visual Odometry and SLAM\r\n    * Visual Odometry (Monocular, Direct Version)\r\n  * c.f. The above examples need [Ceres Solver][] for bundle adjustment.\r\n* __Correspondence Problem__\r\n  * Line Fitting with RANSAC: [line_fitting_ransac.cpp][]\r\n  * Line Fitting with M-estimators: [line_fitting_m_est.cpp][]\r\n* **Appendix**\r\n  * Line Fitting\r\n  * Planar Homograph Estimation\r\n  * Fundamental Matrix Estimation\r\n\r\n### Dependencies\r\n* [OpenCV][] (> 3.0.0, 3-clause BSD License)\r\n  * _OpenCV_ is a base of all example codes for basic computer vision algorithms, linear algebra, image/video manipulation, and GUI.\r\n* [Ceres Solver][] (3-clause BSD License): A numerical optimization library\r\n  * _Ceres Solver_ is additionally used by m-estimator, bundle adjustment, structure-from-motion, and visual odometry/SLAM.\r\n\r\n### License\r\n* [Beerware](http://en.wikipedia.org/wiki/Beerware)\r\n\r\n### Authors\r\n* [Sunglok Choi](http://sites.google.com/site/sunglok/) (sunglok AT hanmail DOT net)\r\n\r\n### Acknowledgement\r\nThe authors thank the following contributors and projects.\r\n\r\n* [Jae-Yeong Lee](https://sites.google.com/site/roricljy/): He motivated many examples.\r\n* [Giseop Kim](https://sites.google.com/view/giseopkim): He contributed the initial version of SfM codes with [Toy-SfM](https://github.com/royshil/SfM-Toy-Library) and [cvsba](https://www.uco.es/investiga/grupos/ava/node/39).\r\n* [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/): The KITTI odometry dataset #07 was used to demonstrate visual odometry and SLAM.\r\n* [Russell Hewett](https://courses.engr.illinois.edu/cs498dh3/fa2013/projects/stitching/ComputationalPhotograph_ProjectStitching.html): His two hill images were used to demonstrate image stitching.\r\n* [Kang Li](http://www.cs.cmu.edu/~kangli/code/Image_Stabilizer.html): His shaking CCTV video was used to demonstrate video stabilization.\r\n* [Richard Blais](http://www.richardblais.net/): His book cover and video in [the OpenCV tutorial](http://docs.opencv.org/3.1.0/dc/d16/tutorial_akaze_tracking.html) were used to demonstrate camera pose estimation and augmented reality.\r\n\r\n[OpenCV]: http://opencv.org/\r\n[Ceres Solver]: http://ceres-solver.org/\r\n\r\n[object_localization.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/object_localization.cpp\r\n[image_formation.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/image_formation.cpp\r\n[distortion_correction.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/distortion_correction.cpp\r\n[camera_calibration.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/camera_calibration.cpp\r\n[pose_estimation_chessboard.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_chessboard.cpp\r\n[pose_estimation_book1.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book1.cpp\r\n[pose_estimation_book2.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book2.cpp\r\n[pose_estimation_book3.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book3.cpp\r\n[perspective_correction.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/perspective_correction.cpp\r\n[image_stitching.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/image_stitching.cpp\r\n[video_stabilization.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/video_stabilization.cpp\r\n[vo_epipolar.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/vo_epipolar.cpp\r\n[triangulation.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/triangulation.cpp\r\n[bundle_adjustment_global.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/bundle_adjustment_global.cpp\r\n[bundle_adjustment_inc.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/bundle_adjustment_inc.cpp\r\n[sfm_global.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/sfm_global.cpp\r\n[sfm_inc.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/sfm_inc.cpp\r\n[line_fitting_ransac.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/line_fitting_ransac.cpp\r\n[line_fitting_m_est.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/line_fitting_m_est.cpp\r\n"
 },
 {
  "repo": "AlphaQi/MTCNN-light",
  "language": "C++",
  "readme_contents": "# MTCNN-light\n## Introduction\nthis repository is the implementation of MTCNN with no framework,  Just need opencv and openblas.  \n\"Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks\", implemented with C++\uff0cno framework  \nit is very easy for you to use.  \nit is can be a part of your project with no framework, like caffe and mxnet.  \nit is real time for VGA, and you can improve it's runtime.  \n\n## Time Cost\nThe average time cost is about 68ms per frame(640,480).The result is generated by testing a camera. mini_size is 40     \ncpu   i5-4590  \nos    windows10   64bit\n\n## Dependencies\nopencv  2.0+  \nopenblas  \n\n##ubuntu   \n### opencv    \nyou can find many tutorials.\n\n### openblas\nIt is very easy to install  \n1 download the source code from https://github.com/xianyi/OpenBLAS  \n2 Extract it and type \"cd xxx\", xxx means the directory  \n3 type \"make\"   \n4 type \"make install PREFIX=your_installation_directory\"   \n\n### if you don't have cmake \napt-get install cmake\n\n## usage\ncd root_directory   \nvim CMakeLists.txt   \nchange   include_directories(the_directory_of_openblas_include_of_yours)  \nchange   link_directories(the_directory_of_openblas_lib_of_yours)  \nsave and exit\n\ncmake .   \nmake   \n./main   \n\n## for windows    \n### opencv and openblas    \nthere is binary packages of openblas for windows, you just need download it   \nBut you should to be careful, if you download the 64bit ,you need configure    \nthe opencv and vs project environment with 64bit, don't choose x86\n"
 },
 {
  "repo": "FaceAR/OpenFaceIOS",
  "language": "C++",
  "readme_contents": "\u6bd5\u4e1a\u5b63\u4e86\uff0c\u6700\u8fd1\u6bd4\u8f83\u5fd9\uff0c\u5148\u653e\u5728\u8fd9\u91cc\uff0c\u540e\u9762\u6709\u673a\u4f1a\u5728\u66f4\u65b0\uff0c\u62b1\u6b49\u4e86\u5404\u4f4d\u5b9d\u5b9d\u3002\n\n#OpenFaceIOS\n\nOpenFace is get from https://github.com/TadasBaltrusaitis/OpenFace, a state-of-the art open source tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. And I create an ios project. Delete Dlib,TBB, Only use OpenCV . And add Face Swap function, It can swap face to face or swap model to face. \n\nMore details about the project - http://www.cl.cam.ac.uk/research/rainbow/projects/openface/\nCode - https://github.com/TadasBaltrusaitis/OpenFace\n\n## Functionality\n\nThe system is capable of performing a number of facial analysis tasks:\n\n- Facial Landmark Detection\n\n![Sample facial landmark detection image](https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/multi_face_img.png)\n\n- Facial Landmark and head pose tracking (links to YouTube videos)\n\n<a href=\"https://www.youtube.com/watch?v=V7rV0uy7heQ\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/V7rV0uy7heQ/0.jpg\" alt=\"Multiple Face Tracking\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/watch?v=vYOa8Pif5lY\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/vYOa8Pif5lY/0.jpg\" alt=\"Multiple Face Tracking\" width=\"240\" height=\"180\" border=\"10\" /></a>\n\n- Gaze tracking (image of it in action)\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/gaze_ex.png\" height=\"378\" width=\"567\" >\n\n- Facial Feature Extraction (aligned faces and HOG features)\n\n![Sample aligned face and HOG image](https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/appearance.png)\n\n- IOS Facial Landmark Detection & head pose tracking & Gaze tracking\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen1.png\" height=\"382\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen2.png\" height=\"382\" width=\"222\" >\n\n- Face Swap\n\nUser\uff1aDooonut https://github.com/Dooonut\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen3.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen4.png\" height=\"282\" width=\"222\" >\n\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen5.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen6.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen7.png\" height=\"282\" width=\"222\" >\n"
 },
 {
  "repo": "hihozhou/php-opencv",
  "language": "C++",
  "readme_contents": "# PHP-OPENCV - PHP extension for Opencv\n\n\n\n[![Build Status](https://travis-ci.org/hihozhou/php-opencv.svg?branch=master)](https://travis-ci.org/hihozhou/php-opencv) [![Minimum PHP Version](https://img.shields.io/badge/php-%3E%3D%207.0-8892BF.svg)](https://php.net/)\n\n\n> **\u26a0\ufe0f\u26a0\ufe0f NO LONGER IN ACTIVE DEVELOPMENT | \u9879\u76ee\u4e0d\u518d\u7ef4\u62a4 \u26a0\ufe0f\u26a0\ufe0f** \n>Due to personal scheduling reasons, the project has been suspended for maintenance and it is hoped that the project code will provide useful value.\n\n\n## Document\n- [PHP OpenCV Document](https://doc.phpopencv.org)\n\n\n## Requirements\n\n- OpenCV 4.0.0+\n- PHP7.0+\n\n\n\n## Installation\n\n### Use OpenCV docker(commendatory)\n\nIf you don't know how to install OpenCV, you can use my OpenCV docker image(only 300M, including opencv_contrib).\n\n```bash\ndocker pull hihozhou/php-opencv\n```\n\n### Compile and install php-opencv extension\n\n```bash\ngit clone https://github.com/hihozhou/php-opencv.git\ncd php-opencv\nphpize\n./configure --with-php-config=your php-config path\nmake\nmake install\n```\n\n## Configure\n\nphp.ini\n\n```\nextension=\"your opencv.so path\"\n```\n## Example\n\n### LBPH face recognition\n\n```php\nuse CV\\Face\\LBPHFaceRecognizer;\n//use ...;\n\n$src = imread('facePic.jpg');\n$gray = cvtColor($src, COLOR_BGR2GRAY);\nequalizeHist($gray, $gray);\n$faceRecognizer = LBPHFaceRecognizer::create();\n/* ... */ //get $images and $labels for train\n$faceRecognizer->train($images, $labels);//How to get $image and $labels, see the document\n/* ... */ //Face detection using CascadeClassifier\n$faceLabel = $faceRecognizer->predict($gray);\n/* ... */ //draw face and name\n```\n\nresult:\n\n![predict](tests/face_recognizer.jpg)\n\n\n### Image Processing\n\n```php\n//Obama.php\nuse function CV\\{ imread, imshow, waitkey, namedWindow};\n\n$im = imread('Obama.png');//load image\nnamedWindow('This is Obama id card',WINDOW_FULLSCREEN);//create window\nimshow('This is Obama id card',$im);//show image on window\n\nwaitkey(0);\n\n```\n\nresult:\n\n![Obama](tests/Obama.png)\n\nLoad image by gray\n\n```php\n$gray = imread('Obama.png',IMREAD_GRAYSCALE);\n//or\nuse  function CV\\{ cvtColor};\n$gray = cvtColor($im, COLOR_BGR2GRAY);\n\n```\n\n![Obama_gray](tests/Obama_gray.png)\n\n\nYou can draw something.  \ne.g:  \n\n```php\nuse CV\\{Mat,Scalar, Point, Size};\nuse function CV\\{ellipse, imwrite, waitKey};\nuse const CV\\{CV_8UC3};\n\n$windowWidth = 600;\n$thickness = 2;\n$lineType = 8;\n$matScalar = new Scalar(0,0,0);\n$mat = new Mat($windowWidth, $windowWidth, CV_8UC3, $matScalar);\n$point=new Point($windowWidth/2, $windowWidth/2);\n$size=new Size($windowWidth/4, $windowWidth/16);\n$scalar=new Scalar(255, 129, 0);\nfor($i = 0; $i <= 360; $i += 45){\n    ellipse($mat,$point,$size,$i,0,360,$scalar,$thickness,$lineType);\n}\nimwrite('./tests/ellipse.png',$mat);\n\n```\n\nresult:\n\n![ellipse](tests/ellipse.png)\n\n\n\n## Features\n- [x] 1.[core](http://phpopencv.org/zh-cn/docs/mat.html)\n- [x] 2.[imgproc](http://phpopencv.org/zh-cn/docs/gausian_median_blur_bilateral_filter.html)\n- [x] 3.highgui\n- [ ] 4.contrib\n- [ ] 5.features2d\n- [ ] 6.flann\n- [ ] 7.gpu\n- [ ] 8.calib3d\n- [ ] 9.legacy\n- [x] 10.ml\n- [ ] 11.nonfree\n- [x] 12.objdetect\n- [ ] 13.ocl\n- [ ] 14.photo\n- [ ] 15.stitching\n- [ ] 16.superres\n- [ ] 17.ts\n- [x] 18.video\n- [ ] 19.Videostab\n\n\n\n## Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](https://github.com/hihozhou/php-opencv/graphs/contributors)].\n    \n## \u611f\u8c22\n\n\u611f\u8c22[\u97e9\u5929\u5cf0](https://github.com/matyhtf)\u8001\u5927\u7684\u6307\u5bfc\uff0c  \n\u611f\u8c22[\u97e9\u5929\u5cf0](https://github.com/matyhtf)\u8001\u5927\u7684\u6307\u5bfc\uff0c\n\u611f\u8c22[\u76d8\u53e4\u5927\u53d4](https://github.com/pangudashu)\u7684[php7-internal](https://github.com/pangudashu/php7-internal)\u9879\u76ee\u4ee5\u53ca\u5e73\u5e38\u7684\u6307\u5bfc\uff0c\n\u611f\u8c22`\u6728\u6876\u6280\u672f\u5347\u7ea7\u4ea4\u6d41\u7fa4`\u548c`\u9ed1\u591c\u8def\u4eba\u6280\u672f\u7fa4`\u3001\u4ee5\u53ca`PHP\u5185\u6838\u4ea4\u6d41`\u7684\u7fa4\u53cb\u5bf9\u6280\u672f\u7684\u5e2e\u52a9\u3002\n"
 },
 {
  "repo": "takmin/OpenCV-Marker-less-AR",
  "language": "C++",
  "readme_contents": "2012/01/10\nMarker less AR is using natural images as markers instead of monochromatic markers.   This program includes object recognition, tracking, and overlay of 3D model functions.\nYou can use this program source code under MIT license.\nhttp://www.opensource.org/licenses/mit-license.php\n\nThis program was written in C++ using OpenCV 2.3.1 and GLUT 3.7.6.  You should install these libraries before you compile this source code. \n\nGLUT\nhttp://www.opengl.org/resources/libraries/glut/\n\nOpenCV\nhttps://sourceforge.net/projects/opencvlibrary/\n\nThis program also includes \"GLMetaseq\" which had been developed by Sunao Hashimoto and Keisuke Konishi.\nGLMetaseq is 3D model loader of mqo format.\n\nGLMetaseq\nhttp://kougaku-navi.net/ARToolKit.html\n\nThe Windows demo program \"ARengine.exe\" includes a 3D models \"mikuX.mqo\", I obtained these files created by Zusa-san from:\nhttp://nanoha.kirara.st/3dcg/file/dlrank.php\n\nThis program consists of 3 parts: Object Recognition, Tracking, and Overlay.  You can use Object Recognition function apart from tracking and overlay.\n\nIf you are a Windows user, I strongly recommend to begin with demo program.  This demo program is build on Windows 7 (32bit).  You can find how to use it in \"HowToUse.pdf\" in WinDemo.zip.\nThis source code is still alpha version and I've not written enough documents yet.  \nThis code is still on the way of refactoring, so API interface design will be changed later.  I will write API documents after refactoring.\nAt the current version, the only document \"HowToUse.pdf\" may be helpful to understand this application.\n\nIf you have any question, please contact here:\nTakuya MINAGAWA (z.takmin@gmail.com)\n\n"
 },
 {
  "repo": "PINTO0309/OpenVINO-YoloV3",
  "language": "Python",
  "readme_contents": "# OpenVINO-YoloV3\nYoloV3 / tiny-YoloV3 + RaspberryPi3 / Ubuntu LaptopPC + NCS/NCS2 + USB Camera + Python\n  \nInspired from **https://github.com/mystic123/tensorflow-yolo-v3.git**\n  \n**Performance comparison as a mobile application (Based on sensory comparison)**  \n\u25ef=HIGH, \u25b3=MEDIUM, \u00d7=LOW  \n\n|No.|Model|Speed|Accuracy|Adaptive distance|\n|:-:|:-|:-:|:-:|:-|\n|1|SSD|\u00d7|\u25ef|ALL|\n|2|**[MobileNet-SSD](https://github.com/PINTO0309/MobileNet-SSD-RealSense.git)**|\u25b3|\u25b3|Short distance|\n|3|**[YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u00d7|\u25ef|ALL|\n|4|**[tiny-YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u25ef|\u25b3|Long distance|\n\n  \n![05](media/05.png)\n\n## My articles\n1. [[24 FPS] Boost RaspberryPi3 with four Neural Compute Stick 2 (NCS2) MobileNet-SSD / YoloV3 [48 FPS for Core i7]](https://qiita.com/PINTO/items/94d5557fca9911cc892d#24-fps-boost-raspberrypi3-with-four-neural-compute-stick-2-ncs2-mobilenet-ssd--yolov3-48-fps-for-core-i7)\n2. [[13 FPS] NCS2 x 4 + Full size YoloV3 performance has been tripled](https://qiita.com/PINTO/items/c766ac9614052f4d6304#13-fps-ncs2-x-4--full-size-yolov3-performance-has-been-tripled)\n3. [Support for local training and OpenVINO of One Class tiny-YoloV3 with a proprietary data set](https://qiita.com/PINTO/items/7dd7135085a7249bf17a#support-for-local-training-and-openvino-of-one-class-tiny-yolov3-with-a-proprietary-data-set)\n\n## Change history\n[Mar 01, 2019]\u3000Improve accuracy. Fixed preprocessing and postprocessing bug.  \n[Mar 17, 2019]\u3000Added a training procedure with your own data set.  \n[Apr 03, 2019]\u3000Work on OpenVINO 2019 R1 started.  \n[Apr 14, 2019]\u3000Compatible with 2019 R1.  \n[Apr 26, 2019]\u3000Compatible with 2019 R1.0.1.  \n\n## Operation sample\n**<CPP + YoloV3 - Intel Core i7-8750H, CPU Only, 4 FPS - 5 FPS>**  \n[<img src=\"media/01.gif\" width=80%>](https://youtu.be/vOcj_3ByK68)  \n  \n**<CPP + tiny-YoloV3 - Intel Core i7-8750H, CPU Only, 60 FPS>**  \n[<img src=\"media/02.gif\" width=80%>](https://youtu.be/md4udC4baZA)  \n\n**<Python + tiny-YoloV3 + USBCamera, Core i7-8750H, CPU Only, 30 FPS>**  \n[<img src=\"media/03.gif\" width=60%>](https://youtu.be/HTDzpFFpBbc)  \n\n**<Python + tiny-YoloV3 + Async + USBCamera, Core i7-8750H, NCS2, 30 FPS+>**  \n**To raise the detection rate, lower the threshold by yourself.**  \n**The default threshold is 40%.**  \n[<img src=\"media/11.gif\" width=60%>](https://youtu.be/7zkb413HCAs) \n\n**<Python + YoloV3 + MP4, Core i7-8750H, NCS2 x4, 13 FPS>  \n\u3010Note\u3011 Due to the performance difference of ARM <-> Core series, performance is degraded in RaspberryPi3.**  \n[<img src=\"media/06.gif\" width=60%>](https://youtu.be/AT75LBIOAck)  \n  \n## Python Version YoloV3 / tiny-YoloV3 (Dec 28, 2018 Operation confirmed)\n### YoloV3\n```bash\n$ python3 openvino_yolov3_test.py\n```\n### tiny-YoloV3 + NCS2 MultiStick\n```bash\n$ python3 openvino_tiny-yolov3_MultiStick_test.py -numncs 1\n```\n### YoloV3 + NCS2 MultiStick (Pretty slow)\n```bash\n$ python3 openvino_yolov3_MultiStick_test.py -numncs 4\n```\n\n\n## CPP Version YoloV3 / tiny-YoloV3 (Dec 16, 2018 Operation confirmed)\n**[cpp version is here](cpp)** \"cpp/object_detection_demo_yolov3_async\"\n  \n## Environment\n\n- LattePanda Alpha (Intel 7th Core m3-7y30) or LaptopPC (Intel 8th Core i7-8750H)\n- Ubuntu 16.04 x86_64\n- RaspberryPi3\n- Raspbian Stretch armv7l\n- OpenVINO toolkit 2019 R1.0.1 (2019.1.133)\n- Python 3.5\n- OpenCV 4.1.0-openvino\n- Tensorflow v1.12.0 or Tensorflow-GPU v1.12.0 (pip install)\n- YoloV3 (MS-COCO)\n- tiny-YoloV3 (MS-COCO)\n- USB Camera (PlaystationEye) / Movie file (mp4)\n- Intel Neural Compute Stick v1 / v2\n  \n## OpenVINO Supported Layers (As of Apr 14, 2019)\n\n- [Supported Framework Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html)\n- [Supported Caffe Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html#caffe-supported-layers)\n- [Supported TensorFlow Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#tensorflow-supported-layers)\n- [Supported MXNet Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html#mxnet-supported-layers)\n- [Supported ONNX Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html#supported-onnx-layers)\n\n**Supported Devices (https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_layers)**\n<table class=\"doxtable\">\n<tr>\n<th align=\"left\">Layers </th><th align=\"center\">GPU </th><th align=\"center\">CPU </th><th align=\"center\">MYRIAD(VPU) </th><th align=\"center\">GNA </th><th align=\"center\">FPGA </th><th align=\"center\">ShapeInfer  </th></tr>\n<tr>\n<td align=\"left\">Activation-Clamp </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ELU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-Leaky ReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-PReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ReLU6 </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-Sigmoid/Logistic </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-TanH </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ArgMax </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">BatchNormalization </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Concat </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Const </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Dilated </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Dilated 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Grouped </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Grouped 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Ordinary </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Ordinary 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Crop </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">CTCGreedyDecoder </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Deconvolution </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Deconvolution 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">DetectionOutput </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Max </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Mul </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Sum </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Flatten </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">FullyConnected (Inner Product) </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Gather </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Gemm </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">GRN </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Interp </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">LRN (Norm) </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">LSTMCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">GRUCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">RNNCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">LSTMSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">GRUSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">RNNSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Memory </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">MVN </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Normalize </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pad </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Permute </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pooling(AVG,MAX) </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pooling(AVG,MAX) 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Power </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PriorBox </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PriorBoxClustered </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Proposal </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PSROIPooling </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">RegionYolo </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ReorgYolo </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Resample </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Reshape </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">RNN </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">ROIPooling </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ScaleShift </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SimplerNMS </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Slice </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SoftMax </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SpatialTransformer </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Split </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">TensorIterator </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Tile </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Unpooling </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Upsampling </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n</table>\n  \n## OpenVINO - Python API\n**https://docs.openvinotoolkit.org/latest/_inference_engine_ie_bridges_python_docs_api_overview.html**\n  \n<br>\n<br>\n\n# Environment construction procedure\n### 1. Work with LaptopPC (Ubuntu 16.04)\n1.OpenVINO 2019R1.0.1 Full-Install. Execute the following command.\n```bash\n$ cd ~\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1ciX7cHqCh8lLFYI0HKkhC3r_fMirrlKk\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1ciX7cHqCh8lLFYI0HKkhC3r_fMirrlKk\" -o l_openvino_toolkit_p_2019.1.133.tgz\n$ tar -zxf l_openvino_toolkit_p_2019.1.133.tgz\n$ rm l_openvino_toolkit_p_2019.1.133.tgz\n$ cd l_openvino_toolkit_p_2019.1.133\n$ sudo -E ./install_openvino_dependencies.sh\n\n## GUI version installer\n$ sudo ./install_GUI.sh\n or\n## CUI version installer\n$ sudo ./install.sh\n```\n2.Configure the Model Optimizer. Execute the following command.\n```bash\n$ cd /opt/intel/openvino/install_dependencies/\n$ sudo -E ./install_openvino_dependencies.sh\n$ nano ~/.bashrc\nsource /opt/intel/openvino/bin/setupvars.sh\n\n$ source ~/.bashrc\n$ cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/\n$ sudo ./install_prerequisites.sh\n```\n3.\u3010Optional execution\u3011 Additional installation steps for the Intel\u00ae Movidius\u2122 Neural Compute Stick v1 and Intel\u00ae Neural Compute Stick v2\n```bash\n$ sudo usermod -a -G users \"$(whoami)\"\n$ cat <<EOF > 97-usbboot.rules\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2150\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2485\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"f63b\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nEOF\n\n$ sudo cp 97-usbboot.rules /etc/udev/rules.d/\n$ sudo udevadm control --reload-rules\n$ sudo udevadm trigger\n$ sudo ldconfig\n$ rm 97-usbboot.rules\n```\n4.\u3010Optional execution\u3011 Additional installation steps for processor graphics (GPU, Intel HD Graphics series only)\n```bash\n$ cd /opt/intel/openvino/install_dependencies/\n$ sudo -E su\n$ uname -r\n4.15.0-42-generic #<--- display kernel version sample\n\n### Execute only when the kernel version is older than 4.14\n$ ./install_4_14_kernel.sh\n\n$ ./install_NEO_OCL_driver.sh\n$ sudo reboot\n```\n\n### 2. Work with RaspberryPi (Raspbian Stretch)\n**[Note] Only the execution environment is introduced.**  \n  \n1.Execute the following command.\n```bash\n$ sudo apt update\n$ sudo apt upgrade\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1NFt6g6ZkneHioU2P7rUJ8BFpQhIazbym\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1NFt6g6ZkneHioU2P7rUJ8BFpQhIazbym\" -o l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ tar -zxvf l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ rm l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ sed -i \"s|<INSTALLDIR>|$(pwd)/inference_engine_vpu_arm|\" inference_engine_vpu_arm/bin/setupvars.sh\n```\n2.Execute the following command.\n```bash\n$ nano ~/.bashrc\n### Add 1 row below\nsource /home/pi/inference_engine_vpu_arm/bin/setupvars.sh\n\n$ source ~/.bashrc\n### Successful if displayed as below\n[setupvars.sh] OpenVINO environment initialized\n\n$ sudo usermod -a -G users \"$(whoami)\"\n$ sudo reboot\n```\n3.Update USB rule.\n```bash\n$ sh inference_engine_vpu_arm/install_dependencies/install_NCS_udev_rules.sh\n### It is displayed as follows\nUpdate udev rules so that the toolkit can communicate with your neural compute stick\n[install_NCS_udev_rules.sh] udev rules installed\n```\n**[Note] OpenCV 4.1.0 will be installed without permission when the work is finished.\nIf you do not want to affect other environments, please edit environment variables after installation is completed.**  \n\n# Training with your own data set  \nSee the article below.  \nA sample of one-class training with Darknet and tiny-YoloV3.  \n**https://qiita.com/PINTO/items/7dd7135085a7249bf17a#support-for-local-training-and-openvino-of-one-class-tiny-yolov3-with-a-proprietary-data-set**\n<br>\n<br>\n<br>\n<br>\n\n# How to install Bazel (version 0.17.2, x86_64 only)\n### 1. Bazel introduction command\n```bash\n$ cd ~\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1dvR3pdM6vtkTWqeR-DpgVUoDV0EYWil5\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1dvR3pdM6vtkTWqeR-DpgVUoDV0EYWil5\" -o bazel\n$ sudo cp ./bazel /usr/local/bin\n$ rm ./bazel\n```\n### 2. Supplementary information\n**https://github.com/PINTO0309/Bazel_bin.git**\n\n# How to check the graph structure of a \".pb\" file [Part.1]\nSimple structure analysis.\n### 1. Build and run graph structure analysis program\n```bash\n$ cd ~\n$ git clone -b v1.11.0 https://github.com/tensorflow/tensorflow.git\n$ cd tensorflow\n$ git checkout -b v1.11.0\n$ bazel build tensorflow/tools/graph_transforms:summarize_graph\n$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=xxxx.pb\n```\n### 2. Sample of display result\nYoloV3\n```bash\nFound 1 possible inputs: (name=inputs, type=float(1), shape=[?,416,416,3]) \nNo variables spotted.\nFound 1 possible outputs: (name=output_boxes, op=ConcatV2) \nFound 62002034 (62.00M) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 536 Const, 372 Identity, 87 Mul, 75 Conv2D, 72 FusedBatchNorm, 72 Maximum, 28 Add, \\\n24 Reshape, 14 ConcatV2, 9 Sigmoid, 6 Tile, 6 Range, 5 Pad, 4 SplitV, 3 Pack, 3 RealDiv, 3 Fill, \\\n3 Exp, 3 BiasAdd, 2 ResizeNearestNeighbor, 2 Sub, 1 Placeholder\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\nbazel run tensorflow/tools/benchmark:benchmark_model -- \\\n--graph=/home/b920405/git/OpenVINO-YoloV3/pbmodels/frozen_yolo_v3.pb \\\n--show_flops \\\n--input_layer=inputs \\\n--input_layer_type=float \\\n--input_layer_shape=-1,416,416,3 \\\n--output_layer=output_boxes\n```\ntiny-YoloV3\n```bash\nFound 1 possible inputs: (name=inputs, type=float(1), shape=[?,416,416,3]) \nNo variables spotted.\nFound 1 possible outputs: (name=output_boxes, op=ConcatV2) \nFound 8858858 (8.86M) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 134 Const, 63 Identity, 21 Mul, 16 Reshape, 13 Conv2D, 11 FusedBatchNorm, 11 Maximum, \\\n10 ConcatV2, 6 Sigmoid, 6 MaxPool, 4 Tile, 4 Add, 4 Range, 3 RealDiv, 3 SplitV, 2 Pack, 2 Fill, \\\n2 Exp, 2 Sub, 2 BiasAdd, 1 Placeholder, 1 ResizeNearestNeighbor\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\nbazel run tensorflow/tools/benchmark:benchmark_model -- \\\n--graph=/home/b920405/git/OpenVINO-YoloV3/pbmodels/frozen_tiny_yolo_v3.pb \\\n--show_flops \\\n--input_layer=inputs \\\n--input_layer_type=float \\\n--input_layer_shape=-1,416,416,3 \\\n--output_layer=output_boxes\n```\n\n# How to check the graph structure of a \".pb\" file [Part.2]\nConvert to text format.\n### 1. Run graph structure analysis program\n```bash\n$ python3 tfconverter.py\n### \".pbtxt\" in ProtocolBuffer format is output.\n### The size of the generated text file is huge.\n```\n\n# How to check the graph structure of a \".pb\" file [Part.3]\nUse Tensorboard.\n### 1. Run log output program for Tensorboard\n```python\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nwith tf.Session() as sess:\n    model_filename =\"xxxx.pb\"\n    with gfile.FastGFile(model_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        g_in = tf.import_graph_def(graph_def)\n\n    LOGDIR=\"path/to/logs\"\n    train_writer = tf.summary.FileWriter(LOGDIR)\n    train_writer.add_graph(sess.graph)\n```\n### 2. Starting Tensorboard\n```bash\n$ tensorboard --logdir=path/to/logs\n```\n### 3. Display of Tensorboard\nAccess `http://localhost:6006` from the browser.\n  \n# How to check the graph structure of a \".pb\" file [Part.4]\nUse **[netron](https://github.com/lutzroeder/netron.git)**.\n### 1. Install netron\n```bash\n$ sudo -H pip3 install netron\n```\n### 2. Starting netron\n```bash\n$ netron -b [MODEL_FILE]\n```\n### 3. Display of netron\nAccess `http://localhost:8080` from the browser.  \n![07](media/07.png)\n# Neural Compute Stick 2\n**https://ncsforum.movidius.com/discussion/1302/intel-neural-compute-stick-2-information**\n\n# Issue\n**[OpenVINO failing on YoloV3's YoloRegion, only one working on FP16, all working on FP32](https://software.intel.com/en-us/forums/computer-vision/topic/804019)**  \n**[Regarding YOLO family networks on NCS2. Possibly a work-around](https://software.intel.com/en-us/forums/computer-vision/topic/805425)**  \n**[Convert YOLOv3 Model to IR](https://software.intel.com/en-us/forums/computer-vision/topic/805370)**  \n\n# Reference\n**https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend**\n**https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#raspbian-stretch**\n"
 },
 {
  "repo": "ucisysarch/opencvjs",
  "language": "C++",
  "readme_contents": "# OpenCV.js\n\nThis is a JavaScript binding that exposes OpenCV library to the web. This project is made possible by support of Intel corporation. Currently, this is based on OpenCV 3.1.0.\n\n### How to Build\n1. Get the source code\n\n  ```\n  git clone https://github.com/ucisysarch/opencvjs.git\n  cd opencvjs\n  git clone https://github.com/opencv/opencv\n  cd opencv\n  git checkout 3.1.0\n  ```\n2. Install emscripten. You can obtain emscripten by using [Emscripten SDK](https://kripken.github.io/emscripten-site/docs/getting_started/downloads.html).\n\n  ```\n  ./emsdk update\n  ./emsdk install sdk-master-64bit --shallow\n  ./emsdk activate sdk-master-64bit\n  source ./emsdk_env.sh\n  ```\n3. Patch Emscripten & Rebuild.\n\n  ```\n  patch -p1 < PATH/TO/patch_emscripten_master.diff\n  ```\n4. Rebuild emscripten\n  ```\n  ./emsdk install sdk-master-64bit --shallow\n  ```\n\n5. Compile OpenCV and generate bindings by executing make.py script.\n\n  ```\n    python make.py\n  ```\n\n### Tests\nTest suite contains several tests and examples demonstrating how the API can be used. Run the tests by launching test/tests.html file usig a browser.\n\n### Exported OpenCV Subset\nClasses and functions that are intended for binding generators (i.e. come with wrapping macros such as CV_EXPORTS_W and CV_WRAP) are exposed. Hence, supported OpenCV subset is comparable to OpenCV for Python. Also, enums with exception of anonymous enums are also exported.\n\nCurrently, the following modules are supported. You can modify the make script to exclude certain modules.\n\n1. Core\n2. Image processing\n3. Photo\n4. Shape\n5. Video\n6. Object detection\n7. Features framework\n8. Image codecs\n\n### At a glance\nThe following example demonstrates how to apply a gaussian blur filter on an image. Note that everything is wrapped in a JavaScript module ('cv').\n\n```Javascript\n  // Gaussian Blur\n  var mat1 = cv.Mat.ones(7, 7, cv.CV_8UC1),\n      mat2 = new cv.Mat();\n\n  cv.GaussianBlur(mat1, mat2, [3, 3], 0, 0, cv.BORDER_DEFAULT);\n\n  mat1.delete();\n  mat2.delete();\n```\nNext example shows how to calculate image keypoints and their descriptors using ORB (Oriented Brief) method.\n```Javascript\n  var numFeatures = 900,\n\t    scaleFactor = 1.2,\n\t    numLevels = 8,\n\t    edgeThreshold = 31,\n\t\t  firstLevel =0,\n\t\t  WTA_K= 2,\n\t\t  scoreType = 0, //ORB::HARRIS_SCORE\n\t\t  patchSize = 31,\n\t\t  fastThreshold=20,\n\t\t  keyPoints = new cv.KeyPointVector(),\n\t\t  descriptors = new cv.Mat();\n\n\tvar orb = new cv.ORB(numFeatures, scaleFactor, numLevels, edgeThreshold, firstLevel,\n\t\t\t\t\t\t\t\t\t     WTA_K, scoreType, patchSize, fastThreshold);\n\n  // image and mask are of type cv.Mat\n\torb.detect(image, keyPoints, mask);\n\torb.compute(image, keyPoints, descriptors);\n\n\tkeyPoints.delete();\n\tdescriptors.delete();\n\torb.delete();\n```\n\nFunctions work on cv::Mat and various vectors. The following vectors are registered and can be used.\n\n```cpp\n  register_vector<int>(\"IntVector\");\n  register_vector<unsigned char>(\"UCharVector\"););\n  register_vector<float>(\"FloatVector\");\n  register_vector<std::vector<Point>>(\"PointVectorVector\");\n  register_vector<cv::Point>(\"PointVector\");\n  register_vector<cv::Mat>(\"MatVector\");\n  register_vector<cv::KeyPoint>(\"KeyPointVector\");\n  register_vector<cv::Rect>(\"RectVector\");\n  register_vector<cv::Point2f>(\"Point2fVector\");\n```\n### Memory management\nAll the allocated objects should be freed manually by calling delete() method. To avoid manual memory management for basic types, the following data types are exported as JavaScript value arrays.\n\n```\ncv.Size\ncv.Point\n```\n\n### File System Access\nIf your OpenCV application needs to access a file, for instance a dataset or a previoulsy trained classifier, you can modify the make script and attach the files by using emscripten \"--preload-file\" flag.\n\n\n### Limitations\n1. MatExpr is not exported.\n2. No support for default parameters yet.\n2. Constructor overloading are implemented by number of paramteres and not their types. Hence, only following Mat constructors are exported.\n\n```cpp\n  cv::Mat()\n  cv::Mat(const std::vector<unsigned char>& data)\n  cv::Mat(Size size, int type)\n  cv::Mat(int rows, int cols, int type)\n  cv::Mat(Size size, int type, void* data, size_t step)\n```\n"
 },
 {
  "repo": "EdjeElectronics/OpenCV-Playing-Card-Detector",
  "language": "Python",
  "readme_contents": "# OpenCV-Playing-Card-Detector\nThis is a Python program that uses OpenCV to detect and identify playing cards from a PiCamera video feed on a Raspberry Pi. Check out the YouTube video that describes what it does and how it works:\n\nhttps://www.youtube.com/watch?v=m-QPjO-2IkA\n\n## Usage\nDownload this repository to a directory and run CardDetector.py from that directory. Cards need to be placed on a dark background for the detector to work. Press 'q' to end the program.\n\nThe program was originally designed to run on a Raspberry Pi with a Linux OS, but it can also be run on Windows 7/8/10. To run on Windows, download and install Anaconda (https://www.anaconda.com/download/, Python 3.6 version), launch Anaconda Prompt, and execute the program by launching IDLE (type \"idle\" and press ENTER in the prompt) and opening/running the CardDetector.py file in IDLE. The Anaconda environment comes with the opencv and numpy packages installed, so you don't need to install those yourself. If you are running this on Windows, you will also need to change the program to use a USB camera, as described below.\n\nThe program allows you to use either a PiCamera or a USB camera. If using a USB camera, change line 38 in CardDetector.py to:\n```\nvideostream = VideoStream.VideoStream((IM_WIDTH,IM_HEIGHT),FRAME_RATE,2,0).start()\n```\n\nThe card detector will work best if you use isolated rank and suit images generated from your own cards. To do this, run Rank_Suit_Isolator.py to take pictures of your cards. It will ask you to take a picture of an Ace, then a Two, and so on. Then, it will ask you to take a picture of one card from each of the suits (Spades, Diamonds, Clubs, Hearts). As you take pictures of the cards, the script will automatically isolate the rank or suit and save them in the Card_Imgs directory (overwriting the existing images).\n\n\n## Files\nCardDetector.py contains the main script\n\nCards.py has classes and functions that are used by CardDetector.py\n\nPiVideoStream.py creates a video stream from the PiCamera, and is used by CardDetector.py\n\nRank_Suit_Isolator.py is a standalone script that can be used to isolate the rank and suit from a set of cards to create train images\n\nCard_Imgs contains all the train images of the card ranks and suits\n\n## Dependencies\nPython 3.6\n\nOpenCV-Python 3.2.0 and numpy 1.8.2:\nSee https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/\nfor how to build and install OpenCV-Python on the Raspberry Pi\n\npicamera library:\n```\nsudo apt-get update\nsudo apt-get install python-picamera python3-picamera\n```\n\n\n"
 },
 {
  "repo": "chandrikadeb7/Face-Mask-Detection",
  "language": "Python",
  "readme_contents": "<h1 align=\"center\">Face Mask Detection</h1>\n\n<div align= \"center\">\n  <h4>Face Mask Detection system built with OpenCV, Keras/TensorFlow using Deep Learning and Computer Vision concepts in order to detect face masks in static images as well as in real-time video streams.</h4>\n</div>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n![Python](https://img.shields.io/badge/python-v3.6+-blue.svg)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/chandrikadeb7/Face-Mask-Detection/issues)\n[![Forks](https://img.shields.io/github/forks/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/network/members)\n[![Stargazers](https://img.shields.io/github/stars/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/stargazers)\n[![Issues](https://img.shields.io/github/issues/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/issues)\n[![MIT License](https://img.shields.io/github/license/chandrikadeb7/Face-Mask-Detection.svg?style=flat-square)](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/LICENSE)\n[![LinkedIn](https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555)](https://www.linkedin.com/in/chandrika-deb/)\n\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n![Live Demo](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Demo.gif)\n\n\n\n## :innocent: Motivation\nIn the present scenario due to Covid-19, there is no efficient face mask detection applications which are now in high demand for transportation means, densely populated areas, residential districts, large-scale manufacturers and other enterprises to ensure safety. Also, the absence of large datasets of __\u2018with_mask\u2019__ images has made this task more cumbersome and challenging. \n\n \n## :hourglass: Project Demo\n:movie_camera: [YouTube Demo Link](https://www.youtube.com/watch?v=AAkNyZlUae0)\n\n:computer: [Dev Link](https://dev.to/chandrikadeb7/face-mask-detection-my-major-project-3fj3)\n\n[![Already deployed version](https://raw.githubusercontent.com/vasantvohra/TrashNet/master/hr.svg)](https://face-mask--detection-app.herokuapp.com/)\n\n\n\n<p align=\"center\"><img src=\"https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Screen%20Shot%202020-05-14%20at%208.49.06%20PM.png\" width=\"700\" height=\"400\"></p>\n\n\n## :warning: TechStack/framework used\n\n- [OpenCV](https://opencv.org/)\n- [Caffe-based face detector](https://caffe.berkeleyvision.org/)\n- [Keras](https://keras.io/)\n- [TensorFlow](https://www.tensorflow.org/)\n- [MobileNetV2](https://arxiv.org/abs/1801.04381)\n\n## :star: Features\nOur face mask detector didn't use any morphed masked images dataset. The model is accurate, and since we used the MobileNetV2 architecture, it\u2019s also\u00a0computationally efficient and thus making it easier to deploy the model to embedded systems (Raspberry Pi, Google Coral, etc.).\n\nThis system can therefore be used in real-time applications which require face-mask detection for safety purposes due to the outbreak of Covid-19. This project can be integrated with embedded systems for application in airports, railway stations, offices, schools, and public places to ensure that public safety guidelines are followed.\n\n## :file_folder: Dataset\nThe dataset used can be downloaded here - [Click to Download](https://drive.google.com/drive/folders/1XDte2DL2Mf_hw4NsmGst7QtYoU7sMBVG?usp=sharing)\n\nThis dataset consists of\u00a0__3835 images__\u00a0belonging to two classes:\n*\t__with_mask: 1916 images__\n*\t__without_mask: 1919 images__\n\nThe images used were real images of faces wearing masks. The images were collected from the following sources:\n\n* __Bing Search API__ ([See Python script](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/search.py))\n* __Kaggle datasets__ \n* __RMFD dataset__ ([See here](https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset))\n\n## :key: Prerequisites\n\nAll the dependencies and required libraries are included in the file <code>requirements.txt</code> [See here](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/requirements.txt)\n\n## \ud83d\ude80&nbsp; Installation\n1. Clone the repo\n```\n$ git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git\n```\n\n2. Change your directory to the cloned repo and create a Python virtual environment named 'test'\n```\n$ mkvirtualenv test\n```\n\n3. Now, run the following command in your Terminal/Command Prompt to install the libraries required\n```\n$ pip3 install -r requirements.txt\n```\n\n## :bulb: Working\n\n1. Open terminal. Go into the cloned project directory and type the following command:\n```\n$ python3 train_mask_detector.py --dataset dataset\n```\n\n2. To detect face masks in an image type the following command: \n```\n$ python3 detect_mask_image.py --image images/pic1.jpeg\n```\n\n3. To detect face masks in real-time video streams type the following command:\n```\n$ python3 detect_mask_video.py \n```\n## :key: Results\n\n#### Our model gave 93% accuracy for Face Mask Detection after training via <code>tensorflow-gpu==2.0.0</code>\n\n![](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Screenshot%202020-06-01%20at%209.48.27%20PM.png)\n\n#### We got the following accuracy/loss training curve plot\n![](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/plot.png)\n\n## Streamlit app\n\nFace Mask Detector webapp using Tensorflow & Streamlit\n\ncommand\n```\n$ streamlit run app.py \n```\n## Images\n\n<p align=\"center\">\n  <img src=\"Readme_images/1.PNG\">\n</p>\n<p align=\"center\">Upload Images</p>\n\n<p align=\"center\">\n  <img src=\"Readme_images/2.PNG\">\n</p>\n<p align=\"center\">Results</p>\n\n## :clap: And it's done!\nFeel free to mail me for any doubts/query \n:email: chandrikadeb7@gmail.com\n\n## :handshake: Contribution\nFeel free to **file a new issue** with a respective title and description on the the [Face-Mask-Detection](https://github.com/chandrikadeb7/Face-Mask-Detection/issues) repository. If you already found a solution to your problem, **I would love to review your pull request**! \n\n## :heart: Owner\nMade with :heart:&nbsp;  by [Chandrika Deb](https://github.com/chandrikadeb7)\n\n## :+1: Credits\n* [https://www.pyimagesearch.com/](https://www.pyimagesearch.com/)\n* [https://www.tensorflow.org/tutorials/images/transfer_learning](https://www.tensorflow.org/tutorials/images/transfer_learning)\n\n## :eyes: License\nMIT \u00a9 [Chandrika Deb](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/LICENSE)\n"
 },
 {
  "repo": "estherjk/face-detection-node-opencv",
  "language": "JavaScript",
  "readme_contents": "# face-detection-node-opencv\n\nReal-time face detection using OpenCV, Node.js, and WebSockets.\n\nClick [here](http://youtu.be/v2SY0naPBFw) to see it in action.\n\n## Requirements\n\n* [Node.js](http://nodejs.org/)\n* [OpenCV 3.4.0](http://opencv.org/)\n    * May also work with older versions of OpenCV, but most recently tested with OpenCV 3.4.0\n* A webcam, e.g. laptop-integrated webcam, USB webcam\n\n## Installing Node.js packages\n\n* Navigate to the `server` directory\n* To install the packages: `npm install`\n\n## Running the demo\n\n* Make sure you are still in the `server` directory\n* To run the server: `node server.js`\n* To run the demo locally, open a browser and go to `localhost:8080`\n\nThe app should be up and running!\n"
 },
 {
  "repo": "glassechidna/zxing-cpp",
  "language": "C++",
  "readme_contents": "# ZXing C++ Port\n\n[![Build Status](https://travis-ci.org/glassechidna/zxing-cpp.svg?branch=master)](https://travis-ci.org/glassechidna/zxing-cpp)\n\n[ZXing](https://github.com/zxing/zxing) is/was a Java library.\n\nAt some point a complete C++ port/rewrite was created and maintained in the official [ZXing](https://github.com/zxing/zxing) repo. However, at the time of writing the C++ port is no longer maintained and has been removed from the official ZXing repo.\n\nThis project was forked from the [last ZXing commit](https://github.com/zxing/zxing/commit/00f6340) to contain the C++ project, with the following exceptions\n\n * scons (Python) build system has been deleted.\n * Deleted black box tests, because they refer to a large test data in ZXing repo.\n * Added appropriate copyright/licensing details (based on those in the ZXing repo).\n * Updated README.md\n\nRemoval of build systems was done to minimise maintenance burden.\n\nIf tests and XCode projects (other than those produced automatically be CMake) are desired, then another repo should be created and this repo referenced as a submodule. \n\n# Building using CMake\n\nCMake is a tool, that generates native makefiles and workspaces. It integrates well with a number of IDEs including Qt Creator and Visual Studio.\n\nUsage with CLion or Qt Creator:\n\n  1. Simply open `CMakeLists.txt` as a new project\n  2. Additional command line arguments can be specified (see below)\n\nUsage with Makefiles, Visual Studio, etc. (see `cmake --help` for a complete list of generators):\n\n  1. `mkdir build`\n  2. `cd` to `build`\n  3. Unix: run `cmake -G \"Unix Makefiles\" ..`\n  3. Windows: run `cmake -G \"Visual Studio 10\" ..`\n  \nYou can switch between build modes by specifying:\n\n  - `-DCMAKE_BUILD_TYPE=Debug` or\n  - `-DCMAKE_BUILD_TYPE=Release`\n\n# OpenCV integration\n\nWhen build on a system where opencv is installed the open cv bridge classes and executable will be built too.\n\n# Development tips\n\nTo profile the code (very useful to optimize the code):\n\n  1. Install Valgrind\n  2. Run `valgrind --tool=callgrind build/zxing - path/to/test/data/*.jpg > report.html`\n  3. Analyze output using KCachegrind\n"
 },
 {
  "repo": "JunshengFu/vehicle-detection",
  "language": "Python",
  "readme_contents": "# **Vehicle Detection for Autonomous Driving** \n\n## Objective\n\n#### A demo of Vehicle Detection System: a monocular camera is used for detecting vehicles. \n\n\n#### [**(1) Highway Drive (with Lane Departure Warning)**](https://youtu.be/Brh9-uab7Qs) (Click to see the full video)\n\n[![gif_demo1][demo1_gif]](https://youtu.be/Brh9-uab7Qs)\n\n#### [**(2) City Drive (Vehicle Detection only)**](https://youtu.be/2wOxK86LcaM) (Click to see the full video)\n[![gif_demo2][demo2_gif]](https://youtu.be/2wOxK86LcaM)\n\n---\n\n### Code & Files\n\n#### 1. My project includes the following files\n\n* [main.py](main.py) is the main code for demos\n* [svm_pipeline.py](svm_pipeline.py) is the car detection pipeline with SVM\n* [yolo_pipeline.py](yolo_pipeline.py) is the car detection pipeline with a deep net [YOLO (You Only Look Once)](https://arxiv.org/pdf/1506.02640.pdf)\n* [visualization.py](visualizations.py) is the function for adding visalization\n\n---\nOthers are the same as in the repository of [Lane Departure Warning System](https://github.com/JunshengFu/autonomous-driving-lane-departure-warning):\n* [calibration.py](calibration.py) contains the script to calibrate camera and save the calibration results\n* [lane.py](model.h5) contains the lane class \n* [examples](examples) folder contains the sample images and videos\n\n\n#### 2. Dependencies & my environment\n\nAnaconda is used for managing my [**dependencies**](https://github.com/udacity/CarND-Term1-Starter-Kit).\n* You can use provided [environment-gpu.yml](environment-gpu.yml) to install the dependencies.\n* OpenCV3, Python3.5, tensorflow, CUDA8  \n* OS: Ubuntu 16.04\n\n#### 3. How to run the code\n\n(1) Download weights for YOLO\n\nYou can download the weight from [here](https://drive.google.com/open?id=0B5WIzrIVeL0WS3N2VklTVmstelE) and save it to\nthe [weights](weights) folder.\n\n(2) If you want to run the demo, you can simply run:\n```sh\npython main.py\n```\n\n#### 4. Release History\n\n* 0.1.1\n    * Fix two minor bugs and update the documents\n    * Date 18 April 2017\n\n* 0.1.0\n    * The first proper release\n    * Date 31 March 2017\n\n---\n\n### **Two approaches: Linear SVM vs Neural Network**\n\n### 1. Linear SVM Approach\n`svm_pipeline.py` contains the code for the svm pipeline.\n\n**Steps:**\n\n* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n* A color transform is applied to the image and append binned color features, as well as histograms of color, to HOG feature vector. \n* Normalize your features and randomize a selection for training and testing.\n* Implement a sliding-window technique and use SVM classifier to search for vehicles in images.\n* Run pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n* Estimate a bounding box for detected vehicles.\n\n[//]: # (Image References)\n[image1]: ./examples/car_not_car.png\n[image2]: ./examples/hog_1.png\n[image2-1]: ./examples/hog_2.png\n[image3]: ./examples/search_windows.png\n[image4]: ./examples/heat_map1.png\n[image5]: ./examples/heat_map2.png\n[image6]: ./examples/labels_map.png\n[image7]: ./examples/svn_1.png\n[image8]: ./examples/yolo_1.png\n[image_yolo1]: ./examples/yolo1.png\n[image_yolo2]: ./examples/yolo2.png\n[video1]: ./project_video.mp4\n[demo1_gif]: ./examples/demo1.gif\n[demo2_gif]: ./examples/demo2.gif\n\n#### 1.1 Extract Histogram of Oriented Gradients (HOG) from training images\nThe code for this step is contained in the function named `extract_features` and codes from line 464 to 552 in `svm_pipeline.py`. \n If the SVM classifier exist, load it directly. \n \n Otherwise, I started by reading in all the `vehicle` and `non-vehicle` images, around 8000 images in each category.  These datasets are comprised of \n images taken from the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html) and \n [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/).\n Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n\n![alt text][image1]\n\n\nI then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n\nHere is an example using the `RGB` color space and HOG parameters of `orientations=9`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n\n![alt text][image2]\n![alt text][image2-1]\n \nTo optimize the HoG extraction, I **extract the HoG feature for the entire image only once**. Then the entire HoG image\nis saved for further processing. (see line 319 to 321 in  `svm_pipeline.py`)\n\n#### 1.2 Final choices of HOG parameters, Spatial Features and Histogram of Color.\n\nI tried various combinations of parameters and choose the final combination as follows \n(see line 16-27 in `svm_pipeline.py`):\n* `YCrCb` color space\n* orient = 9  # HOG orientations\n* pix_per_cell = 8 # HOG pixels per cell\n* cell_per_block = 2 # HOG cells per block, which can handel e.g. shadows\n* hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n* spatial_size = (32, 32) # Spatial binning dimensions\n* hist_bins = 32    # Number of histogram bins\n* spatial_feat = True # Spatial features on or off\n* hist_feat = True # Histogram features on or off\n* hog_feat = True # HOG features on or off\n\nAll the features are **normalized** by line 511 to 513 in `svm_pipeline.py`, which is a critical step. Otherwise, classifier \nmay have some bias toward to the features with higher weights.\n#### 1.3. How to train a classifier\nI randomly select 20% of images for testing and others for training, and a linear SVM is used as classifier (see line\n520 to 531 in `svm_pipeline.py`)\n\n#### 1.4 Sliding Window Search\nFor this SVM-based approach, I use two scales of the search window (64x64 and 128x128, see line 41) and search only between \n[400, 656] in y axis (see line 32 in `svm_pipeline.py`). I choose 75% overlap for the search windows in each scale (see \nline 314 in `svm_pipeline.py`). \n\nFor every window, the SVM classifier is used to predict whether it contains a car nor not. If yes, save this window (see \nline 361 to 366 in `svm_pipeline.py`). In the end, a list of windows contains detected cars are obtianed.\n\n![alt text][image3]\n\n#### 1.5 Create a heat map of detected vehicles\nAfter obtained a list of windows which may contain cars, a function named `generate_heatmap` (in line 565 in \n`svm_pipeline.py`) is used to generate a heatmap. Then a threshold is used to filter out the false positives.\n\n![heatmap][image4]\n![heatmap][image5]\n\n#### 1.6 Image vs Video implementation\n**For image**, we could directly use the result from the filtered heatmap to create a bounding box of the detected \nvehicle. \n\n**For video**, we could further utilize neighbouring frames to filter out the false positives, as well as to smooth \nthe position of bounding box. \n* Accumulate the heatmap for N previous frame.  \n* Apply weights to N previous frames: smaller weights for older frames (line 398 to 399 in `svm_pipeline.py`).\n* I then apply threshold and use `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  \n* I then assume each blob corresponded to a vehicle and constructe bounding boxes to cover the area of each blob detected.  \n\n\n#### Example of test image\n\n![alt text][image7]\n\n---\n\n\n### 2. Neural Network Approach (YOLO)\n`yolo_pipeline.py` contains the code for the yolo pipeline. \n\n[YOLO](https://arxiv.org/pdf/1506.02640.pdf) is an object detection pipeline baesd on Neural Network. Contrast to prior work on object detection with classifiers \nto perform detection, YOLO frame object detection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n\n![alt text][image_yolo2]\n\nSteps to use the YOLO for detection:\n* resize input image to 448x448\n* run a single convolutional network on the image\n* threshold the resulting detections by the model\u2019s confidence\n\n![alt text][image_yolo1]\n\n`yolo_pipeline.py` is modified and integrated based on this [tensorflow implementation of YOLO](https://github.com/gliese581gg/YOLO_tensorflow).\nSince the \"car\" is known to YOLO, I use the precomputed weights directly and apply to the entire input frame.\n\n#### Example of test image\n![alt text][image8]\n\n---\n\n### Discussion\nFor the SVM based approach, the accuray is good, but the speed (2 fps) is an problem due to the fact of sliding window approach \nis time consuming! We could use image downsampling, multi-threads, or GPU processing to improve the speed. But, there are probably\na lot engineering work need to be done to make it running real-time. Also, in this application, I limit the vertical searching \nrange to control the number of searching windows, as well as avoid some false positives (e.g. cars on the tree).\n\nFor YOLO based approach, it achieves real-time and the accuracy are quite satisfactory. Only in some cases, it may failure to\n detect the small car thumbnail in distance. My intuition is that the original input image is in resolution of 1280x720, and it needs to be downscaled\n to 448x448, so the car in distance will be tiny and probably quite distorted in the downscaled image (448x448). In order to \n correctly identify the car in distance, we might need to either crop the image instead of directly downscaling it, or retrain \n the network.\n"
 },
 {
  "repo": "pageauc/speed-camera",
  "language": "Python",
  "readme_contents": "# SPEED CAMERA - Object Motion Tracker [![Mentioned in Awesome <INSERT LIST NAME>](https://awesome.re/mentioned-badge.svg)](https://github.com/thibmaek/awesome-raspberry-pi)\n### RPI, Unix and Windows Speed Camera Using python, openCV, USB Cam or RPI camera module\n## For Details See [Program Features](https://github.com/pageauc/speed-camera/wiki/Program-Description#program-features) and [Wiki Instructions](https://github.com/pageauc/speed-camera/wiki) and [YouTube Videos](https://github.com/pageauc/speed-camera#links)\n\n## RPI Quick Install or Upgrade   \n***IMPORTANT*** - A raspbian **sudo apt-get update** and **sudo apt-get upgrade** will\n**NOT** be performed as part of   \n**speed-install.sh** so it is recommended you run these prior to install\nto ensure your system is up-to-date.     \n\n***Step 1*** With mouse left button highlight curl command in code box below. Right click mouse in **highlighted** area and Copy.     \n***Step 2*** On RPI putty SSH or terminal session right click, select paste then Enter to download and run script.  \n\n    curl -L https://raw.github.com/pageauc/speed-camera/master/speed-install.sh | bash\n\nThis will download and run the **speed-install.sh** script. If running under python3 you will need opencv3 installed.\nSee my Github [menu driven compile opencv3 from source](https://github.com/pageauc/opencv3-setup) project\n\n## Program Description   \nThis is a raspberry pi, Windows, Unix Distro computer openCV object speed camera demo program.\nIt is written in python and uses openCV to detect and track the x,y coordinates of the \nlargest moving object in the camera view above a minimum pixel area.\nUser variables are stored in the [***config.py***](https://github.com/pageauc/speed-camera/blob/master/config.py) file.\nMotion detection is restricted between ***y_upper***, ***y_lower***, ***x_left***, ***x_right*** variables  (road or area of interest).\nIf a track is longer than ***track_len_trig*** variable then average speed will be \ncalculated based on ***cal_obj_px*** and ***cal_obj_mm*** variables and a speed photo will be\ntaken and saved in ***media/images*** dated subfolders per variable ***imageSubDirMaxFiles*** = ***1000*** \n(see config.py). \n\nIf ***log_data_to_CSV*** = ***True*** then a ***speed-cam.csv*** file will be created/updated with event data stored in\nCSV (Comma Separated Values) format. This can be imported into a spreadsheet, database, Etc program for further processing.\nRelease 8.9 adds a **sqlite3** database to store speed data. Default is ***data/speed_cam.db*** with data in the ***speed*** table .\nthere is a simple report ***sql_speed_gt.sh*** that can query for records with greater than a specified speed.\nI plan on doing more but this should be a good start. Take a look at the code for details.\n\nAlso included are \n  \n* [***menubox.sh***](https://github.com/pageauc/speed-camera/wiki/Admin-and-Settings#manage-settings-using-menuboxsh)\nscript is a whiptail menu system to allow easier management of program settings and operation. \n* [***webserver.py***](https://github.com/pageauc/speed-camera/wiki/How-to-View-Data#how-to-view-images-and-or-data-from-a-web-browser)\nAllows viewing images and/or data from a web browser (see config.py for webserver settings)\n* [***rclone***](https://github.com/pageauc/speed-camera/wiki/Manage-rclone-Remote-Storage-File-Transfer)\nfor optional remote file sync to a remote storage service like google drive, DropBox and many others. \n* [***watch-app.sh***](https://github.com/pageauc/speed-camera/wiki/watch-app.sh-Remote-Manage-Config)\nfor administration of settings from a remote storage service. Plus application monitoring.\n* [***sql_speed_gt.py***](https://github.com/pageauc/speed-camera/blob/master/sql_speed_gt.sh) Prompts for a speed value and runs a simple\nsqlite3 query to show all record that exceed the specified speed. Output can be found in media/reports folder and is available from browser. \n* [***sql_hour_count.py***](https://github.com/pageauc/speed-camera/blob/master/sql_hour_count.py) Run report for count by hour.\nalso produces a graph using gnuplot. Query output html report and .png graph can be found in media/reports folder and is available from browser.\n* [***alpr-speed.py***](https://github.com/pageauc/speed-camera/wiki/alpr-speed.py---Process-speed-images-with-OPENALPR-Automatic-License-Plate-Reader)\nProcess speed camera images with [OPENALPR](https://github.com/openalpr/openalpr) License plate reader\n* [***speed-search.py***](https://github.com/pageauc/rpi-speed-camera/wiki/How-to-Run-speed-search.py)\nallows searching for similar target object images using opencv template matching. \n* [***makehtml.py***](https://github.com/pageauc/speed-camera/wiki/How-to-View-Data#view-combined-imagedata-html-pages-on-a-web-browser)\ncreates html files that combine csv and image data for easier viewing from a web browser.\n(Does not work with ***secpicam480.py*** or ***secwebcam480.py*** plugins enabled.\n\n## Requirements\n[***Raspberry Pi computer***](https://www.raspberrypi.org/documentation/setup/) and a [***RPI camera module installed***](https://www.raspberrypi.org/documentation/usage/camera/)\nor USB Camera plugged in. Make sure hardware is tested and works. Most [RPI models](https://www.raspberrypi.org/products/) will work OK. \nA quad core RPI will greatly improve performance due to threading. A recent version of \n[Raspbian operating system](https://www.raspberrypi.org/downloads/raspbian/) is Recommended.   \nor  \n***MS Windows or Unix distro*** computer with a USB Web Camera plugged in and a\n[recent version of python installed](https://www.python.org/downloads/)\nFor Details See [***Wiki details***](https://github.com/pageauc/speed-camera/wiki/Prerequisites-and-Install#windows-or-non-rpi-unix-installs).\n\nIt is recommended you upgrade to OpenCV version 3.x.x  For Easy compile of opencv 3.4.2 from source \nSee https://github.com/pageauc/opencv3-setup\n\n## Windows or Non RPI Unix Installs\nFor Windows or Unix computer platforms (non RPI or Debian) ensure you have the most\nup-to-date python version. For Downloads visit https://www.python.org/downloads    \n\nThe latest python versions includes numpy and recent opencv version that is required to run this code. \nYou will also need a USB web cam installed and working. \nTo install this program access the GitHub project page at https://github.com/pageauc/speed-camera\nSelect the ***green Clone or download*** button. The files will be cloned or zipped\nto a speed-camera folder. You can run the code from python IDLE application (recommended), GUI desktop\nor command prompt terminal window. Note bash .sh shell scripts will not work with windows unless \nspecial support for bash is installed for windows Eg http://win-bash.sourceforge.net/  http://www.cygwin.com/\nNote I have Not tested these.   \n\n***IMPORTANT*** speed-cam.py ver 8.x or greater Requires Updated config.py and plugins.\n\n    cd ~/speed-camera\n    cp config.py config.py.bak\n    cp config.py.new config.py\n    \nTo replace plugins rename (or delete) plugins folder per below\n\n    cd ~/speed-camera\n    mv plugins pluginsold   # renames plugins folder\n    rm -r plugins           # deletes plugins folder\n\nThen run ***menubox.sh*** UPGRADE menu pick.\n \n## Manual Install or Upgrade   \nFrom logged in RPI SSH session or console terminal perform the following. Allows you to review install code before running\n\n    cd ~\n    wget https://raw.github.com/pageauc/speed-camera/master/speed-install.sh\n    more speed-install.sh       # You can review code if you wish\n    chmod +x speed-install.sh\n    ./speed-install.sh  # runs install script.\n    \n## Run to view verbose logging \n\n    cd ~/speed-camera    \n    ./speed-cam.py\n    \nSee [***How to Run***](https://github.com/pageauc/speed-camera/wiki/How-to-Run) speed-cam.py wiki section\n\n***IMPORTANT*** Speed Camera will start in ***calibrate*** = ***True*** Mode.    \nReview settings in ***config.py*** file and edit variables with nano as required.\nYou will need to perform a calibration to set the correct value for config.py ***cal_obj_px*** and ***cal_obj_mm*** \nvariables based on the distance from camera to objects being measured for speed.\nSee [***Calibration Procedure***](https://github.com/pageauc/speed-camera/wiki/Calibrate-Camera-for-Distance) for more details.     \n\nThe config.py motion tracking variable called track_counter = can be adjusted for your system and opencv version.\ndefault is 5 but a quad core RPI3 and latest opencv version eg 3.4.2 can be 10-15 or possibly greater. \n    \n## Run menubox.sh \n\n    cd ~/speed-camera\n    ./menubox.sh\n\nAdmin speed-cam Easier using menubox.sh (Once calibrated and/or testing complete)  \n![menubox main menu](https://github.com/pageauc/speed-camera/blob/master/menubox.png)     \n\nView speed-cam data and trends from web browser per sample screen shots\n\n![Speed Camera Web Recent View](https://github.com/pageauc/speed-camera/blob/master/speed_web_recent.png)   \n![Speed Camera Web html speed list Report](https://github.com/pageauc/speed-camera/blob/master/speed_web_sqlite.png)   \n![Speed Camera Web Recent View](https://github.com/pageauc/speed-camera/blob/master/speed_web_gnuplot.png)   \n\n## Links\n* YouTube Speed Lapse Video https://youtu.be/-xdB_x_CbC8\n* YouTube Speed Camera Video https://youtu.be/eRi50BbJUro\n* YouTube motion-track video https://youtu.be/09JS7twPBsQ\n* Speed Camera RPI Forum post https://www.raspberrypi.org/forums/viewtopic.php?p=1004150#p1004150\n* YouTube Channel https://www.youtube.com/user/pageaucp \n* Speed Camera GitHub Repo https://github.com/pageauc/speed-camera      \n\n## Credits  \nSome of this code is based on a YouTube tutorial by\nKyle Hounslow using C here https://www.youtube.com/watch?v=X6rPdRZzgjg\n\nThanks to Adrian Rosebrock jrosebr1 at http://www.pyimagesearch.com \nfor the PiVideoStream Class code available on github at\nhttps://github.com/jrosebr1/imutils/blob/master/imutils/video/pivideostream.py\n  \nHave Fun   \nClaude Pageau    \nYouTube Channel https://www.youtube.com/user/pageaucp   \nGitHub Repo https://github.com/pageauc\n"
 },
 {
  "repo": "goncalopp/simple-ocr-opencv",
  "language": "Python",
  "readme_contents": "# Simple Python OCR\n[![Build Status](https://travis-ci.org/goncalopp/simple-ocr-opencv.svg?branch=master)](https://travis-ci.org/goncalopp/simple-ocr-opencv)\n\nA simple pythonic OCR engine using opencv and numpy.\n\nOriginally inspired by [this stackoverflow question](http://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python)\n\n### Essential Concepts\n\n#### Segmentation\n\nIn order for OCR to be performed on a image, several steps must be \nperformed on the source image. Segmentation is the process of \nidentifying the regions of the image that represent characters. \n\nThis project uses rectangles to model segments. \n\n#### Supervised learning with a classification problem\n\nThe [classification problem][] consists in identifying to which class a \nobservation belongs to (i.e.: which particular character is contained \nin a segment).\n\n[Supervised learning][] is a way of \"teaching\" a machine. Basically, an \nalgorithm is *trained* through *examples* (i.e.: this particular \nsegment contains the character `f`). After training, the machine \nshould be able to apply its acquired knowledge to new data.\n\nThe [k-NN algorithm], used in this project, is one of the simplest  \nclassification algorithm.\n\n#### Grounding\n\nCreating a example image with already classified characters, for \ntraining purposes.\nSee [ground truth][].\n\n[classification problem]: https://en.wikipedia.org/wiki/Statistical_classification\n[Supervised learning]: https://en.wikipedia.org/wiki/Supervised_learning\n[k-NN algorithm]: https://en.wikipedia.org/wiki/K-nearest_neighbors_classification\n[ground truth]: https://en.wikipedia.org/wiki/Ground_truth\n\n#### How to understand this project\n\nUnfortunately, documentation is a bit sparse at the moment (I \ngladly accept contributions).\nThe project is well-structured, and most classes and functions have \ndocstrings, so that's probably a good way to start.\n\nIf you need any help, don't hesitate to contact me. You can find my \nemail on my github profile.\n\n\n#### How to use\n\nPlease check `example.py` for basic usage with the existing pre-grounded images.\n\nYou can use your own images, by placing them on the `data` directory. \nGrounding images interactively can be accomplished by using `grounding.UserGrounder`.\nFor more details check `example_grounding.py`\n\n#### Copyright and notices\n\nThis project is available under the [GNU AGPLv3 License](https://www.gnu.org/licenses/agpl-3.0.txt), a copy\nshould be available in LICENSE. If not, check out the link to learn more.\n \n    Copyright (C) 2012-2017 by the simple-ocr-opencv authors\n    All authors are the copyright owners of their respective additions\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU AGPLv3 License, as found in LICENSE.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.    \n  \n"
 },
 {
  "repo": "lincolnhard/head-pose-estimation",
  "language": "C++",
  "readme_contents": "# head-pose-estimation\nReal-time head pose estimation built with OpenCV and dlib \n\n<b>2D:</b><br>Using dlib for facial features tracking, modified from http://dlib.net/webcam_face_pose_ex.cpp.html\n<br>The algorithm behind it is described in http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf\n<br>It applies cascaded regression trees to predict shape(feature locations) change in every frame.\n<br>Splitting nodes of trees are trained in random, greedy, maximizing variance reduction fashion.\n<br>The well trained model can be downloaded from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 \n<br>Training set is based on i-bug 300-W datasets. It's annotation is shown below:<br><br>\n![ibug](https://cloud.githubusercontent.com/assets/16308037/24229391/1910e9cc-0fb4-11e7-987b-0fecce2c829e.JPG)\n<br><br>\n<b>3D:</b><br>To match with 2D image points(facial features) we need their corresponding 3D model points. \n<br>http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp provides a similar 3D facial feature model.\n<br>It's annotation is shown below:<br><br>\n![gl](https://cloud.githubusercontent.com/assets/16308037/24229340/ea8bad94-0fb3-11e7-9e1d-0a2217588ba4.jpg)\n<br><br>\nFinally, with solvepnp function in OpenCV, we can achieve real-time head pose estimation.\n<br><br>\n\n"
 },
 {
  "repo": "macmade/OpenCV-iOS",
  "language": "Makefile",
  "readme_contents": "OpenCV-iOS\n==========\n\n[![Build Status](https://img.shields.io/travis/macmade/OpenCV-iOS.svg?branch=master&style=flat)](https://travis-ci.org/macmade/OpenCV-iOS)\n[![Issues](http://img.shields.io/github/issues/macmade/OpenCV-iOS.svg?style=flat)](https://github.com/macmade/OpenCV-iOS/issues)\n![Status](https://img.shields.io/badge/status-inactive-lightgray.svg?style=flat)\n![License](https://img.shields.io/badge/license-bsd-brightgreen.svg?style=flat)\n[![Contact](https://img.shields.io/badge/contact-@macmade-blue.svg?style=flat)](https://twitter.com/macmade)  \n[![Donate-Patreon](https://img.shields.io/badge/donate-patreon-yellow.svg?style=flat)](https://patreon.com/macmade)\n[![Donate-Gratipay](https://img.shields.io/badge/donate-gratipay-yellow.svg?style=flat)](https://www.gratipay.com/macmade)\n[![Donate-Paypal](https://img.shields.io/badge/donate-paypal-yellow.svg?style=flat)](https://paypal.me/xslabs)\n\nOverview\n--------\n\nOpenCV (Open Source Computer Vision) is a library of programming functions for real time computer vision.\nThis project is a port of the OpenCV library for Apple iOS. It includes two XCode projects: one for iPhone, the other one for iPad.\n\nOpenCV is released under the BSD License, it is free for both academic and commercial use.\n\nThe official OpenCV documentation can be found at the following address:\nhttp://opencv.willowgarage.com/wiki/\n\nPlease read the [documentation][1] to learn how to start developping iPhone and iPad applications powered by OpenCV, or start developing your applications now by [downloading][2] the XCode project files.\n\nYou can use the XCode project files freely.\nNote that you still need to give credits to OpenCV, as stated by the BSD License.\nThat said, we'll be very happy if you drop us a line when releasing your app!\n\nProject Status\n--------------\n\nThis project is no longer maintained.  \nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\n\n[1]: http://www.eosgarden.com/en/opensource/opencv-ios/documentation/   \"Documentation\"\n[2]: http://www.eosgarden.com/en/opensource/opencv-ios/download/        \"Download\"\n\nRepository Infos\n----------------\n\n    Owner:\t\t\tJean-David Gadina - XS-Labs\n    Web:\t\t\twww.xs-labs.com\n    Blog:\t\t\twww.noxeos.com\n    Twitter:\t\t@macmade\n    GitHub:\t\t\tgithub.com/macmade\n    LinkedIn:\t\tch.linkedin.com/in/macmade/\n    StackOverflow:\tstackoverflow.com/users/182676/macmade\n"
 },
 {
  "repo": "billmccord/OpenCV-Android",
  "language": "C++",
  "readme_contents": "= OpenCV-Android\n\nDedicated to providing an optimized port of OpenCV for the Google Android OS.\n\n== Requirements\n\nIn order to use OpenCV-Android, you will need to download and install both the Android SDK 1.6 and NDK r5.  It may or may not work with a higher / lower version. It has been confirmed that this doesn't work with older versions of Android SDK, so please use 1.6 or higher.\n\nIn addition to having the SDK or NDK you will also need to have one of the following:\n* An Android Phone Dev Phone (Might work with other phones, has not been tested)\n* The QuickTime Java Libraries (optional if you want to use the QTWebcamBroadcaster)\n\nFor those of you running on the emulator, I built a very simple Socket-based Camera server that will send images over a socket connection.  This uses the QuickTime libraries which are present by default on Mac OS X, but I haven't tested on other OSes.  If it doesn't work for you, you can always try the original WebcamBroadcaster I derived mine from which uses the JMF (which doesn't work with Mac OS X, hence the QTWebcamBroadcaster):\nhttp://www.tomgibara.com/android/camera-source\n\n== Build\n\nBuilding is now even easier with NDK r5.  I'm going to assume that you are familiar with Android if you are reading this and keep it rather short.\n\n* Make sure that ndk-build is in your path.\n\n* Go to the OpenCV-Android project directory and run 'ndk-build'\n  \nBy default, this will build the opencv library and push it into:\n[OPENCV_ANDROID_ROOT]/tests/VideoEmulation/libs\n\nYou can change where the lib is delivered by modifying the APP_PROJECT_PATH in:\n[OPENCV_ANDROID_ROOT]/Application.mk\n\nOnce you have built the OpenCV library, you can now build and [OPENCV_ANDROID_ROOT]/tests/VideoEmulation.\n\n<b>NOTE:</b> If you plan to use the Socket Camera, you will need to build and run the [OPENCV_ANDROID_ROOT]/tests/QTWebcamBroadcaster first.  Also, if you use Eclipse to develop Android, there are already projects defined for both of these applications.  You can simply import them into your workspace.\n\n<b>IMPORTANT NEW NOTE:</b> In order for the QTWebcamBroadcaster to work, you must ensure that you run it in a 32-bit VM.  Failure to run in 32-bit will result in an empty screen in your emulator (no image will appear.)  You can easily force the VM that runs this application to 32-bit by adding the VM flag:\n-d32\n\n\n== Setup\n\nIf you want to test face tracking, then you need to have a Haar Classifier Cascade XML.  I have provided one for use and it is stored in:\ntests/haarcascade_frontalface_alt.xml\n\nBefore attempting to run the VideoEmulator application, you must first copy this XML file into the emulator in the following location:\n/data/data/org.siprop.opencv/files/haarcascade_frontalface_alt.xml\n\nCurrently, this is a hard-coded path that we look up.  Hopefully, this can be remedied in a future version.\n\n== Run\n\nIn order to use the VideoEmulator, you have to use the emulator (hence the name.)  If you have a Dev Phone, you can play around with the old 'OpenCVSample' test or modify the VideoEmulator to support a real camera.  This is something we will work on resolving in the future.\n\nUsing the emulator there are two slightly different 'flavors' of running.  Both are socket based cameras, but one is written in C++ and emulates a real OpenCV Capture while the other (loosely) emulates a camera implementation in Java.  The C++ version is the default as it is slightly faster and takes a little less memory.  Also, the ultimate goal is to hook up with a real camera in C++ so that we don't have to pass huge amounts of data (images) back and forth through the JNI interface.\n\n<b>NOTE:</b> For all of these examples you cannot use localhost or 127.0.0.1 as your address for the socket camera.  The reason is because when the client is running on the Android emulator, both of these map to Android's localhost, not the machine you are running the emulator on.  This means you have to be connected to a network in order to use the socket camera, a limitation.\n\n=== C++\n\nSince this is the default, we have made it pretty easy...\n\n* Start the WebcamBroadcaster - this is a socket server that grabs images from your camera and serves them up\n* Start the VideoEmulator - this runs the Android application that allows you to try out the various pieces implemented thus far\n* Once the application comes up, you will have to configure for your machine address and port for the socket camera to work.\n* Leave Use C++ SocketCapture CHECKED!\n* Choose which test you want to run.\n\n=== Java\n\nTo use Java, you have to make a small code change.  Eventually we will make this a configurable option without having to make a code change.  The reason is because when we send data over a socket from Java to Java it is faster to send serialized buffered images.  However, when we send data to C++, we have to send a raw byte array.\n\n* Modify the WebcamBroadcaster by changing the default value assigned to RAW to false.\n* Start the WebcamBroadcaster - this is a socket server that grabs images from your camera and serves them up\n* Start the VideoEmulator - this runs the Android application that allows you to try out the various pieces implemented thus far\n* Once the application comes up, you will have to configure for your machine address and port for the socket camera to work.\n* UNCHECK Use C++ SocketCapture!\n* Choose which test you want to run.\n"
 },
 {
  "repo": "Web-Sight/WebSight",
  "language": "JavaScript",
  "readme_contents": "WebSight demonstrates a comparison of performance between JavaScript, <a href=\"http://asmjs.org/\">asm.js</a>, and <a href=\"http://webassembly.org/\">WebAssembly</a>. A user uploaded static image or live video is displayed for each target. Performance is measured by the length of time it takes to detect face(s) or eyes in the image or video.\n\nEach target is run in its own <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API\">web worker</a>. The popular open source computer vision library, <a href=\"http://opencv.org/\">OpenCV</a>, was compiled using <a href=\"http://kripken.github.io/emscripten-site/\">Emscripten</a> to asm.js and wasm (WebAssembly module) to utilize the <a href=\"https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework\">Viola-Jones algorithm</a> for object detection. <a href=\"https://github.com/foo123/HAAR.js\">HAAR.js</a> was modified to remove the DOM manipulation, so it could be used in the JavaScript web worker.\n\nThanks to <a href=\"https://github.com/shamadee/web-dsp\">WebDSP</a> for the inspiration, and to <a href=\"https://github.com/ucisysarch/opencvjs\">uscisysarch</a>, <a href=\"https://github.com/njor/opencvjs\">njor</a>, and <a href=\"https://github.com/foo123/HAAR.js\">foo123</a> for giving us a starting point.\n\nContributors to this project are <a href=\"https://github.com/BrianJFeldman\">BrianJFeldman</a>, <a href=\"https://github.com/DebraD\">DebraD</a>, <a href=\"https://github.com/MarkGeeRomano\">MarkGeeRomano</a> and <a href=\"https://github.com/YervantB\">YervantB</a>.\n"
 },
 {
  "repo": "qupath/qupath",
  "language": "Java",
  "readme_contents": "[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fqupath.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/qupath)\n\nQuPath\n======\n\n**QuPath is open source software for bioimage analysis**.\n\nFeatures include:\n\n* Extensive tools to annotate and view images, including whole slide images\n* Workflows for both IHC and H&E analysis\n* New algorithms for common tasks, e.g. cell segmentation, tissue microarray dearraying\n* Interactive machine learning, e.g. for cell & texture classification\n* Customization, batch-processing & data interrogation by scripting\n* Easy integration with other tools, including ImageJ\n\n\nTo **download QuPath**, go to the [Latest Releases](https://github.com/qupath/qupath/releases/latest) page.\n\nFor **documentation**, see [https://qupath.readthedocs.io](https://qupath.readthedocs.io)\n\nFor **help & support**, try [image.sc](https://forum.image.sc/tag/qupath) or the [links here](https://qupath.readthedocs.io/en/latest/docs/starting/help.html)\n\nTo **build QuPath from source** see [here](https://qupath.readthedocs.io/en/latest/docs/reference/building.html).\n\n\n*QuPath is an academic project intended for research use only.*\n*The software has been made freely available under the terms of the [GPLv3](https://github.com/qupath/qupath/blob/master/LICENSE.md) in the hope it is useful for this purpose, and to make analysis methods open and transparent.*\n\nIf you find QuPath useful in work that you publish, please [cite the publication](https://qupath.readthedocs.io/en/latest/docs/intro/citing.html).\n\n\n\n## Developers\n\nQuPath is being actively developed at the University of Edinburgh by:\n\n* Pete Bankhead (creator)\n* Melvin Gelbard\n\nFor all contributors, see [here](https://github.com/qupath/qupath/graphs/contributors).\n\n\n----\n\n## Background\n\nQuPath was designed, implemented and documented by Pete Bankhead while at Queen's University Belfast, with additional code and testing by Jose Fernandez.\n\nVersions up to v0.1.2 are copyright 2014-2016 The Queen's University of Belfast, Northern Ireland.\nThese were written as part of projects that received funding from:\n\n* Invest Northern Ireland (RDO0712612)\n* Cancer Research UK Accelerator (C11512/A20256)\n\n\n![Image](https://raw.githubusercontent.com/wiki/qupath/qupath/images/qupath_demo.jpg)\n"
 },
 {
  "repo": "MicrocontrollersAndMore/OpenCV_3_License_Plate_Recognition_Python",
  "language": "Python",
  "readme_contents": "The video pretty much explains it all:\nhttps://www.youtube.com/watch?v=fJcl6Gw1D8k\n"
 },
 {
  "repo": "meiqua/shape_based_matching",
  "language": "C++",
  "readme_contents": "# shape_based_matching  \n\nupdate:   \n**[fusion implementation to run faster!](https://github.com/meiqua/shape_based_matching/issues/77)**  \n**[icp is also refined to be faster and easier to use](https://github.com/meiqua/shape_based_matching/issues/100)**  \n\n[Transforms in shape-based matching](./Transforms%20in%20shape-based%20matching.pdf)  \n[pose refine with icp branch](https://github.com/meiqua/shape_based_matching/tree/icp2D), 0.1-0.5 degree accuracy   \n[icp + subpixel branch](https://github.com/meiqua/shape_based_matching/tree/subpixel), < 0.1 degree accuracy  \n[icp + subpixel + sim3(previous is so3) branch](https://github.com/meiqua/shape_based_matching/tree/sim3), deal with scale error  \n\ntry to implement halcon shape based matching, refer to machine vision algorithms and applications, page 317 3.11.5, written by halcon engineers  \nWe find that shape based matching is the same as linemod. [linemod pdf](Gradient%20Response%20Maps%20for%20Real-TimeDetection%20of%20Textureless%20Objects.pdf)  \n\nhalcon match solution guide for how to select matching methods([halcon documentation](https://www.mvtec.com/products/halcon/documentation/#reference_manual)):  \n![match](./match.png)  \n\n## steps\n\n1. change test.cpp line 9 prefix to top level folder\n\n2. in cmakeList line 23, change /opt/ros/kinetic to somewhere opencv3 can be found(if opencv3 is installed in default env then don't need to)\n\n3. cmake make & run. To learn usage, see different tests in test.cpp. Particularly, scale_test are fully commented.\n\nNOTE: On windows, it's confirmed that visual studio 17 works fine, but there are some problems with MIPP in vs13. You may want old codes without [MIPP](https://github.com/aff3ct/MIPP): [old commit](https://github.com/meiqua/shape_based_matching/tree/fc3560a1a3bc7c6371eacecdb6822244baac17ba)  \n\n## thoughts about the method\n\nThe key of shape based matching, or linemod, is using gradient orientation only. Though both edge and orientation are resistant to disturbance,\nedge have only 1bit info(there is an edge or not), so it's hard to dig wanted shapes out if there are too many edges, but we have to have as many edges as possible if we want to find all the target shapes. It's quite a dilemma.  \n\nHowever, gradient orientation has much more info than edge, so we can easily match shape orientation in the overwhelming img orientation by template matching across the img.  \n\nSpeed is also important. Thanks to the speeding up magic in linemod, we can handle 1000 templates in 20ms or so.  \n\n[Chinese blog about the thoughts](https://www.zhihu.com/question/39513724/answer/441677905)  \n\n## improvment\n\nComparing to opencv linemod src, we improve from 6 aspects:  \n\n1. delete depth modality so we don't need virtual func, this may speed up  \n\n2. opencv linemod can't use more than 63 features. Now wo can have up to 8191  \n\n3. simple codes for rotating and scaling img for training. see test.cpp for examples  \n\n4. nms for accurate edge selection  \n\n5. one channel orientation extraction to save time, slightly faster for gray img\n\n6. use [MIPP](https://github.com/aff3ct/MIPP) for multiple platforms SIMD, for example, x86 SSE AVX, arm neon.\n   To have better performance, we have extended MIPP to uint8_t for some instructions.(Otherwise we can only use\n   half feature points to avoid int8_t overflow)  \n\n7. rotate features directly to speed up template extractions; selectScatteredFeatures more \nevenly; exautive select all features if not enough rather than abort templates(but features <= 4 will abort)\n\n## some test\n\n### Example for circle shape  \n\n#### You can imagine how many circles we will find if use edges  \n![circle1](test/case0/1.jpg)\n![circle1](test/case0/result/1.png)  \n\n#### Not that circular  \n![circle2](test/case0/2.jpg)\n![circle2](test/case0/result/2.png)  \n\n#### Blur  \n![circle3](test/case0/3.png)\n![circle3](test/case0/result/3.png)  \n\n### circle template before and after nms  \n\n#### before nms\n\n![before](test/case0/features/no_nms_templ.png)\n\n#### after nms\n\n![after](test/case0/features/nms_templ.png)  \n\n### Simple example for arbitary shape\n\nWell, the example is too simple to show the robustness  \nrunning time: 1024x1024, 60ms to construct response map, 7ms for 360 templates  \n\ntest img & templ features  \n![test](./test/case1/result.png)  \n![templ](test/case1/templ.png)  \n\n\n### noise test  \n\n![test2](test/case2/result/together.png)  \n\n## some issues you may want to know  \nWell, issues are not clearly classified and many questions are discussed in one issue sometimes. For better reference, some typical discussions are pasted here.  \n\n[object too small?](https://github.com/meiqua/shape_based_matching/issues/13#issuecomment-474780205)  \n[failure case?](https://github.com/meiqua/shape_based_matching/issues/19#issuecomment-481153907)  \n[how to run even faster?](https://github.com/meiqua/shape_based_matching/issues/21#issuecomment-489664586)  \n\n"
 },
 {
  "repo": "A9T9/RPA",
  "language": "JavaScript",
  "readme_contents": "# UI.Vision [RPA](https://ui.vision/rpa) (formerly Kantu)\n\n- Modern Robotic Process Automation plus Selenium IDE++\n\nQuestions? Suggestions? - Meet us in the UI.Vision [RPA user forum](https://forum.ui.vision).\n\nEvery user benefits from the questions and answers provided in the forum, that is why we would ask you to post the question [in the RPA forum](https://forum.ui.vision) first if a public forum is appropriate for your question. The forum is monitored by active users, tech support and the developers, so we would like to concentrate the discussion \"over there\" in one place\n\n\n# Are you interested in becoming a UI.Vision RPA beta tester?\n\nAs beta tester, we will email you (hidden) install links for new  UI.Vision RPA versions before they go live in the Chrome store. **Beta versions never overwrite a regular  UI.Vision RPA version**. The beta channel is technically a separate extension with its own toolbar icon. So beta-testing  UI.Vision RPA does not interfere with your regular RPA projects and macros. We will notify you by email once a new version is available so you can grab it. \n\n[If you are interested in helping out, please sign-up here](http://eepurl.com/dm0cTX).\n\nThe link goes to a Mailchimp signup form. \n\n\n# How to install  UI.Vision RPA:\n\n UI.Vision RPA for Chrome and Firefox is modern cross-platform RPA software for macOS, Linux and Windows. It includes a Selenium IDE and Web Macro Recorder. You find the latest version always in the Chrome and Firefox stores. You can use it _completely free for private and commercial purposes_: \n\n- [UI.Vision RPA in the Google Chrome Webstore](https://chrome.google.com/webstore/detail/uivision-rpa/gcbalfbdmfieckjlnblleoemohcganoc)\n\n- [UI.Vision RPA in the Firefox Webstore](https://addons.mozilla.org/en-US/firefox/addon/rpa/)\n\n- [UI.Vision RPA plus Selenium IDE Homepage](https://ui.vision/rpa)\n\n- List of supported [Selenium IDE commands](https://ui.vision/rpa/docs/selenium-ide/)\n\n\n# Building the Chrome and Firefox Extension\n\nWe use Node V8.11.1 and NPM V5.6.0.\n\nYou can [install UI.Vision RPA directly from the Chrome or Firefox stores](https://ui.vision/rpa), which is the easiest and the recommended way of using the UI.Vision RPA software. But of course developers can also build it directly from the source code with this command line:\n\n```\nnpm i\nnpm run build (or build-ff for Firefox)\n```\n\nOnce done, the ready-to-use extension code appears in the /dist directory (Chrome) or /dist_ff directory (Firefox).\n\n# Need the very latest source code version?\n\nPlease note that we use an internal source code repository for our daily development work. The very latest source code snapshot can always be requested directly from the development team. Please contact us at team AT ui.vision. We are looking forward talking to you. And of course, if you want to join the development you are welcome.\n"
 },
 {
  "repo": "adipandas/multi-object-tracker",
  "language": "Python",
  "readme_contents": "[cars-yolo-output]: examples/assets/cars.gif \"Sample Output with YOLO\"\n[cows-tf-ssd-output]: examples/assets/cows.gif \"Sample Output with SSD\"\n\n# Multi-object trackers in Python\nEasy to use implementation of various multi-object tracking algorithms.\n\n[![DOI](https://zenodo.org/badge/148338463.svg)](https://zenodo.org/badge/latestdoi/148338463)\n\n\n`YOLOv3 + CentroidTracker` |  `TF-MobileNetSSD + CentroidTracker`\n:-------------------------:|:-------------------------:\n![Cars with YOLO][cars-yolo-output]  |  ![Cows with tf-SSD][cows-tf-ssd-output]\nVideo source: [link](https://flic.kr/p/L6qyxj) | Video source: [link](https://flic.kr/p/26WeEWy)\n\n\n## Available Multi Object Trackers\n\n```\nCentroidTracker\nIOUTracker\nCentroidKF_Tracker\nSORT\n```\n\n## Available OpenCV-based object detectors:\n\n```\ndetector.TF_SSDMobileNetV2\ndetector.Caffe_SSDMobileNet\ndetector.YOLOv3\n```\n\n## Installation\n\nPip install for OpenCV (version 3.4.3 or later) is available [here](https://pypi.org/project/opencv-python/) and can be done with the following command:\n\n```\ngit clone https://github.com/adipandas/multi-object-tracker\ncd multi-object-tracker\npip install -r requirements.txt\npip install -e .\n```\n\n**Note - for using neural network models with GPU**  \nFor using the opencv `dnn`-based object detection modules provided in this repository with GPU, you may have to compile a CUDA enabled version of OpenCV from source.  \n* To build opencv from source, refer the following links:\n[[link-1](https://docs.opencv.org/master/df/d65/tutorial_table_of_content_introduction.html)],\n[[link-2](https://www.pyimagesearch.com/2020/02/03/how-to-use-opencvs-dnn-module-with-nvidia-gpus-cuda-and-cudnn/)]\n\n## How to use?: Examples\n\nPlease refer [examples](https://github.com/adipandas/multi-object-tracker/tree/master/examples) folder of this repository.\nYou can clone and run the examples as shown [here](examples/readme.md).\n\n## Pretrained object detection models\n\nYou will have to download the pretrained weights for the neural-network models. \nThe shell scripts for downloading these are provided [here](https://github.com/adipandas/multi-object-tracker/tree/master/examples/pretrained_models) below respective folders.\nPlease refer [DOWNLOAD_WEIGHTS.md](DOWNLOAD_WEIGHTS.md) for more details.\n\n### Notes\n* There are some variations in implementations as compared to what appeared in papers of `SORT` and `IoU Tracker`.\n* In case you find any bugs in the algorithm, I will be happy to accept your pull request or you can create an issue to point it out.\n\n## References and Credits\n\nPlease see [REFERENCES.md](REFERENCES.md).\n\n## Citation\n\nIf you use this repository in your work, please consider citing it with:\n```\n@misc{multiobjtracker_amd2018,\n  author = {Deshpande, Aditya M.},\n  title = {Multi-object trackers in Python},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/adipandas/multi-object-tracker}},\n}\n```\n\n```\n@software{aditya_m_deshpande_2020_3951169,\n  author       = {Aditya M. Deshpande},\n  title        = {Multi-object trackers in Python},\n  month        = jul,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v1.0.0},\n  doi          = {10.5281/zenodo.3951169},\n  url          = {https://doi.org/10.5281/zenodo.3951169}\n}\n```\n"
 },
 {
  "repo": "avidLearnerInProgress/pyCAIR",
  "language": "Python",
  "readme_contents": ".. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME937258 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME707802 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME2888 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n"
 },
 {
  "repo": "PatWie/tensorflow-cmake",
  "language": "CMake",
  "readme_contents": "# TensorFlow CMake/C++ Collection\n\nLooking at the official docs: What do you see? The usual fare?\nNow, guess what: This is a bazel-free zone. We use CMake here!\n\nThis collection contains **reliable** and **dead-simple** examples to use TensorFlow in C, C++, Go and Python: load a pre-trained model or compile a custom operation with or without CUDA. All builds are tested against the most recent stable TensorFlow version and rely on CMake with a custom [FindTensorFlow.cmake](https://github.com/PatWie/tensorflow-cmake/blob/master/cmake/modules/FindTensorFlow.cmake). This cmake file includes common work arounds for bugs in specific TF versions.\n\n| TensorFlow | Status |\n| ------ | ------ |\n|   1.14.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.14.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.13.1 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.13.1/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.12.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.12.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.11.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.11.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.10.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.10.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.9.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.9.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n\n\n\nThe repository contains the following examples.\n\n| Example| Explanation |\n| ------ | ------ |\n| [custom operation](./custom_op)   | build a custom operation for TensorFLow in C++/CUDA (requires only pip) |\n| [inference  (C++)](./inference/cc) | run inference in C++ |\n| [inference  (C)](./inference/c) | run inference in C |\n| [inference  (Go)](./inference/go) | run inference in Go |\n| [event writer](./examples/event_writer)  | write event files for TensorBoard in C++ |\n| [keras cpp-inference example](./examples/keras)  | run a Keras-model in C++ |\n| [simple example](./examples/simple)  | create and run a TensorFlow graph in C++ |\n| [resize image example](./examples/resize)  | resize an image in TensorFlow with/without OpenCV |\n\n\n## Custom Operation\n\nThis example illustrates the process of creating a custom operation using C++/CUDA and CMake. It is *not* intended to show an implementation obtaining peak-performance. Instead, it is just a boilerplate-template.\n\n```console\nuser@host $ pip install tensorflow-gpu --user # solely the pip package is needed\nuser@host $ cd custom_op/user_ops\nuser@host $ cmake .\nuser@host $ make\nuser@host $ python test_matrix_add.py\nuser@host $ cd ..\nuser@host $ python example.py\n```\n\n## TensorFlow Graph within C++\n\nThis example illustrates the process of loading an image (using OpenCV or TensorFlow), resizing the image  saving the image as a JPG or PNG (using OpenCV or TensorFlow).\n\n```console\nuser@host $ cd examples/resize\nuser@host $ export TENSORFLOW_BUILD_DIR=...\nuser@host $ export TENSORFLOW_SOURCE_DIR=...\nuser@host $ cmake .\nuser@host $ make\n```\n\n\n## TensorFlow-Serving\n\nThere are two examples demonstrating the handling of TensorFlow-Serving: using a vector input and using an encoded image input.\n\n```console\nserver@host $ CHOOSE=basic # or image\nserver@host $ cd serving/${CHOOSE}/training\nserver@host $ python create.py # create some model\nserver@host $ cd serving/server/\nserver@host $ ./run.sh # start server\n\n# some some queries\n\nclient@host $ cd client/bash\nclient@host $ ./client.sh\nclient@host $ cd client/python\n# for the basic-example\nclient@host $ python client_rest.py\nclient@host $ python client_grpc.py\n# for the image-example\nclient@host $ python client_rest.py /path/to/img.[png,jpg]\nclient@host $ python client_grpc.py /path/to/img.[png,jpg]\n```\n\n## Inference\n\nCreate a model in Python, save the graph to disk and load it in C/C+/Go/Python to perform inference. As these examples are based on the TensorFlow C-API they require the `libtensorflow_cc.so` library which is *not* shipped in the pip-package (tensorfow-gpu). Hence, you will need to build TensorFlow from source beforehand, e.g.,\n\n```console\nuser@host $ ls ${TENSORFLOW_SOURCE_DIR}\n\nACKNOWLEDGMENTS     bazel-genfiles      configure          pip\nADOPTERS.md         bazel-out           configure.py       py.pynano\nANDROID_NDK_HOME    bazel-tensorflow    configure.py.bkp   README.md\n...\nuser@host $ cd ${TENSORFLOW_SOURCE_DIR}\nuser@host $  ./configure\nuser@host $  # ... or whatever options you used here\nuser@host $ bazel build -c opt --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow:libtensorflow.so\nuser@host $ bazel build -c opt --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow:libtensorflow_cc.so\n\nuser@host $ export TENSORFLOW_BUILD_DIR=/tensorflow_dist\nuser@host $ mkdir ${TENSORFLOW_BUILD_DIR}\nuser@host $ cp ${TENSORFLOW_SOURCE_DIR}/bazel-bin/tensorflow/*.so ${TENSORFLOW_BUILD_DIR}/\nuser@host $ cp ${TENSORFLOW_SOURCE_DIR}/bazel-genfiles/tensorflow/cc/ops/*.h ${TENSORFLOW_BUILD_DIR}/includes/tensorflow/cc/ops/\n```\n\n### 1. Save Model\n\nWe just run a very basic model\n\n```python\nx = tf.placeholder(tf.float32, shape=[1, 2], name='input')\noutput = tf.identity(tf.layers.dense(x, 1), name='output')\n```\n\nTherefore, save the model like you regularly do. This is done in `example.py` besides some outputs\n\n```console\nuser@host $ python example.py\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32_ref>]\ninput            [[1. 1.]]\noutput           [[2.1909506]]\ndense/kernel:0   [[0.9070684]\n [1.2838823]]\ndense/bias:0     [0.]\n```\n\n### 2. Run Inference\n\n#### Python\n\n```console\nuser@host $ python python/inference.py\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32_ref>]\ninput            [[1. 1.]]\noutput           [[2.1909506]]\ndense/kernel:0   [[0.9070684]\n [1.2838823]]\ndense/bias:0     [0.]\n```\n\n#### C++\n\n```console\nuser@host $ cd cc\nuser@host $ cmake .\nuser@host $ make\nuser@host $ cd ..\nuser@host $ ./cc/inference_cc\n\ninput           Tensor<type: float shape: [1,2] values: [1 1]>\noutput          Tensor<type: float shape: [1,1] values: [2.19095063]>\ndense/kernel:0  Tensor<type: float shape: [2,1] values: [0.907068372][1.28388226]>\ndense/bias:0    Tensor<type: float shape: [1] values: 0>\n```\n\n#### C\n\n```console\nuser@host $ cd c\nuser@host $ cmake .\nuser@host $ make\nuser@host $ cd ..\nuser@host $ ./c/inference_c\n\n2.190951\n\n```\n\n\n#### Go\n\n```console\nuser@host $ go get github.com/tensorflow/tensorflow/tensorflow/go\nuser@host $ cd go\nuser@host $ ./build.sh\nuser@host $ cd ../\nuser@host $ ./inference_go\n\ninput           [[1 1]]\noutput          [[2.1909506]]\ndense/kernel:0  [[0.9070684] [1.2838823]]\ndense/bias:0    [0]\n```\n"
 },
 {
  "repo": "andrewssobral/vehicle_detection_haarcascades",
  "language": "C++",
  "readme_contents": "# Vehicle Detection with Haar Cascades\n\nLast page update: **19/10/2016**\n\nLast version: **1.0.0** (see Release Notes for more info)\n\nHello everyone,\nAn easy way to perform vehicle detection is by using Haar Cascades. Currently, I don't have a detailed tutorial about it, but you can get some extra information in the OpenCV homepage, see [Cascade Classifier](http://docs.opencv.org/2.4/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html) page. See also [Cascade Classifier Training](http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html) for training your own cascade classifier.\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=c4LobbqeKZc\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/vehicle_detection_haarcascades/master/doc/images/vehicle_detection_haarcascades.png\" border=\"0\" />\n</a>\n</p>\n\nThe haar-cascade **cars.xml** was trained using 526 images of cars from the rear (360 x 240 pixels, no scale).\nThe images were extracted from the Car dataset proposed by Brad Philip and Paul Updike taken of the freeways of southern California.\n\nFor more information, please see:\n\n* Train Your Own OpenCV Haar Classifier\n * http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html\n * https://github.com/mrnugget/opencv-haar-classifier-training\n\n* Related paper:\n * Oliveira, M.; Santos, V. Automatic Detection of Cars in Real Roads using Haar-like Features ([PDF](https://github.com/andrewssobral/vehicle_detection_haarcascades/raw/master/doc/Automatic_Detection_of_Cars_in_Real_Roads_using_Haar-like_Features.pdf))\n\n* Some additional resources:\n * http://lars.mec.ua.pt/public/Media/ResearchDevelopmentProjects/HaarFeatures_RoadFilms/HaarFeaturesTests/CarsRear/\n * http://lars.mec.ua.pt/public/Media/ResearchDevelopmentProjects/HaarFeatures_RoadFilms/HaarFeaturesTests/\n\nFor Windows users\n-----------------\n* Check if your OpenCV is installed at: C:\\OpenCV2.4.10\n* There is a Visual Studio 2013 template project in the **vs2013/** folder. Open it in the Visual Studio IDE and select [Release]-[Win32] or [Release]-[x64] mode. Next, click on **run_vehicle_detection.bat** and enjoy!\n\nFor Linux users\n-----------------\n* For Linux and Mac users, a Makefile is provided to compile the source code.\n* * Requirements: OpenCV 2.4.x (it only works with this version).\n* * Check out the latest project source code and compile it:\n```\n~/git clone https://github.com/andrewssobral/vehicle_detection_haarcascades.git\n~/cd vehicle_detection_haarcascades\n~/vehicle_detection_haarcascades/ chmod +x run_vehicle_detection_video1.sh\n~/vehicle_detection_haarcascades/ chmod +x run_vehicle_detection_video2.sh\n~/vehicle_detection_haarcascades/cd build\n~/vehicle_detection_haarcascades/build/ cmake ..\n~/vehicle_detection_haarcascades/build/ make\n```\n* * Run demos:\n```\n~/vehicle_detection_haarcascades/run_vehicle_detection_video1.sh\n~/vehicle_detection_haarcascades/run_vehicle_detection_video2.sh\n```\n\n\nDocker image\n----------------------------------------\n* Docker image is available at:\n* * **Ubuntu 16.04 + VNC + OpenCV 2.4.13 + Python 2.7 + Vehicle Detection, Tracking and Counting**\nhttps://hub.docker.com/r/andrewssobral/vehicle_detection_tracking_counting/\n\nRelease Notes:\n* Version 1.0.0:\nFirst version.\n"
 },
 {
  "repo": "andrewssobral/simple_vehicle_counting",
  "language": "C++",
  "readme_contents": "Vehicle Detection, Tracking and Counting\n========================================\n\nLast page update: **12/04/2017** (Added Python API & OpenCV 3.x support)\n\nLast version: **1.0.0** (see Release Notes for more info)\n\nHi everyone,\n\nThere are several ways to perform vehicle detection, tracking and counting.\nHere is a step-by-step of a simplest way to do this:\n\n1. First, you will need to detect the moving objects. An easy way to do vehicle detection is by using a Background Subtraction (BS) algorithm. You can try to use a background subtraction library like [BGSLibrary](https://github.com/andrewssobral/bgslibrary#bgslibrary).\n2. For vehicle tracking, you will need to use a tracking algorithm. A simplest way to do this is by using a blob tracker algorithm (see [cvBlob](https://code.google.com/p/cvblob/) or [OpenCVBlobsLib](http://opencvblobslib.github.io/opencvblobslib/)). So, send the foreground mask to **cvBlob** or **OpenCVBlobsLib**. For example, the **cvBlob** library provide some methods to get the **centroid**, the **track** and the **ID** of the moving objects. You can also set to draw a **bounding box**, the **centroid** and the **angle** of the tracked object.\n3. And then, check if the **centroid** of the moving object has crossed a **region of interest** (i.e. virtual line) in your video.\n4. Voil\u00e0! enjoy it :)\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/andrewssobral/simple_vehicle_counting/master/doc/images/vehicle_counting_screen.png\" /></p>\n\n\nCitation\n--------\nIf you use this code for your publications, please cite it as:\n```\n@ONLINE{vdtc,\n    author = \"Andrews Sobral\",\n    title  = \"Vehicle Detection, Tracking and Counting\",\n    year   = \"2014\",\n    url    = \"https://github.com/andrewssobral/simple_vehicle_counting\"\n}\n```\n\n\nFor Windows users\n-----------------\n* There is no Visual Studio 2013 template project anymore. Please, use CMAKE instead.\n\n#### Compiling with OpenCV 3.x and Visual Studio 2015 from CMAKE\n\n**Dependencies:**\n* OpenCV 3.x (tested with OpenCV 3.2.0)\n* GIT (tested with git version 2.7.2.windows.1).\n* CMAKE for Windows (tested with cmake version 3.1.1).\n* Microsoft Visual Studio (tested with VS2015).\n\n*Note: the procedure is similar for OpenCV 2.4.x and Visual Studio 2013.*\n\nPlease follow the instructions below:\n\n1) Go to Windows console.\n\n2) Clone git repository:\n```\ngit clone --recursive https://github.com/andrewssobral/simple_vehicle_counting.git\n```\n\n3) Go to **simple_vehicle_counting/build** folder.\n\n4) Set your OpenCV PATH:\n```\nset OpenCV_DIR=C:\\OpenCV3.2.0\\build\n```\n\n5) Launch CMAKE:\n```\ncmake -DOpenCV_DIR=%OpenCV_DIR% -G \"Visual Studio 14 Win64\" ..\n```\n\n6) Include OpenCV binaries in the system path:\n```\nset PATH=%PATH%;%OpenCV_DIR%\\x64\\vc14\\bin\n```\n\n7) Open the **bgs.sln** file in your Visual Studio and switch to **'RELEASE'** mode\n\n8) Click on **'ALL_BUILD'** project and build!\n\n9) If everything goes well, copy **simple_vehicle_counting.exe** to **simple_vehicle_counting/** and run!\n\n\nFor Linux users\n-----------------\n* For Linux and Mac users, a CMakefile is provided to compile the source code.\n\n* * Check out the latest project source code and compile it:\n```\n~/git clone --recursive https://github.com/andrewssobral/simple_vehicle_counting.git\n~/cd simple_vehicle_counting\n~/simple_vehicle_counting/cd build\n~/simple_vehicle_counting/build/ cmake ..\n~/simple_vehicle_counting/build/ make\n```\n* * Run demo:\n```\n~/simple_vehicle_counting/run_simple_vehicle_counting.sh\n```\n\n\nDocker image\n----------------------------------------\n* Docker image is available at:\n* * **Ubuntu 16.04 + VNC + OpenCV 2.4.13 + Python 2.7 + Vehicle Detection, Tracking and Counting**\nhttps://hub.docker.com/r/andrewssobral/vehicle_detection_tracking_counting/\n\nExample code\n------------\n```C++\n#include <iostream>\n#include <opencv2/opencv.hpp>\n\n#include \"package_bgs/PBAS/PixelBasedAdaptiveSegmenter.h\"\n#include \"package_tracking/BlobTracking.h\"\n#include \"package_analysis/VehicleCouting.h\"\n\nint main(int argc, char **argv)\n{\n  /* Open video file */\n  CvCapture *capture = 0;\n  capture = cvCaptureFromAVI(\"dataset/video.avi\");\n  if(!capture){\n    std::cerr << \"Cannot open video!\" << std::endl;\n    return 1;\n  }\n\n  /* Background Subtraction Algorithm */\n  IBGS *bgs;\n  bgs = new PixelBasedAdaptiveSegmenter;\n\n  /* Blob Tracking Algorithm */\n  cv::Mat img_blob;\n  BlobTracking* blobTracking;\n  blobTracking = new BlobTracking;\n\n  /* Vehicle Counting Algorithm */\n  VehicleCouting* vehicleCouting;\n  vehicleCouting = new VehicleCouting;\n\n  std::cout << \"Press 'q' to quit...\" << std::endl;\n  int key = 0;\n  IplImage *frame;\n  while(key != 'q')\n  {\n    frame = cvQueryFrame(capture);\n    if(!frame) break;\n\n    cv::Mat img_input = cv::cvarrToMat(frame);\n    cv::imshow(\"Input\", img_input);\n\n    // bgs->process(...) internally process and show the foreground mask image\n    cv::Mat img_mask;\n    bgs->process(img_input, img_mask);\n\n    if(!img_mask.empty())\n    {\n      // Perform blob tracking\n      blobTracking->process(img_input, img_mask, img_blob);\n\n      // Perform vehicle counting\n      vehicleCouting->setInput(img_blob);\n      vehicleCouting->setTracks(blobTracking->getTracks());\n      vehicleCouting->process();\n    }\n\n    key = cvWaitKey(1);\n  }\n\n  delete vehicleCouting;\n  delete blobTracking;\n  delete bgs;\n\n  cvDestroyAllWindows();\n  cvReleaseCapture(&capture);\n\n  return 0;\n}\n```\n\n\nPython API\n----------------------------------------\nA [python demo](python/demo.py) shows how to call the Python API.\nIt is similar as the [C++ demo](Demo.cpp).\n\nTo use the Python API, you should copy [\"python\" directory](python) to overwrite the generated one.\n\n```\n~/simple_vehicle_counting/cd build\n~/simple_vehicle_counting/build/cmake ..\n~/simple_vehicle_counting/build/make -j 8\n~/simple_vehicle_counting/build/cp -r ../python/* python/\n~/simple_vehicle_counting/build/../run_python_demo.sh\n```\n\nIf you have previously built the project at the project root,\nmake sure there are no previously generated libraries in the [\"python\" directory](python) by ```make clean```.\n\n\nRelease Notes:\n--------------\n* 12/04/2017: Added OpenCV 3.x support. Removed vs2013 template project (use CMAKE instead).\n\n* 07/04/2017: Added Python API, thanks to [@kyu-sz](https://github.com/kyu-sz).\n\n* Version 1.0.0: First version.\n"
 },
 {
  "repo": "royshil/SfM-Toy-Library",
  "language": "C++",
  "readme_contents": "# Toy Structure From Motion Library using OpenCV\n\nThis is a reference implementation of a Structure-from-Motion pipeline in OpenCV, following the work of Snavely et al. [2] and Hartley and Zisserman [1].\n\n*Note:* This is not a complete and robust SfM pipeline implementation. The purpose of this code is to serve as a tutorial and reference for OpenCV users and a soft intro to SfM in OpenCV. If you are looking for a more complete solution with many more options and parameters to tweak, check out the following:\n\n * OpenMVG http://openmvg.readthedocs.io/en/latest/#\n * libMV https://developer.blender.org/tag/libmv/\n * VisualSFM http://ccwu.me/vsfm/\n * Bundler http://www.cs.cornell.edu/~snavely/bundler/\n\nSfM-Toy-Library is now using OpenCV 3, which introduced many new convenience functions to Structure from Motion (see my [blog post](http://www.morethantechnical.com/2016/10/17/structure-from-motion-toy-lib-upgrades-to-opencv-3/) for details), making the implementation much cleaner and simpler. \n\nCeres solver was chosen to do bundle adjustment, for its simple API, straightforward modeling of the problem and long-term support.\n\nDoxygen-style documentation comments appear throughout.\n\n## Compile\n\nTo compile use CMake: http://www.cmake.org\n\n### Prerequisite\n- OpenCV 3.x: http://www.opencv.org\n- Ceres Solver (for bundle adjustment): http://ceres-solver.org/\n- Boost C++ libraries v1.54+: http://www.boost.org/\n\n### How to make\n\n#### On OSX Using XCode\n\nGet Boost and Ceres using homebrew: `brew install boost ceres-solver` (you will need to tap `homebrew/science` for Ceres)\n\n\tmkdir build\n\tcd build\n\tcmake -G \"Xcode\" ..\n\topen SfMToyExample.xcodeproj\n\t\n#### On Linux (or OSX) via a Makefile\n\nObtain Boost (with e.g. `apt-get install libboost-all-dev`) and Ceres (probably need to clone and compile), or on OSX view homebrew as mentioned before.\n\n\tmkdir build\n\tcd build\n\tcmake -G \"Unix Makefiles\" ..\n\tmake \n\n#### On Windows\n\nUse Cmake's GUI to create a MSVC solution, and build it.\n\n## Usage\n\n### Execute\n\n    USAGE ./build/SfMToyUI [options] <input-directory>\n      -h [ --help ]                   produce help message\n      -d [ --console-debug ] arg (=2) Debug output to console log level (0 = Trace,\n                                      4 = Error).\n      -v [ --visual-debug ] arg (=3)  Visual debug output to screen log level (0 = \n                                      All, 4 = None).\n      -s [ --downscale ] arg (=1)     Downscale factor for input images\n      -p [ --input-directory ] arg    Directory to find input images\n\n### Datasets\n\nHere's a place with some standard datasets for SfM: http://cvlabwww.epfl.ch/data/multiview/denseMVS.html\n\nAlso, you can use the \"Crazy Horse\" (A national memorial site in South Dakota) dataset, that I pictured myself, included in the repo.\n\n### Other\n\nSome relevant blog posts from over the years:\n- http://www.morethantechnical.com/2016/10/17/structure-from-motion-toy-lib-upgrades-to-opencv-3/\n- http://www.morethantechnical.com/2015/03/16/bootstrapping-planar-ar-tracking-without-markers-wcode/\n- http://www.morethantechnical.com/2013/11/04/moving-to-qt-on-the-sfm-toy-library-project/\n- http://www.morethantechnical.com/2012/08/09/checking-for-co-planarity-of-3d-points-in-opencv-wcode/\n- http://www.morethantechnical.com/2012/08/09/decomposing-the-essential-matrix-using-horn-and-eigen-wcode/\n- http://www.morethantechnical.com/2012/02/07/structure-from-motion-and-3d-reconstruction-on-the-easy-in-opencv-2-3-w-code/\n- http://www.morethantechnical.com/2012/01/04/simple-triangulation-with-opencv-from-harley-zisserman-w-code/\n\n## References\n\n1. Multiple View Geometry in Computer Vision, Hartley, R. I. and Zisserman, A., 2004, Cambridge University Press [http://www.robots.ox.ac.uk/~vgg/hzbook/]\n2. Modeling the World from Internet Photo Collections, N. Snavely, S. M. Seitz, R. Szeliski, IJCV 2007 [http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf]\n3. Triangulation, R.I. Hartley, P. Strum, 1997, Computer vision and image understanding [http://perception.inrialpes.fr/Publications/1997/HS97/HartleySturm-cviu97.pdf]\n4. Recovering baseline and orientation from essential matrix, B.K.P. Horn, 1990, J. Optical Society of America [http://people.csail.mit.edu/bkph/articles/Essential_Old.pdf]\n5. On benchmarking camera calibration and multi-view stereo for high resolution imagery. Strecha, Christoph, et al. IEEE Computer Vision and Pattern Recognition (CVPR) 2008. [http://infoscience.epfl.ch/record/126393/files/strecha_cvpr_2008.pdf]\n"
 },
 {
  "repo": "SSARCandy/Coherent-Line-Drawing",
  "language": "C++",
  "readme_contents": "# Coherent Line Drawing\n\n[![C/C++ CI](https://github.com/SSARCandy/Coherent-Line-Drawing/workflows/C/C++%20CI/badge.svg)](https://github.com/SSARCandy/Coherent-Line-Drawing/actions)\n[![Build Status](https://travis-ci.org/SSARCandy/Coherent-Line-Drawing.svg?branch=master)](https://travis-ci.org/SSARCandy/Coherent-Line-Drawing)\n[![codecov](https://codecov.io/gh/SSARCandy/Coherent-Line-Drawing/branch/master/graph/badge.svg)](https://codecov.io/gh/SSARCandy/Coherent-Line-Drawing)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/SSARCandy/Coherent-Line-Drawing/blob/master/LICENSE)\n\n\nThis project implemented a non-photorealistic rendering technique presented by Kang et al, that can automatically generates a line drawing from a photograph. This project provide an easy-to-use, real-time interactive graphic user interface system.\n\n\n- [Original academic paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.559&rep=rep1&type=pdf)\n- [Introduction in Chinese](https://ssarcandy.tw/2017/06/26/Coherent-Line-Drawing/)\n\n![demo](./demo/4.jpg)\n\n## Workflow (Youtube)\n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=48fTXKUTM-8\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/48fTXKUTM-8/0.jpg\" \nalt=\"Coherent Line Drawing\" width=\"800\" border=\"10\" /></a>\n\n\n## Build\n\n### Requirement\n\n- C++ 17\n- CMake\n- OpenCV 3\n- WxWidget 3 (not required for cmd application)\n- Boost (not required for gui application)\n\n_If you prefer C++ 11 version, there is a branch called [cpp11](https://github.com/SSARCandy/Coherent-Line-Drawing/tree/cpp11), the function is equivalent._  \n\nIt should work fine in Windows/Linux/MacOS.  \nI provided some scripts that can use in Linux:\n\n\n```sh\n# Usage: build.sh [options]\n# Options:\n#   -c, --clean       Clean build\n#   -d, --debug       Build with debug mode\n#   -j, --jobs        Use N cores to build\n$ ./build.sh\n\n# Usage: linter.sh [options]\n# Check code style\n# Options:\n#   -i                In-place format\n$ ./linter.sh\n```\n\n### Command Line Version\n\nI provide another command line application that can directly use without graphic interface, the entry point is `src/cmd.cpp`. The options is list as following, also you can refer to `./test.sh` to see how to use:\n\n```sh\n# Coherent-Line-Drawing Options:\n#   -h [ --help ]            Help message\n#   -s [ --src ] arg         Source image path\n#   -o [ --output ] arg      Output image path\n#   --ETF_kernel arg (=5)    ETF kernel size\n#   --ETF_iter arg (=1)      Refining n times ETF\n#   --CLD_iter arg (=1)      Iterate n times FDoG\n#   --sigma_c arg (=1)       Line width\n#   --sigma_m arg (=3)       Degree of coherence\n#   --rho arg (=0.997)       Noise\n#   --tau arg (=0.8)         Thresholding\n```\n\n### Pre-Build GUI Version\n\nYou can download pre-build version at [here](https://github.com/SSARCandy/Coherent-Line-Drawing/releases).  \nIncluding Windows and macOS versions.\n\n**Screenshots**\n\n![demo](./demo/1.jpg)\n![demo](./demo/2.jpg)\n![demo](./demo/3.jpg)\n"
 },
 {
  "repo": "pageauc/pi-timolo",
  "language": "Python",
  "readme_contents": "# PI-TIMOLO [![Mentioned in Awesome <INSERT LIST NAME>](https://awesome.re/mentioned-badge.svg)](https://github.com/thibmaek/awesome-raspberry-pi)\n### Raspberry (Pi)camera, (Ti)melapse, (Mo)tion, (Lo)wlight \n## For Details See [Program Features](https://github.com/pageauc/pi-timolo/wiki/Introduction#program-features) and [Wiki Instructions](https://github.com/pageauc/pi-timolo/wiki) and [YouTube Videos](https://www.youtube.com/playlist?list=PLLXJw_uJtQLa11A4qjVpn2D2T0pgfaSG0)\n\n***IMPORTANT:*** Raspbian Stretch and pi-timolo.py ver 11.11 and earlier has long exposure low light \ncamera freezing issue due to kernel panic that requires a reboot to gain\ncontrol of camera back per https://github.com/waveform80/picamera/issues/528 \npi-timolo.py ver 11.12 has a fix to resolve issue but\nrequires the latest Raspbian firmware. If you encounter camera freeze with latest Stretch image then\nyou will need to run ***sudo rpi-update*** to update Stretch to latest firmware.  Normal\nbackup precautions are advised before doing the firmware update.  See [wiki](https://github.com/pageauc/pi-timolo/wiki/Basic-Trouble-Shooting#raspbian-stretch-kernel-panic-and-camera-freeze)\n***Note:*** Raspbian Jessie works fine and does Not encounter freezing issue with long exposure low light operation.\n\n* ***Release 9.x*** New Features have been Added. See Wiki Details below    \n [plugins Setup and Operation](https://github.com/pageauc/pi-timolo/wiki/How-to-Use-Plugins)   \n [Rclone Setup and Media Sync](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-rclone) (Replaces gdrive)    \n [watch-app.sh Remote Configuration Management](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-config.py-Remote-Configuration)   \n [python3 Support Details](https://github.com/pageauc/pi-timolo/wiki/Prerequisites#python-3-support)   \n* ***Release 10.x*** Added Sched Start to Motion Track, Timelapse and VideoRepeat. See Wiki Details below    \n [How To Schedule Motion, Timelapse or VideoRepeat](https://github.com/pageauc/pi-timolo/wiki/How-to-Schedule-Motion,-Timelapse-or-VideoRepeat)  \n This release requires config.py be updated by the user with config.py.new since new variables have been added.\n* ***Release 11.12*** Added adhoc fix for Debian Stretch kernel panic and camera freeze issue when running\n under very low light conditions.  Note a ***sudo rpi-update*** may be required to update firmware if freezing\n still occurs under pi-timolo.py ver 11.12 or greater\n* ***Release 11.55*** Added config.py setting ***nightTwilightModeOn***  True is the normal twilight mode for outside conditions.\nFalse is used for indoors conditions where there is no twilight.  This setting will avoid overexposure when lights\nor sudden lighting changes are encountered.  If you have previously experienced overexposure when camera is indoors then\nthis setting should help. \n* ***Release 12.0*** Added [pantilthat panoramic image stitching Feature](https://github.com/pageauc/pi-timolo/wiki/Panoramic-Images-Stitching-Feature) \nwith support for Pimoroni and [Waveshare pantilthat](https://github.com/pageauc/waveshare.pantilthat) (and compatilble) hardware.\nThis release also renames all the [config.py](https://github.com/pageauc/pi-timolo/blob/master/source/config.py) variable Constants to snake_case and does some code cleanup.  You need to update to\nthe new config.py.  Built in instructions will prompt you if needed.  Please raise github issue if there are\nproblems. See config.py comments and revised wiki for details. \n \n## Requirements\nRequires a [***Raspberry Pi computer***](https://www.raspberrypi.org/documentation/setup/) and a \n[***RPI camera module installed***](https://www.raspberrypi.org/documentation/usage/camera/).\nMake sure hardware is tested and works. Most [RPI models](https://www.raspberrypi.org/products/) will work OK. \nA quad core RPI will greatly improve performance due to threading. A recent version of \n[Raspbian operating system](https://www.raspberrypi.org/downloads/raspbian/) is Recommended.\n \n## Quick Install or Upgrade\n**IMPORTANT** - It is suggested you do a Raspbian ***sudo apt-get update*** and ***sudo apt-get upgrade***\nbefore curl install, since it is **No longer** performed by the pi-timolo-install.sh script\n\n***Step 1*** With mouse left button highlight curl command in code box below. Right click mouse in **highlighted** area and Copy.     \n***Step 2*** On RPI putty SSH or terminal session right click, select paste then Enter to download and run script.     \n\n    curl -L https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo-install.sh | bash\n\nThe command above will download and Run the GitHub ***pi-timolo-install.sh*** script. \nAn upgrade will not overwrite configuration files.   \n\n* ***NOTICE*** gdrive is no longer installed with pi-timolo-install.sh, I have been testing\nrclone and it is Now the Default. Some ***rclone-*** samples are included. Make a copy of one, rename and edit for\nyour own needs.  See [Wiki - How to Setup Rclone](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-rclone).\nNote: If a ***/usr/local/bin/gdrive*** File Exists, It Will Remain. Older files are still available on this GitHub Repo.   \n\n## Test Install\nTo Test Run default config.py - motion track(HD image) plus timelapse(5 min interval). \n \n    cd ~/pi-timolo\n    ./pi-timolo.py\n\n### For More Details see [Basic Trouble Shooting](https://github.com/pageauc/pi-timolo/wiki/Basic-Trouble-Shooting) or [pi-timolo Wiki](https://github.com/pageauc/pi-timolo/wiki)\n\n## Description\nPI-TIMOLO is primarily designed for ***headless operation*** and includes rclone that\ncan securely synchronize specified media folders and files with a users remote storage service of choice. This works well for remote security and monitoring\ncameras. Camera config.py and conf settings can be easily administered remotely from a designated sync directory using ***watch-app.sh***\nscript using a crontab entry to periodically check for updates between the pi-timolo camera and a users remote storage rclone service name. \n\npi-timolo is python 2/3 compatible and can take timelapse and/or motion tracking images/videos, separately or together. Will take\nlong exposure Night (lowlight) images for Time Lapse and/or Motion. Has relatively smooth twilight transitions based on a threshold light\nsetting, so a real time clock is not required. Customization settings are saved in a ***config.py*** and conf files and optional special\npurpose plugin config files. Optional plugin feature allows overlaying config.py settings with custom settings for specific tasks.  \n\nIncludes ***makevideo.sh*** to create timelapse or motion lapse videos from images, ***convid.sh*** to convert/combine \nh264 to mp4 format, a simple minumum or no setup web server to view images or videos and ***menubox.sh*** \nto admin settings and stop start pi-timolo and webserver as background tasks. \n       \nFor more Details see [Github Wiki](https://github.com/pageauc/pi-timolo/wiki)   \n\n## Minimal Upgrade\nIf you are just interested in a minimal upgrade (must have pi-timolo previously installed)\nfrom a logged in ssh or terminal session execute the following commands.  \n\n    cd ~/pi-timolo\n    sudo apt-get install python-opencv\n    cp config.py config.py.old\n    cp pi-timolo.py pi-timolo.py.old\n    wget -O config.py https://raw.github.com/pageauc/pi-timolo/master/source/config.py\n    wget -O pi-timolo.py https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo.py    \n    \nEdit config.py to transfer any customized settings from config.py.old  \n    \n## Manual Install or Upgrade  \nFrom logged in RPI SSH session or console terminal perform the following. You can review\nthe pi-timolo-install.sh script code before executing.\n\n    cd ~\n    wget https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo-install.sh\n    more pi-timolo-install.sh    # Review code if required\n    chmod +x pi-timolo-install.sh\n    ./pi-timolo-install.sh\n    \n## Menubox\npi-timolo has a whiptail administration menu system. The menu's allow\nstart/stop of pi-timolo.py and/or webserver.py as background tasks, as well as\nediting configuration files, making timelapse videos from jpg images, converting or joining mp4 files Etc.    \n\nTo run menubox.sh from ssh console or terminal session execute commands below.\n\n    cd ~/pi-timolo\n    ./menubox.sh\n\n![menubox main menu](menubox.png)\n \n## Webserver\nI have also written a standalone LAN based webserver.py to allow easy access to pi-timolo image and video files\non the Raspberry from another LAN computer web browser.  There is no setup required but the display\nsettings can be customized via variables in the config.py file or via menubox admin menuing.     \n***NOTE:*** webserver.py is normally run in background using menubox.sh, webserver.sh or from /etc/rc.local     \nTo Test Run from ssh console or terminal session. \n    \n    cd ~/pi-timolo\n    ./webserver.py\n\n![webserver browser screen shot](webserver.jpg)\n \n## Reference Links  \nDetailed pi-timolo Wiki https://github.com/pageauc/pi-timolo/wiki  \n[my pi-timolo and other YouTube Videos playlist](https://www.youtube.com/playlist?list=PLLXJw_uJtQLa11A4qjVpn2D2T0pgfaSG0)    \n[MagPi Object Recognition using pi-timolo](https://magpi.raspberrypi.org/articles/wildlife-camera-object-recognition)    \n[makezine night vision project using pi-timolo](https://makezine.com/2016/05/26/spy-on-garden-critters-with-raspberry-pi-powered-night-vision/)    \n[hackster facial recognition using pi-timolo](https://www.hackster.io/gr1m/raspberry-pi-facial-recognition-16e34e)    \n[Neverending project timelapse using pi-timolo](https://www.theneverendingprojectslist.com/raspberrypiprojects/timelapse/)       \n[hedgehog camera using pi-timolo](http://www.sconemad.com/blog/hedgeycam/) and [step by step](https://oraclefrontovik.com/2016/08/28/a-step-by-step-guide-to-building-a-raspberry-pi-hedgehog-camera/)    \n[Museum Insect activity monitoring using pi-timolo](https://www.vam.ac.uk/blog/caring-for-our-collections/making-a-simple-insect-activity-monitor-using-a-raspberry-pi)    \n[Brett Beeson timelapse cloud project using pi-timolo](https://brettbeeson.com.au/timelapse-cloud/)    \n\t\nGood Luck\nClaude Pageau \n"
 },
 {
  "repo": "niw/iphone_opencv_test",
  "language": "C++",
  "readme_contents": "Using OpenCV on iPhone\n----------------------\nThis source repository includes pre-compiled OpenCV library and headeres so that you can get started easily!\nMore documents you can see on [this article](http://niw.at/articles/2009/03/14/using-opencv-on-iphone/).\n\nBuilding Static Link Version of OpenCV\n--------------------------------------\nIf you want to build it from source code, you can do by next steps.\n\n1.  Building OpenCV requiers [CMake](http://www.cmake.org/).\n    You can easily install it by using [Homebrew](http://mxcl.github.com/homebrew/) or [MacPorts](http://www.macports.org/).\n\n        # Using Homebrew\n        % brew install cmake\n        # Using MacPorts\n        % sudo port install cmake\n\n2.  Clone this project from github.com, then move into the project directory\n\n        % git clone git://github.com/niw/iphone_opencv_test.git\n\n3.  Getting source code from sourceforge. I tested with [OpenCV-2.2.0.tar.bz2](http://sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.2/OpenCV-2.2.0.tar.bz2/download).\n\n4.  Extract downloaded archive on the top of demo project directory\n\n        % tar xjvf OpenCV-2.2.0.tar.bz2\n\n5.  Apply patch for iPhone SDK\n\n        % cd OpenCV-2.2.0\n        % patch -p1 < ../OpenCV-2.2.0.patch\n\n6.  Following next steps to build OpenCV static library for simulator.\n    All files are installed into ``opencv_simulator`` directory.\n    When running ``make`` command, you've better assign ``-j`` option and number according to number of your CPU cores.\n    Without ``-j`` option, it takes a long time.\n\n        % cd ..\n        % mkdir build_simulator\n        % cd build_simulator\n        % ../opencv_cmake.sh Simulator ../OpenCV-2.2.0\n        % make -j 4\n        % make install\n\n7.  Following next steps to build OpenCV static library for device\n    All files are installed into ``opencv_device`` directory.\n\n        % cd ..\n        % mkdir build_device\n        % cd build_device\n        % ../opencv_cmake.sh Device ../OpenCV-2.2.0\n        % make -j 4\n        % make install\n\nBuild support script\n--------------------\n\nuild support script ``opencv_cmake.sh`` has some options to build OpenCV with iOS SDK.\nTry ``--help`` option to get the all options of it.\n\nChange Log\n----------\n *  04/11/2011 - Supprot OpenCV 2.2.0 + iOS SDK 4.3 + XCode 4\n *  10/30/2010 - Support iOS SDK 4.1\n *  08/22/2010 - Support OpenCV 2.1.0 + iOS SDK 4.0\n *  12/21/2009 - Support Snow Leopard + iPhone SDK 3.1.2, Thank you Hyon!\n *  11/15/2009 - Support OpenCV to 2.0.0 + iPhone SDK 3.x\n *  03/14/2009 - Release this project with OpenCV 1.0.0 + iPhone SDK 2.x\n"
 },
 {
  "repo": "pibooth/pibooth",
  "language": "Python",
  "readme_contents": "|Pibooth| |BeerPay|\n\n|PythonVersions| |PypiPackage| |Downloads|\n\nThe ``pibooth`` project provides a photobooth application *out-of-the-box* in pure Python\nfor Raspberry Pi. Have a look to the `wiki <https://github.com/pibooth/pibooth/wiki>`_\nto discover some realizations from GitHub users, and don't hesitate to send us photos of your version.\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/background_samples.png\n   :align: center\n   :alt: Settings\n\n.. note:: Even if designed for a Raspberry Pi, this software may be installed on any Unix/Linux\n          based OS (tested on Ubuntu 16 and Mac OSX 10.14.6).\n\n.. contents::\n\nFeatures\n--------\n\n* Interface available in Danish, Dutch, English, French, German, Hungarian, Norwegian and Spanish (customizable)\n* Capture from 1 to 4 photos and concatenate them in a final picture\n* Support all cameras compatible with gPhoto2, OpenCV and Raspberry Pi\n* Support for hardware buttons and lamps on Raspberry Pi GPIO\n* Fully driven from hardware buttons / keyboard / mouse / touchscreen\n* Auto-start at the Raspberry Pi startup\n* Animate captures from the last sequence during idle time\n* Store final pictures and the individual captures\n* Printing final pictures using CUPS server (printing queue indication)\n* Custom texts can be added on the final picture (customizable fonts, colors, alignments)\n* Custom background(s) and overlay(s) can be added on the final picture\n* All settings available in a configuration file (most common options in a graphical interface)\n* Highly customizable thanks to its plugin system, you can develop your own plugin\n\nGallery\n-------\n\nYou can see some examples of the output picture formats you can get with ``pibooth`` on the following page.\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/gallery.png\n   :align: center\n   :alt: gallery\n   :target: https://github.com/pibooth/pibooth/blob/master/docs/examples.rst\n   :height: 200px\n\nRequirements\n------------\n\nThe requirements listed below are the ones used for the development of ``pibooth``, but\nother configuration may work fine. **All hardware buttons, lights and printer are optional**,\nthe application can be entirely controlled using a keyboard, a mouse or a touchscreen.\n\n.. warning:: Using a Pi Camera, the preview is visible only on a screen connected to the HDMI or\n             DSI connectors (the preview is an overlay managed at GPU low level).\n\nHardware\n^^^^^^^^\n\n* 1 Raspberry Pi 3 Model B (or higher)\n* 1 Camera (Raspberry Pi Camera v2.1 8 MP 1080p\n  or any DSLR camera `compatible with gPhoto2 <http://www.gphoto.org/proj/libgphoto2/support.php>`_\n  or any webcam `compatible with OpenCV <https://opencv.org>`_ )\n* 2 push buttons\n* 2 LEDs\n* 2 resistors of 100 Ohm\n* 1 printer\n\nSoftware\n^^^^^^^^\n\n* Raspbian ``Buster with desktop and recommended software``\n* Python ``3.5.3``\n* libgphoto2 ``2.5.23``\n* libcups ``2.2.1``\n\nInstall\n-------\n\nA brief description on how to set-up a Raspberry Pi to use this software.\n\n1. Download the Raspbian image and set-up an SD-card. You can follow\n   `these instructions <https://www.raspberrypi.org/documentation/installation/installing-images/README.md>`_.\n\n2. Insert the SD-card into the Raspberry Pi and fire it up. Use the ``raspi-config`` tool\n   to configure your system (e.g., expand partition, change hostname, password, enable SSH,\n   configure to boot into GUI, etc.).\n\n   .. hint:: Don't forget to enable the camera in raspi-config.\n\n3. Upgrade all installed software:\n\n   ::\n\n        $ sudo apt-get update\n        $ sudo apt-get upgrade\n\n4. Optionally install the last stable ``gPhoto2`` version (required only for DSLR camera):\n\n   ::\n\n        $ sudo wget raw.github.com/gonzalo/gphoto2-updater/master/gphoto2-updater.sh\n        $ sudo chmod 755 gphoto2-updater.sh\n        $ sudo ./gphoto2-updater.sh\n\n5. Optionally install ``CUPS`` to handle printers (more instructions to add a new printer can be found\n   `here <https://www.howtogeek.com/169679/how-to-add-a-printer-to-your-raspberry-pi-or-other-linux-computer>`_):\n\n   ::\n\n        $ sudo apt-get install cups libcups2-dev\n\n6. Optionally install ``OpenCV`` to improve images generation efficiency or if a Webcam is used:\n\n   ::\n\n        $ sudo apt-get install python3-opencv\n\n7. Install ``pibooth`` from the `pypi repository <https://pypi.org/project/pibooth/>`_:\n\n   ::\n\n        $ sudo pip3 install pibooth[dslr,printer]\n\n   .. hint:: If you don't have ``gPhoto2`` and/or ``CUPS`` installed (steps 5. and/or 6. skipped), remove\n             printer or dslr under the ``[]``\n\n.. note:: An editable/customizable version of ``pibooth`` can be installed by following\n          these `instructions <https://github.com/pibooth/pibooth/blob/master/docs/dev.rst>`_ .\n          Be aware that the code on the `master` branch may be unstable.\n\nRun\n---\n\nStart the photobooth application using the command::\n\n    $ pibooth\n\nAll pictures taken are stored in the folder defined in ``[GENERAL][directory]``. They are named\n**YYYY-mm-dd-hh-mm-ss_pibooth.jpg** which is the time when first capture of the sequence was taken.\nA subfolder **raw/YYYY-mm-dd-hh-mm-ss** is created to store the single raw captures.\n\n.. note:: if you have both ``Pi`` and ``DSLR`` cameras connected to the Raspberry Pi, **both are used**,\n          this is called the **Hybrid** mode. The preview is taken using the ``Pi`` one for a better\n          video rendering and the capture is taken using the ``DSLR`` one for better picture rendering.\n\nYou can display a basic help on application options by using the command::\n\n    $ pibooth --help\n\nStates and lights management\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe application follows the states sequence defined in the simplified diagram\nbelow:\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/state_sequence.png\n   :align: center\n   :alt: State sequence\n\nThe states of the **LED 1** and **LED 2** are modified depending on the actions available\nfor the user.\n\nDetailed state diagram can be found `on this page <https://github.com/pibooth/pibooth/blob/master/docs/plugin.rst>`_.\n\nCommands\n^^^^^^^^\n\nAfter the graphical interface is started, the following actions are available:\n\n======================= ================ =====================\nAction                  Keyboard key     Physical button\n======================= ================ =====================\nToggle Full screen      Ctrl + F         \\-\nChoose layout           LEFT or RIGHT    Button 1 or Button 2\nTake pictures           P                Button 1\nExport Printer/Cloud    Ctrl + E         Button 2\nOpen/close settings     ESC              Button 1 + Button 2\nSelect option           UP or DOWN       Button 1\nChange option value     LEFT or RIGHT    Button 2\n======================= ================ =====================\n\nFinal picture rendering\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``pibooth`` application handle the rendering of the final picture using 2 variables defined in\nthe configuration (see `Configuration`_ below):\n\n* ``[CAMERA][resolution] = (width, height)`` is the resolution of the captured picture in pixels.\n  As explained in the configuration file, the preview size is directly dependent from this parameter.\n* ``[PICTURE][orientation] = auto/landscape/portrait`` is the orientation of the final picture\n  (after concatenation of all captures). If the value is **auto**, the orientation is automatically\n  chosen depending on the resolution.\n\n.. note:: The resolution is an important parameter, it is responsible for the quality of the final\n          picture. Have a look to `picamera possible resolutions <http://picamera.readthedocs.io/en/latest/fov.html#sensor-modes>`_ .\n\nImage effects can be applied on the capture using the ``[PICTURE][effect]`` variable defined in the\nconfiguration.\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Effect applied on all captures\n    captures_effects = film\n\nInstead of one effect name, a list of names can be provided. In this case, the effects are applied\nsequentially on the captures sequence.\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Define a rolling sequence of effects. For each capture the corresponding effect is applied.\n    captures_effects = ('film', 'cartoon', 'washedout', 'film')\n\nHave a look to the predefined effects available depending on the camera used:\n\n* `picamera effects <https://picamera.readthedocs.io/en/latest/api_camera.html#picamera.PiCamera.image_effect>`_\n* `gPhoto2 effects (PIL based) <https://pillow.readthedocs.io/en/latest/reference/ImageFilter.html>`_\n\nTexts can be defined by setting the option ``[PICTURE][footer_text1]`` and ``[PICTURE][footer_text2]``\n(lets them empty to hide any text). For each one, the font, the color and the alignment can be chosen.\nFor instance:\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Same font applied on footer_text1 and footer_text2\n    text_fonts = Amatic-Bold\n\nThis key can also take two names or TTF file paths:\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # 'arial' font applied on footer_text1, 'Roboto-BoldItalic' font on footer_text2\n    text_fonts = ('arial', 'Roboto-BoldItalic')\n\nThe available fonts can be listed using the following the command::\n\n \u00a0  $ pibooth --fonts\n\nTo regenerate the final pictures afterwards, from the originals captures present in the\n``raw`` folder, use the command::\n\n    $ pibooth-regen\n\nIt permits to adjust the configuration to enhance the previous pictures with better\nparameters (title, more effects, etc...)\n\nConfiguration\n-------------\n\nAt the first run, a configuration file is generated in ``~/.config/pibooth/pibooth.cfg``\nwhich permits to configure the behavior of the application.\n\nA quick configuration GUI menu (see `Commands`_ ) gives access to the most common options:\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/settings.png\n   :align: center\n   :alt: Settings\n\nMore options are available by editing the configuration file which is easily\ndone using the command::\n\n \u00a0  $ pibooth --config\n\nThe default configuration can be restored with the command (strongly recommended when\nupgrading ``pibooth``)::\n\n \u00a0 \u00a0$ pibooth --reset\n\nSee the `default configuration file <https://github.com/pibooth/pibooth/blob/master/docs/config.rst>`_\nfor further details.\n\nCustomize using plugins\n^^^^^^^^^^^^^^^^^^^^^^^\n\nSeveral plugins maintained by the community are available. They add extra features to\n``pibooth``. Have a look to the `plugins on PyPI  <https://pypi.org/search/?q=pibooth>`_.\n\nYou can also easily develop your own plugin, and declare it in the ``[GENERAL][plugins]``\nkey of the configuration. See guidelines to\n`develop custom plugin <https://github.com/pibooth/pibooth/blob/master/docs/plugin.rst>`_.\n\nGUI translations\n^^^^^^^^^^^^^^^^\n\nThe graphical interface texts are available in 8 languages by default: Danish, Dutch, English, \nFrench, German, Hungarian, Norwegian and Spanish. The default translations can be easily edited using the command::\n\n \u00a0  $ pibooth --translate\n\nA new language can be added by adding a new section (``[alpha-2-code]``).\nIf you want to have ``pibooth`` in your language feel free to send us the corresponding keywords via a GitHub issue.\n\nPrinter\n^^^^^^^\n\nThe print button (see `Commands`_) and print states are automatically activated/shown if:\n\n* `pycups <https://pypi.python.org/pypi/pycups>`_ and `pycups-notify <https://github.com/anxuae/pycups-notify>`_ are installed\n* at least one printer is configured in ``CUPS``\n* the key ``[PRINTER][printer_name]`` is equal to ``default`` or an existing printer name\n\nTo avoid paper waste, set the option ``[PRINTER][max_duplicates]`` to the maximum\nof identical pictures that can be sent to the printer.\n\nSet the option ``[PRINTER][max_pages]`` to the number of paper sheets available on the\nprinter. When this number is reached, the print function will be disabled and an icon\nindicates the printer failure. To reset the counter, open then close the settings\ngraphical interface (see `Commands`_).\n\nHere is the default configuration used for this project in CUPS, it may depend on\nthe printer used:\n\n================ =============================\nOptions          Value\n================ =============================\nMedia Size       10cm x 15cm\nColor Model      CMYK\nMedia Type       Glossy Photo Paper\nResolution       Automatic\n2-Sided Printing Off\nShrink page ...  Shrink (print the whole page)\n================ =============================\n\nCircuit diagram\n---------------\n\nHere is the diagram for hardware connections. Please refer to the\n`default configuration file <https://github.com/pibooth/pibooth/blob/master/docs/config.rst>`_\nto know the default pins used (`physical pin numbering <https://pinout.xyz>`_).\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/sketch.png\n   :align: center\n   :alt: Electronic sketch\n\nAn extra button can be added to start and shutdown properly the Raspberry Pi.\nEdit the file ``/boot/config.txt`` and set the line::\n\n    dtoverlay=gpio-shutdown\n\nThen connect a push button between physical *pin 5* and *pin 6*.\n\nTerms and conditions\n--------------------\n\nSee the LICENSE file to have details on the terms and coniditions.\n\nGDPR advices\n^^^^^^^^^^^^\n\n``pibooth`` was developed for a private usage with no connection to a professional or commercial activity,\nas a consequence GDPR does not apply.\nHowever if you are using photobooth in Europe, it is your responsability to check that your usage and\nmore particularly the usage of the pictures generated by ``pibooth`` follows the GDPR rules, especially make\nsure that the people that will use the ``pibooth`` are aware that the image will be stored on the device.\n\nCredits\n^^^^^^^\n\nPibooth icon from `Artcore Illustrations <https://www.iconspedia.com/icon/photobooth-icon-29464.html>`_\n\nIcons from the Noun Project (https://thenounproject.com/)\n\n- Polaroid by icon 54\n- Up hand drawn arrow by Kid A\n- Cameraman and Friends Posing For Camera by Gan Khoon Lay\n- Camera by Alfa Design\n- Print Photo by Kmg Design\n- Pointer hand by Peter van Driel\n\nSupport us on Beerpay\n---------------------\n\nIf you want to help us you can by clicking on the following links!\n\n|BeerPay| |BeerPay2|\n\n.. |BeerPay| image:: https://beerpay.io/werdeil/pibooth/badge.svg?style=beer-square\n   :align: middle\n   :target: https://beerpay.io/werdeil/pibooth\n\n.. |BeerPay2| image:: https://beerpay.io/werdeil/pibooth/make-wish.svg?style=flat-square\n   :align: middle\n   :target: https://beerpay.io/werdeil/pibooth?focus=wish\n\n.. |Pibooth| image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/pibooth.png\n   :align: middle\n\n.. |PythonVersions| image:: https://img.shields.io/badge/python-2.7+ / 3.6+-red.svg\n   :target: https://www.python.org/downloads\n   :alt: Python 2.7+/3.6+\n\n.. |PypiPackage| image:: https://badge.fury.io/py/pibooth.svg\n   :target: https://pypi.org/project/pibooth\n   :alt: PyPi package\n\n.. |Downloads| image:: https://img.shields.io/pypi/dm/pibooth?color=purple\n   :target: https://pypi.org/project/pibooth\n   :alt: PyPi downloads\n"
 },
 {
  "repo": "RobertSasak/react-native-openalpr",
  "language": "C++",
  "readme_contents": "[![Build Status](https://travis-ci.com/RobertSasak/react-native-openalpr.svg?branch=master)](https://travis-ci.com/RobertSasak/react-native-openalpr)\n[![Gitter](https://badges.gitter.im/react-native-openalpr/community.svg)](https://gitter.im/react-native-openalpr/community)\n\n# react-native-openalpr\n\n[OpenALPR](https://github.com/openalpr/openalpr) integration for React Native. Provides a camera component that recognizes license plates in real-time. Supports both iOS and Android.\n\n<img alt=\"OpenALPR iOS Demo Video\" src=\"https://cdn-images-1.medium.com/max/800/1*u1nTJMFc34aDLTPCIr0-cQ.gif\" width=200 height=350 /> <img alt=\"OpenALPR Android Demo Video\" src=\"https://user-images.githubusercontent.com/10334791/27850595-62dc852e-615e-11e7-875c-57a017dbb28c.gif\" width=200 height=350 />\n\n## Requirements\n\n- iOS 9+\n- Android 5.0+\n- RN 0.60+\n\n## Installation\n\n### Installation with React Native\n\nStart by adding the package and linking it.\n\n```sh\n$ yarn add react-native-openalpr\n```\n\n### iOS Specific Setup\n\n#### Install react-native-permissions\n\nIt is a good practice to check and request CAMERA permission. Check full implementation in example folder.\n\n```sh\nyarn add react-native-permissions\n```\n\nAdd camera permission into your podfile.\n\n```\npod 'Permission-Camera', :path => \"../node_modules/react-native-permissions/ios/Camera.podspec\"\n```\n\n#### Install pods\n\n```sh\n$ cd ios && pod install && cd ...\n```\n\n#### Camera Permissions\n\n- Add an entry for `NSCameraUsageDescription` in your `info.plist` explaining why your app will use the camera. If you forget to add this, your app will crash!\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  ...\n \t<key>NSCameraUsageDescription</key>\n \t<string>We use your camera for license plate recognition to make it easier for you to add your vehicle.</string>\n</dict>\n```\n\n#### Bitcode\n\nBecause the OpenCV binary framework release is compiled without bitcode, the other frameworks built by this script are also built without it, which ultimately means your Xcode project also cannot be built with bitcode enabled. [Per this message](http://stackoverflow.com/a/32728516/868173), it sounds like we want this feature disabled for OpenCV anyway.\n\nTo disable bitcode in your project:\n\n- In `Build Settings` \u2192 `Build Options`, search for `Enable Bitcode` and set it to `No`.\n\n### Android-specific Setup\n\n#### Camera Permissions\n\n- Add permissions for `CAMERA` and `FLASHLIGHT` and the related features (below) to `AndroidManifest.xml`. If you forget to add these permissions, your app will crash!\n\n```xml\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  <!-- Camera Permissions -->\n  <uses-permission android:name=\"android.permission.CAMERA\" />\n\n  <uses-feature\n      android:name=\"android.hardware.camera\"\n      android:required=\"false\" />\n  <uses-feature\n      android:name=\"android.hardware.camera.autofocus\"\n      android:required=\"false\" />\n\n  <uses-permission android:name=\"android.permission.FLASHLIGHT\" />\n```\n\n#### Add to Gradle\n\n###### Your `android/settings.gradle` file should have following lines:\n\n```gradle\n\nrootProject.name = 'RNOpenALPRExample'\napply from: file(\"../node_modules/@react-native-community/cli-platform-android/native_modules.gradle\"); applyNativeModulesSettingsGradle(settings)\ninclude ':app'\n\n// Add these lines\ninclude ':openalpr'\nproject(':openalpr').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-openalpr/android/libraries/openalpr')\ninclude ':opencv'\nproject(':opencv').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-openalpr/android/libraries/opencv')\n```\n\n#### Linking\n\nThe library is linked automatically with leptonica, opencv, tesseract, and openalpr ([openalpr](https://github.com/SandroMachado/openalpr-android)).\nTo make it work, copy and paste the directory with the runtime needed data to your project at path `android/app/src/main/assets/runtime_data`.\n\nThe `runtime_data` file can be found in `/example/android/app/src/main/assets/` in this repo. Open `runtime_data/openalpr.conf` file and replace `com.rnopenalprexample` with your package name\n\n```\n[common]\n\n; Specify the path to the runtime data directory\nruntime_dir = /data/data/com.rnopenalprexample/runtime_data\n\n\nocr_img_size_percent = 1.33333333\nstate_id_img_size_percent = 2.0\n...\n```\n\n## Usage\n\nOpenALPR exposes a camera component (based on [react-native-camera](https://github.com/lwansbrough/react-native-camera)) that is optimized to run OpenALPR image processing on a live camera stream. Among other parameters, the camera accepts a callback, `onPlateRecognized`, for when a plate is recognized.\n\n```js\nimport React, { Component } from 'react'\nimport { StyleSheet, Text, View } from 'react-native'\n\nimport Camera, {\n  Aspect,\n  CaptureQuality,\n  TorchMode,\n} from 'react-native-openalpr'\n\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n  },\n  textContainer: {\n    position: 'absolute',\n    top: 100,\n    left: 50,\n  },\n  text: {\n    textAlign: 'center',\n    fontSize: 20,\n  },\n})\n\nexport default class PlateRecognizer extends React.Component {\n  state = {\n    plate: 'Scan a plate',\n  }\n\n  onPlateRecognized = ({ plate, confidence }) => {\n    this.setState({\n      plate,\n    })\n  }\n\n  render() {\n    return (\n      <View style={styles.container}>\n        <Camera\n          style={styles.preview}\n          aspect={Aspect.fill}\n          captureQuality={CaptureQuality.medium}\n          country=\"us\"\n          onPlateRecognized={this.onPlateRecognized}\n          plateOutlineColor=\"#ff0000\"\n          showPlateOutline\n          zoom={0}\n          torchMode={TorchMode.off}\n          touchToFocus\n        />\n        <View style={styles.textContainer}>\n          <Text style={styles.text}>{this.state.plate}</Text>\n        </View>\n      </View>\n    )\n  }\n}\n```\n\n### Options\n\n#### `zoom`\n\nThe zoon of the camera (Android only). Can be :\n\n0 to 99\n\n#### `aspect`\n\nThe aspect ratio of the camera. Can be one of:\n\n- `Aspect.stretch`\n- `Aspect.fit`\n- `Aspect.fill`\n\n#### `captureQuality`\n\nThe resolution at which video frames are captured and analyzed. For completeness, several options are provided. However, it is strongly recommended that you stick with one of the following for the best frame rates and accuracy:\n\n- `CaptureQuality.medium` (480x360)\n- `CaptureQuality.480p` (640x480)\n\n#### `country`\n\nSpecifies which OpenALPR config file to load, corresponding to the country whose plates you wish to recognize. Currently supported values are:\n\n- `au`\n- `br`\n- `eu`\n- `fr`\n- `gb`\n- `kr`\n- `mx`\n- `sg`\n- `us`\n- `vn2`\n\n#### `onPlateRecognized`\n\nThis callback receives a hash with keys:\n\n- `plate`, representing the recognized license plate string\n- `confidence`, OpenALPR's confidence(%) in the result\n\n#### `plateOutlineColor`\n\nHex string specifying the color of the border to draw around the recognized plate. Example: `#ff0000` for red.\n\n#### `showPlateOutline`\n\nIf true, this draws an outline over the recognized plate\n\n#### `torchMode`\n\nTurns the flashlight on or off. Can be one of:\n\n- `TorchMode.on`\n- `TorchMode.off`\n- `TorchMode.auto`\n\n#### `touchToFocus`\n\nIf true, this focuses the camera where the user taps\n\n## Examples\n\n- [Example Project](https://github.com/RobertSasak/react-native-openalpr/tree/master/example)\n\n## Development\n\n- This project works with iOS and Android. It may have some bugs depending on how the underlying native components are updated\n\n### Running the Example project on Android While Developing\n\n1. Clone the repo and enter the `example` directory\n\n```ssh\ngit clone https://github.com/RobertSasak/react-native-openalpr.git\ncd react-native-openalpr\ncd example\n```\n\n2. From the `example` directory, run `yarn`\n\n3. Copy the `android` folder from `/react-native-openalpr/android` to `/react-native-openalpr/example/node_modules/react-native-openalpr/`\n\n4. Open Android Studio and import the project `react-native-openalpr/example/android` and wait until Android Studio indexes and links.\n\n5. Run `npm start` from dir /react-native-openalpr/example/\n\n6. Open the path in your browser `http://localhost:8081/index.android.bundle?platform=android&dev=true&hot=false&minify=false`\n\n7. Create file the `/react-native-openalpr/example/android/app/src/main/assets/index.android.bundle`. Copy and paste the data from browser window to the file you just created and save.\n\n8. Return to Android Studio and run project on your development device.\n\nNote: If you are getting errors, double check that you have completed all of the steps above. If you are having issues running `npm start` on Mac OSX and are using homebrew, [this issue might help](https://github.com/facebook/react-native/issues/910).\n\n## Credits\n\n- OpenALPR built from [OpenALPR-iOS](https://github.com/twelve17/openalpr-ios)\n- Project scaffold based on [react-native-camera](https://github.com/lwansbrough/react-native-camera)\n"
 },
 {
  "repo": "JetsonHacksNano/CSI-Camera",
  "language": "Python",
  "readme_contents": "# CSI-Camera\nSimple example of using a MIPI-CSI(2) Camera (like the Raspberry Pi Version 2 camera) with the NVIDIA Jetson Nano Developer Kit. This is support code for the article on JetsonHacks: https://wp.me/p7ZgI9-19v\n\nThe camera should be installed in the MIPI-CSI Camera Connector on the carrier board. The pins on the camera ribbon should face the Jetson Nano module, the stripe faces outward.\n\nThe new Jetson Nano B01 developer kit has two CSI camera slots. You can use the sensor_mode attribute with nvarguscamerasrc to specify the camera. Valid values are 0 or 1 (the default is 0 if not specified), i.e.\n\n```\nnvarguscamerasrc sensor_id=0\n```\n\nTo test the camera:\n\n```\n# Simple Test\n#  Ctrl^C to exit\n# sensor_id selects the camera: 0 or 1 on Jetson Nano B01\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! nvoverlaysink\n\n# More specific - width, height and framerate are from supported video modes\n# Example also shows sensor_mode parameter to nvarguscamerasrc\n# See table below for example video modes of example sensor\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! \\\n   'video/x-raw(memory:NVMM),width=3280, height=2464, framerate=21/1, format=NV12' ! \\\n   nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=720' ! \\\n   nvvidconv ! nvegltransform ! nveglglessink -e\n\nNote: The cameras appear to report differently than show below on some Jetsons. You can use the simple gst-launch example above to determine the camera modes that are reported by the sensor you are using. As an example the same camera from below may report differently on a Jetson Nano B01:\n\nGST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000 \n\nYou should adjust accordingly. As an example, for 3264x2464 @ 21 fps on sensor_id 1 of a Jetson Nano B01:\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=1 ! \\\n   'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! \\\n   nvvidconv flip-method=0 ! 'video/x-raw, width=816, height=616' ! \\\n   nvvidconv ! nvegltransform ! nveglglessink -e\n\nAlso, it's been noticed that the display transform is sensitive to width and height (in the above example, width=816, height=616). If you experience issues, check to see if your display width and height is the same ratio as the camera frame size selected (In the above example, 816x616 is 1/4 the size of 3264x2464).\n```\n\nThere are several examples:\n\nNote: You may need to install numpy for the Python examples to work, ie $ pip3 install numpy\n\nsimple_camera.py is a Python script which reads from the camera and displays to a window on the screen using OpenCV:\n\n$ python simple_camera.py\n\nface_detect.py is a python script which reads from the camera and uses  Haar Cascades to detect faces and eyes:\n\n$ python face_detect.py\n\nHaar Cascades is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. The function is then used to detect objects in other images. \n\nSee: https://docs.opencv.org/3.3.1/d7/d8b/tutorial_py_face_detection.html \n\nThe third example is a simple C++ program which reads from the camera and displays to a window on the screen using OpenCV:\n\n```\n$ g++ -std=c++11 -Wall -I/usr/lib/opencv simple_camera.cpp -L/usr/lib -lopencv_core -lopencv_highgui -lopencv_videoio -o simple_camera\n\n$ ./simple_camera\n```\n\nThe final example is dual_camera.py. This example is for the newer rev B01 of the Jetson Nano board, identifiable by two CSI-MIPI camera ports. This is a simple Python program which reads both CSI cameras and displays them in a window. The window is 960x1080. For performance, the script uses a separate thread for reading each camera image stream. To run the script:\n\n```\n$ python3 dual_camera.py\n```\n\nThe directory 'instrumented' contains instrumented code which can help adjust performance and frame rates.\n\n<h2>Notes</h2>\n\n<h3>Camera Image Formats</h3>\nYou can use v4l2-ctl to determine the camera capabilities. v4l2-ctl is in the v4l-utils:\n\n$ sudo apt-get install v4l-utils\n\nFor the Raspberry Pi V2 camera, typically the output is (assuming the camera is /dev/video0):\n\n```\n$ v4l2-ctl --list-formats-ext\nioctl: VIDIOC_ENUM_FMT\n\tIndex       : 0\n\tType        : Video Capture\n\tPixel Format: 'RG10'\n\tName        : 10-bit Bayer RGRG/GBGB\n\t\tSize: Discrete 3280x2464\n\t\t\tInterval: Discrete 0.048s (21.000 fps)\n\t\tSize: Discrete 3280x1848\n\t\t\tInterval: Discrete 0.036s (28.000 fps)\n\t\tSize: Discrete 1920x1080\n\t\t\tInterval: Discrete 0.033s (30.000 fps)\n\t\tSize: Discrete 1280x720\n\t\t\tInterval: Discrete 0.017s (60.000 fps)\n\t\tSize: Discrete 1280x720\n\t\t\tInterval: Discrete 0.017s (60.000 fps)\n```\n\n<h3>GStreamer Parameter</h3>\nFor the GStreamer pipeline, the nvvidconv flip-method parameter can rotate/flip the image. This is useful when the mounting of the camera is of a different orientation than the default.\n\n```\n\nflip-method         : video flip methods\n                        flags: readable, writable, controllable\n                        Enum \"GstNvVideoFlipMethod\" Default: 0, \"none\"\n                           (0): none             - Identity (no rotation)\n                           (1): counterclockwise - Rotate counter-clockwise 90 degrees\n                           (2): rotate-180       - Rotate 180 degrees\n                           (3): clockwise        - Rotate clockwise 90 degrees\n                           (4): horizontal-flip  - Flip horizontally\n                           (5): upper-right-diagonal - Flip across upper right/lower left diagonal\n                           (6): vertical-flip    - Flip vertically\n                           (7): upper-left-diagonal - Flip across upper left/low\n```\n\n<h2>OpenCV and Python</h2>\nStarting with L4T 32.2.1 / JetPack 4.2.2, GStreamer support is built in to OpenCV.\nThe OpenCV version is 3.3.1 for those versions. Please note that if you are using\nearlier versions of OpenCV (most likely installed from the Ubuntu repository), you\nwill get 'Unable to open camera' errors.\n<br>\nIf you can open the camera in GStreamer from the command line, and have issues opening the camera in Python, check the OpenCV version. \n\n```\n>>>cv2.__version__\n```\n\n<h2>Release Notes</h2>\n\nv3.1 Release March, 2020\n* L4T 32.3.1 (JetPack 4.3)\n* OpenCV 4.1.1\n* Tested on Jetson Nano B01\n* Tested with Raspberry Pi v2 cameras\n\nv3.0 December 2019\n* L4T 32.3.1\n* OpenCV 4.1.1.\n* Tested with Raspberry Pi v2 camera\n\nv2.0 Release September, 2019\n* L4T 32.2.1 (JetPack 4.2.2)\n* OpenCV 3.3.1\n* Tested on Jetson Nano\n\nInitial Release (v1.0) March, 2019\n* L4T 32.1.0 (JetPack 4.2)\n* Tested on Jetson Nano\n\n\n"
 },
 {
  "repo": "gloomyfish1998/opencv_tutorial",
  "language": "C++",
  "readme_contents": "## OpenCV 4.0 C++/python SDK tutorial\n- include dnn module code example\n- very useful case study\n- not only for beginer but also for expericence developer\n- include most opencv modules and API usages.\n- data \u5305\u542b\u6f14\u793a\u56fe\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\n- \u65b0\u589e\u76ee\u5f55Python\uff0c\u6dfb\u52a0\u4e86OpenCV Python\u8bed\u8a00\u6559\u7a0b\n\n## B\u7ad9\u514d\u8d39\u770b\uff01OpenCV C++\u4e0ePython \u5f00\u53d1\u73af\u5883\u642d\u5efa\u89c6\u9891\nhttp://space.bilibili.com/365916694/#/\n\n## \u8d3e\u5fd7\u521a\n2004\u6bd5\u4e1a\u4e8e\u5c71\u4e1c\u5927\u5b66\u9f50\u9c81\u8f6f\u4ef6\u5b66\u9662\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u4e13\u4e1a\u3002\u4e3b\u8981\u4e13\u6ce8\u4e8e\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u5b66\u4e60\u4e0e\u7814\u7a76\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5e94\u7528\u5f00\u53d1\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5e94\u7528\u3002\u4e24\u672c\u4e66\u7c4d\u300aJava\u6570\u5b57\u56fe\u50cf\u5904\u7406-\u7f16\u7a0b\u6280\u5de7\u4e0e\u5e94\u7528\u5b9e\u8df5\u300b\u3001\u300aOpenCV  Android\u5f00\u53d1\u5b9e\u6218\u300b\u4f5c\u8005\u300151CTO\u5b66\u9662\u91d1\u724c\u8bb2\u5e08\u3001CSDN\u535a\u5ba2\u4e13\u5bb6\u3001\u4e13\u6ce8\u56fe\u50cf\u5904\u7406\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7814\u7a76\u4e0e\u5f00\u53d1\u591a\u5e74\u3001\u4e13\u4e1a\u6280\u672f\u535a\u5ba2\u8bbf\u95ee\u91cf\u8d85\u8fc7320\u4e07\u6b21\u3001\u7cbe\u901aOpenCV\u3001ImageJ\u5f00\u6e90\u6846\u67b6\u3001\u56fe\u50cf\u5bf9\u8c61\u68c0\u6d4b\u4e0e\u8bc6\u522b\u7b49\u5e94\u7528\u5f00\u53d1\u6280\u672f\u3002\u5728\u5bf9\u8c61\u68c0\u6d4b\u3001\u533b\u5b66\u7ec6\u80de\u68c0\u6d4b\u4e0e\u8bc6\u522b\u3001\u6587\u672c\u5904\u7406\u3001\u56fe\u50cf\u641c\u7d22\u3001\u4eba\u8138\u7f8e\u5316\u7b97\u6cd5\u65b9\u9762\u6709\u6df1\u5165\u7814\u7a76\u3001\u5f00\u53d1\u8fc7\u591a\u4e2a\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u6a21\u5757\u5e76\u6210\u529f\u5e94\u7528\u5728\u533b\u5b66\u68c0\u6d4b\u4e0e\u5de5\u4e1a\u9886\u57df\u3002\n\n\n### \u6b22\u8fce\u5927\u5bb6\u52a0\u5165\u4eba\u5de5\u667a\u80fd\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\n### OpenCV\u7814\u4e60\u793e\n\u52a0\u5165\u5373\u9001400+\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8bba\u6587\u3002\n- \u6bcf\u5468\u4e00\u5230\u6bcf\u5468\u4e94\u5206\u4eabOpenCV/tensorflow\u77e5\u8bc6\u70b9\u5b66\u4e60\uff08\u97f3\u9891+\u6587\u5b57+\u6e90\u7801\uff09\n- OpenCV+tensorflow\u771f\u5b9e\u6848\u4f8b\u4ee3\u7801\u5206\u4eab\n- \u76f4\u63a5\u5411\u8001\u5e08\u63d0\u95ee\u3001\u6bcf\u5929\u7b54\u7591\u8f85\u5bfc\n- \u7cfb\u7edf\u5316\u5b66\u4e60\u7ea6300\u4e2a\u77e5\u8bc6\u70b9\uff0c\u4ece\u6613\u5230\u96be\u3001\u7531\u6d45\u5165\u6df1\n- \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u77e5\u8bc6\u957f\u6587\u5206\u4eab\n- \u6bcf\u5929\u4e0d\u52303\u6bdb\u94b1\uff0c\u62e5\u62b1\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\n- Python\u4e0eC++\u53cc\u8bed\u77e5\u8bc6\u70b9\u5206\u4eab\n- \u5efa\u7acb\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u4eba\u8109\uff0c\u8ba4\u8bc6\u66f4\u591a\u540c\u884c\uff0c\u4e00\u8d77\u5b66\u4e60\u4ea4\u6d41\uff01\n- \u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587PDF\u4e0e\u5b66\u4e60\u8d44\u6599PPT\u5206\u4eab\n\u5c11\u770b\u4e00\u573a\u7535\u5f71\uff0c\u5c11\u5403\u4e00\u987f\u5927\u9910\uff0c\u5c31\u53ef\u4ee5\u52a0\u6301\u672a\u6765\uff0c99\u5143\u5b66\u4e60\u5c31\u53ef\u4ee5\u505a\u5230\uff01\n![](OpenCVRD.png)\n\n\n## \u89c6\u9891\u8bfe\u7a0b\n\n| \u8bfe\u7a0b\u540d\u79f0\u00a0 \u00a0 \u00a0 \u00a0 | \u8bed\u8a00 \u00a0 \u00a0 \u00a0 \u00a0  | \u5730\u5740\u00a0 \u00a0 \u00a0 \u00a0  |\n| ------------- |:-------------:| :-------------:|\n|Tensorflow+OpenCV\u5b9e\u6218\u884c\u4eba\u68c0\u6d4b|Python|https://edu.51cto.com/course/16898.html|\n| Tensorflow Object Detection API\u5bf9\u8c61\u68c0\u6d4b\u6559\u7a0b| Python| http://edu.51cto.com/course/15208.html |\n| Tensorflow\u96f6\u57fa\u7840\u5165\u95e8\u8bfe\u7a0b| Python| http://edu.51cto.com/course/14584.html |\n| OpenCV Python\u56fe\u50cf\u4e0e\u89c6\u9891\u5206\u6790\u6559\u7a0b| Python| http://edu.51cto.com/course/14029.html |\n| OpenCV Python\u56fe\u50cf\u5904\u7406\u8fdb\u9636\u6559\u7a0b\u89c6\u9891\u8bfe\u7a0b| Python| http://edu.51cto.com/course/13789.html |\n| OpenCV Python\u96f6\u57fa\u7840\u5165\u95e8\u89c6\u9891\u6559\u7a0b| Python| http://edu.51cto.com/course/13680.html |\n| OpenCV4Android\u6444\u50cf\u5934\u5e94\u7528\u6848\u4f8b\u5b9e\u6218\u9891\u6559\u7a0b| Android| http://edu.51cto.com/course/12700.html |\n| Python+OpenCV3.3\u56fe\u50cf\u5904\u7406\u89c6\u9891\u6559\u7a0b| Python| http://edu.51cto.com/course/11324.html |\n| OpenCV3.3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc(DNN)\u6a21\u5757-\u5e94\u7528\u89c6\u9891\u6559\u7a0b| C++| http://edu.51cto.com/course/11516.html |\n| \u4eba\u5de5\u667a\u80fd\u4e4bOpenCV\u4eba\u8138\u8bc6\u522b\u6848\u4f8b\u5b9e\u6218\u89c6\u9891\u6559\u7a0b| C++| http://edu.51cto.com/course/10759.html |\n| OpenCV\u56fe\u50cf\u5206\u5272\u5b9e\u6218\u89c6\u9891\u6559\u7a0b|C++| http://edu.51cto.com/course/10166.html |\n| OpenCV\u89c6\u9891\u5206\u6790\u4e0e\u5bf9\u8c61\u8ddf\u8e2a\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8837.html |\n| OpenCV\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3\u4e0e\u4f7f\u7528\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8645.html |\n| OpenCV\u56fe\u50cf\u5904\u7406-\u5c0f\u6848\u4f8b\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8354.html |\n| OpenCV \u7279\u5f81\u63d0\u53d6\u4e0e\u68c0\u6d4b\u5b9e\u6218\u89c6\u9891\u8bfe\u7a0b|C++| http://edu.51cto.com/course/8305.html |\n| OpenCV\u56fe\u50cf\u5904\u7406\u89c6\u9891\u8bfe\u7a0b|C++| http://edu.51cto.com/course/7521.html |\n| OpenCV For Android\u57fa\u7840\u5165\u95e8\u5b9e\u6218\u89c6\u9891\u8bfe\u7a0b|Android | http://edu.51cto.com/course/8012.html |\n| OpenCV3.2\u5e94\u7528\u5b9e\u6218\u7cfb\u5217\u89c6\u9891\u8bfe\u7a0b-Android\u5e73\u53f0\u94f6\u884c\u5361\u5361\u53f7\u8bc6\u522b|Android| http://edu.51cto.com/course/9170.html |\n| OpenCV3.2\u670d\u52a1\u5668\u7aef\u56fe\u50cf\u5904\u7406\u4e0eJava\u56fe\u50cf\u5904\u7406\u89c6\u9891\u6559\u7a0b|Java| http://edu.51cto.com/course/10475.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u4e8c\u503c\u56fe\u50cf\u5206\u6790\u89c6\u9891\u8bfe\u7a0b(Java\u8bed\u8a00\u63cf\u8ff0)|Java| http://edu.51cto.com/course/6901.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u7a7a\u95f4\u57df\u5377\u79ef(Java\u8bed\u8a00\u63cf\u8ff0)\u89c6\u9891\u8bfe\u7a0b|Java| http://edu.51cto.com/course/6464.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u57fa\u7840\u5165\u95e8\u89c6\u9891\u8bfe\u7a0b(Java\u8bed\u8a00\u63cf\u8ff0)|Java| http://edu.51cto.com/course/6335.html |\n\n## \u8054\u7cfb\u65b9\u5f0f\nQQ:57558865 \u6ce8\u660e\uff1aopencv\u5373\u53ef\n\n\u90ae\u4ef6:57558865@qq.com\n\n\u5fae\u4fe1\uff1agloomy_fish \u6ce8\u660e \u5408\u4f5c\n\n<b>\u975e\u6280\u672f\u5408\u4f5c\u8bf7\u52ff\u52a0\u5fae\u4fe1</b>\n\n\u5fae\u535a \uff1a http://weibo.com/u/3181256271\n\nCSDN\u535a\u5ba2\u5730\u5740\uff1a http://blog.csdn.net/jia20003\n\n51CTO\u5b66\u9662\u8bfe\u7a0b\u4e3b\u9875\uff1ahttp://edu.51cto.com/lecturer/8837804.html \n\n### OpenCV\u5b66\u5802 \u5fae\u4fe1\u516c\u4f17\u53f7\n![](CVSCHOOL.jpg)\n\n\n\n\n"
 },
 {
  "repo": "surya-veer/movement-tracking",
  "language": "Python",
  "readme_contents": "# Realtime Face Movement Tracking ![](https://bit.ly/surya-veer-movement-tracking)\n90 Lines of code to convert your face movement into keyboard commands.\n\n# Description\nThis is a basic face movement tracking that can convert face movement into keyboard commands like **UP - DOWN - LEFT  - RIGHT**. I used facial landmarks to detect face and get the nose out of it for better referencing. I have created two versions of it, v1 is using a fixed reference boundary which not work expected properly because we need to come at the same position after each movement. To save this I created V2 which uses position change with respect to the previous position. This is more dynamic and easy to control the moves. No need to set position again and again.\n\n## movement-v1.py\nIn version1, I used a fixed reference boundary. If nose reference is out of boundary then I calculate the direction of movement. After getting direction I am converting it into keyboard commands using the keyboard library.\n\n## movement-v2.py\nIn version2, I am using reference change with respect to the previous position in a particular time window and then calculating the direction vector to get direction and converting it to keyboard command.\n\n## Dependencies\nThis is the list of dependencies for running this application. Use pip to install them.\n * **opencv**\n * **keyboard**\n\n ```\n $ pip install -r requirements.txt\n ```\n\n## How to use\n1. Download or clone this repository.\n2. Extract to some location.\n3. First, run **```movement-v1.py```** (for fix boundary) or run **```movement-v2.py```**(for dynamic movement) <br>\n  NOTE: If you are getting 215 assertion failed!! on line 81 check this (https://github.com/surya-veer/movement-tracking/issues/4#issuecomment-664018021)\n\n4. open any online atari game like Subway surfers or temple run.\n5. Start doing movements to play game. It will press up-down-left-right based on your movements.\n\n# Fun with face movements\nOpen any online game on the browser which needs UP-DOWN-LEFT-RIGHT movements following games, you can find many games if you search on google.\n1. Subway surfer https://www.kiloo.com/subway-surfers/\n2. Temple run https://m.plonga.com/adventure/Temple-Run-2-Online-Tablet\n\n### You can do a lot more things by the small code change.\n\n### ** SUPPORT OPEN SOURCE **\n"
 },
 {
  "repo": "shouzhong/Scanner",
  "language": "Java",
  "readme_contents": "# Scanner\n\n## \u8054\u7cfb\u6211\nQQ\u7fa4 777891894\uff08\u95ee\u9898\u4ea4\u6d41\uff0c\u7b54\u6848\uff1aandroid\uff09\n\n## \u8bf4\u660e\n\u8bc6\u522b\u5e93\uff0c\u8bc6\u522b\u5668\u53ef\u9009\u62e9\uff0c\u8fd9\u91cc\u6709\u4f60\u5e38\u7528\u7684\u4e8c\u7ef4\u7801/\u6761\u7801\u8bc6\u522b\uff0c\u8fd8\u6709\u4f60\u53ef\u80fd\u7528\u5230\u7684\u8eab\u4efd\u8bc1\u8bc6\u522b\u3001\u94f6\u884c\u5361\u8bc6\u522b\u3001\u8f66\u724c\u8bc6\u522b\u3001\u56fe\u7247\u6587\u5b57\u8bc6\u522b\u3001\u9ec4\u56fe\u8bc6\u522b\u3001\u9a7e\u9a76\u8bc1\u8bc6\u522b\uff0c\u5982\u679c\u6ca1\u6709\u4f60\u60f3\u8981\u7684\uff0c\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\u3002\u8be5\u5e93\u53ea\u8bc6\u522b\u626b\u63cf\u6846\u5185\u7684\u56fe\u50cf\uff0c\u8bc6\u522b\u901f\u7387\u4e0a\u5927\u5927\u63d0\u9ad8\uff0c\u800c\u4e14\u8fd9\u4e2a\u5e93\u6bd4\u8d77\u5176\u5b83\u7684\u5e93\u5c31\u662f\u89e3\u51b3\u4e86\u6444\u50cf\u5934\u9884\u89c8\u53d8\u5f62\uff0c\u9884\u89c8\u9875\u9762\u9ad8\u5ea6\u81ea\u5b9a\u4e49\uff0c\u4f60\u53ef\u4ee5\u50cf\u5e38\u89c4\u4e00\u6837\u6574\u4e2a\u9875\u9762\u90fd\u662f\u9884\u89c8\uff0c\u6216\u8005\u4f60\u53ef\u4ee5\u9009\u62e9\u5728\u4efb\u4f55\u4f4d\u7f6e\u5b9a\u4e49\u4efb\u4f55\u5c3a\u5bf8\u7684\u9884\u89c8\uff0c\u626b\u63cf\u6846\u4e5f\u9ad8\u5ea6\u81ea\u5b9a\u4e49\uff0c\u4f60\u53ef\u4ee5\u50cf\u5e38\u89c4\u4e00\u6837\u5c45\u4e2d\uff0c\u6216\u8005\u4f60\u4e5f\u53ef\u4ee5\u5728\u9884\u89c8\u7684\u4efb\u4f55\u4f4d\u7f6e\u5b9a\u4e49\u4efb\u4f55\u5c3a\u5bf8\u7684\u626b\u63cf\u6846\uff08\u5b9e\u9645\u8bc6\u522b\u7684\u626b\u63cf\u6846\u548c\u753b\u4e0a\u53bb\u7684\u626b\u63cf\u6846\u4e0d\u4e00\u5b9a\u662f\u4e00\u6837\u7684\uff0c\u7531\u4f60\u81ea\u5df1\u51b3\u5b9a\uff09\u3002\n## \u6548\u679c\u56fe\n\n<table>\n    <tr>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/1.jpg\"/></td>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/2.jpg\"/></td>\n    </tr>\n    <tr>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/3.jpg\"/></td>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/4.jpg\"/></td>\n    </tr>\n</table>\n\n## [\u4e0b\u8f7d apk-demo](http://downgit.zhoudaxiaa.com/#/home?url=https://github.com/shouzhong/Scanner/blob/master/app/release/app-release.apk)\n\n## \u4f7f\u7528\n### \u4f9d\u8d56\n```\nimplementation 'com.shouzhong:Scanner:1.1.3'\n```\n\u4ee5\u4e0b\u9009\u62e9\u81ea\u5df1\u9700\u8981\u7684\n```\n// zxing\nimplementation 'com.google.zxing:core:3.3.3'\n// zbar\nimplementation 'com.shouzhong:ScannerZBarLib:1.0.0'\n// \u94f6\u884c\u5361\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerBankCardLib:1.0.3'\n// \u8eab\u4efd\u8bc1\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerIdCardLib:1.0.4'\n// \u8f66\u724c\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerLicensePlateLib:1.0.3'\n// \u56fe\u7247\u6587\u5b57\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerTextLib:1.0.0'\n// \u9ec4\u56fe\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerNsfwLib:1.0.0'\n// \u9a7e\u9a76\u8bc1\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerDrivingLicenseLib:1.0.1'\n// \u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\nimplementation 'com.shouzhong:ScannerIdCard2Lib:1.0.0'\n```\n### \u4ee3\u7801\n\u57fa\u672c\u4f7f\u7528\n```\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n    <com.shouzhong.scanner.ScannerView\n        android:id=\"@+id/sv\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"1080px\"\n        android:background=\"#000000\"/>\n</RelativeLayout>\n```\n```\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_scanner);\n    scannerView = findViewById(R.id.sv);\n    scannerView.setViewFinder(new ViewFinder(this));\n    scannerView.setSaveBmp(true);\n    scannerView.setEnableZXing(true);\n    scannerView.setEnableZBar(true);\n    scannerView.setEnableBankCard(true);\n    scannerView.setEnableIdCard(true);\n    scannerView.setEnableLicensePlate(true);\n    scannerView.setCallback(new Callback() {\n        @Override\n        public void result(Result result) {\n            tvResult.setText(\"\u8bc6\u522b\u7ed3\u679c\uff1a\\n\" + result.toString());\n            scannerView.restartPreviewAfterDelay(2000);\n        }\n    });\n}\n\n@Override\nprotected void onResume() {\n    super.onResume();\n    scannerView.onResume();\n}\n\n@Override\nprotected void onPause() {\n    super.onPause();\n    scannerView.onPause();\n}\n```\n\u5f00\u542f\u6216\u8005\u5173\u95ed\u67d0\u4e2a\u8bc6\u522b\u5668\n```\n// \u542f\u7528zxing\u8bc6\u522b\u5668\nscannerView.setEnableZXing(true);\n// \u542f\u7528zbar\u8bc6\u522b\u5668\nscannerView.setEnableZBar(true);\n// \u542f\u7528\u94f6\u884c\u5361\u8bc6\u522b\u5668\nscannerView.setEnableBankCard(true);\n// \u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff08\u8fd9\u91cc\u53ea\u652f\u63012\u4ee3\u8eab\u4efd\u8bc1\uff09\nscannerView.setEnableIdCard(true);\n// \u542f\u7528\u8f66\u724c\u8bc6\u522b\nscannerView.setEnableLicensePlate(true);\n// \u542f\u7528\u9a7e\u9a76\u8bc1\u8bc6\u522b\nscannerView.setEnableDrivingLicense(true);\n// \u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\nscannerView.setEnableIdCard2(true);\n```\n\u5982\u679c\u4f60\u60f3\u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\n```\nscannerView.setScanner(new IScanner() {\n    /**\n     * \u8fd9\u91cc\u5b9e\u73b0\u81ea\u5df1\u7684\u8bc6\u522b\u5668\uff0c\u5e76\u628a\u8bc6\u522b\u7ed3\u679c\u8fd4\u56de\n     *\n     * @param data \u77e9\u5f62\u6846\u5185nv21\u56fe\u50cf\u6570\u636e\n     * @param width \u56fe\u50cf\u5bbd\u5ea6\n     * @param height \u56fe\u50cf\u9ad8\u5ea6\n     * @return\n     * @throws Exception\n     */\n    @Override\n    public Result scan(byte[] data, int width, int height) throws Exception {\n        // \u5982\u679c\u4f60\u60f3\u8f6c\u4e3aBitmap\uff0c\u8bf7\u4f7f\u7528NV21.nv21ToBitmap(byte[] nv21, int width, int height)\n        return null;\n    }\n});\n```\n\u8fd9\u91cc\u6ca1\u7ed9\u9ed8\u8ba4\u7684\u9884\u89c8\u9875\u9762\uff0c\u9700\u8981\u81ea\u5df1\u81ea\u5b9a\u4e49\uff0c\u8bf7\u53c2\u8003[demo](https://github.com/shouzhong/Scanner/blob/master/app/src/main/java/com/shouzhong/scanner/demo/TestViewFinder.java)\n\n## \u56de\u8c03\u8bf4\u660e\n\nResult\n\n\u5c5e\u6027 | \u8bf4\u660e\n------------ | -------------\nTYPE_CODE | \u7c7b\u578b\uff1a\u4e8c\u7ef4\u7801/\u6761\u7801\nTYPE_ID_CARD_FRONT | \u7c7b\u578b\uff1a\u8eab\u4efd\u8bc1\u4eba\u5934\u9762\nTYPE_ID_CARD_BACK | \u7c7b\u578b\uff1a\u8eab\u4efd\u8bc1\u56fd\u5fbd\u9762\nTYPE_BANK_CARD | \u7c7b\u578b\uff1a\u94f6\u884c\u5361\nTYPE_LICENSE_PLATE | \u7c7b\u578b\uff1a\u8f66\u724c\nTYPE_DRIVING_LICENSE | \u7c7b\u578b\uff1a\u9a7e\u9a76\u8bc1\ntype | \u7ed3\u679c\u7c7b\u578b\npath | \u4fdd\u5b58\u7684\u56fe\u7247\u8def\u5f84\ndata | \u6570\u636e\n```\n// \u4ee5\u4e0b\u662f\u5bf9data\u7684\u8bf4\u660e\n// \u5f53type\u4e3aTYPE_CODE\uff0cTYPE_BANK_CARD\uff0cTYPE_LICENSE_PLATE\u65f6\uff0cdata\u4e3a\u5b57\u7b26\u4e32\n// \u5f53type\u4e3aTYPE_ID_CARD_FRONT\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"cardNumber\": \"21412412421\",// \u8eab\u4efd\u8bc1\u53f7\n\t\"name\": \"\u5f20\u4e09\",// \u59d3\u540d\n\t\"sex\": \"\u7537\",// \u6027\u522b\n\t\"nation\": \"\u6c49\",// \u6c11\u65cf\n\t\"birth\": \"1999-01-01\",// \u51fa\u751f\n\t\"address\": \"\u5730\u5740\"// \u5730\u5740\n}\n// \u5f53type\u4e3aTYPE_ID_CARD_BACK\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"organization\": \"\u7b7e\u53d1\u673a\u5173\",// \u7b7e\u53d1\u673a\u5173\n\t\"validPeriod\": \"20180101-20380101\"// \u6709\u6548\u671f\u9650\n}\n// \u5f53type\u4e3aTYPE_DRIVING_LICENSE\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"cardNumber\": \"43623446432\",// \u8bc1\u53f7\n\t\"name\": \"\u5f20\u4e09\",// \u59d3\u540d\n\t\"sex\": \"\u7537\",// \u6027\u522b\n\t\"nationality\": \"\u4e2d\u56fd\",// \u56fd\u7c4d\n\t\"address\": \"\u5730\u5740\",// \u5730\u5740\n\t\"birth\": \"1999-01-01\",// \u51fa\u751f\u65e5\u671f\n\t\"firstIssue\": \"2018-01-01\",// \u521d\u6b21\u9886\u8bc1\u65e5\u671f\n\t\"_class\": \"C1\",// \u51c6\u9a7e\u8f66\u578b\n\t\"validPeriod\": \"20180101-20240101\"// \u6709\u6548\u671f\u9650\n}\n```\n\nBankCardInfoBean\n\n\u5c5e\u6027\u6216\u65b9\u6cd5 | \u8bf4\u660e\n------------ | -------------\ncardNumber | \u94f6\u884c\u5361\u53f7\ncardType | \u94f6\u884c\u5361\uff08\u82f1\u6587\uff09\u7c7b\u578b\nbank | \u94f6\u884c\uff08\u82f1\u6587\uff09\u540d\u79f0\ngetCNBankName | \u83b7\u53d6\u94f6\u884c\uff08\u4e2d\u6587\uff09\u540d\u79f0\ngetBankId | \u83b7\u53d6\u94f6\u884c\u7f16\u53f7\ngetCNCardType | \u83b7\u53d6\u94f6\u884c\u5361\uff08\u4e2d\u6587\uff09\u7c7b\u578b\n\n## \u65b9\u6cd5\u8bf4\u660e\n\nScannerView\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\nsetViewFinder | \u626b\u63cf\u533a\u57df\nsetCallback | \u626b\u7801\u6210\u529f\u540e\u7684\u56de\u8c03\nsetCameraDirection | \u6444\u50cf\u5934\u65b9\u5411\uff0c\u540e\u7f6e\u4e3aCamera.CameraInfo.CAMERA_FACING_BACK\uff0c\u524d\u7f6e\u4e3aCamera.CameraInfo.CAMERA_FACING_FRONT\nsetEnableZXing | \u662f\u5426\u542f\u7528zxing\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableZBar | \u662f\u5426\u542f\u7528zbar\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableQrcode | \u662f\u5426\u542f\u52a8\u4e8c\u7ef4\u7801\u8bc6\u522b\uff0c\u9ed8\u8ba4true\uff0c\u53ea\u6709\u5728zxing\u6216\u8005zbar\u5f00\u542f\u65f6\u6709\u6548\nsetEnableBarcode | \u662f\u5426\u542f\u52a8\u6761\u7801\u8bc6\u522b\uff0c\u9ed8\u8ba4true\uff0c\u53ea\u6709\u5728zxing\u6216\u8005zbar\u5f00\u542f\u65f6\u6709\u6548\nsetEnableBankCard | \u662f\u5426\u542f\u7528\u94f6\u884c\u5361\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableIdCard | \u662f\u5426\u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableIdCard2 | \u662f\u5426\u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\uff0c\u9ed8\u8ba4false\nsetEnableDrivingLicense | \u662f\u5426\u542f\u7528\u9a7e\u9a76\u8bc1\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableLicensePlate | \u662f\u5426\u542f\u7528\u8f66\u724c\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetScanner | \u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\nonResume | \u5f00\u542f\u626b\u63cf\nonPause | \u505c\u6b62\u626b\u63cf\nrestartPreviewAfterDelay | \u8bbe\u7f6e\u591a\u5c11\u6beb\u79d2\u540e\u91cd\u542f\u626b\u63cf\nsetFlash | \u5f00\u542f/\u5173\u95ed\u95ea\u5149\u706f\ntoggleFlash | \u5207\u6362\u95ea\u5149\u706f\u7684\u70b9\u4eae\u72b6\u6001\nisFlashOn | \u95ea\u5149\u706f\u662f\u5426\u88ab\u70b9\u4eae\nsetShouldAdjustFocusArea | \u8bbe\u7f6e\u662f\u5426\u8981\u6839\u636e\u626b\u7801\u6846\u7684\u4f4d\u7f6e\u53bb\u8c03\u6574\u5bf9\u7126\u533a\u57df\u7684\u4f4d\u7f6e\uff0c\u90e8\u5206\u624b\u673a\u4e0d\u652f\u6301\uff0c\u9ed8\u8ba4false\nsetSaveBmp | \u8bbe\u7f6e\u662f\u5426\u4fdd\u5b58\u8bc6\u522b\u7684\u56fe\u7247\uff0c\u9ed8\u8ba4false\nsetRotateDegree90Recognition | \u662f\u5426\u5728\u539f\u6765\u8bc6\u522b\u7684\u56fe\u50cf\u57fa\u7840\u4e0a\u65cb\u8f6c90\u5ea6\u7ee7\u7eed\u8bc6\u522b\uff0c\u9ed8\u8ba4false\n\nScannerUtils\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\ndecodeCode | \u4e8c\u7ef4\u7801/\u6761\u7801\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeBank | \u94f6\u884c\u5361\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ngetBankCardInfo | \u83b7\u53d6\u94f6\u884c\u5361\u4fe1\u606f\uff0c\u8bf7\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeIdCard | \u8eab\u4efd\u8bc1\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeId2Card | \u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeDrivingLicense | \u9a7e\u9a76\u8bc1\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeLicensePlate | \u8f66\u724c\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeText | \u56fe\u7247\u6587\u5b57\u8bc6\u522b\uff0c\u8bf7\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeNsfw | \u9ec4\u56fe\u8bc6\u522b\uff0c\u5927\u4e8e0.3\u53ef\u4ee5\u8bf4\u56fe\u7247\u6d89\u9ec4\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ncreateBarcode | \u6761\u7801\u751f\u6210\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ncreateQRCode | \u4e8c\u7ef4\u7801\u751f\u6210\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\naddLogo | \u5f80\u56fe\u7247\u4e2d\u95f4\u52a0logo\n\nNV21\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\nnv21ToBitmap | nv21\u8f6cbitmap\nbitmapToNv21 | bitmap\u8f6cnv21\n\n## \u600e\u4e48\u628a\u6211\u7684\u6574\u4e2a\u9879\u76ee\u5bfc\u8fdb\u53bb\n1. \u8be5\u9879\u76ee\u4f7f\u7528opencv-3.4.6\uff0c[\u70b9\u51fb\u4e0b\u8f7d](https://nchc.dl.sourceforge.net/project/opencvlibrary/3.4.6/opencv-3.4.6-android-sdk.zip)\n2. NDK\u7248\u672cr16\n3. \u628alicennseplate\u7684CMakeLists.txt\u7684\u7b2c12\u884c\u66ff\u6362\u6210\u81ea\u5df1\u7684opencv-android-sdk\u7684JNI\u8def\u5f84\n4. \u5220\u9664\u6240\u6709gradle\u91cc\u7684 apply from: 'bintray.gradle'\n5. \u5220\u9664bankcard\u7684build.gradle\u91cc\u7684android->externalNativeBuild\u4ee5\u53caandroid->defaultConfig->ndk\u548cexternalNativeBuild\u6807\u7b7e\n6. \u5220\u9664text\u7684build.gradle\u91cc\u7684android->externalNativeBuild\u4ee5\u53caandroid->defaultConfig->ndk\u548cexternalNativeBuild\u6807\u7b7e\n7. \u5982\u679c\u662flinux\u7528\u6237\uff0c\u8bf7\u5728licennseplate\u7684build.gradle\u6dfb\u52a0\u4ee5\u4e0b\n```\nandroid {\n...\n  defaultConfig {\n      ...\n      externalNativeBuild {\n          cmake {\n              cppFlags \"-std=c++11\"\n              // linux\u8bf7\u6dfb\u52a0\u4ee5\u4e0b\n              arguments \"-DANDROID_TOOLCHAIN=gcc\", \"-DANDROID_ARM_NEON=TRUE\", \"-DANDROID_STL_FORCE_FEATURES=OFF\"\n          }\n      }\n  }\n}\n```\n\n## \u6ce8\u610f\u4e8b\u9879\n1. so\u8d44\u6e90\u53ea\u6709arm\u683c\u5f0f\u7684\uff0cScannerDrivingLicenseLib\u548cScannerIdCard2Lib\u65e0arm64-v8a\u683c\u5f0f\n\n## \u6c42star\n#### [BaseLib](https://github.com/shouzhong/BaseLib)\uff0cui\u5f00\u53d1\u57fa\u7840\u5305\n#### [Bridge](https://github.com/shouzhong/Bridge)\uff0c\u8de8\u8fdb\u7a0b\u7ba1\u7406\u5e93\n#### [ScreenHelper](https://github.com/shouzhong/ScreenHelper)\uff0c\u5c4f\u5e55\u9002\u914d\u5e93\n"
 },
 {
  "repo": "cyrildiagne/screenpoint",
  "language": "Python",
  "readme_contents": "# ScreenPoint\n\nFinds the (x,y) coordinates of the centroid of an image (eg: a mobile phone camera image) pointing at another image (eg: a computer screen) using [OpenCV SIFT](https://docs.opencv.org/2.4/modules/nonfree/doc/feature_detection.html).\n\n![debug view](example/match_debug.png)\n\n## Installation\n\nThis library only supports Python 3.6 or Python 3.7. That until SIFT becomes available again in opencv-python-contrib.\n\n```bash\npip install screenpoint\n```\n\n## Usage\n\n```python\nimport screenpoint\nimport cv2\n\n# Load input images.\nscreen = cv2.imread('screen.png', 0)\nview = cv2.imread('view.jpg', 0)\n\n# Project view centroid to screen space.\n# x and y are the coordinate of the `view` centroid in `screen` space.\nx, y = screenpoint.project(view, screen)\n```\n\nSee [example.py](example.py) for more information.\n"
 },
 {
  "repo": "abidrahmank/OpenCV2-Python",
  "language": "Python",
  "readme_contents": "This repo contains some samples codes on OpenCV-Python with new 'cv2' interface.\n\nIt contains two folders:\n1) Official_Tutorial_Python_version : It contains the corresponding Python codes for C++ codes in official tutorials. So explanation of the code can be found at 'http://opencv.itseez.com/doc/tutorials/tutorials.html' \n\n2) Blog_codes : It contains some codes which I have explained in my blog 'www.opencvpython.blogspot.com'. For explanation, visit the blog.\n\nYou can contact me on abidrahman2@gmail.com\n\nWith Regards,\nAbid Rahman K."
 },
 {
  "repo": "BloodAxe/OpenCV-Tutorial",
  "language": "C++",
  "readme_contents": "OpenCV Tutorial\n==========================\n\nThis repository contains source code of OpenCV Tutorial application.\n\nSamples list (done)\n==========================\n* Edge detection (Canny, Sobel, Schaar)\n* Image transformations (Sepia, negative, contrast and brightness adjustments)\n* Feature Detection (SURF, ORB, FREAK)\n* Video tracking (KLT, BRIEF, ORB)\n\nSamples list (plans)\n==========================\n* PTAM  \n\nStep by step tutorials\n==========================\n* [Part 1](http://computer-vision-talks.com/2012-06-23-opencv-tutorial-part-1/)\n* [Part 2](http://computer-vision-talks.com/2012-06-24-opencv-tutorial-part-2/)\n* [Part 3](http://computer-vision-talks.com/2012-06-27-opencv-tutorial-part-3/)\n* [Part 4](http://computer-vision-talks.com/2012-07-07-opencv-tutorial-part-4/)\n* [Part 5](http://computer-vision-talks.com/2012-07-14-opencv-tutorial-part-5/)\n* [Part 6](http://computer-vision-talks.com/2012-07-22-opencv-tutorial-part-6/)\n* [Part 7](http://computer-vision-talks.com/2012-10-22-opencv-tutorial-part-7/)\n\nRoadmap\n==========================\nhttp://computer-vision-talks.com/opencv-tutorial-roadmap/\n\nCopyright\n==========================\nIdea and development by Eugene Khvedchenya <ekhvedchenya@gmail.com>\n\nhttp://computer-vision-talks.com\n\nThis application is provided via BSD licence, it is free for both academic and commercial use.\n\nThis application is provided as-is, with no warranty expressed or implied.  Use this application at your own risk.\nThe author assumes no liability for any loss associated with the use of this application.\nIf you do not agree with the terms of this license, do not install this application.\n\nContributors\n==========================\n * Anton Belodedenko <anton.belodedenko@gmail.com> \n * Emmanuel d'Angelo <http://www.computersdontsee.net>\n * Daniel J. Pinter <datazombies@gmail.com>\n"
 },
 {
  "repo": "Breta01/handwriting-ocr",
  "language": "Jupyter Notebook",
  "readme_contents": "# Handwriting OCR\nThe project tries to create software for recognition of a handwritten text from photos (also for Czech language). It uses computer vision and machine learning. And it experiments with different approaches to the problem. It started as a school project which I got a chance to present on Intel ISEF 2018.\n\n<p align=\"center\"><img src =\"doc/imgs/poster.png?raw=true\" height=\"340\" alt=\"Sublime's custom image\" /></p>\n\n## Program Structure\nProces of recognition is divided into 4 steps. The initial input is a photo of page with text.\n\n1. Detection of page and removal of background\n2. Detection and separation of words\n3. Normalization of words\n4. Separation and recegnition of characters (recognition of words)\n\nMain files combining all the steps are [OCR.ipynb](notebooks/OCR.ipynb) or [OCR-Evaluator.ipynb](notebooks/ocr_evaluator.ipynb). Naming of files goes by step representing - name of machine learning model.\n\n## Getting Started\n### 1. Clone the repository\n```\ngit clone https://github.com/Breta01/handwriting-ocr.git\n```\nAfter downloading the repo, you have to download the datasets and models (for more info look into [data](data/) and [models](models/) folders).\n\n### 2. Requirements\nThe project is created using Python 3.6 with Jupyter Notebook. I recommend using Anaconda. If you have it, you can run the installation as:\n```\nconda create --name ocr-env --file environment.yml\nconda activate ocr-env\n```\nMain libraries (all required libraries are in [environment.yml](environment.yml)):\n* Numpy (1.13)\n* Tensorflow (1.4)\n* OpenCV (3.1)\n* Pandas (0.21)\n* Matplotlib (2.1)\n\n### Run\nWith all required libraries installed and cloned repo, run `jupyter notebook` in the directory of the project. Then you can work on the particular notebook.\n\n## Contributing\nBest way how to get involved is through creating [GitHub issues](https://github.com/Breta01/handwriting-ocr/issues) or solving one! If there aren't any issues you can contact me directly on email.\n\n## License\n**MIT**\n\n## Support the project\nIf this project helped you or you want to support quick answers to questions and issues. Or you just think it is an interesting project. Please consider a small donation.\n\n[![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif)](https://paypal.me/bretahajek/2)\n"
 },
 {
  "repo": "casparwylie/Perceptron",
  "language": "Python",
  "readme_contents": " NOTE: This is an incomplete project. I may decide to spend a lot more time on it if people persist giving it positive attention.\r\n\r\n# Perceptron\r\nA flexible artificial neural network builder to analysis performance, and optimise the best model. \r\n\r\nPerceptron is a software that will help researchers, students, and programmers to\r\ndesign, compare, and test artificial neural networks. As it stands, there are few visual\r\ntools that do this for free, and with simplicity.\r\n<b>This software is largely for educational purposes</b>, allowing people to experiment\r\nand understand how the different parameters within an ANN can result in\r\ndifferent performances and better results. I have not used libraries like TensorFlow specifally so people\r\ncan see what goes on lower level, all within the code. \r\n\r\n![screenshotnew](https://user-images.githubusercontent.com/7353547/27341298-c611ba96-55d4-11e7-9da9-9cfd6045ae5c.png)\r\n\r\n\r\n\r\n![untitled](https://cloud.githubusercontent.com/assets/7353547/25346609/effb5106-290f-11e7-8426-788a10fd4e2f.png)\r\n"
 },
 {
  "repo": "jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV",
  "language": "Jupyter Notebook",
  "readme_contents": "# Tutorial: Computer Vision and Machine Learning with Python, Keras and OpenCV\n\n<a href=\"https://tracking.gitads.io/?repo=Computer-Vision-Basics-with-Python-Keras-and-OpenCV\"><img src=\"https://images.gitads.io/Computer-Vision-Basics-with-Python-Keras-and-OpenCV\" alt=\"GitAds\"/></a>\n\n### Includes a demonstration of concepts with Gesture Recognition.\nThis was created as part of an educational for the [Western Founders Network](https://foundersnetwork.ca/) computer vision and machine learning educational session.\n\n## Demo\n\nThe final demo can be seen [here](https://www.youtube.com/watch?v=IJV11OGTNT8) and below:\n\n<a href=\"https://imgflip.com/gif/22n3o6\"><img src=\"https://i.imgflip.com/22n3o6.gif\"/></a>\n\n## Contents\n[notebook.ipynb](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/blob/master/notebook.ipynb) contains a full tutorial of basic computer vision and machine learning concepts, including:\n\n* *What computers see*\n* Image Filters and Functions\n  - Blurring\n  - Dilating\n  - Erosion\n  - Canny Edge Detectors\n  - Thresholding\n* Background Subtraction Techniques\n  - Using a background image to find differences\n  - Using motion based background subtraction algorithms\n* Contours\n  - Finding and sorting contours\n* Tracking\n* (Deep) Neural Networks \n* (Deep) Convolutional Neural Networks\n* Demo Project: Gesture Recognition\n  - Extracting the subject\n  - Tracking the hand\n  - Collecting data\n  - Building the Neural Network\n  - Preparing Data for Training\n  - Training the Network\n  - Plotting Model History\n  \n*Note: Please check the [issues](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/issues) on this repo if you're having problems with the notebook.*\n\n## Installation Instructions ('$' means run this in terminal/command prompt, do not type '$')\n### Windows:\n* Install Anaconda (https://www.anaconda.com/download/) or Miniconda (https://conda.io/miniconda.html) to save hard drive space\n* Start an Anaconda Prompt. (Search Anaconda in the start menu.)\n#### Option 1: Exact source package installs\n* Use the spec-file.txt provided, install identical packages\n\n        $ conda create -n [ENV_NAME] --file spec-file.txt # create new env with same packages\n    or, if you have an existing environment\n\n        $ conda install -n [ENV_NAME] --file spec-file.txt # install packages into an existing env\n* Then activate the environment\n\n        $ activate cv\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install package\n\n          $ pip install [file path]\n#### Option 2: Package installs\n* Using the environment.yml file provided, run\n\n        $ conda create -n cv --file environment.yml\n    or, if you have an existing environment\n\n        $ conda install -n [ENV_NAME] --file environment.yml # install packages into an existing env\n* Activate the environment\n\n        $ activate cv\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install the package\n\n          $ pip install [file path]\n#### Option 3: Manually installing packages\n* Create and activate a Python 3.5 conda environment called cv.\n\n        $ conda create -n cv python=3.5\n\n        $ activate cv\n* Install Numpy (http://www.numpy.org/)\n\n        $ conda install numpy\n* Install Matplotlib (https://matplotlib.org/)\n\n        $ conda install matplotlib\n* Install Keras (https://keras.io/) \n\n        $ conda install keras\n    - This should also install tensorflow\n* Install h5py (http://www.h5py.org/)\n\n        $ conda install h5py\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install package\n\n          $ pip install [file path]\n* Install Jupyter Notebook (http://jupyter.org/)\n\n        $ conda install jupyter notebook\n* Install IPython (https://ipython.org/)\n\n        $ conda install ipython\n        \n### Mac/Linux: Manually installing packages\n* Install Anaconda (https://www.anaconda.com/download/) or Miniconda (https://conda.io/miniconda.html) to save hard drive space\n#### Mac:\n* For Miniconda, open terminal and navigate to the directory you downloaded Miniconda3-latest-MacOSX-x86_64.sh to and run:\n\n        $ bash Miniconda3-latest-MacOSX-x86_64.sh\n\n* For Anaconda, double click the Anaconda3-5.0.1-MacOSX-x86_64.pkg file you downloaded\n\n#### Linux:\n* For Miniconda, open a terminal and navigate to the directory you downloaded Miniconda3-latest-Linux-x86_64.sh to and run:\n\n        $ bash Miniconda3-latest-Linux-x86_64.sh\n\n* For Anaconda, open a terminal and navigate to the directory you downloaded Anaconda3-5.0.1-Linux-x86_64.sh to and run:\n\n        $ bash Anaconda3-5.0.1-Linux-x86_64.sh\n\n#### Both:\n* Create and activate a Python 3.5 conda environment called cv.\n\n        $ conda create -n cv python=3.5\n\n        $ source activate cv\n* Install Numpy (http://www.numpy.org/)\n\n        $ conda install numpy\n* Install Matplotlib (https://matplotlib.org/)\n\n        $ conda install matplotlib\n* Install Keras (https://keras.io/) \n\n        $ conda install keras\n    - This should also install tensorflow\n* Install h5py (http://www.h5py.org/)\n\n        $ conda install h5py\n* Install Jupyter Notebook (http://jupyter.org/)\n\n        $ conda install jupyter notebook\n* Install IPython (https://ipython.org/)\n\n        $ conda install ipython\n* Install OpenCV3 (https://opencv.org/)\n        \n        $ conda install -c conda-forge opencv \n    \n    if the `import cv2` does not work with this install, try instead:\n    \n        $ conda install -c https://conda.anaconda.org/menpo opencv3\n"
 },
 {
  "repo": "feichtenhofer/gpu_flow",
  "language": "C++",
  "readme_contents": "GPU based optical flow extraction in OpenCV\n====================\n### Features:\n* OpenCV wrapper for Real-Time optical flow extraction on GPU\n* Automatic directory handling using Qt\n* Allows saving of optical flow to disk, \n** either with clipping large displacements \n** or by adaptively scaling the displacements to the radiometric resolution of the output image\n\n### Dependencies\n* [OpenCV 2.4] (http://opencv.org/downloads.html) (if you want OpenCV 3.1, tell me, I'll do the port)\n* [Qt 5.4] (https://www.qt.io/qt5-4/)\n* [cmake] (https://cmake.org/)\n\n### Installation\n1. `mkdir -p build`\n2. `cd build`\n3. `cmake ..`\n4. `make`\n\n### Configuration:\nYou should adjust the input and output directories by passing in `vid_path` and `out_path`. Note that vid_path must exist, Qt will create out_path. Use -h option t for more.\nIn the CMakeLists.txt there is an option called WARP. This selects if you want warped optical flow or not. The warped optical flow file also outputs optical flows as a single BGR image (red is the flow magnitude). In the compute_flow_si_warp file itself there is a warp variable that you can set to false to just compute normal flow. If you want grayscale for images (x and y) use compute_flow.\n\n### Usage:\n```\n./compute_flow [OPTION]...\n```\n```\n./compute_flow_si_warp [OPTION] ..\n```\n\nAvailable options:\n* `start_video`: start with video number in `vid_path` directory structure [1]\n* `gpuID`: use this GPU ID [0]\n* `type`: use this flow method Brox = 0, TVL1 = 1 [1] \n* `skip`: the number of frames that are skipped between flow calcuation [1]\n* `vid_path`: folder with input videos\n* `out_path`: folder where a folder per video containing optical flow frames will be created\n\nAdditional features in `compute_flow.cpp`:\n* `float MIN_SZ = 256`: defines the smallest side of the frame for optical flow computation\n* `float OUT_SZ = 256`: defines the smallest side of the frame for saving as .jpeg \n* `bool clipFlow = true;`: defines whether to clip the optical flow larger than [-20 20] pixels and maps the interval [-20 20] to  [0 255] in grayscale image space. If no clipping is performed the mapping to the image space is achieved by finding the frame-wise minimum and maximum displacement and mapping to [0 255] via an adaptive scaling, where the scale factors are saved as a binary file to `out_path`.\n\n### Example:\n```\n./compute_flow --gpuID=0 --type=1 --vid_path=test --vid_path=test_out --stride=2\n```\n\n\n"
 },
 {
  "repo": "huihut/OpenCV-MinGW-Build",
  "language": null,
  "readme_contents": "# OpenCV-MinGW-Build\n\nMinGW 32bit and 64bit version of OpenCV compiled on Windows.\n\n## Releases\n\n### [OpenCV 4.1.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.1-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-08-07T16:07:07Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (29 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.1/opencv-4.1.1/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.1/opencv-4.1.1-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.1.0-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.0-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.0-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.0 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-04-09T16:48:54Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (29 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.0/opencv-4.1.0/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.0/opencv-4.1.0-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.1.0](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.0) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.0 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.0 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-07-24T13:59:00Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (16 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.0/opencv-4.1.0/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.0/opencv-4.1.0-x86-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.1-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 4.0.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-03-27T15:25:27Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (7 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (13 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.35.100)\n      avformat:                  YES (ver 58.20.100)\n      avutil:                    YES (ver 56.22.100)\n      swscale:                   YES (ver 5.3.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.1/opencv-4.0.1/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.1/opencv-4.0.1-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.0-rc-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.0-rc-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-rc-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-rc-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.0-rc-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 4.0.0-rc =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-11-17T09:33:40Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (6 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (12 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.35.100)\n      avformat:                  YES (ver 58.20.100)\n      avutil:                    YES (ver 56.22.100)\n      swscale:                   YES (ver 5.3.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.0-rc/opencv-4.0.0-rc/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.0-rc/opencv-4.0.0-rc-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.0-alpha-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.0-alpha-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-alpha-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-alpha-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.0-alpha-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x64-4.8.1-release-posix-seh-rev5\n* Windows-10-64bit\n* CMake-3.12.0\n\n```\nGeneral configuration for OpenCV 4.0.0-alpha =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-09-25T08:37:52Z\n    Host:                        Windows 10.0.17134 AMD64\n    CMake:                       3.12.0\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (4 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (10 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/g++.exe  (ver 4.8.1)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    js world\n    Disabled by dependency:      -\n    Unavailable:                 java python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.34)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    Video for Windows:           YES\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.18.100)\n      avformat:                  YES (ver 58.12.100)\n      avutil:                    YES (ver 56.14.100)\n      swscale:                   YES (ver 5.1.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.0-alpha/opencv-4.0.0-alpha/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.0-alpha/opencv-4.0.0-mingw64-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 3.4.9](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.9) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.9.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.9.tar.gz)\n```\ngit clone -b OpenCV-3.4.9 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary>\n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.9 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-12-26T15:33:59Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (16 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.9/opencv-windows/sources/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.9/build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.8 x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.8-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.8-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.8-x64.tar.gz)\n```\ngit clone -b OpenCV-3.4.8-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary>\n\n* MinGW-x64-7.0.0\n* Windows-10-64bit\n* CMake-3.16.0-rc3\n\n```\nGeneral configuration for OpenCV 3.4.8 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-11-19T16:58:28Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.16.0-rc3\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            C:/PROGRA~1/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (28 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                C:/Program Files/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  C:/Program Files/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                C:/Library/opencv-3.4.8/source/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Program Files/python27/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    C:/Library/opencv-3.4.8/build/install\n-----------------------------------------------------------------\n```\n</details>\n\n\n### [OpenCV 3.4.7](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.7) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.7.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.7.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.7 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.7 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-09-01T15:19:48Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (15 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.7/opencv-3.4.7/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.7/opencv-3.4.7-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.6](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.6) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.6.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.6.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.6 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.6 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-07-22T17:25:51Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (15 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.6/opencv-3.4.6/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.6/opencv-3.4.6-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.5](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.5) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.5.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.5.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.5 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 3.4.5 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-02-11T05:45:28Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (7 files):          + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/g++.exe  (ver 5.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.11.2)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.11.2)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.5/opencv-3.4.5/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.5/opencv-3.4.5-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.1-x64)  | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x64-4.8.1-release-posix-seh-rev5\n* Windows-10-64bit\n* CMake-3.12.0\n\n```\nGeneral configuration for OpenCV 3.4.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-07-31T02:14:11Z\n    Host:                        Windows 10.0.17134 AMD64\n    CMake:                       3.12.0\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (3 files):          + SSSE3 SSE4_1\n      SSE4_2 (1 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (2 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (9 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/g++.exe  (ver 4.8.1)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    js world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build (ver 90)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.34)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n\n  Video I/O:\n    Video for Windows:           YES\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  NVIDIA CUDA:                   NO\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv_341/opencv/sources/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Matlab:                        NO\n\n  Install to:                    E:/opencv_341/opencv_mingw64_build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 3.4.1](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.1) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.1 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.9.2\n\n</details>\n\n### [OpenCV 3.3.1](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.3.1) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.3.1.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.3.1.tar.gz)\n\n```\ngit clone -b OpenCV-3.3.1 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.9.2\n\n</details>\n\n## Tutorials\n\n### How to compile OpenCV\n\n* [blog.huihut . OpenCV\u4f7f\u7528CMake\u548cMinGW-w64\u7684\u7f16\u8bd1\u5b89\u88c5](https://blog.huihut.com/2018/07/31/CompiledOpenCVWithMinGW64/)\n* [blog.huihut . OpenCV\u4f7f\u7528CMake\u548cMinGW\u7684\u7f16\u8bd1\u5b89\u88c5\u53ca\u5176\u5728Qt\u914d\u7f6e\u8fd0\u884c](https://blog.huihut.com/2017/12/03/CompiledOpenCVRunInQt/)\n* [wiki.qt . How to setup Qt and openCV on Windows](https://wiki.qt.io/How_to_setup_Qt_and_openCV_on_Windows)\n* [Tutorial: Installation from source for Windows with Mingw-w64](http://visp-doc.inria.fr/doxygen/visp-daily/tutorial-install-win10-mingw64.html)\n\n### Using OpenCV in Visual Studio Code\n\n* [Running a C++ program with OpenCV 3.4.1 using MinGW-w64 g++ in Visual Studio Code on Windows 10 x64](https://stackoverflow.com/questions/51622111/opencv-c-mingw-vscode-fatal-error-to-compile/51801863#51801863)\n\n### Using OpenCV in Visual Studio\n\n* [How to build applications with OpenCV inside the Microsoft Visual Studio](https://docs.opencv.org/2.4/doc/tutorials/introduction/windows_visual_studio_Opencv/windows_visual_studio_Opencv.html)\n\n### Using OpenCV in Qt\n* [wiki.qt . How to setup Qt and openCV on Windows](https://wiki.qt.io/How_to_setup_Qt_and_openCV_on_Windows)\n"
 },
 {
  "repo": "jerry1900/faceRecognition",
  "language": "Python",
  "readme_contents": "# faceRecognition\n\u5229\u7528OpenCV\u3001CNN\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\n\u4e07\u58d1\uff0c497899960@qq.com\n"
 },
 {
  "repo": "lixiaoshaxing/MultiMediaLearn",
  "language": "C",
  "readme_contents": "# MultiMediaLearn\n\u591a\u5a92\u4f53\u5b66\u4e60\uff1a\u56fe\u7247\u5904\u7406\uff0c\u97f3\u89c6\u9891\u5904\u7406\uff0c\u76f8\u673a\u4f7f\u7528\uff0cOpenGL\uff0cOpenSL\uff0cOpenCV\uff0cFFMpeg\u7b49\u5b66\u4e60\uff0c\u6240\u6709\u4ee3\u7801\u90fd\u4e0d\u5c01\u88c5\uff0c\u4e00\u822c\u7531\u4e00\u4e2a\u7c7b\u5b8c\u6210\u529f\u80fd\uff0c\u65b9\u4fbf\u5b66\u4e60\u3002\n+ \u56fe\u7247\u5904\u7406\n    - 1.SurfaceView\uff0cGLSurfaceView,TextureView\uff0cImageView\uff0c\u81ea\u5b9a\u4e49View\u6e32\u67d3Bitmap\n    - 2.Bitmap\u538b\u7f29\uff0c\u526a\u88c1\uff0cRenderscript\u6ee4\u955c\u5904\u7406(\u5f85)\n+ \u76f8\u673a\u4f7f\u7528\n    - 1.SurfaceView\uff0cGLSurfaceView\uff0cTextureView\u9884\u89c8Camera\uff0c\u62cd\u7167\n    - 2.SurfaceView\uff0cGLsurfaceView\uff0cTextureView\u540c\u65f6\u9884\u89c8Camera\uff0c\u5171\u4eab\u6570\u636e\n    - 3.\u591aGLSurfaceView\u5171\u4eabCamera\uff0c\u6a21\u4eff\u5c0f\u7c73\u7cfb\u7edf\u76f8\u673a\u4e5d\u4e2a\u6ee4\u955c\u540c\u65f6\u9884\u89c8\n+ \u97f3\u89c6\u9891\u5904\u7406\n    - 1.MediaPlayer\u7ed3\u5408SurfaceView\uff0cGLSurfaceView\uff0cTextureView\u64ad\u653e\u89c6\u9891\n    - 2.MediaPlayer\uff0cMediaRecord\u5f55\u5236\u89c6\u9891\n    - 3.AudioRecord\uff0cAudioTrack\u5f55\u5236\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\n    - 4.Fmod\u5904\u7406\u97f3\u9891\uff0c\u53d8\u58f0\n    - 5.MediaExtractor\uff0cMediaCodec\u63d0\u53d6\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\uff0c\u7ed3\u5408MediaMux\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\n    - 6.AudioRecoder\uff0cCamera\uff0cMediaCodec\u5355\u72ec\u5f55\u5236\u97f3\u9891\uff0c\u89c6\u9891\n    - 7.AudioRecoder\uff0cCamera\uff0cMediaCodec\uff0cAudioMux\u5f55\u5236\u89c6\u9891\n    - 8.\u5f55\u5236\u89c6\u9891\uff0c\u4f7f\u7528OpenGL\u5904\u7406\uff0c\u6dfb\u52a0\u6ee4\u955c\uff0c\u6c34\u5370\n    - 9.\u89c6\u9891\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\uff0c\u8f6cgif\uff0c\u65ad\u70b9\u5f55\u5236\uff08\u5f85\uff09\n    - 10.\u5f55\u5236\u89c6\u9891\uff0c\u4f7f\u7528FFmpeg\u589e\u52a0\u6ee4\u955c\uff0c\u6c34\u5370\uff0c\u526a\u88c1\uff08\u5f85\uff09\n+ OpenGL\u5b66\u4e60\n    - 1.\u51b0\u7403\u6e38\u620f\uff0c\u57fa\u672c\u56fe\u5f62\u7ed8\u5236\uff0c\u57fa\u672c\u77e9\u9635\u53d8\u6362\uff0c\u89e6\u63a7\n    - 2.\u7c92\u5b50\u55b7\u6cc9\uff0c\u70df\u706b\u7cfb\u7edf\uff0c\u5929\u7a7a\u76d2\uff0c\u52a8\u6001\u58c1\u7eb8\uff0c\u9640\u87ba\u4eea\uff0c\u9ad8\u5ea6\u56fe\u7b49\n    - 3.\u56fe\u7247\u5904\u7406\uff0cOpenGL\u4f7f\u7528glsl\u5904\u7406\u56fe\u7247\uff0c\u6539\u8272\u8c03\uff0c\u653e\u5927\uff0c\u865a\u5316\u7b49\n    - 4.OpenGL\u9ad8\u7ea7\u7279\u6027\uff1aVBO\uff0cPBO\uff0cFBO\u4f7f\u7528\n    - 5.Camera\u9884\u89c8\u754c\u9762\u89e3\u6790ETC\uff0c\u52a8\u753b\uff0c\u6ee4\u955c\uff0c\u7f8e\u989c\uff0c\u6c34\u5370\n    - 6.\u52a0\u8f7dSTL\u6a21\u578b\uff0c\u6e32\u67d33D\u6a21\u578b\n    - 7.\u52a0\u8f7dOBJ\u6a21\u578b\n    - 8.\u52a0\u8f7dOBJ-MTL\u6a21\u578b\uff0c\u5305\u542b\u6a21\u578b\uff0c\u7eb9\u7406\n    - 9.\u7f16\u8bd1assimp\uff0c\u52a0\u8f7d\u6a21\u578b\uff08\u5f85\uff09\n    - 10.\u52a0\u8f7d\u9aa8\u67b6\uff08\u5f85\uff09\n+ FFMpeg\u5b66\u4e60\n    - 1.\u672c\u5730\u89c6\u9891\u63a8\u6d41\uff0c\u4f7f\u7528libRtmp\uff0cffmpeg\u89e3\u7801\uff0clibYuv\u8f6c\u5411\n    - 2.\u76f4\u64ad\uff0c\u91c7\u96c6\u97f3\u9891\uff0c\u89c6\u9891\uff0c\u4f7f\u7528x264\uff0cfaac\u7f16\u7801\uff0clibrtmp\u63a8\u6d41\uff0clibYuv\u8f6c\u5411\n    - 3.\u89e3\u6790\u97f3\u9891\u4e3apcm\uff0cffmpeg\u89e3\u7801\uff0c\u672c\u5730\u65b9\u6cd5\u8c03\u7528java\u5c42AudioTrack\u64ad\u653e\u97f3\u9891\n    - 4.FFMpeg\u89e3\u7801\u672c\u5730\u89c6\u9891\uff0c\u4f7f\u7528\u961f\u5217\uff0c\u4fdd\u6301\u97f3\u89c6\u9891\u540c\u6b65\u64ad\u653e\n    - 5.OpenSL\u5f55\u5236\u97f3\u9891\uff0c\u89e3\u7801\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\n+ OpenCV\u5165\u95e8\n    - 1. \u4f7f\u7528OpenCV\u8fdb\u884c\u8fb9\u6846\u8bc6\u522b\uff0c\u5e76\u526a\u88c1Bitmap"
 },
 {
  "repo": "WL-Amigo/waifu2x-converter-cpp",
  "language": "C++",
  "readme_contents": "# waifu2x (converter only version)\n\nThis is a reimplementation of waifu2x ([original](https://github.com/nagadomi/waifu2x)) converter function, in C++, using OpenCV.\nThis is also a reimplementation of [waifu2x python version](https://marcan.st/transf/waifu2x.py) by [Hector Martin](https://marcan.st/blog/).\nYou can use this as command-line tool of image noise reduction or/and scaling.\n\n\n## Prebuilt binary-form release\n\nPlease see [releases](https://github.com/WL-Amigo/waifu2x-converter-cpp/releases) of this repository.\nThere is only for-windows binary, now. Sorry.\n\n### works using waifu2x-converter\n\n * [waifu2x_win_koroshell](http://inatsuka.com/extra/koroshell/)\n   - waifu2x-converter GUI frontend that is easy to use, and so cute. You need only drag&drop to convert your image. (and also you can set converting mode, noise reduction level, scale ratio, etc..., on GUI)\n   - Both waifu2x-converter x86 and x64 are included this package, and GUI see your windows architecture(x86|x64) and selects automatically which to use. \n   - For windows only.\n\n\n## Dependencies\n\n### Platform\n\n * Ubuntu\n * Mac OS X?\n * Windows\n \n(This program probably can be built under MacOSX, because OpenCV and other libraries support OS X)\n\n### Libraries\n\n * [OpenCV](http://opencv.org/)(C++, version 3.0.0 rc1)\n\nThis programs also depends on libraries shown below, but these are already included in this repository.\n*CUDA Support in OpenCV is optional, since not required. (in version 1.0.0, CUDA Support is not used.)*\n\n * [picojson](https://github.com/kazuho/picojson)\n * [TCLAP(Templatized C++ Command Line Parser Library)](http://tclap.sourceforge.net/)\n\n## How to build\n\n### for Ubuntu\n\nSorry, under construction...\n\nThese are hints for building :\n\n * I recommend to install OpenCV from sources. (build instruction is found [here](http://opencv.org/quickstart.html))\n * include path : `include/` `(/path/to/opencv/installed/directory)/include`\n * library path : `(/path/to/opencv/installed/directory)/lib` \n     - if you have built and installed OpenCV from source, and have changed install directory(by using `CMAKE_INSTALL_PREFIX`), you may need to set environment variable `LD_LIBRARY_PATH` for your OpenCV installed directory.\n * libraries to link : `opencv_core` `opencv_imgproc` `opencv_imgcodecs` `opencv_features2d`\n * standard of C++ : `c++11`\n\n\n\n## Usage\n\nUsage of this program can be seen by executing this with `--help` option.\n\n\n\n(My native language is not English, then I'm sorry for my broken English.)\n"
 },
 {
  "repo": "atulapra/Emotion-detection",
  "language": "Python",
  "readme_contents": "# Emotion detection using deep learning\n\n## Introduction\n\nThis project aims to classify the emotion on a person's face into one of **seven categories**, using deep convolutional neural networks. The model is trained on the **FER-2013** dataset which was published on International Conference on Machine Learning (ICML). This dataset consists of 35887 grayscale, 48x48 sized face images with **seven emotions** - angry, disgusted, fearful, happy, neutral, sad and surprised.\n\n## Dependencies\n\n* Python 3, [OpenCV](https://opencv.org/), [Tensorflow](https://www.tensorflow.org/)\n* To install the required packages, run `pip install -r requirements.txt`.\n\n## Basic Usage\n\nThe repository is currently compatible with `tensorflow-2.0` and makes use of the Keras API using the `tensorflow.keras` library.\n\n* First, clone the repository and enter the folder\n\n```bash\ngit clone https://github.com/atulapra/Emotion-detection.git\ncd Emotion-detection\n```\n\n* Download the FER-2013 dataset from [here](https://drive.google.com/file/d/1X60B-uR3NtqPd4oosdotpbDgy8KOfUdr/view?usp=sharing) and unzip it inside the `src` folder. This will create the folder `data`.\n\n* If you want to train this model, use:  \n\n```bash\ncd src\npython emotions.py --mode train\n```\n\n* If you want to view the predictions without training again, you can download the pre-trained model from [here](https://drive.google.com/file/d/1FUn0XNOzf-nQV7QjbBPA6-8GLoHNNgv-/view?usp=sharing) and then run:  \n\n```bash\ncd src\npython emotions.py --mode display\n```\n\n* The folder structure is of the form:  \n  src:\n  * data (folder)\n  * `emotions.py` (file)\n  * `haarcascade_frontalface_default.xml` (file)\n  * `model.h5` (file)\n\n* This implementation by default detects emotions on all faces in the webcam feed. With a simple 4-layer CNN, the test accuracy reached 63.2% in 50 epochs.\n\n![Accuracy plot](imgs/accuracy.png)\n\n## Data Preparation (optional)\n\n* The [original FER2013 dataset in Kaggle](https://www.kaggle.com/deadskull7/fer2013) is available as a single csv file. I had converted into a dataset of images in the PNG format for training/testing and provided this as the dataset in the previous section.\n\n* In case you are looking to experiment with new datasets, you may have to deal with data in the csv format. I have provided the code I wrote for data preprocessing in the `dataset_prepare.py` file which can be used for reference.\n\n## Algorithm\n\n* First, the **haar cascade** method is used to detect faces in each frame of the webcam feed.\n\n* The region of image containing the face is resized to **48x48** and is passed as input to the CNN.\n\n* The network outputs a list of **softmax scores** for the seven classes of emotions.\n\n* The emotion with maximum score is displayed on the screen.\n\n## Example Output\n\n![Mutiface](imgs/multiface.png)\n\n## References\n\n* \"Challenges in Representation Learning: A report on three machine learning contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n   Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,  \n   X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n   M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n   Y. Bengio. arXiv 2013.\n"
 },
 {
  "repo": "wzh191920/License-Plate-Recognition",
  "language": "Python",
  "readme_contents": "# License-Plate-Recognition\nLicense Plate Recognition For Car With Python And OpenCV\n\n#### \u7528python3+opencv3\u505a\u7684\u4e2d\u56fd\u8f66\u724c\u8bc6\u522b\uff0c\u5305\u62ec\u7b97\u6cd5\u548c\u5ba2\u6237\u7aef\u754c\u9762\uff0c\u53ea\u67092\u4e2a\u6587\u4ef6\uff0csurface.py\u662f\u754c\u9762\u4ee3\u7801\uff0cpredict.py\u662f\u7b97\u6cd5\u4ee3\u7801\uff0c\u754c\u9762\u4e0d\u662f\u91cd\u70b9\u6240\u4ee5\u7528tkinter\u5199\u5f97\u5f88\u7b80\u5355\u3002\n\n### \u4f7f\u7528\u65b9\u6cd5\uff1a\n\u7248\u672c\uff1apython3.4.4\uff0copencv3.4\u548cnumpy1.14\u548cPIL5<br>\n\u4e0b\u8f7d\u6e90\u7801\uff0c\u5e76\u5b89\u88c5python\u3001numpy\u3001opencv\u7684python\u7248\u3001PIL\uff0c\u8fd0\u884csurface.py\u5373\u53ef\n\n### \u7b97\u6cd5\u5b9e\u73b0\uff1a\n\u7b97\u6cd5\u601d\u60f3\u6765\u81ea\u4e8e\u7f51\u4e0a\u8d44\u6e90\uff0c\u5148\u4f7f\u7528\u56fe\u50cf\u8fb9\u7f18\u548c\u8f66\u724c\u989c\u8272\u5b9a\u4f4d\u8f66\u724c\uff0c\u518d\u8bc6\u522b\u5b57\u7b26\u3002\u8f66\u724c\u5b9a\u4f4d\u5728predict\u65b9\u6cd5\u4e2d\uff0c\u4e3a\u8bf4\u660e\u6e05\u695a\uff0c\u5b8c\u6210\u4ee3\u7801\u548c\u6d4b\u8bd5\u540e\uff0c\u52a0\u4e86\u5f88\u591a\u6ce8\u91ca\uff0c\u8bf7\u53c2\u770b\u6e90\u7801\u3002\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\u4e5f\u5728predict\u65b9\u6cd5\u4e2d\uff0c\u8bf7\u53c2\u770b\u6e90\u7801\u4e2d\u7684\u6ce8\u91ca\uff0c\u9700\u8981\u8bf4\u660e\u7684\u662f\uff0c\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\u4f7f\u7528\u7684\u7b97\u6cd5\u662fopencv\u7684SVM\uff0c opencv\u7684SVM\u4f7f\u7528\u4ee3\u7801\u6765\u81ea\u4e8eopencv\u9644\u5e26\u7684sample\uff0cStatModel\u7c7b\u548cSVM\u7c7b\u90fd\u662fsample\u4e2d\u7684\u4ee3\u7801\u3002SVM\u8bad\u7ec3\u4f7f\u7528\u7684\u8bad\u7ec3\u6837\u672c\u6765\u81ea\u4e8egithub\u4e0a\u7684EasyPR\u7684c++\u7248\u672c\u3002\u7531\u4e8e\u8bad\u7ec3\u6837\u672c\u6709\u9650\uff0c\u4f60\u6d4b\u8bd5\u65f6\u4f1a\u53d1\u73b0\uff0c\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\uff0c\u53ef\u80fd\u5b58\u5728\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u7b2c\u4e00\u4e2a\u4e2d\u6587\u5b57\u7b26\u51fa\u73b0\u7684\u8bef\u5dee\u6982\u7387\u8f83\u5927\u3002\u6e90\u7801\u4e2d\uff0c\u6211\u4e0a\u4f20\u4e86EasyPR\u4e2d\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5728train\\\u76ee\u5f55\u4e0b\uff0c\u5982\u679c\u8981\u91cd\u65b0\u8bad\u7ec3\u8bf7\u89e3\u538b\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\uff0c\u5e76\u5220\u9664\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u6587\u4ef6svm.dat\u548csvmchinese.dat\u3002\n\n##### \u989d\u5916\u8bf4\u660e\uff1a\u7b97\u6cd5\u4ee3\u7801\u53ea\u6709500\u884c\uff0c\u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c\u8f66\u724c\u5b9a\u4f4d\u7b97\u6cd5\u7684\u53c2\u6570\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u8272\u504f\u3001\u8f66\u8ddd\u5f71\u54cd\uff08test\u76ee\u5f55\u4e0b\u7684\u8f66\u724c\u7684\u50cf\u7d20\u90fd\u6bd4\u8f83\u5c0f\uff0c\u5176\u4ed6\u56fe\u7247\u5f88\u53ef\u80fd\u56e0\u4e3a\u50cf\u7d20\u7b49\u95ee\u9898\u8bc6\u522b\u4e0d\u4e86\uff0c\u8bc6\u522b\u5176\u4ed6\u50cf\u7d20\u7684\u8f66\u724c\u9700\u8981\u4fee\u6539config\u6587\u4ef6\u91cc\u9762\u7684\u53c2\u6570\uff0c\u6b64\u9879\u76ee\u4ec5\u662f\u629b\u7816\u5f15\u7389\uff0c\u63d0\u4f9b\u4e00\u4e2a\u601d\u8def\uff09\u3002\n##### \u6709\u4efb\u4f55\u7591\u95ee\u8bf7\u90ae\u4ef6\u81f3 wzh191920@sina.com\n\n##### \u754c\u9762\u6548\u679c\uff1a\n![](https://github.com/wzh191920/License-Plate-Recognition/blob/master/Screenshots/3.png)\n![](https://github.com/wzh191920/License-Plate-Recognition/blob/master/Screenshots/5.png)\n"
 }
]