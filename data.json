[
 {
  "repo": "Hironsan/BossSensor",
  "language": "Python",
  "readme_contents": "# BossSensor\nHide your screen when your boss is approaching.\n\n## Demo\nThe boss stands up. He is approaching.\n\n![standup](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/standup.jpg)\n\nWhen he is approaching, the program fetches face images and classifies the image.\n \n![approaching](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/approach.jpg)\n\nIf the image is classified as the Boss, it will monitor changes.\n\n![editor](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/editor.jpg)\n\n## Requirements\n\n* WebCamera\n* Python3.5\n* OSX\n* Anaconda\n* Lots of images of your boss and other person image\n\nPut images into [data/boss](https://github.com/Hironsan/BossSensor/tree/master/data/boss) and [data/other](https://github.com/Hironsan/BossSensor/tree/master/data/other).\n\n## Usage\nFirst, Train boss image.\n\n```\n$ python boss_train.py\n```\n\n\nSecond, start BossSensor. \n\n```\n$ python camera_reader.py\n```\n\n## Install\nInstall OpenCV, PyQt4, Anaconda.\n\n```\nconda create -n venv python=3.5\nsource activate venv\nconda install -c https://conda.anaconda.org/menpo opencv3\nconda install -c conda-forge tensorflow\npip install -r requirements.txt\n```\n\nChange Keras backend from Theano to TensorFlow. \n\n## Licence\n\n[MIT](https://github.com/Hironsan/BossSensor/blob/master/LICENSE)\n\n## Author\n\n[Hironsan](https://github.com/Hironsan)\n"
 },
 {
  "repo": "opencv/opencv",
  "language": "C++",
  "readme_contents": "## OpenCV: Open Source Computer Vision Library\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/master/>\n* Q&A forum: <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib> \n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n"
 },
 {
  "repo": "Ewenwan/MVision",
  "language": "C++",
  "readme_contents": "# MVision\u3000Machine Vision \u673a\u5668\u89c6\u89c9\n[AI\u7b97\u6cd5\u5de5\u7a0b\u5e08\u624b\u518c \u6570\u5b66\u57fa\u7840 \u7edf\u8ba1\u5b66\u4e60 \u6df1\u5ea6\u5b66\u4e60 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5de5\u5177\u4f7f\u7528](http://www.huaxiaozhuan.com/)\n\n[AI \u5b89\u5168\u6570\u636e\u79d1\u5b66\u548c\u7b97\u6cd5 ](https://github.com/Ewenwan/AI-Security-Learning)\n\n[\u6fb3\u5927\u5229\u4e9a\u673a\u5668\u4eba\u89c6\u89c9\u7814\u7a76\u4e2d\u5fc3](https://www.roboticvision.org/)\n\n[NIPS Neural Information Processing Systems](https://papers.nips.cc/)\n\n[icml Proceedings of Machine Learning Research PMLR](http://proceedings.mlr.press/index.html)\n\n[ICDM IEEE International Conference on Data Mining](http://www.cs.uvm.edu/~icdm/)\n\n[Computer Vision and Pattern Recognition arxiv.org \u6700\u65b0\u63d0\u4ea4\u7684\u8bba\u6587](https://arxiv.org/list/cs.CV/recent)\n\n[papercept \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://controls.papercept.net/conferences/scripts/start.pl)\n\n[easychair \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://easychair.org/my/roles.cgi?welcome=1)\n\n[DBLP \u8ba1\u7b97\u673a\u6838\u5fc3\u6280\u672f\u6587\u732e](https://dblp.uni-trier.de/)\n\n[\u6280\u672f\u5218 \u589e\u5f3a\u73b0\u5b9e\u3001\u56fe\u50cf\u8bc6\u522b\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u4eba](http://liuxiao.org/category/robots/)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0a\uff09](https://cloud.tencent.com/developer/article/1005894)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0b\uff09](https://cloud.tencent.com/developer/article/1005893)\n\n[\u4f18\u79c0\u7684\u535a\u5ba2\u8bba\u6587\u7b14\u8bb0](https://github.com/Ewenwan/antkillerfarm.github.com)\n\n[CSCI 1430: Introduction to Computer Vision \u8ba1\u7b97\u673a\u89c6\u89c9\u8bfe\u7a0b](http://cs.brown.edu/courses/csci1430/#schedule)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u7b97\u6cd5 \u4e66\u7c4d](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf)\n\n[Computer vision:models, learning and inference \u4e66\u7c4d](http://web4.cs.ucl.ac.uk/staff/s.prince/book/book.pdf)\n\n[TLD\uff1atracking-learning-detection \u8ddf\u8e2a\u7b97\u6cd5](https://github.com/Ewenwan/opencv_TLD)\n\n[\u673a\u5668\u4eba\u9009\u4fee\u8bfe](http://www.diag.uniroma1.it/%7Elanari/EIR/)\n\n[Andrew Davison\u7684\u8bfe\u7a0b\uff1a Robotics Lecture Course (course code 333)](http://www.doc.ic.ac.uk/~ajd/Robotics/index.html)\n\n[Simultaneous Localization and Mapping: Part I ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial1.pdf)\n\n[Simultaneous Localization and Mapping: Part II ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial2.pdf)\n\n[\u745e\u58eb\u82cf\u9ece\u4e16\u7406\u5de5\u7684\u5b66\u751f\u7ec3\u4e60](http://www.csc.kth.se/~kootstra/index.php?item=313&menu=300)\n\n[\u4e66\u7c4d Robotics, Vision & Control \u63a8\u8350\uff01\uff01\uff01\uff01](http://petercorke.com/wordpress/)\n\n[Robotics, Vision & Control.PDF \u767e\u5ea6\u7f51\u76d8](https://pan.baidu.com/s/1c1TcEgo)\n\n[Robotics, Vision and Control csdn](https://download.csdn.net/download/u013834525/10169878)\n\n[\u4f18\u8fbe\u5b66\u57ce \u673a\u5668\u4eba\u4eba\u5de5\u667a\u80fd\u8bfe\u7a0b](https://classroom.udacity.com/courses/cs373)\n\n[\u5b66\u4e60\u65e0\u4eba\u9a7e\u9a76\u8f66\uff0c\u4f60\u6240\u5fc5\u987b\u77e5\u9053\u7684](https://zhuanlan.zhihu.com/p/27686577)\n\n[\u5f3a\u5316\u5b66\u4e60\u4ece\u5165\u95e8\u5230\u653e\u5f03\u7684\u8d44\u6599](https://zhuanlan.zhihu.com/p/34918639?utm_source=wechat_session&utm_medium=social&wechatShare=1&from=singlemessage&isappinstalled=0)\n\n[\u53f0\u5927 \u673a\u5668\u5b66\u4e60\u6df1\u5ea6\u5b66\u4e60\u8bfe\u7a0b](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)\n\n[\u65af\u5766\u798fCS231\u8ba1\u7b97\u673a\u89c6\u89c92017](http://www.mooc.ai/course/268/learn?lessonid=1819#lesson/1819)\n\n[\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u793a\u4f8b cs231n\u7b49](https://github.com/autoliuweijie/DeepLearning)\n\n[2018 MIT 6.S094 \u9ebb\u7701\u7406\u5de5\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u4e2d\u6587](http://www.mooc.ai/course/483/notes)\n\n[MIT  \u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u82f1\u6587](https://selfdrivingcars.mit.edu/)\n\n[DeepTraffic](https://selfdrivingcars.mit.edu/deeptraffic/)\n\n[SegFuse: Dynamic Driving Scene Segmentation](https://selfdrivingcars.mit.edu/segfuse/)\n\n[DeepTesla - End-to-End Steering Model](https://selfdrivingcars.mit.edu/deeptesla/)\n\n[\u4e2d\u6587slam\u9996\u9875](http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5)\n\n[ORB-LSD-SVO\u6bd4\u8f83-\u5218\u6d69\u654f_bilibili](https://www.bilibili.com/video/av5934066/)\n\n[LSD\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7136033.html)\n\n[SVO\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7135847.html)\n\n[DSO \u534a\u95f2\u5c45\u58eb \u89e3\u6790](https://zhuanlan.zhihu.com/p/29177540)\n\n[\u8def\u5f84\u89c4\u5212A*\u7b97\u6cd5\u53caSLAM\u81ea\u4e3b\u5730\u56fe\u521b\u5efa\u5bfc\u822a\u7b97\u6cd5](http://www.voidcn.com/article/p-yfjpnwte-tz.html)\n\n[\u51af\u5175\u7684blog slam](http://www.fengbing.net)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801\uff08EKF)](https://github.com/ethz-asl/rovio)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801(\u975e\u7ebf\u6027\u4f18\u5316\uff09](https://github.com/ethz-asl/okvis_ros)\n\n[\u53cc\u76ee\u7acb\u4f53\u5339\u914d](https://wenku.baidu.com/view/08f86102e518964bcf847c6c.html)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e00\u4e9b\u5e93\u6587\u4ef6](https://blog.csdn.net/garfielder007/article/details/50533052)\n\n[\u4eba\u8138\u68c0\u6d4b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/52983093)\n\n[\u884c\u4e3a\u8bc6\u522b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/79504214)\n\n[Free-SpaceEstimation \u65e0\u969c\u788d\u7269\u7a7a\u95f4\u4f30\u8ba1 \u7a20\u5bc6\u5730\u56fe \u6805\u683c\u5730\u56fe \u52a8\u6001\u89c4\u5212 \u9ad8\u5ea6\u5206\u5272 \u8def\u9762\u4fe1\u606f\u63d0\u53d6](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_free_space.pdf)\n\n[2D Object Detection 2d\u76ee\u6807\u68c0\u6d4b RCNN ](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_2D_detection.pdf)\n\n[3D Object Detection 3D\u76ee\u6807\u68c0\u6d4b \u52a8\u673a](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf)\n\n[Semantic Segmentation \u8bed\u4e49\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/07_segmentation.pdf)\n\n[Instance-level Segmentation \u5b9e\u4f8b\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/08_instance.pdf)\n\n[Tracking \u8ddf\u8e2a ]()\n\n[Kalibr calibration toolbox \u6807\u5b9a\u591a\u76ee\u76f8\u673a\u7cfb\u7edf\u3001\u76f8\u673a IMU \u76f8 \u5bf9 \u4f4d \u59ff \u548c \u5377 \u5e18 \u5feb \u95e8 \u76f8 \u673a  ](https://github.com/Ewenwan/kalibr)\n\n[\u970d\u592b\u68ee\u6797(Hough Forest) \u968f\u673a\u68ee\u6797\u548c\u970d\u592b\u6295\u7968\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u7528\u5728\u7269\u4f53\u68c0\u6d4b\uff0c\u8ddf\u8e2a\u548c\u52a8\u4f5c\u8bc6\u522b](https://github.com/Ewenwan/HoughForest)\n\n[\u767e\u5ea6\u81ea\u52a8\u9a7e\u9a76\u5f00\u6e90\u6846\u67b6 apollo](https://github.com/Ewenwan/apollo)\n\n[\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u8bc6\u522b \u6570\u636e\u96c6](https://cg.cs.tsinghua.edu.cn/traffic-sign/)\n\n[Halcon \u4f7f\u7528\u53c2\u8003](https://blog.csdn.net/maweifei/article/details/52613392)\n\n[\u6709\u4ee3\u7801\u7684\u8bba\u6587](https://github.com/Ewenwan/pwc)\n\n[\u56fe\u50cf\u5904\u7406\u57fa\u672c\u7b97\u6cd5\u4ee3\u7801](http://www.cnblogs.com/Imageshop/p/3430742.html)\n\n# \u611f\u8c22\u652f\u6301\n\n![](https://github.com/Ewenwan/EwenWan/blob/master/zf.jpg)\n\n# \u65e0\u4eba\u9a7e\u9a76\u7684\u5404\u4e2a\u65b9\u9762\u77e5\u8bc6\n[\u53c2\u8003](https://blog.csdn.net/qq_40027052/article/details/78485120)\n\n    1. \u611f\u77e5\uff08Perception\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u573a\u666f\u7406\u89e3\u3001\u4ea4\u901a\u72b6\u51b5\u5206\u6790\u3001\u8def\u9762\u68c0\u6d4b\u3001\u7a7a\u95f4\u68c0\u6d4b\u3001\n        \u969c\u788d\u7269\u68c0\u6d4b\u3001\u884c\u4eba\u68c0\u6d4b\u3001\u8def\u6cbf\u68c0\u6d4b\u3001\u8f66\u9053\u68c0\u6d4b\u3002\u8fd8\u6709\u4e00\u4e2a\u6bd4\u8f83\u65b0\u9896\u6709\u8da3\u7684\u662f\u901a\u8fc7\u80ce\u538b\u53bb\u68c0\u6d4b\u9053\u8def\u8d28\u91cf\u3002\n        \u5728\u65e0\u4eba\u9a7e\u9a76\u884c\u4e1a\uff0c\u6709\u4e00\u5957\u901a\u7528\u7684\u6570\u636e\u96c6\u2014\u2014KITTI\u6570\u636e\u96c6\uff0c\u91cc\u9762\u6709\u4e0d\u540c\u7684\u6570\u636e\uff0c\u5305\u62ec\u53cc\u76ee\u89c6\u89c9\u7684\u6570\u636e\u3001\u5b9a\u4f4d\u5bfc\u822a\u7684\u6570\u636e\u7b49\u3002\n        \u7269\u4f53\u68c0\u6d4b\uff08Object Detection\uff09\uff1a\n            \u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u662f\u9488\u5bf9\u56fa\u5b9a\u7269\u4f53\u7684\u68c0\u6d4b\u3002\u4e00\u822c\u7684\u65b9\u6cd5\u662fHOG\uff08 \u65b9\u5411\u68af\u5ea6\u76f4\u65b9\u56fe\uff09\uff0c\u7136\u540e\u518d\u52a0\u4e00\u4e2aSVM\u7684\u5206\u7c7b\u5668\u3002\n            \u800c\u5bf9\u4e8e\u52a8\u6001\u7269\u4f53\u7684\u68c0\u6d4b\uff0c\u4e3b\u8981\u4f7f\u7528\u7684\u662fDPM\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5148\u628a\u624b\u548c\u811a\u8bc6\u522b\u51fa\u6765\uff0c\u518d\u8fdb\u884c\u7ec4\u5408\u3002\n            \u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 RCNN YOLO\n       \u573a\u666f\u5206\u5272\uff08Segmentation\uff09 \u00a0\uff1a\n            \u4eba\u884c\u9053\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u9053\u8def\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u5728\u573a\u666f\u4e2d\u5bf9\u4e0d\u540c\u7684\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u662f\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u95ee\u9898\u3002\n            \u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u91c7\u7528CRF\uff08 \u6761\u4ef6\u968f\u673a\u573a\uff09\uff0c\u57fa\u672c\u539f\u7406\u5728\u4e8e\u56fe\u50cf\u90fd\u662f\u7531\u50cf\u7d20\u70b9\u7ec4\u6210\u7684\uff0c\n            \u82e5\u4e24\u4e2a\u50cf\u7d20\u70b9\u90fd\u6bd4\u8f83\u50cf\u8f66\uff0c\u90a3\u5c31\u628a\u4e8c\u8005\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u5bf9\u8f66\u8f86\u7684\u8bc6\u522b\u3002\n\n            \u8fd0\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5219\u4f7f\u7528\u7684\u662f\u53e6\u4e00\u79cd\u6a21\u578b\uff0c\u88ab\u79f0\u4e3aPSPnet\uff08\u8bed\u4e49\u5206\u5272\uff09\u3002\n            \u8fd9\u662f\u91d1\u5b57\u5854\u578b\u7684\u573a\u666f\u5206\u89e3\u6a21\u578b\uff0c\u5c06\u4e00\u4e2a\u573a\u666f\u4e0d\u65ad\u5730\u538b\u7f29\uff0c\u628a\u7c7b\u4f3c\u7684\u7269\u4f53\u805a\u7c7b\uff0c\u7136\u540e\u518d\u505a\u5224\u65ad\u3002\n       \u53cc\u76ee \u5149\u6d41\uff08Optical Flow\uff09\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff1a\n            \u5149\u6d41\u662f\u9488\u5bf92D\u56fe\u50cf\u6765\u8bf4\u7684\uff0c\u5982\u679c\u8bf4\u4e00\u4e2a\u56fe\u7247\u6d41\u5230\u53e6\u5916\u4e00\u4e2a\u56fe\u7247\uff0c\u90fd\u662f2D\u7684\u7269\u4f53\u79fb\u52a8\uff0c\u90a3\u5c31\u7528\u5149\u6d41\u6765\u505a\u3002\n            \u5982\u679c\u662f3D\u7684\u7269\u4f53\u6d41\u52a8\uff0c\u90a3\u6211\u4eec\u5c31\u7528\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff0c\u573a\u666f\u6d41\u5728\u4f20\u7edf\u7684\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528\u7684\u662fSGBM\uff0c\n            \u5229\u7528\u7684\u662f\u53cc\u76ee\u6210\u50cf\u7684\u6280\u672f\uff0c\u628a\u5de6\u56fe\u548c\u53f3\u56fe\u5408\u8d77\u6765\u63d0\u53d6\u51fa\u7a7a\u95f4\u7684\u70b9\uff0c\u7528\u5149\u6d41\u5728\u4e0a\u9762\u505a\uff0c\u5c31\u80fd\u628a\u573a\u666f\u7684\u6d41\u52a8\u5206\u6790\u51fa\u6765\u3002\n\n            \u5149\u6d41\u4e5f\u53ef\u4ee5\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u6765\u505a\uff0c\u628a\u5de6\u53f3\u4e24\u56fe\u7528\u540c\u6837\u7684\u6a21\u578b\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u7ecf\u8fc7\u8ba1\u7b97\u5c31\u80fd\u5f97\u51fa\u4e00\u4e2a\u6df1\u5ea6\u7684\u4fe1\u606f\u3002\n            \u4f46\u662f\u8fd9\u4e2a\u65b9\u5f0f\u7684\u8ba1\u7b97\u91cf\u975e\u5e38\u5927\u3002\n\n       \u7269\u4f53\u8ffd\u8e2a\uff08Object Tracking\uff09\uff1a \u00a0 \u00a0\n            \u8fd9\u4e5f\u662f\u65e0\u4eba\u9a7e\u9a76\u4e2d\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u6280\u672f\u3002\u5982\u4f55\u9884\u6d4b\u884c\u4eba\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3001\u600e\u4e48\u53bb\u8ddf\u8e2a\u8fd9\u4e2a\u884c\u4eba\uff0c\u4e5f\u6709\u4e00\u7cfb\u5217\u95ee\u9898\u3002\n            \u91cc\u9762\u7528\u5230\u7684\u662f\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e2a\u6280\u672f\u53eb\u505aMDP\uff0c\u8ddf\u8e2a\u4e00\u4e2a\u4eba\uff0c\u968f\u65f6\u8ddf\u8e2a\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u9884\u6d4b\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3002\n            \u4ee5\u4e0a\u5176\u5b9e\u90fd\u662f\u4e00\u4e9b\u4f20\u7edf\u7684\u611f\u77e5\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u5e74\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u5e94\u7528\u4e5f\u975e\u5e38\u5e7f\u6cdb\u3002\n            \n    2. \u8fd0\u52a8\u89c4\u5212\uff08Motion Planning\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u8fd0\u52a8\u89c4\u5212\u3001\u8f68\u8ff9\u89c4\u5212\u3001\u901f\u5ea6\u89c4\u5212\u3001\u8fd0\u52a8\u6a21\u578b\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u901a\u8fc7\u8d5b\u8f66\u6e38\u620f\u53bb\u5b66\u4e60\u57fa\u4e8e\u7f51\u683c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u91cd\u91cf\u7ea7\u8d27\u8f66\u7684\u907f\u969c\u89c4\u5212\uff0c\u666e\u4e16\u7684\u9002\u7528\u4e8e\u65e0\u4eba\u9a7e\u9a76\u7684\u53cc\u8f6e\u6a21\u578b\u7b49\u7b49\u3002\n\n    3. \u9632\u78b0\u649e\uff08CollisionAvoidance\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u8f66\u5185\u7684\u611f\u77e5\u7cfb\u7edf\u4ee5\u53caV2X \u7cfb\u7edf\u53bb\u8f85\u52a9\u9632\u78b0\u649e\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5b9e\u65f6\u5730\u53bb\u8bc4\u4f30\u5f53\u524d\u9a7e\u9a76\u884c\u4e3a\u7684\u5371\u9669\u6027\uff0c\u5982\u4f55\u901a\u8fc7\u5f53\u524d\u9053\u8def\u7684\u62d3\u6251\u53bb\u589e\u5f3a\u81ea\u884c\u8f66\u9a91\u58eb\u7684\u5b89\u5168\u6027\u7b49\u7b49\u3002\n\n    4. \u5730\u56fe\u4e0e\u5b9a\u4f4d\uff08Mapping andLocalization\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7684\u4f20\u611f\u5668\uff0c\u5305\u62ec\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\u3001GNSS\uff0c\u4ee5\u53ca V2X \u53bb\u5efa\u56fe\u4e0e\u5b9a\u4f4d\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u53bb\u5b9a\u4f4d\uff0c\u6bd4\u5982\u5728\u957f\u96a7\u9053\u91cc\u9762\uff0c\u65e2\u6ca1\u6709 GNSS \u4fe1\u53f7\uff0c\u4e5f\u6ca1\u6709\u592a\u597d\u7684\u6fc0\u5149\u6216\u8005\u89c6\u89c9\u7279\u5f81\u7684\u65f6\u5019\u5982\u4f55\u5b9a\u4f4d\u3002\n\n    5. \u5408\u4f5c\u7cfb\u7edf\uff08CooperativeSystems\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u534f\u540c\u591a\u4e2a\u65e0\u4eba\u8f66\u53bb\u5b8c\u6210\u4e00\u4e9b\u4efb\u52a1\uff0c\u6bd4\u5982\u5728\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u51fa\u73b0\u65f6\u5982\u4f55\u8c03\u5ea6\uff0c\n        \u8fd8\u6709\u5c31\u662f\u5f53\u6709\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u505c\u8f66\u573a\u8bd5\u5982\u4f55\u6709\u5e8f\u7684\u505c\u8f66\u3002\n\n    6. \u63a7\u5236\u7b56\u7565\uff08Control Strategy\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5728\u4e0d\u540c\u7684\u7ec6\u5206\u573a\u666f\u4e0b\u7684\u63a7\u5236\u7b56\u7565\uff0c\u6bd4\u5982\u5728\u5341\u5b57\u8def\u53e3\u5982\u4f55\u63a7\u5236\uff0c\u8f6c\u7ebf\u5982\u4f55\u63a7\u5236\uff0c\u5728\u611f\u77e5\u6570\u636e\u4e0d\u53ef\u9760\u65f6\u5982\u4f55\u5c3d\u91cf\u5b89\u5168\u7684\u63a7\u5236\u7b49\u7b49\u3002\n\n    7. \u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff08VehicleDetection and Tracking\uff09\uff1a\n        \u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\uff0c\u4ee5\u53ca\u6beb\u7c73\u6ce2\u96f7\u8fbe\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u5de5\u4f5c\u5305\u62ec\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6df1\u5ea6\u89c6\u89c9\u7684\u7ed3\u5408\u8fdb\u884c\u8f66\u8f86\u8ddf\u8e2a\uff0c\n        \u901a\u8fc7\u5355\u76ee\u89c6\u89c9\u6df1\u5ea6\u5b66\u4e60\u53bb\u5c3d\u91cf\u4f30\u8ba1\u8f66\u4f53\u5927\u5c0f\uff0c\u901a\u8fc7\u4f20\u7edf\u89c6\u89c9\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u53bb\u5224\u65ad\u662f\u5426\u8f66\u4f53\u7b49\u7b49\u3002\n\n    8. \u9759\u6001\u7269\u4f53\u68c0\u6d4b\uff08Static ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u4ee5\u53ca\u6fc0\u5149\u96f7\u8fbe\u53bb\u68c0\u6d4b\u4e00\u4e9b\u9759\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u4ea4\u901a\u706f\u3001\u4ea4\u901a\u6307\u793a\u724c\u3001\u8def\u6cbf\u3001\u8def\u9762\u7b49\u7b49\uff0c\u6bcf\u4e2a\u7269\u4f53\u54c1\u7c7b\u7684\u68c0\u6d4b\u90fd\u662f\u4e00\u4e2a\u7ec6\u5206\u65b9\u5411\u3002\n\n    9. \u52a8\u6001\u7269\u4f53\u68c0\u6d4b\uff08Moving ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u4ee5\u53ca\u4f20\u611f\u5668\u878d\u5408\u7684\u65b9\u6cd5\u53bb\u68c0\u6d4b\u4e00\u4e9b\u52a8\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u884c\u4eba\u3001\u8f66\u8f86\u3001\u81ea\u884c\u8f66\u9a91\u58eb\u7b49\u7b49\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u52a8\u6001\u7269\u4f53\u7684\u52a8\u4f5c\u53bb\u9884\u6d4b\u884c\u4e3a\u3002\n\n    10. \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\uff08Road andIntersection Detection\uff09\uff1a\n        \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\u7531\u4e8e\u5176\u7279\u6b8a\u6027\u4ee5\u53ca\u5bf9\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u88ab\u5355\u72ec\u5217\u51fa\u4f5c\u4e3a\u4e00\u4e2a\u7ec6\u5206\u7684\u5c0f\u65b9\u5411\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u4e00\u4e9b\u7ec6\u5206\u573a\u666f\uff0c\u6bd4\u5982\u5efa\u7b51\u5de5\u5730\u7684\u68c0\u6d4b\u3001\u505c\u8f66\u4f4d\u7684\u68c0\u6d4b\u7b49\u7b49\u3002\n\n    11. \u51b3\u7b56\u7cfb\u7edf\uff08Planning andDecision\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u6bcf\u4e2a\u65e0\u4eba\u8f66\u7684\u52a8\u4f5c\u7684\u51b3\u7b56\uff0c\u6bd4\u5982\u52a0\u901f\u3001\u5239\u8f66\u3001\u6362\u7ebf\u3001\u8d85\u8f66\u3001\u8c03\u5934\u7b49\u7b49\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u5728\u9ad8\u901f\u884c\u9a76\u4e2d\u5982\u4f55\u5b89\u5168\u7684\u6362\u7ebf\uff0c\u5728\u901a\u8fc7\u89c6\u89c9\u7406\u89e3\u4e86\u573a\u666f\u540e\u5982\u4f55\u51b3\u7b56\uff0c\u5728\u611f\u77e5\u4fe1\u606f\u7f3a\u5931\u7684\u65f6\u5019\uff08\u6bd4\u5982\u5728\u96a7\u9053\u91cc\u9762\uff09\u5982\u4f55\u51b3\u7b56\u7b49\u7b49\u3002\n\n    12. \u4e3b\u52a8\u4e0e\u88ab\u52a8\u5b89\u5168\uff08Active andPassive Safety\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u4f20\u611f\u5668\u7684\u611f\u77e5\u53bb\u786e\u4fdd\u65e0\u4eba\u9a7e\u9a76\u4ee5\u53ca\u884c\u4eba\u5b89\u5168\uff0c\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u901a\u8fc7\u5bf9 CAN \u603b\u7ebf\u7684\u5f02\u5e38\u68c0\u6d4b\u53bb\u8bc4\u4f30\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5bf9\u505c\u8f66\u573a\u7684\u89c6\u9891\u76d1\u63a7\u53bb\u8bad\u7ec3\u81ea\u52a8\u6cca\u8f66\u6a21\u578b\u7b49\u7b49\u3002\n\n    13. \u65e0\u4eba\u8f66\u4e0e\u4ea4\u901a\u7684\u4ea4\u4e92\uff08AutonomousVehicles: Interaction with Traffic\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u65e0\u4eba\u8f66\u5982\u4f55\u4e0e\u73b0\u6709\u7684\u4ea4\u901a\u751f\u6001\u5171\u5b58\uff0c\u7279\u522b\u662f\u4f20\u7edf\u8f66\u4e0e\u65e0\u4eba\u8f66\u7684\u5171\u5b58\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec V2X \u865a\u62df\u4ea4\u901a\u6807\u5fd7\uff0c\u901a\u8fc7\u89c6\u89c9\u53bb\u8bc4\u4f30\u65c1\u8fb9\u8f66\u9053\u53f8\u673a\u7684\u9a7e\u9a76\u884c\u4e3a\u7b49\u7b49\u3002\n\n    14. \u89c6\u89c9\u5b9a\u4f4d\uff08SLAM and VisualOdometry\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u7528\u89c6\u89c9\u4e0e\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u89c6\u89c9\u7684\u7ebf\u4e0a\u6821\u51c6\uff0c\u4f7f\u7528\u8f66\u9053\u7ebf\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7b49\u7b49\u3002\n\n    15. \u73af\u5883\u5b66\u4e60\u4e0e\u5efa\u56fe\uff08Mapping andLearning the Environment\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u5efa\u7acb\u7cbe\u51c6\u7684\u73af\u5883\u4fe1\u606f\u56fe\u3002\u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u4f7f\u7528\u4f4e\u7a7a\u65e0\u4eba\u673a\u53bb\u521b\u5efa\u7ed9\u65e0\u4eba\u9a7e\u9a76\u4f7f\u7528\u7684\u5730\u56fe\uff0c\n        \u4ee5\u53ca\u901a\u8fc7\u505c\u8f66\u573a\u76d1\u63a7\u6444\u50cf\u5934\u5efa\u7acb\u8f85\u52a9\u81ea\u52a8\u6cca\u8f66\u7684\u5730\u56fe\u7b49\u7b49\u3002\n\n## \u65e0\u4eba\u9a7e\u9a76\u9762\u8bd5\u77e5\u8bc6\u70b9\n[\u53c2\u8003\u535a\u5ba2](https://blog.csdn.net/xiangxianghehe/article/details/82528180)\n```\n1. \u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\n    \u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u533a\u522b\uff0c\u5404\u81ea\u9002\u7528\u4e8e\u4ec0\u4e48\u95ee\u9898\n    CNN\u57fa\u672c\u539f\u7406\uff0cCNN\u7684\u90a3\u4e9b\u90e8\u5206\u662f\u795e\u7ecf\u5143\n    CNN\u53bb\u6389\u6fc0\u6d3b\u51fd\u6570\u4f1a\u600e\u4e48\u6837\n    \u4ecb\u7ecdYOLO/SSD/RCNN/Faster-RCNN/Mask-RCNN\u7b97\u6cd5\n    YOLO v1/v2/v3 \u533a\u522b\u7ec6\u8282\uff0cSSD\u5982\u4f55\u6539\u8fdb\u6709\u601d\u8003\u8fc7\u561b\uff0c\u77e5\u9053DSSD\u548cFSSD\u561b\n    \u662f\u5426\u4e86\u89e3RPN\uff0cRoI pooling,\u548cRoIAlign\n    YOLO/SSD\u91cc\u9762\u6709\u5168\u8fde\u63a5\u5c42\u561b\n    YOLO/SSD\u7b97\u6cd5\u601d\u60f3\u5982\u4f55\u7528\u5230\u4e09\u7ef4\u70b9\u4e91\u76ee\u6807\u68c0\u6d4b\n    \u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5one-stage\u548ctwo-stage\u533a\u522b\u70b9\u5728\u54ea\u91cc\n    two-stage\u7b97\u6cd5\u76f8\u6bd4\u4e8eone-stage\u6709\u4f55\u4f18\u52bf\n    \u5355\u5f20\u56fe\u7247\u7269\u4f53\u8d8a\u591a\u8d8a\u5bc6\u96c6\uff0cYOLO/SSD/Faster-RCNN\u4e2d\u8ba1\u7b97\u91cf\u662f\u5426\u4e5f\u968f\u7740\u589e\u52a0\n    CVPR/ECCV 2018 \u6700\u65b0\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u6709\u4e86\u89e3\u8fc7\u561b\n    \u5982\u4f55\u7406\u89e3\u4e0a\u91c7\u6837\uff0c\u548c\u4e0b\u91c7\u6837\u7684\u533a\u522b\u662f\u4ec0\u4e48\n    \u4e0a\u91c7\u6837(UNSampling)\u4e0e\u4e0a\u6c60\u5316(UnPooling)\u533a\u522b\n    \u5168\u8fde\u63a5\u5c42\u7406\u8bba\u4e0a\u53ef\u4ee5\u66ff\u4ee3\u5377\u79ef\u5c42\u561b\n    \u795e\u7ecf\u7f51\u7edc\u91cc\u9762\u53ef\u4ee5\u7528\u4ec0\u4e48\u65b9\u6cd5\u66ff\u6362\u6389pooling\n    \u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\u7684\u65b9\u5f0f\u6709\u54ea\u4e9b\n    \u4ecb\u7ecd\u4e0b\u4f60\u4e86\u89e3\u7684\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\n    \u7f51\u7edc\u6a21\u578b\u538b\u7f29\u65b9\u9762\u7684\u526a\u679d\uff0c\u91cf\u5316\u548c\u4e8c\u503c\u5316\u7f16\u7801\n    \u57fa\u4e8e\u89c6\u9891\u7684C3D\u4e09\u7ef4\u7f51\u7edc\u6a21\u578b\u6709\u542c\u8bf4\u8fc7\u561b\n    2.5D\u5377\u79ef\u5462\n    \u4ec0\u4e48\u662f\u7a7a\u6d1e\u5377\u79ef\uff0c\u4ec0\u4e48\u662f\u53cd\u5377\u79ef\uff0c\u4f5c\u7528\u662f\u4ec0\u4e48\n    \u5982\u4f55\u4e00\u5f20RGB\u56fe\u7247\u751f\u6210\u4e09\u7ef4\u6a21\u578b\n    PNG/JPG\u5b58\u50a8\u56fe\u50cf\u7684\u539f\u7406\n    global average pooling \u548caverage pooling\u533a\u522b\n    FPN\u7684\u539f\u7406\uff0c\u4e3a\u4ec0\u4e48\u4e0d\u540c\u5c3a\u5ea6feature map\u878d\u5408\u4f1a\u6709\u6548\u679c\u63d0\u5347\n    \u65e0\u76d1\u7763/\u534a\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6709\u4e86\u89e3\u8fc7\u561b\n    GAN\u7684\u539f\u7406\n    \u57fa\u4e8eRGB\u56fe\u7684\u6df1\u5ea6\u4fe1\u606f\u4f30\u8ba1\u6709\u4e86\u89e3\u8fc7\u561b\n    MobileNet V1/V2\u533a\u522b\n    ShuffleNet\u548cSqueezeNet\n    \u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u6709\u54ea\u4e9b\n    \u53cc\u7ebf\u6027\u63d2\u503c\uff0c\u91cf\u5316\u5bf9\u9f50\n    Relu\u4e3a\u4ec0\u4e48\u6bd4sigmod\u597d\n    \u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u5e38\u7528\u8bc4\u6d4b\u65b9\u5f0f\n    IOU\u548cmAP\uff0cAUC\u548cROC\u5206\u522b\u662f\u4ec0\u4e48\n    \u4ecb\u7ecd\u4e0b\u5e38\u89c1\u635f\u5931\u51fd\u6570\uff0csoftmax\u4e00\u822c\u548c\u54ea\u4e2a\u6fc0\u6d3b\u51fd\u6570\u4f7f\u7528\n    \u4ecb\u7ecd\u4e0bPointNet/PointNet++/VoxelNet\u4ee5\u53ca\u4ed6\u4eec\u7684\u4f18\u7f3a\u70b9\n    PointCNN\u4ecb\u7ecd\u4e00\u4e0b\n    \u65cb\u8f6c\u77e9\u9635\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u6027\u8d28\uff0cPointNet\u4e2dT-Net\u65cb\u8f6c\u77e9\u9635\u7684\u635f\u5931\u51fd\u6570\u5982\u4f55\u8bbe\u8ba1\n    \u5982\u4f55\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\n    \u4ecb\u7ecd\u4e0b\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u53c2\u6570\u7c7b\u7b97\u6cd5\u548c\u975e\u53c2\u6570\u7c7b\u7b97\u6cd5\n    \u968f\u673a\u68af\u5ea6\u4e0b\u964d\n    \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5982\u4f55\u89e3\u51b3\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\n    L1\u6b63\u5219\u5316\u548cL2\u6b63\u5219\u5316\u533a\u522b\uff0c\u5177\u4f53\u6709\u4f55\u7528\u9014\n    L1\u6b63\u5219\u5316\u76f8\u6bd4\u4e8e L2\u6b63\u5219\u5316\u4e3a\u4f55\u5177\u6709\u7a00\u758f\u89e3\n    \n2. C++\u5f00\u53d1\u76f8\u5173\n    c++\u5e38\u89c1\u5bb9\u5668\uff0cvector\u5bb9\u5668capacity\u548csize\u533a\u522b\uff0c\u5982\u4f55\u52a8\u6001\u589e\u957f\n    vector\u904d\u5386\u6709\u54ea\u51e0\u79cd\u65b9\u5f0f\uff08\u5c3d\u53ef\u80fd\u591a\uff09\n    cv:Mat \u6709\u51e0\u79cd\u8bbf\u95ee\u65b9\u5f0f\n    map\u5bb9\u5668\u589e\u5220\u6539\u67e5\uff0c\u548cunorder_map\u533a\u522b\uff0cmap\u5e95\u5c42\u5982\u4f55\u5b9e\u73b0\n    c++\u667a\u80fd\u6307\u9488\n    c++14/17\u65b0\u7279\u6027\n    c++\u548cc\u8bed\u8a00\u533a\u522b\n    c++\u5982\u4f55\u5b9e\u73b0\u591a\u6001\uff0c\u6709\u51e0\u79cd\u65b9\u5f0f\uff0c\u52a8\u6001\u591a\u6001\u548c\u9759\u6001\u591a\u6001\u533a\u522b\n    \u6a21\u677f\u4e86\u89e3\u561b\n    c++\u7ee7\u627f\u591a\u6001\n    c++\u6df1\u62f7\u8d1d\u4e0e\u6d45\u62f7\u8d1d\n    \u62f7\u8d1d\u6784\u9020\u51fd\u6570\u548c\u59d4\u6258\u6784\u9020\u51fd\u6570\n    c++\u9762\u5411\u5bf9\u8c61\n    \u53f3\u503c\u5f15\u7528\uff0cmove\u8bed\u4e49\uff0c\u5b8c\u7f8e\u8f6c\u53d1\n    emplace_back\u548cpush_back\u533a\u522b\n    Eigen\u5e93\u4e86\u89e3\u561b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2ac++\u7684\u5355\u4f8b\u6a21\u5f0f\n    \u5185\u8054\u51fd\u6570\u548c\u5b8f\u7684\u533a\u522b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2a\u53ea\u5728\u5806\u6216\u8005\u6808\u4e0a\u521d\u59cb\u5316\u7684\u7c7b\n    \u5982\u4f55\u67e5\u627e\u5bb9\u5668\u5185\u6240\u6709\u7b26\u5408\u6761\u4ef6\u7684\u5143\u7d20\n    \n3. Python\u5f00\u53d1\u76f8\u5173\n    list tuple\u533a\u522b\n    \u751f\u6210\u5668\u548c\u8fed\u4ee3\u5668\n    Python\u7c7b\u7684\u5b9a\u4e49\u548c\u5b9e\u4f8b\u5316\u65b9\u6cd5\n    \n4. \u6570\u636e\u7ed3\u6784\u76f8\u5173\n    \u7ea2\u9ed1\u6811\u7ed3\u6784\uff0c\u67e5\u627e\u65f6\u95f4\u590d\u6742\u5ea6\n    \u5806\u6392\u5e8f\u7684\u65f6\u95f4\u590d\u6742\u5ea6\n    Top K\u6392\u5e8f\n    \u5982\u4f55\u7528O(1)\u590d\u6742\u5ea6\u67e5\u627e\u5230stack\u91cc\u9762\u7684\u6700\u5c0f\u503c\n    \u516b\u7687\u540e\n    C++\u81ea\u5df1\u5b9e\u73b0\u4e00\u4e2a\u961f\u5217\n    \u6570\u7ec4\u548c\u94fe\u8868\u7684\u533a\u522b\n    \u4ec0\u4e48\u662fkd-tree\uff0c\u5982\u4f55\u5b9e\u73b0\n    \u9752\u86d9\u8df3\u53f0\u9636\u7684\u9012\u5f52\u548c\u975e\u9012\u5f52\u5b9e\u73b0\n    \n5. \u64cd\u4f5c\u7cfb\u7edf\u76f8\u5173\n    \u5982\u4f55\u8c03\u8bd5\u6808\u6ea2\u51fa\n    \u8ba1\u7b97\u673a\u5185\u5b58\u5806\u548c\u6808\u7684\u533a\u522b\n    \u7ebf\u7a0b\u540c\u6b65\u7684\u65b9\u5f0f\uff0c\u4e92\u65a5\u9501\u548c\u4fe1\u53f7\u91cf\u7684\u5bf9\u6bd4\n    \u8fdb\u7a0b\u548c\u7ebf\u7a0b\u7684\u533a\u522b\n    \u56fe\u7247\u5b58\u50a8\u539f\u7406\u4ecb\u7ecd\u4e00\u4e0b\n    \n6. \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u5173\n    Tensorflow\u7ed3\u6784\u6846\u67b6\uff0c\u5982\u4f55\u7528Tensorflow\u5b9e\u73b0\u4e00\u4e2a\u53cd\u5411\u6c42\u68af\u5ea6\n    Tensorflow\u5982\u4f55\u5408\u5e76\u4e24\u4e2aTensor\n    caffe\u548cPytorch\u4e86\u89e3\u561b\n    caffe\u548cTensorflow\u533a\u522b\u5728\u4ec0\u4e48\u5730\u65b9\n    Tensorflow serving\u548cTensorRT\u6709\u4e86\u89e3\u8fc7\u561b\n    caffe\u7ed3\u6784\u6846\u67b6\n    \n7. \u89c6\u89c9SLAM\u76f8\u5173\n    SLAM\u4e3b\u8981\u5206\u4e3a\u54ea\u51e0\u4e2a\u6a21\u5757\n    ORB-SLAM2\u7684\u4f18\u7f3a\u70b9\u5206\u6790\uff0c\u5982\u4f55\u6539\u8fdb\n    ORB\u548cFAST\u5bf9\u6bd4\n    BA\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\n    ORB-SLAM2\u7684\u4e09\u4e2a\u7ebf\u7a0b\u662f\u4ec0\u4e48\n    ORB-SLAM2\u7684\u5b9a\u4f4d\u5982\u4f55\u5b9e\u73b0\n    \u5982\u4f55\u7406\u89e3ORB-SLAM2\u7684\u56fe\u4f18\u5316\n    \u7ed3\u6784\u5149\u3001TOF\u3001\u53cc\u76ee\u89c6\u89c9\u539f\u7406\n    \u76f4\u63a5\u6cd5\u3001\u534a\u76f4\u63a5\u6cd5\u3001\u7279\u5f81\u70b9\u6cd5\u533a\u522b\u4e0e\u8054\u7cfb\n    Apollo\u7684\u611f\u77e5\u6a21\u5757\u539f\u7406\n    Apollo\u76842D\u548c3D\u8ddf\u8e2a\n    \u5982\u4f55\u6c42\u89e3\u65cb\u8f6c\u77e9\u9635\n    \u5982\u679c\u53ea\u670932\u7ebf\u96f7\u8fbe\uff0c\u4e2a\u6570\u4e0d\u9650\uff0c\u80fd\u5b9e\u73b0360\u5ea6\u89c6\u89d2\u8986\u76d6\u5417\uff0c\u5982\u4f55\u5b9e\u73b0\uff0c64\u7ebf\u5462\uff1f\n```\n\n##  \u516c\u53f8\n[\u89c6\u89c9\u9886\u57df\u7684\u90e8\u5206\u56fd\u5185\u516c\u53f8](http://www.ipcv.org/cvcom/)\n###  \u521d\u521b\u516c\u53f8\uff1a\n[\u56fe\u666e\u79d1\u6280](http://www.tuputech.com/)---[Face++](http://www.faceplusplus.com.cn/)---[Linkface](http://www.linkface.cn/index.html)---[Minieye](http://www.minieye.cc/cn/)---[\u77e5\u56feCogtu](http://www.cogtu.com/?lang=zh)---[\u5546\u6c64\u79d1\u6280Sensetime](http://www.sensetime.com/cn)---[\u4eae\u98ce\u53f0Hiscene](http://www.hiscene.com/)---[\u638c\u8d62\u79d1\u6280](http://www.zhangying.mobi/index.html)---[\u683c\u7075\u6df1\u77b3DeepPG](http://www.deepglint.com/)---[\u51cc\u611f\u79d1\u6280usens](http://www.lagou.com/gongsi/j114187.html)---[\u56fe\u68eeTuSimple](http://www.tusimple.com/)---[\u4e2d\u79d1\u89c6\u62d3Seetatech(\u5c71\u4e16\u5149)](http://www.seetatech.com/)---[\u7b2c\u56db\u8303\u5f0f](https://www.4paradigm.com/product/prophet)\n\n### \u4e0a\u5e02\u516c\u53f8\uff1a\n[\u767e\u5ea6DL\u5b9e\u9a8c\u5ba4](http://idl.baidu.com/)---[\u817e\u8baf\u4f18\u56fe](http://youtu.qq.com/)---[\u963f\u91cc\u9ad8\u5fb7](http://www.newsmth.net/nForum/#!article/Career_Upgrade/429476)---[\u66b4\u98ce\u9b54\u955c](http://www.newsmth.net/nForum/#!article/Career_PHD/225254)---[\u641c\u72d7](http://www.newsmth.net/nForum/#!article/Career_PHD/224449)---[\u4e50\u89c6tv](http://www.newsmth.net/nForum/#!article/Career_PHD/222651)---[\u5947\u864e360](http://www.newsmth.net/nForum/#!article/Career_PHD/222379)---[\u4eac\u4e1c\u5b9e\u9a8c\u5ba4](http://www.newsmth.net/nForum/#!article/Career_PHD/223133/a>)---[\u963f\u91cc\u5df4\u5df4](http://www.newsmth.net/nForum/#!article/Career_PHD/222007)---[\u8054\u60f3\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/220225)---[\u534e\u4e3a\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/225976)\n\n### \u77e5\u540d\u5916\u4f01\uff1a\n[\u4f73\u80fd\u4fe1\u606f](http://www.newsmth.net/nForum/#!article/Career_PHD/222548)---[\u7d22\u5c3c\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/223437)---[\u5bcc\u58eb\u901a\u7814\u53d1\u4e2d\u5fc3](http://www.newsmth.net/nForum/#!article/Career_PHD/220654)---[\u5fae\u8f6f\u7814\u7a76\u9662](https://careers.microsoft.com/?rg=cn)---[\u82f1\u7279\u5c14\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/221175)---[\u4e09\u661f\u7814\u7a76\u9662](http://www.yingjiesheng.com/job-001-742-124.html)\n\n\n\n## 0 \u8ba1\u7b97\u6444\u5f71\u3000\u6444\u5f71\u51e0\u4f55\n[\u8ba1\u7b97\u6444\u5f71\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/cp-lecture/)\n[\u76f8\u673a\u5185\u90e8\u56fe\u50cf\u5904\u7406\u6d41\u7a0b](http://www.comp.nus.edu.sg/~brown/ICIP2013_Brown.html)\n[pdf](http://www.comp.nus.edu.sg/~brown/ICIP2013_Tutorial_Brown.pdf)\n\n    \u76f8\u673a = \u5149\u6d4b\u91cf\u88c5\u7f6e(Camera = light-measuring device)\n        \u7167\u660e\u5149\u6e90(Illumination source)\uff08\u8f90\u5c04(radiance)\uff09 --> \n        \u573a\u666f\u5143\u7d20(Scene Element)   --->\n        \u6210\u50cf\u7cfb\u7edf(Imaging System)  --->\n        \u5185\u90e8\u56fe\u50cf\u5e73\u9762(Internal Image Plane) --->\n        \u8f93\u51fa\uff08\u6570\u5b57\uff09\u56fe\u50cf(Output (digital) image) \n    \u56fe\u50cf = \u8f90\u5c04\u80fd\u91cf\u6d4b\u91cf(Image = radiant-energy measurement)  \n    \n    \n    \u73b0\u4ee3\u6444\u5f71\u6d41\u6c34\u7ebf\u3000Modern photography pipeline \n    \u573a\u666f\u8f90\u5c04\u3000--->\u3000\u76f8\u673a\u524d\u7aef(\u955c\u5934\u8fc7\u6ee4\u5668 \u955c\u5934Lens \u5feb\u95e8Shutter \u5b54\u5f84)\u3000--->\u3000\n    \u76f8\u673a\u5185\u90e8(ccd\u54cd\u5e94response\uff08RAW\uff09 CCD\u63d2\u503cDemosaicing \uff08\u539f\uff09)\u3000--->\u3000\n    \u76f8\u673a\u540e\u7aef\u5904\u7406(\u76f4\u65b9\u56fe\u5747\u8861Hist equalization\u3001\u7a7a\u95f4\u626d\u66f2Spatial warping)--->\u3000\u8f93\u51fa\n    \n\n\n    \u900f\u8fc7\u68f1\u955c\u7684\u767d\u5149 \u3000\u201cWhite light\u201d through a prism  ------> \u6298\u5c04\u5149(Refracted light)----> \u5149\u8c31 Spectral \u3000\n    \u6211\u4eec\u7684\u773c\u775b\u6709\u4e09\u4e2a\u53d7\u4f53\uff08\u9525\u7ec6\u80de\uff09\uff0c\u5b83\u4eec\u5bf9\u53ef\u89c1\u5149\u4f5c\u51fa\u53cd\u5e94\u5e76\u4ea7\u751f\u989c\u8272\u611f\u3002\n    \n[CSC320S: Introduction to Visual Computing \u89c6\u89c9\u8ba1\u7b97\u5bfc\u8bba ](http://www.cs.toronto.edu/~kyros/courses/320/)\n\n[Facebook surround 360 \u300a\u5168\u666f\u56fe\u62fc\u63a5\u300b](https://github.com/facebook/Surround360)\n\n        \u8f93\u5165\uff1a17\u5f20raw\u56fe\u50cf\uff0c\u5305\u62ec14\u5f20side images\u30012\u5f20top images\u30011\u5f20bottom image\n        \u8f93\u51fa\uff1a3D\u7acb\u4f53360\u5ea6\u5168\u666f\u56fe\u50cf  \n[\u535a\u5ba2\u7b14\u8bb0](https://blog.csdn.net/electech6/article/details/53618965)   \n        \n\n[\u6df1\u5ea6\u6444\u5f71\u98ce\u683c\u8f6c\u6362 Deep Photo Style Transfer](https://github.com/luanfujun/deep-photo-styletransfer)\n\n### \u56fe\u50cf\u5f62\u53d8 Image warping\n[\u53c2\u8003](http://www.ipcv.org/image-warping/)\n### \u8272\u5f69\u589e\u5f3a/\u8f6c\u6362\u3000Color transfer\n[\u53c2\u8003](http://www.ipcv.org/colortransfer/)\n### \u56fe\u50cf\u4fee\u8865 Image repair\n[\u53c2\u8003](http://www.ipcv.org/imagerepair/)\n### \u56fe\u50cf\u53bb\u566a Image denoise\n[\u53c2\u8003](http://www.ipcv.org/imagedenoise/)\n### \u56fe\u50cf\u53bb\u6a21\u7cca Image deblur \n[\u53c2\u8003](http://www.ipcv.org/imagedeblur/)\n###  \u56fe\u50cf\u6ee4\u6ce2 Image filter\n[\u53c2\u8003](http://www.ipcv.org/imagefilter/)\n\n###  \u8d85\u5206\u8fa8\u7387 Super-resolution\n[\u53c2\u8003](http://www.ipcv.org/code-superresolution/)  \n\n\n## 1\u3000\u4e09\u7ef4\u91cd\u5efa 3D Modeling\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/3dd/)\n### \u76f8\u673a\u77eb\u6b63\u3000Camera calibration\n[\u53c2\u8003](http://www.ipcv.org/poseestimation/)\n### \u975e\u521a\u4f53\u91cd\u5efa\u3000Non-rigid modeling\u3000\n[\u53c2\u8003](http://www.ipcv.org/nonnigitreco/)\n### \u4e09\u7ef4\u91cd\u6784 3D modeling\n[\u53c2\u8003](http://www.ipcv.org/3dmodeling/)\n[\u89c6\u89c9SLAM](http://www.ipcv.org/on-visual-slam/)\n\n[Self-augmented Convolutional Neural Networks](https://github.com/msraig/self-augmented-net)\n\n[\u8fd0\u52a8\u4f30\u8ba1 motion estimation](http://www.ipcv.org/on-motion-estimation/)\n\n[\u9762\u90e8\u53d8\u5f62\u3000face morphing\u3000](http://www.ipcv.org/about-face-morphing/)\n\n[\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u7684\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/people-3d-modeling/)\n\n\n## 2  \u5339\u914d/\u8ddf\u8e2a Matching & Tracking\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/tracking/)\n\n### 2.a \u7279\u5f81\u63d0\u53d6 Feature extraction\n[\u53c2\u8003](http://www.ipcv.org/featureextraction/)\n\n### 2.b \u7279\u5f81\u5339\u914d Feature matching\n[\u53c2\u8003](http://www.ipcv.org/code-featmatching/)\n\n### 2.c \u65f6\u7a7a\u5339\u914d Space-time matching\n[\u53c2\u8003](http://www.ipcv.org/code-spacetimematching/)\n\n### 2.d \u533a\u57df\u5339\u914d Region matching\n[\u53c2\u8003](http://www.ipcv.org/code-regionmatching/)\n\n### 2.e \u8f6e\u5ed3\u5339\u914d Contour matching\n[\u53c2\u8003](http://www.ipcv.org/code-coutourmatching/)\n\n### 2.f \u7acb\u4f53\u5339\u914d Stereo matching \n[\u53c2\u8003](http://www.ipcv.org/code-stereomatching/)\n\n[\u53cc\u76ee\u89c6\u89c9\u81ea\u52a8\u9a7e \u573a\u666f\u7269\u4f53\u8ddf\u8e2apaper](http://www.cvlibs.net/publications/Menze2015CVPR.pdf)\n\n[kitti\u53cc\u76ee\u6570\u636e\u96c6\u89e3\u51b3\u65b9\u6848](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo)\n\n[\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5c062D\u7535\u5f71\u8f6c\u6362\u4e3a3D\u7535\u5f71\u7684\u8f6c\u6362](https://github.com/piiswrong/deep3d)\n\n[\u795e\u7ecf\u7f51\u7edc\u3000\u53cc\u76ee\u5339\u914d](https://github.com/jzbontar/mc-cnn)\n\n\n[\u4e2d\u5c71\u5927\u5b66\u5f20\u5f1b\u535a\u58eb](http://chizhang.me/)\n\n    MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation\n    1\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\uff08Stereo Matching\uff09\u95ee\u9898\u7684\u5b9a\u4e49\u53ca\u5176\u4e0e\u4eba\u773c\u611f\u77e5\u6df1\u5ea6\u7684\u5173\u7cfb\uff1b\n    2\u3001\u5bf9Matching Cost Volume\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u5206\u6790\uff0c\u4ee5\u671f\u671b\u8fbe\u5230\u542c\u8005\u5bf9\u5176\u7684\u76f4\u89c2\u4e14\u672c\u8d28\u7684\u7406\u89e3\uff1b\n    3\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\u95ee\u9898\u4e2d\u7684\u56db\u4e2a\u7ecf\u5178\u65b9\u6cd5\uff08\n        Graph Cut\uff0cAdaptive Support Weight Aggregation, \n        Semi-Global Matching, \n        \u4ee5\u53ca PatchMatch Stereo\uff09\uff1b\n    4\u3001\u8ba8\u8bba\u4e86MeshStereo\u7684\u8bd5\u56fe\u7edf\u4e00disparity\u6c42\u89e3\u4ee5\u53ca\u7f51\u683c\u751f\u6210\u4e24\u4e2a\u6b65\u9aa4\u7684motivation\uff0c\n        \u4ee5\u53caformulate\u8fd9\u6837\u4e00\u4e2aunified model\u4f1a\u9047\u5230\u7684\u56f0\u96be\uff1b\n    5\u3001\u8ba8\u8bba\u4e86MeshStereo\u5f15\u5165splitting probability\u7684\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u4f18\u5316\u8fc7\u7a0b\u3002\n    \n    Webinar\u6700\u540e\u5c55\u793a\u4e86MeshStereo\u5728\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca\u65b0\u89c6\u89d2\u6e32\u67d3\u4e24\u4e2a\u4efb\u52a1\u4e2d\u7684\u7ed3\u679c\u3002\n\n\n[Stereo Matching Using Tree Filtering non-local\u7b97\u6cd5\u5728\u53cc\u76ee\u7acb\u4f53\u5339\u914d\u4e0a\u7684\u5e94\u7528 ](https://blog.csdn.net/wsj998689aa/article/details/45584725)\n\n\n\n\n### 2.g \u6df1\u5ea6\u5339\u914d depth matching \n[\u6df1\u5ea6\u5339\u914d depth matching ](http://www.ipcv.org/on-depth-matching/)\n\n### 2.h \u59ff\u6001\u8ddf\u8e2a Pose tracking \n[\u53c2\u8003](http://www.ipcv.org/code-posetracking/)\n\n### 2.i \u7269\u4f53\u8ddf\u8e2a Object tracking\n[\u53c2\u8003](http://www.ipcv.org/code-objtracking/)\n\n### 2.j \u7fa4\u4f53\u5206\u6790 Crowd analysis\n[\u53c2\u8003](http://www.ipcv.org/code-crowdanalysis/)\n[\u7fa4\u4f53\u8fd0\u52a8\u5ea6\u91cf](https://github.com/metalbubble/collectiveness)\n\n### 2.k \u5149\u6d41\u573a\u8ddf\u8e2a Optical flow\n[\u53c2\u8003](http://www.ipcv.org/code-opticalflow/)\n\n\n## 3 \u8bed\u4e49/\u5b9e\u4f8b\u5206\u5272&\u89e3\u6790\u3000Segmentation & Parsing\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/parsing/)\n### 3.a \u89c6\u9891\u5206\u5272  Video  segmentation\n[\u53c2\u8003](http://www.ipcv.org/video-segmentation/)\n\n### 3.b \u4eba\u4f53\u89e3\u6790  Person parsing\n[\u53c2\u8003](http://www.ipcv.org/code-poseparsing/)\n\n[person parsing](http://www.ipcv.org/about-person-parsing/)\n\n### 3.c \u573a\u666f\u89e3\u6790  Scene  parsing\n[scene parsing](http://www.ipcv.org/about-scene-parsing/)\n[\u53c2\u8003](http://www.ipcv.org/code-sceneparsing/)\n\n### 3.d \u8fb9\u7f18\u68c0\u6d4b  Edge   detection\n[\u53c2\u8003](http://www.ipcv.org/code-edgedetection/)\n[\u8fb9\u7f18\u68c0\u6d4b](http://www.ipcv.org/on-edge-detection/)\n\n\n### 3.e \u56fe\u50cf\u7269\u4f53\u5206\u5272 Image object segmentation \n[\u53c2\u8003](http://www.ipcv.org/code-imobjseg/)\n \n### 3.f \u89c6\u9891\u7269\u4f53\u5206\u5272 Video object segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-viobseg/)\n[object segmentation](http://www.ipcv.org/about-video-object-segmentation/)\n\n\n### 3.g \u4ea4\u4e92\u5f0f\u5206\u5272   Interactive segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-intseg/)\n\n### 3.h \u5171\u5206\u5272      Co-segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-cosegmentation/)\n\n### 3.i \u80cc\u666f\u5dee      Background subtraction \n[\u53c2\u8003](http://www.ipcv.org/code-backsub/)\n\n### 3.j \u56fe\u50cf\u5206\u5272\u65b9\u9762 Image segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-imgseg/)\n \n\n## 4 \u8bc6\u522b/\u68c0\u6d4b\u3000Recognition & Detection\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/detect/)\n###  4.a \u5176\u4ed6\u8bc6\u522b Other recognition\n[\u53c2\u8003](http://www.ipcv.org/otherrecog/)\n\n### 4.b \u56fe\u50cf\u68c0\u7d22 Image retrieval\n[\u53c2\u8003](http://www.ipcv.org/%e5%9b%be%e5%83%8f%e6%a3%80%e7%b4%a2/)\n\n### 4.c \u663e\u8457\u68c0\u6d4b Saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saldetection/)\n\n### 4.d \u901a\u7528\u7269\u4f53\u68c0\u6d4b Object proposal\n[\u53c2\u8003](http://www.ipcv.org/code-objproposal/)\n\n### 4.e \u884c\u4e3a\u8bc6\u522b Action recognition\n[\u53c2\u8003](http://www.ipcv.org/code-actionrecogntion/)\n\n### 4.f \u7269\u4f53\u8bc6\u522b Object recognition\n[\u53c2\u8003](http://www.ipcv.org/code-objrecogntion/)\n\n### 4.g \u884c\u4eba\u68c0\u6d4b Human detection\n[\u53c2\u8003](http://www.ipcv.org/code-humandetection/)\n\n### 4.h \u4eba\u8138\u89e3\u6790 Face Parsing\n[\u53c2\u8003](http://www.ipcv.org/code-facerecog/)\n\n### 4.i \u7eb9\u7406\u5206\u6790 Texture Analysis\n[\u7eb9\u7406\u5206\u6790 Texture Analysis](http://www.ipcv.org/on-texture-analysis/)\n[\u76f8\u5173\u4eba\u7269](http://www.ipcv.org/people-reidentity/)\n\n## 5 \u673a\u5668\u5b66\u4e60 Maching Learning\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/ml/)\n\n### 5.a \u751f\u6210\u5bf9\u6297\u7f51\u7edc GAN Generative Adversarial Networks\n[\u53c2\u8003](http://www.ipcv.org/adversarial-networks/)\n\n### 5.b \u6df1\u5ea6\u5b66\u4e60    Deep learning \n[\u53c2\u8003](http://www.ipcv.org/deeplearning/)\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/lecture-deeplearning/)\n\n[CNN Models \u5377\u79ef\u7f51\u7edc\u6a21\u578b](http://www.ipcv.org/on-object-detection/)\n\n[Deep Learning Libraries\u3000\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u5e93](http://www.ipcv.org/deep_learning_libraries/)\n\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/dl-researcher/)\n\n### 5.c \u80fd\u91cf\u4f18\u5316    Energy optimization \n[\u53c2\u8003](http://www.ipcv.org/energyopt/)\n\n### 5.d \u6a21\u578b\u8bbe\u8ba1    Model design\n[\u53c2\u8003](http://www.ipcv.org/modelbuilding/)\n\n### 5.e \u7a7a\u95f4\u964d\u7ef4    Dimention reduction \n[\u53c2\u8003](http://www.ipcv.org/dimention/)\n\n### 5.f \u805a\u7c7b       Clustering \n[\u53c2\u8003](http://www.ipcv.org/clustering/)\n\n### 5.g \u5206\u7c7b\u5668     Classifier\n[\u53c2\u8003](http://www.ipcv.org/classifier/)\n\n## 6 \u5f00\u6e90\u5e93\u3000Open library\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/lib/)\n###\n###\n\n\n## 7 \u6570\u636e\u96c6\u3000Public dataset\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/dataset/)\n### 7.a \u5176\u4ed6\u65b9\u9762 Other datasets\n[\u53c2\u8003](http://www.ipcv.org/otherdb/)\n[\u4eba\u8138\u68c0\u6d4bFace tracking and recognition database ](http://seqam.rutgers.edu/site/index.php?option=com_content&view=article&id=65&Itemid=76)\n\n[\u4eba\u8138\u68c0\u6d4bCaltech 10,000 Web Faces](http://vision.caltech.edu/archive.html)\n\n[\u4eba\u8138\u68c0\u6d4bHelen dataset](http://www.ifp.illinois.edu/~vuongle2/helen/)\n\n[\u6df1\u5ea6\u56fe RGB-D dataset](http://mobilerobotics.cs.washington.edu/projects/kdes/)\n\n[\u89c6\u9891\u5206\u5272 2010 ECCV Efficient Hierarchical Graph Based Video Segmentation](http://www.cc.gatech.edu/cpl/projects/videosegmentation/)\n\n[\u624b\u52bf\u8ddf\u8e2a Hand dataset](https://engineering.purdue.edu/RVL/Database.html)\n\n[\u624b\u52bf\u8ddf\u8e2a2](http://www.robots.ox.ac.uk/~vgg/research/hands/index.html)\n\n[\u8f66\u8f86\u68c0\u6d4b 2002 ECCV Learning a sparse representation for object detection](http://cogcomp.cs.illinois.edu/Data/Car/)\n\n### 7.b \u4eba\u4f53\u68c0\u6d4b dataset on human annotation\n[\u53c2\u8003](http://www.ipcv.org/humandetection/)\n[Caltech Pedestrian Detection Benchmark](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n\n[Ethz](http://www.vision.ee.ethz.ch/~aess/dataset/)\n[bleibe](http://www.vision.ee.ethz.ch/~bleibe/data/datasets.html)\n\n[RGB-D People Dataset](http://www.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)\n\n[TUD Campus](http://www.d2.mpi-inf.mpg.de/tud-brussels)\n[382](https://www.d2.mpi-inf.mpg.de/node/382)\n\n[PSU HUB Dataset](http://www.cse.psu.edu/~rcollins/software.html)\n\n[Pedestrian parsing](http://vision.ics.uci.edu/datasets/)\n\n[Human Eva](http://vision.cs.brown.edu/humaneva/)\n\n\n### 7.c \u7269\u4f53\u8bc6\u522b dataset on object recognition\n\n[\u53c2\u8003](http://www.ipcv.org/objectrecognition/)\n\n[ e-Lab Video Data Set](https://engineering.purdue.edu/elab/eVDS/)\n\n[Image Net](http://www.image-net.org/)\n\n[Places2 Database](http://places2.csail.mit.edu)\n\n[Microsoft CoCo: Common Objects in Context](http://mscoco.org/)\n\n[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n[MIT\u2019s Place2]( http://places2.csail.mit.edu/)\n\n### 7.d \u663e\u8457\u68c0\u6d4b\u65b9\u9762 dataset on saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saliencydetection/)\n\n[\u89c6\u89c9\u663e\u8457\u6027\u68c0\u6d4b\u6280\u672f\u53d1\u5c55\u60c5\u51b5](http://blog.csdn.net/anshan1984/article/details/8657176)\n\n[2012 ECCV Salient Objects Dataset (SOD)](http://elderlab.yorku.ca/SOD/)\n\n[2012 ECCV Neil D. B. Bruce Eye Tracking Data](http://cs.umanitoba.ca/~bruce/datacode.html)\n\n[2012 ECCV DOVES:A database of visual eye movements](http://live.ece.utexas.edu/research/doves/)\n\n[2012 ECCV MSRA:Salient Object Database](http://research.microsoft.com/en-us/um/people/jiansun/SalientObject/salient_object.htm)\n\n[2012 ECCV NUS: Predicting Saliency Beyond Pixels](http://www.ece.nus.edu.sg/stfpage/eleqiz/predicting.html)\n\n[2012 ECCV saliency benchmark](http://people.csail.mit.edu/tjudd/SaliencyBenchmark/index.html)\n\n[2010 ECCV The DUT-OMRON Image Dataset](http://ice.dlut.edu.cn/lu/DUT-OMRON/Homepage.htm)\n\n[2010 ECCV An eye fixation database for saliency detection in images](http://mmas.comp.nus.edu.sg/NUSEF.html)\n\n### 7.e \u884c\u4e3a\u8bc6\u522b dataset on action recognition\n[\u53c2\u8003](http://www.ipcv.org/actionrecognition/)\n\n[UCF](http://www.cs.ucf.edu/~liujg/YouTube_Action_dataset.html)\n[ChaoticInvariants](http://www.cs.ucf.edu/~sali/Projects/ChaoticInvariants/index.html)\n[datasetsActions](http://vision.eecs.ucf.edu/datasetsActions.html)\n\n[Hollywood Human Actions dataset](http://www.di.ens.fr/~laptev/download.html)\n[data](http://lear.inrialpes.fr/data)\n\n[Weizmann: Actionsas Space-Time Shapes](http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html)\n\n[KTH](http://www.nada.kth.se/cvap/actions/)\n\n[UMD](http://www.umiacs.umd.edu/~zhuolin/Keckgesturedataset.html)\n\n[HMDB: A Large Video Database for Human Motion Recognition](http://serre-lab.clps.brown.edu/resources/HMDB/related_data/)\n\n[Collective Activity Dataset](http://www.eecs.umich.edu/vision/activity-dataset.html)\n\n[MSR Action Recognition Datasets and Codes](http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/default.htm)\n\n[Visual Event Recognition in Videos](http://vc.sce.ntu.edu.sg/index_files/VisualEventRecognition/VisualEventRecognition.html)\n\n\n\n### 7.f \u7269\u4f53\u5206\u5272 dataset on object segmentation\n[\u53c2\u8003](http://www.ipcv.org/objectsegmentation/)\n[microsoft MSRC-V2](http://research.microsoft.com/en-us/projects/objectclassrecognition/)\n\n[2010 CVPR iCoseg: Interactive cosegmentation by touch](http://chenlab.ece.cornell.edu/projects/touch-coseg/)\n\n[2010 CVPR Caltech-UCSD Birds 200](http://www.vision.caltech.edu/visipedia/CUB-200.html)\n\n[2010 CVPR Flower Datasets](http://www.robots.ox.ac.uk/~vgg/data/flowers/)\n\n[2009 ICCV An efficient algorithm for co-segmentation](http://www.biostat.wisc.edu/~vsingh/)\n\n[2008 CVPR Unsupervised Learning of Probabilistic Object Models (POMs) for Object Classification, Segmentation and Recognition](http://people.csail.mit.edu/leozhu/)\n\n[2008 CVPR Caltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)\n\n[2004 ECCV The Weizmann Horse Database](http://www.msri.org/people/members/eranb/)\n\n\n### 7.g \u573a\u666f\u89e3\u6790 dataset on scene parsing\n[\u53c2\u8003](http://www.ipcv.org/sceneparsing/)\n\n[ImageNet](http://www.image-net.org/)\n\n[ADE 20k](http://sceneparsing.csail.mit.edu/)\n\n[Cityscapes](https://www.cityscapes-dataset.com/)\n\n[COCO](http://cocodataset.org/#home)\n\n[Lab, Koch](http://www.mis.tu-darmstadt.de/tudds)\n\n[uiuc, D hoiem](http://www.cs.illinois.edu/homes/dhoiem/)\n\n[mit, cbcl](http://cbcl.mit.edu/software-datasets/streetscenes/)\n\n[mit LabelMeVideo](http://labelme.csail.mit.edu/LabelMeVideo/)\n\n[2013 BMVC Hierarchical Scene Annotation](http://www.vision.caltech.edu/~mmaire/)\n\n[2010 ECCV SuperParsing: Scalable Nonparametric Image Parsing with Superpixels](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/)\n\n[2009 CVPR Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment](ttp://people.csail.mit.edu/celiu/CVPR2009/)\n\n[2009 Scene Understanding Datasets](http://dags.stanford.edu/projects/scenedataset.html)\n\n[2008 IJCV 6D-Vision](http://www.6d-vision.com/scene-labeling)\n\n[2008 IJCV The Daimler Urban Segmentation Dataset](http://www.6d-vision.com/scene-labeling)\n\n[2008 ECCV Motion-based Segmentation and Recognition Dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec))/CamVid/\n\n[2008 York Urban Dataset](http://www.elderlab.yorku.ca/YorkUrbanDB/)\n\n[2008 IJCV LabelMe](http://labelme.csail.mit.edu/LabelMeToolbox/index.html)\n\n\n## 8 \u4f1a\u8bae\u3000\u671f\u520a\u3000\n### CVPR Computer vision  and  Pattern Reconition \u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6a21\u5f0f\u8bc6\u522b\n### ECCV European Conference on Computer Vision   \u6b27\u6d32\u8ba1\u7b97\u673a\u89c6\u89c9\u56fd\u9645\u4f1a\u8bae \n### ICCV IEEE International Conference on Computer Vision  \u56fd\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u5927\u4f1a\n### \u5176\u4ed6\n[\u5176\u4ed6](http://www.ipcv.org/otherpaper/)\n###\n###\n###\n###\n\n\n## AR&VR\n[\u53c2\u8003](http://www.ipcv.org/category/top-dir/arvr/)\n\n\n## \n"
 },
 {
  "repo": "PySimpleGUI/PySimpleGUI",
  "language": "Python",
  "readme_contents": "\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Logo%20with%20text%20for%20GitHub%20Top.png\" alt=\"\u4eba\u9593\u306e\u305f\u3081\u306ePythonGUI \">\n  <h2 align=\"center\">\u4eba\u9593\u306e\u305f\u3081\u306ePython\u306eGUI</h2>\n</p>\n\ntkinter\u3001Qt\u3001WxPython\u3001\u304a\u3088\u3073Remi(\u30d6\u30e9\u30a6\u30b6\u30d9\u30fc\u30b9)\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u3001\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\u306f\u521d\u5fc3\u8005\u304c\u7406\u89e3\u3059\u308bPython\u30b3\u30a2\u30c7\u30fc\u30bf\u578b (\u30ea\u30b9\u30c8\u3068\u8f9e\u66f8) \u3092\u4f7f\u7528\u3057\u3066\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u6e21\u3059\u30e2\u30c7\u30eb\u306b\u30a4\u30d9\u30f3\u30c8\u51e6\u7406\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3055\u3089\u306b\u5358\u7d14\u5316\u304c\u884c\u308f\u308c\u307e\u3059\u3002 \n\n\u30b3\u30fc\u30c9\u306f\u3088\u308a\u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b\u306e\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u6301\u3064*\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093*\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u7406\u89e3\u3057\u3084\u3059\u3044\u3082\u306e\u3067\u3059\u304c\u3001\u5fc5\u305a\u3057\u3082*\u5358\u7d14*\u306a\u554f\u984c\u3060\u3051\u306b\u5236\u9650\u3055\u308c\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u305f\u3060\u3057\u3001\u4e00\u90e8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fPySimpleGUI\u306b\u306f\u9069\u3057\u3066\u3044\u307e\u305b\u3093\u3002 \u5b9a\u7fa9\u4e0a\u3001PySimpleGUI \u306f\u57fa\u76e4\u3068\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304cPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u3066\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u9069\u3057\u3066\u3044\u306a\u3044\u304b\u3092\u6b63\u78ba\u306b\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3067\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8a73\u7d30\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\u3002\u30a8\u30af\u30bb\u30eb\u3092\u8a73\u7d30\u306b\u8907\u88fd\u3059\u308b\u3053\u3068\u306fPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u306a\u3044\u3082\u306e\u306e\u4f8b\u3067\u3059\u3002\n\n<hr>\n\n# \u7d71\u8a08 :chart_with_upwards_trend:\n\n## PyPI \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n<p align=\"center\">\ntkinter <img src=\"http://pepy.tech/badge/pysimplegui?color=blue&style=for-the-badge\" width=\"100px\"  align=\"center\">\ntkinter 2.7 <img src=\"https://pepy.tech/badge/pysimplegui27?color=blue&style=for-the-badge\"  align=\"center\"><br>\nQt <img src=\"https://pepy.tech/badge/pysimpleguiqt?color=blue&style=for-the-badge\"  align=\"center\">\nWxPython<img src=\"https://pepy.tech/badge/pysimpleguiwx?color=blue&style=for-the-badge\"  align=\"center\">\nWeb (Remi) <img src=\"https://pepy.tech/badge/pysimpleguiweb?color=blue&style=for-the-badge\"  align=\"center\">\n</p>\n\n\n## GitHub\n\n<p align=\"center\">\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\" width=\"180px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-closed-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\"  width=\"200px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/commit-activity/m/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"  width=\"260px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/last-commit/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"width=\"200px\"></a>\n<a href=\"\"><img src=\"http://ForTheBadge.com/images/badges/makes-people-smile.svg\" alt=\"img\"width=\"190px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/stars/PySimpleGUI/PySimpleGUI.svg?style=social&label=Star&maxAge=2592000\" alt=\"img\"width=\"140x\"></a>\n</p>\n\n\n<p align=\"center\">\n  <img src=\"https://github-readme-stats.vercel.app/api/?username=PySimpleGUI&bg_color=3e7bac&title_color=ffdd55&icon_color=ffdd55&text_color=ffdd55&show_icons=true&count_private=true\">\n</p>\n\n## \u6700\u65b0\u306e PyPI \u30d0\u30fc\u30b8\u30e7\u30f3\n\n\n<p align=\"center\">\ntkinter\n<a href=\"pypi tkinter\"><img src=\"https://img.shields.io/pypi/v/pysimplegui.svg?style=for-the-badge&color=red\" alt=\"img\" align=\"center\" width=\"150px\"></a>\nQt\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWeb\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWxPython\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\n</p>\n\n<hr>\n\n# PySimpleGUI\u3068\u306f\u4f55\u3067\u3059\u304b:question:\n\nPySimpleGUI\u306f\u3042\u3089\u3086\u308b\u30ec\u30d9\u30eb\u306ePython\u30d7\u30ed\u30b0\u30e9\u30de\u304cGUI\u3092\u4f5c\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308bPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080 \u300c\u30ec\u30a4\u30a2\u30a6\u30c8\u300d\u3092\u4f7f\u7528\u3057\u3066 GUI \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u6307\u5b9a\u3057\u307e\u3059 (PySimpleGUI \u3067\u306f\u300c\u30a8\u30ec\u30e1\u30f3\u30c8\u300d\u3068\u547c\u3073\u307e\u3059)\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b4\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u8868\u793a\u3084\u64cd\u4f5c\u3059\u308b\u306e\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u30b5\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001tkinter\u3001Qt\u3001WxPython\u3001WxPython\u307e\u305f\u306fRemi\u304c\u542b\u307e\u308c\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u300c\u30e9\u30c3\u30d1\u30fc\u300d\u3068\u3044\u3046\u7528\u8a9e\u304c\u4f7f\u308f\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\n\nPySimpleGUI\u306f\u300c\u30dc\u30a4\u30e9\u30fc\u30d7\u30ec\u30fc\u30c8\u30b3\u30fc\u30c9\u300d\u306e\u591a\u304f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u57fa\u3068\u306a\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082\u5358\u7d14\u3067\u77ed\u304b\u3044\u30b3\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002\n\u3055\u3089\u306b\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306f\u3001\u671b\u3093\u3060\u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u3067\u304d\u308b\u3060\u3051\u5c11\u306a\u304f\u3059\u308b\u3088\u3046\u306b\u5358\u7d14\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u3082\u3088\u308a\u307e\u3059\u304c\u3001PySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u76f4\u63a5\u4f7f\u7528\u3057\u3066\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u3088\u308a\u3082\u3001\u30b3\u30fc\u30c9\u306e\u91cf\u306f1/2\u304b\u30891/10\u7a0b\u5ea6\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u76ee\u6a19\u306f\u4f7f\u7528\u3057\u3066\u3044\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u4e0a\u306e\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3084\u30b3\u30fc\u30c9\u3092\u30ab\u30d7\u30bb\u30eb\u5316/\u975e\u8868\u793a\u306b\u3059\u308b\u3053\u3068\u3067\u3059\u304c\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3084\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u30a2\u30af\u30bb\u30b9\u3067\u304d\u307e\u3059\u3002\n\u8a2d\u5b9a\u3084\u6a5f\u80fd\u304c\u307e\u3060\u516c\u958b\u3055\u308c\u3066\u304a\u3089\u305a\u3001PySimpleGUI API\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u906e\u65ad\u3055\u308c\u3066\u307e\u305b\u3093\u3002PySimpleGUI\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u81ea\u4f53\u3092\u76f4\u63a5\u5909\u66f4\u305b\u305a\u306b\u6a5f\u80fd\u3092\u62e1\u5f35\u3067\u304d\u307e\u3059\u3002\n## \u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7\u300d\u3092\u57cb\u3081\u308b\n\nPython \u306f\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u591a\u304f\u306e\u4eba\u3005\u3092\u62db\u3044\u3066\u3044\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6570\u3068\u6271\u3046\u9818\u57df\u306e\u7bc4\u56f2\u306f\u6c17\u304c\u9060\u304f\u306a\u308a\u307e\u3059 \u3057\u304b\u3057\u591a\u304f\u306e\u5834\u5408\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3068\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u4e00\u63e1\u308a\u306e\u4eba\u3005\u4ee5\u5916\u306e\u624b\u306e\u5c4a\u304b\u306a\u3044\u3068\u3053\u308d\u306b\u3042\u308a\u307e\u3059\u3002Python \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u5927\u534a\u306f\"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\"\u30d9\u30fc\u30b9\u3067\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u7cfb\u306e\u4eba\u306f\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3092\u4ecb\u3057\u3066\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u3053\u3068\u306b\u6163\u308c\u3066\u3044\u3066\u3001\u3053\u306e\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u307b\u3068\u3093\u3069\u306e\u300c\u666e\u901a\u306e\u4eba\u300d\u306f\u554f\u984c\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30b8\u30bf\u30eb\u30fb\u30c7\u30a3\u30d0\u30a4\u30c9\u3001\u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7 \u300d\u304c\u751f\u307f\u51fa\u3055\u308c\u307e\u3059\u3002\n\u30d7\u30ed\u30b0\u30e9\u30e0\u306bGUI\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u591a\u304f\u306e\u4eba\u306b\u77e5\u3063\u3066\u3082\u3089\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u89aa\u3057\u307f\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002GUI\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u6163\u308c\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u3067\u3042\u3063\u3066\u3082\u3001\u3044\u304f\u3064\u304b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u64cd\u4f5c\u3092\u7c21\u5358\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u305d\u3057\u3066\u6700\u5f8c\u306bGUI\u3092\u5fc5\u8981\u3068\u3059\u308b\u554f\u984c\u3082\u3042\u308a\u307e\u3059\u3002   \n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/GUI%20Gap%202020.png\" width=\"600px\">\n</p>\n\n\n<hr>\n\n# \u79c1\u306b\u3064\u3044\u3066 :wave:\n\u3053\u3093\u306b\u3061\u306f\uff01 \u79c1\u306f\u30de\u30a4\u30af\u3067\u3059\u3002 GitHub\u306ePySimpleGUI\u3067\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066PySimpleGUI\u3092\u7d99\u7d9a\u7684\u306b\u524d\u9032\u3055\u305b\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u663c\u3068\u591c\u3068\u9031\u672b\u3082\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068PySimpleGUI\u30e6\u30fc\u30b6\u30fc\u306b\u6367\u3052\u3066\u304d\u307e\u3057\u305f\u3002\u79c1\u305f\u3061\u306e\u6210\u529f\u306f\u6700\u7d42\u7684\u306b\u5171\u6709\u3055\u308c\u307e\u3059\u3002 \u3042\u306a\u305f\u304c\u6210\u529f\u3057\u305f\u3068\u304d\u306b\u79c1\u306f\u6210\u529f\u3057\u3066\u3044\u307e\u3059\u3002\n\nPython\u3067\u306f\u76f8\u5bfe\u7684\u306a\u65b0\u4eba\u3067\u3059\u304c\u300170\u5e74\u4ee3\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u3002 \u79c1\u306e\u30ad\u30e3\u30ea\u30a2\u306e\u5927\u534a\u306f\u30b7\u30ea\u30b3\u30f3\u30d0\u30ec\u30fc\u3067\u306e\u88fd\u54c1\u958b\u767a\u306b\u8cbb\u3084\u3055\u308c\u307e\u3057\u305f\u3002PySimpleGUI\u306b\u306f\u81ea\u5206\u304c\u958b\u767a\u3057\u305f\u4f01\u696d\u88fd\u54c1\u3068\u540c\u3058\u3088\u3046\u306a\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30ea\u30ba\u30e0\u3068\u732e\u8eab\u3092\u3082\u305f\u3089\u3057\u307e\u3059\u3002\u4eca\u3001\u3042\u306a\u305f\u306f\u79c1\u306e\u9867\u5ba2\u3067\u3059\u3002\n\n\n## \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u6a19 :goal_net:\n\nPySimpleGUI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u91cd\u8981\u306a\u76ee\u6a19\u306f\u4ee5\u4e0b\u306e2\u3064\u3067\u3059\u3002\n\n* \u697d\u3057\u3080\u3053\u3068\n* \u3042\u306a\u305f\u306e\u6210\u529f\n\n\u771f\u9762\u76ee\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30b4\u30fc\u30eb\u3068\u3057\u3066**\u697d\u3057\u3080**\u3068\u3044\u3046\u306e\u306f\u5909\u306b\u805e\u3053\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u306f\u771f\u9762\u76ee\u306a\u76ee\u6a19\u3067\u3059\u3002\u79c1\u306f\u3053\u308c\u3089\u306eGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u304f\u3053\u3068\u306f\u3068\u3066\u3082\u697d\u3057\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u306e\u7406\u7531\u306e1\u3064\u306f\u3001\u5b8c\u5168\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u306b\u304b\u304b\u308b\u6642\u9593\u304c\u3044\u304b\u306b\u77ed\u3044\u304b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3082\u3057\u79c1\u9054\u304c\u30d7\u30ed\u30bb\u30b9\u3092\u697d\u3057\u3093\u3067\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u8ab0\u304b\u304c\u3042\u304d\u3089\u3081\u3066\u3044\u307e\u3059\u3002\n\n\u81a8\u5927\u306a\u91cf\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3001\u30af\u30c3\u30af\u30d6\u30c3\u30af\u3001\u3059\u3050\u306b\u4f7f\u3048\u308b100\u7a2e\u985e\u4ee5\u4e0a\u306e\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3001\u8a73\u7d30\u306a\u30b3\u30fc\u30eb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3001YouTube\u306e\u30d3\u30c7\u30aa\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u306eTrinket\u306e\u30c7\u30e2\u306a\u3069\u3001\u3059\u3079\u3066\u304c\u697d\u3057\u3044\u4f53\u9a13\u3092\u751f\u307f\u51fa\u3059\u305f\u3081\u306b\u4f5c\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n**\u3042\u306a\u305f\u306e\u6210\u529f**\u306f\u5171\u901a\u306e\u76ee\u6a19\u3067\u3059\u3002 PySimpleGUI \u306f\u958b\u767a\u8005\u5411\u3051\u306b\u69cb\u7bc9\u3055\u308c\u307e\u3057\u305f\u3002\u3042\u306a\u305f\u306f\u79c1\u306e\u4ef2\u9593\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u3068PySimpleGUI\u306e\u5171\u540c\u4f5c\u696d\u306e\u7d50\u679c\u3092\u898b\u308b\u306e\u306f\u4e88\u60f3\u5916\u306e\u5831\u916c\u3067\u3057\u305f\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3084\u305d\u306e\u4ed6\u306e\u8cc7\u6599\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u69cb\u7bc9\u306b\u5f79\u7acb\u3066\u3066\u304f\u3060\u3055\u3044\u3002\u30c8\u30e9\u30d6\u30eb\u306b\u906d\u9047\u3057\u305f\u5834\u5408\u306f\u3001[PySimpleGUI GitHub \u306e\u554f\u984c](http://Issues.PySimpleGUI.org)\u3067Issue \u3092\u958b\u3044\u3066\u30d8\u30eb\u30d7\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u30b5\u30dd\u30fc\u30c8\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002\n\n<hr>\n\n# \u6559\u80b2\u30ea\u30bd\u30fc\u30b9 :books:\n\nwww.PySimpleGUI.org \u306f\u899a\u3048\u3084\u3059\u304f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5834\u6240\u3067\u3059\u3002\u4e0a\u90e8\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u7570\u306a\u308b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8868\u3059\u30bf\u30d6\u304c\u3042\u308a\u307e\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u300cRead The Docs\u300d\u306b\u8a18\u8f09\u3055\u308c\u3066\u304a\u308a\u3001\u5404\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u76ee\u6b21\u304c\u3042\u308a\u691c\u7d22\u304c\u7c21\u5358\u3067\u3059\u3002\n\n\u6570\u767e\u30da\u30fc\u30b8\u306e\u6587\u66f8\u5316\u3055\u308c\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u6570\u767e\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u3042\u308a\u3001\u3042\u306a\u305f\u304c\u975e\u5e38\u306b\u901f\u304f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\u5358\u4e00\u306e GUI \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5b66\u3076\u306e\u306b\u6570\u65e5\u307e\u305f\u306f\u6570\u9031\u9593\u6295\u8cc7\u3059\u308b\u3088\u308a\u3082\u3001PySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u5348\u5f8c\u4e00\u56de\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b8c\u6210\u3055\u305b\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\n## \u4f8b 1 - \u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u30bf\u30a4\u30d7\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c1\u56de\u8868\u793a\u3055\u308c\u3066\u53ce\u96c6\u3055\u308c\u305f\u5024\u304c\u9589\u3058\u3089\u308c\u308b\u305f\u3081\u3001\u300c\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u300d\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u547c\u3070\u308c\u307e\u3059\u3002 \u30ef\u30fc\u30c9\u30d7\u30ed\u30bb\u30c3\u30b5\u306e\u3088\u3046\u306b\u9577\u3044\u9593\u958b\u3044\u305f\u307e\u307e\u306b\u306a\u3063\u3066\u3044\u307e\u305b\u3093\u3002\n### \u5358\u7d14\u306aPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u89e3\u5256\u5b66\n\nPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f5\u3064\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\n\n\n\n```python\nimport PySimpleGUI as sg                                 # \u30d1\u30fc\u30c8 1 - \u30a4\u30f3\u30dd\u30fc\u30c8\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [  [sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],     # \u30d1\u30fc\u30c8 2 - \u30ec\u30a4\u30a2\u30a6\u30c8\n            [sg.Input()],\n            [sg.Button('\u306f\u3044')] ]\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb', layout)      # \u30d1\u30fc\u30c8 3- \u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\n                                                \n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nevent, values = window.read()                   # \u30d1\u30fc\u30c8 4- \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u307e\u305f\u306f Window.read \u547c\u3073\u51fa\u3057\n\n# \u53ce\u96c6\u3055\u308c\u305f\u60c5\u5831\u3067\u4f55\u304b\u3092\u3059\u308b\nprint('\u30cf\u30ed\u30fc ', values[0], \"! PySimpleGUI\u3092\u8a66\u3057\u3066\u304f\u308c\u3066\u3042\u308a\u304c\u3068\u3046\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()                                  #\u30d1\u30fc\u30c8 5 - \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\n```\n\n\u30b3\u30fc\u30c9\u306f\u3001\u4ee5\u4e0b\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank1.jpg\">\n</p>\n\n\n<hr>\n\n## \u4f8b 2 - \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\u304b\u3001\u307e\u305f\u306f [\u7d42\u4e86] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u307e\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u753b\u9762\u4e0a\u306b\u6b8b\u308a\u307e\u3059\u3002 \u5148\u307b\u3069\u898b\u305f\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e3b\u306a\u9055\u3044\u306f\u3001\u300c\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u300d\u306e\u8ffd\u52a0\u3067\u3059\u3002\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u304b\u3089\u30a4\u30d9\u30f3\u30c8\u3068\u5165\u529b\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002 \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4e2d\u5fc3\u306f\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],\n          [sg.Input(key='-\u5165\u529b-')],\n          [sg.Text(size=(55,1), key='-\u51fa\u529b-')],\n          [sg.Button('\u306f\u3044'), sg.Button('\u7d42\u4e86')]]\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb',\u30ec\u30a4\u30a2\u30a6\u30c8)\n\n# \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nwhile True:\n    event, values = window.read()\n# \u30e6\u30fc\u30b6\u30fc\u304c\u7d42\u4e86\u3057\u305f\u3044\u306e\u304b\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n\n    # Output a message to the window\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()\n```\n\n\u4ee5\u4e0b\u306f\u306f\u3001\u4f8b2\u304c\u4f5c\u6210\u3059\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank.jpg\">\n</p>\n\n\n\n\u5165\u529b\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u5024\u3092\u5165\u529b\u3057\u3066 [OK] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u5f8c\u306e\u8868\u793a\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorld1.jpg\">\n</p>\n\n\n\u3053\u306e\u4f8b\u3068\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u9055\u3044\u306b\u6c17\u3065\u304f\u3067\u3057\u3087\u3046\u3002 \u7279\u306b2\u3064\u306e\u5909\u66f4\u304c\u91cd\u8981\u3067\u3059\u3002 1\u3064\u306f`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u3068`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e1\u3064\u306b`key`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u300ckey\u300d\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u540d\u524d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 \u307e\u305f\u306f\u3001Python\u306e\u8a00\u8449\u3067\u306f\u3001\u8f9e\u66f8\u30ad\u30fc\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 `Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ad\u30fc\u306f\u3001\u30b3\u30fc\u30c9\u306e\u5f8c\u534a\u3067\u8f9e\u66f8\u30ad\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\n\n\n\u3082\u30461\u3064\u306e\u9055\u3044\u306f\u3001\u3053\u306e `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u8ffd\u52a0\u3067\u3059:\n```python\n          [sg.Text(size=(40,1), key='-OUTPUT-')],\n```\n\n\u3059\u3067\u306b\u30ab\u30d0\u30fc\u3057\u3066\u3044\u308b\u300c\u30ad\u30fc\u300d\u3068\u3044\u30462\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002 `Size`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u6587\u5b57\u6570\u306e\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5e4540\u6587\u5b57\u3001\u9ad8\u30551\u6587\u5b57\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u306e\u6587\u5b57\u5217\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u7a7a\u767d\u306b\u306a\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4f5c\u6210\u3055\u308c\u305f\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306f\u7a7a\u767d\u884c\u304c\u7c21\u5358\u306b\u898b\u308c\u307e\u3059\u3002\n\n\u307e\u305f \u3001[\u7d42\u4e86]\u30dc\u30bf\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306f\u3001\u304a\u306a\u3058\u307f\u306e`window.read()`\u547c\u3073\u51fa\u3057\u3057\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u8aad\u307f\u8fbc\u3093\u3060\u5f8c\u306b\u7d9a\u304f\u306e\u306f\u3001\u3053\u306eif\u6587\u3067\u3059\u3002\n```python\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n```\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c \u300cX\uff08\u9589\u3058\u308b\uff09\u300d \u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u305f\u304b\u3001\u307e\u305f\u306f\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306e\u3044\u305a\u308c\u304b\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u30b3\u30fc\u30c9\u306f\u30a4\u30d9\u30f3\u30c8 \u30eb\u30fc\u30d7\u304b\u3089\u629c\u3051\u51fa\u3057\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305a\u3001\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u304c\u30af\u30ea\u30c3\u30af\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u52d5\u4f5c\u304c\u7d99\u7d9a\u3055\u308c\u307e\u3059\u3002 \u8d77\u3053\u308a\u3046\u308b\u552f\u4e00\u306e\u4e8b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u300cOK\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u3053\u3068\u3067\u3059\u3002 \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306e\u6700\u5f8c\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\n\n\n\n```python\n    window['-OUTPUT-'].update('\u30cf\u30ed\u30fc  ' + values['-INPUT-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n```\n\n\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u3001`-OUTPUT-`\u30ad\u30fc \u3092\u6301\u3064`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u6587\u5b57\u5217\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002`window['-OUTPUT-']`\u306f`-OUTPUT-`\u30ad\u30fc\u3092\u6301\u3064\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u691c\u7d22\u3057\u307e\u3059\u3002 \u30ad\u30fc\u306f\u3001\u7a7a\u767d\u306e`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5c5e\u3057\u307e\u3059\u3002 \u30a8\u30ec\u30e1\u30f3\u30c8\u304c\u691c\u7d22\u304b\u3089\u8fd4\u3055\u308c\u308b\u3068\u3001\u305d\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update`\u30e1\u30bd\u30c3\u30c9\u304c\u547c\u3073\u51fa\u3055\u308c\u307e\u3059\u3002 \u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306f`update`\u30e1\u30bd\u30c3\u30c9\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u5024\u3084\u69cb\u6210\u3092\u5909\u66f4\u3057\u305f\u308a\u3059\u308b\u306e\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u30c6\u30ad\u30b9\u30c8\u3092\u9ec4\u8272\u306b\u3057\u305f\u3044\u5834\u5408\u306f\u3001`update`\u30e1\u30bd\u30c3\u30c9\u306b`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u51e6\u7406\u3057\u307e\u3059\u3002\n```python\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\", text_color='yellow')\n```\n\n`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u305f\u5f8c\u3001\u3053\u308c\u304c\u65b0\u3057\u3044\u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorldYellow.jpg\">\n</p>\n\n\n\u5404\u30a8\u30ec\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306f[call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](http://calls.PySimpleGUI.org)\u3068docstrings \u3068\u4e21\u65b9\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306b\u306f\u3001\u5229\u7528\u53ef\u80fd\u306a\u3059\u3079\u3066\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u8c4a\u5bcc\u306a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002 `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update'`\u30e1\u30bd\u30c3\u30c9\u3092\u691c\u7d22\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9a\u7fa9\u304c\u898b\u3064\u304b\u308a\u307e\u3059:\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/TextUpdate.jpg\">\n</p>\n\n\n\u3054\u89a7\u306e\u3088\u3046\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u3082\u306e\u306f\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306fPySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u7c21\u5358\u306b\u3059\u308b\u8cb4\u91cd\u306a\u30ea\u30bd\u30fc\u30b9\u3067\u3059\u3002\n\n<hr>\n\n##\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u9762\u767d\u3044\u3067\u3059 LOL! :laughing:\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u300c\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u300d(LOL)\u3067\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u300c\u884c\u300d\u306b\u5206\u5272\u3055\u308c\u307e\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5404\u884c\u306f\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u30ea\u30b9\u30c8\u306b\u306a\u308a\u307e\u3059\u3002 \u3059\u3079\u3066\u306e\u30ea\u30b9\u30c8\u3092\u9023\u7d50\u3059\u308b\u3068\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002...\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\u884c\u306e\u5b9a\u7fa9\u65b9\u6cd5\u3092\u7c21\u5358\u306b\u78ba\u8a8d\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5404\u884c\u306b\u8ffd\u52a0\u306e 'Text' \u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u305f\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u3001\u4ee5\u524d\u3068\u540c\u3058\u3067\u3059:\n\n```python\nlayout = [  [sg.Text('\u30e9\u30a4\u30f3 1'), sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\")],\n            [sg.Text('\u30e9\u30a4\u30f3 2'), sg.Input()],\n            [sg.Text('\u30e9\u30a4\u30f3 3'), sg.Button('\u306f\u3044')] ]\n```\n\n\u3053\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5404\u884c\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u306e\u884c\u306b\u8868\u793a\u3055\u308c\u308b\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/rows.jpg\">\n</p>\n\n\n\n\u30ea\u30b9\u30c8\u3092\u4f7f\u7528\u3057\u3066GUI\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u4ed6\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066GUI\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u884c\u3046\u65b9\u6cd5\u306b\u304f\u3089\u3079\u3066\u3044\u304f\u3064\u304b\u5927\u304d\u306a\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001Python \u306e\u30ea\u30b9\u30c8\u5185\u5305\u8868\u8a18\u3092\u5229\u7528\u3057\u3066\u30011 \u884c\u306e\u30b3\u30fc\u30c9\u3067\u30dc\u30bf\u30f3\u306e\u30b0\u30ea\u30c3\u30c9\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\n\n\u6b21\u306e3\u884c\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\nlayout = [[sg.Button(f'{row}, {col}') for col in range(4)] for row in range(4)]\n\nevent, values = sg.Window('List Comprehensions', layout).read(close=True)\n```\n\n\u30dc\u30bf\u30f3\u306e4 x 4\u30b0\u30ea\u30c3\u30c9\u3092\u6301\u3064\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/4x4grid.jpg\">\n</p>\n\n\u300c\u697d\u3057\u3080\u300d\u304c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u306e\uff11\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Python\u306e\u5f37\u529b\u306a\u57fa\u672c\u6a5f\u80fd\u3092GUI\u306e\u554f\u984c\u306b\u76f4\u63a5\u9069\u7528\u3059\u308b\u306e\u306f\u697d\u3057\u3044\u3067\u3059\u3002GUI\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u306e\u30da\u30fc\u30b8\u306e\u4ee3\u308f\u308a\u306b\u3001\u6570\u884c (\u307e\u305f\u306f\u591a\u304f\u306e\u5834\u54081\u884c) \u306e\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n## \u30b3\u30fc\u30c9\u306e\u6298\u308a\u305f\u305f\u307f\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30b3\u30fc\u30c9\u30921\u884c\u306e\u30b3\u30fc\u30c9\u306b\u51dd\u7e2e\u3067\u304d\u307e\u3059\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5b9a\u7fa9\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u3001\u8868\u793a\u3001\u304a\u3088\u3073\u30c7\u30fc\u30bf\u53ce\u96c6\u306f\u3059\u3079\u3066\u3001\u6b21\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u307e\u3059\u3002\n```python\nevent, values = sg.Window('Window Title', [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],[sg.Input()],[sg.Button('\u306f\u3044')]]).read(close=True)\n```\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourName.jpg\">\n</p>\n\n\n\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u8868\u793a\u3055\u308c\u3001PySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u793a\u3059\u4f8b\u3068\u540c\u3058\u5024\u304c\u8fd4\u3055\u308c\u307e\u3059\u3002 \u975e\u5e38\u306b\u5c11\u306a\u3044\u91cf\u3067\u591a\u304f\u306e\u3053\u3068\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001Python\u30b3\u30fc\u30c9\u306b\u3059\u3070\u3084\u304f\u7c21\u5358\u306bGUI\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u9078\u629e\u3092\u5f97\u305f\u3044\u5834\u5408\u306f\u30011\u30da\u30fc\u30b8\u306e\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u77ed\u7e2e\u30a8\u30a4\u30ea\u30a2\u30b9\u3092\u4f7f\u7528\u3057\u3066\u3088\u308a\u5c11\u306a\u3044\u6587\u5b57\u6570\u3067\u30b3\u30fc\u30c9\u306e\u30b9\u30da\u30fc\u30b9\u3092\u3055\u3089\u306b\u77ed\u304f\u3067\u304d\u307e\u3059\u3002  \u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u306f\u3001\u4f7f\u7528\u3067\u304d\u308b\u77ed\u3044\u540d\u524d\u304c\uff11\u3064\u4ee5\u4e0a\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5358\u306b`T`\u3068\u3057\u3066\u66f8\u3051\u307e\u3059\u3002`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f `I`\u3001`Button`\u306f`B`\u3068\u66f8\u3051\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306b\u306b\u306a\u308a\u307e\u3059:\n\n```python\nevent, values = sg.Window('Window Title', [[sg.T(\"\u3042\u306a\u305f\u306e\u540d\u524d\u306f\u4f55\u3067\u3059\u304b?\")],[sg.I()],[sg.B('\u306f\u3044')]]).read(close=True)\n```\n\n\n### \u30b3\u30fc\u30c9\u306e\u79fb\u690d\u6027\n\nPySimpleGUI\u306f\u73fe\u5728\u30014\u3064\u306ePython\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002 \u4f7f\u7528\u3059\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001import\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u6307\u5b9a\u3057\u307e\u3059\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3092\u5909\u66f4\u3059\u308b\u3068\u3001\u57fa\u672c\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3088\u3063\u3066\u306f\u3001\u5225\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306fimport \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u4ee5\u5916\u306e\u5909\u66f4\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002 \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u3001\u30a4\u30f3\u30dd\u30fc\u30c8\u3092`PySimpleGUI`\u304b\u3089`PySimpleGUIQt`\u3001`PySimpleGUIWx`\u3001`PySimpleGUIWeb`\u3001`PySimpleGUIWeb`\u306b\u5909\u66f4\u3059\u308b\u3068\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\n\n| \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 | \u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6 |\n|--|--|\n| PySimpleGUI |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-tkinter.jpg) |\n| PySimpleGUIQt |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Qt.jpg) |\n| PySimpleGUIWx |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-WxPython.jpg) |\n| PySimpleGUIWeb |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Remi.jpg) |\n\n\n\nGUI\u306e\u30b3\u30fc\u30c9\u3092\u3042\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u5225\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u79fb\u690d\u3059\u308b (\u4f8b\u3048\u3070\u3001\u30b3\u30fc\u30c9\u3092tkinter\u304b\u3089Qt\u306b\u79fb\u52d5\u3059\u308b) \u306b\u306f\u3001\u901a\u5e38\u306f\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u63db\u3048\u304c\u5fc5\u8981\u3067\u3059\u3002  PySimpleGUI \u306f\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u306e\u7c21\u5358\u306a\u79fb\u52d5\u3092\u53ef\u80fd\u306b\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3044\u304f\u3064\u304b\u306e\u5909\u66f4\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u76ee\u7684\u306f\u6700\u5c0f\u9650\u306e\u5909\u66f4\u3067\u79fb\u690d\u6027\u306e\u9ad8\u3044\u30b3\u30fc\u30c9\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \n\n\u30b7\u30b9\u30c6\u30e0 \u30c8\u30ec\u30a4 \u30a2\u30a4\u30b3\u30f3\u306a\u3069\u306e\u4e00\u90e8\u306e\u6a5f\u80fd\u306f\u3001\u3059\u3079\u3066\u306e\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u306a\u3044\u3067\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u6a5f\u80fd\u306fQt\u304a\u3088\u3073WxPython\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306ftkinter\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u306f\u3001PySimpleGUIWeb\u30dd\u30fc\u30c8\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u307e\u305b\u3093\u3002\n\n##  \u30e9\u30f3\u30bf\u30a4\u30e0\u74b0\u5883\n\n|\u74b0\u5883 |\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b |\n|--|--|\n|\u30d1\u30a4\u30bd\u30f3| Python  3.4+ |\n|\u30aa\u30da\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0 \u30b7\u30b9\u30c6\u30e0 |\u30a6\u30a3\u30f3\u30c9\u30a6\u30ba, Linux, \u30de\u30c3\u30af |\n|\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2 |\u30c7\u30b9\u30af\u30c8\u30c3\u30d7 PC, \u30ce\u30fc\u30c8\u30d1\u30bd\u30b3\u30f3, \u30e9\u30ba\u30d9\u30ea\u30fc\u30d1\u30a4, PyDroid3 \u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30a2\u30f3\u30c9\u30ed\u30a4\u30c9\u30c7\u30d0\u30a4\u30b9 |\n|\u30aa\u30f3\u30e9\u30a4\u30f3 |repli.it\u3001Trinket.com (\u3069\u3061\u3089\u3082\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067tkinter\u3092\u5b9f\u884c\u3059\u308b) |\n|GUI \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af |tkinter, pyside2, WxPython, Remi |\n\n\n## \u7d71\u5408\n200 \u4ee5\u4e0a\u306e\u300c\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u300d\u306e\u4e2d\u306b\u306f\u3001\u591a\u304f\u306e\u4eba\u6c17\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u3092GUI\u306b\u7d71\u5408\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u304c\u898b\u3064\u304b\u308a\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306bMatplotlib\u306e\u63cf\u753b\u3092\u57cb\u3081\u8fbc\u307f\u305f\u3044\u3067\u3059\u304b?  \u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3001 \u30c7\u30e2\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5373\u5ea7\u306b\u3042\u306a\u305f\u306e\u5922\u306eMatplotlib\u306e\u63cf\u753b\u3092\u3042\u306a\u305f\u306eGUI\u306b\u7d44\u307f\u8fbc\u3081\u307e\u3059\u3002  \n\n\u3053\u308c\u3089\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3084\u305d\u306e\u4ed6\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3001\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30c7\u30e2\u30ec\u30dd\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001GUI\u306b\u5165\u308c\u308b\u6e96\u5099\u304c\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\n|\u30d1\u30c3\u30b1\u30fc\u30b8 |\u8aac\u660e |\n|--|--|\n Matplotlib |\u30b0\u30e9\u30d5\u3084\u30d7\u30ed\u30c3\u30c8\u306e\u591a\u304f\u306e\u7a2e\u985e |\n OpenCV |\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3 (AI\u3067\u3088\u304f\u4f7f\u7528) |\n VLC |\u30d3\u30c7\u30aa\u518d\u751f |\n pymunk |\u7269\u7406\u30a8\u30f3\u30b8\u30f3|\n psutil |\u30b7\u30b9\u30c6\u30e0\u74b0\u5883\u306e\u7d71\u8a08 |\n prawn |Reddit  API |\njson |PySimpleGUI \u306f\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u8a2d\u5b9a\u300d\u3092\u683c\u7d0d\u3059\u308b\u7279\u5225\u306aAPI\u3092\u30e9\u30c3\u30d7\u3057\u307e\u3059\u3002 |\n weather |\u304a\u5929\u6c17\u30a2\u30d7\u30ea\u3092\u4f5c\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u5929\u6c17API\u3068\u7d71\u5408 |\n mido |MIDI \u518d\u751f |\n beautiful soup |\u30a6\u30a7\u30d6\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0 (GitHub issue\u30a6\u30a9\u30c3\u30c1\u30e3\u30fc\u3067\u306e\u4f8b) |\n\n<hr>\n\n# \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb :floppy_disk:\n\n\nPySimpleGUI\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4e00\u822c\u7684\u306b2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\n\n1. PyPI\u304b\u3089pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\n2. PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u914d\u7f6e\u3057\u307e\u3059\n\n\n### Pip\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n\u73fe\u5728\u63d0\u6848\u3055\u308c\u3066\u3044\u308b`pip`\u30b3\u30de\u30f3\u30c9\u3092\u547c\u3073\u51fa\u3059\u65b9\u6cd5\u306f\u3001Python\u3092\u4f7f\u3063\u3066\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u4ee5\u524d\u306f\u3001`pip`\u307e\u305f\u306f`pip3`\u30b3\u30de\u30f3\u30c9\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3/\u30b7\u30a7\u30eb\u4e0a\u3067\n\u76f4\u63a5\u5b9f\u884c\u3055\u308c\u307e\u3057\u305f\u3002 \u63d0\u6848\u3055\u308c\u305f\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\nWindows \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python -m pip install PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python3 -m pip install PySimpleGUI`\n\n`pip`\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3059\u308b\u306b\u306f\u3001\u5358\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf`--upgrade --no-cache-dir`\u3092\u6307\u5b9a\u3059\u308b\u3060\u3051\u3067\u3059\u3002\n\nWindows \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python3 -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\n\n### \u5358\u4e00\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nPySimpleGUI\u306fRaspberry Pi \u306e\u3088\u3046\u306a\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306b\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u30b7\u30b9\u30c6\u30e0\u306b\u3082\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5358\u4e00\u306e .py \u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u306b\u7f6e\u304f\u3060\u3051\u3067\u3059\u3002Python \u306f\u30a4\u30f3\u30dd\u30fc\u30c8\u6642\u306b\u30ed\u30fc\u30ab\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n.py\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f\u3001PyPI\u304b\u3089\u5165\u624b\u3059\u308b\u304b\u3001\u6700\u65b0\u306e\u672a\u30ea\u30ea\u30fc\u30b9\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u306fGitHub\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\nPyPI\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306b\u306f\u3001wheel\u307e\u305f\u306f .gz \u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u89e3\u51cd\u3057\u307e\u3059\u3002 .whl\u30d5\u30a1\u30a4\u30eb\u3092.zip\u306b\u30ea\u30cd\u30fc\u30e0\u3059\u308b\u3068\u3001\u901a\u5e38\u306ezip\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u3058\u3088\u3046\u306b\u958b\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d5\u30a9\u30eb\u30c0\u306e\u4e2d\u306bPySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5b8c\u4e86\u3067\u3059\u3002\n\ntkinter \u30d0\u30fc\u30b8\u30e7\u30f3\u306e PySimpleGUI \u306e PyPI \u30ea\u30f3\u30af\u3067\u3059\nhttps://pypi.org/project/PySimpleGUI/#files\n\nGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u3001\u3053\u3061\u3089\u3067\u78ba\u8a8d\u3067\u304d\u307e\u3059\nhttps://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/PySimpleGUI.py\n\n\n\u300c\u305d\u3046\u3060\u3051\u3069\u3001\u5de8\u5927\u306a\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u3064\u3060\u3051\u6301\u3064\u306e\u306f\u306a\u3093\u3066\u3072\u3069\u3044\u8003\u3048\u3067\u3059\u300d\u3068\u4eca\u3001\u8003\u3048\u3066\u3044\u308b\u4eba\u3082\u3044\u308b\u3067\u3057\u3087\u3046\u3002 \u3053\u308c\u306f*\u6642\u306b\u306f*\u3072\u3069\u3044\u8003\u3048\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u4eca\u56de\u306f\u3001\u30e1\u30ea\u30c3\u30c8\u306f\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u5927\u5e45\u306b\u4e0a\u56de\u308a\u307e\u3057\u305f\u3002 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6982\u5ff5\u306e\u591a\u304f\u306f\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u307e\u305f\u306f\u4e3b\u89b3\u7684\u306a\u3082\u306e\u3067\u3059\u3002 \u4e00\u90e8\u306e\u4eba\u304c\u671b\u3080\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u3001\u3059\u3079\u3066\u304c\u767d\u9ed2\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u591a\u304f\u306e\u5834\u5408\u3001\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u306f\u300c\u6b21\u7b2c\u300d\u3067\u3059\u3002\n\n\n\n## \u30ae\u30e3\u30e9\u30ea\u30fc :art:\n\n\u30e6\u30fc\u30b6\u30fc\u304c\u6295\u7a3f\u3057\u305fGUI\u3068GitHub\u306b\u3042\u308bGUI\u306e\u3088\u308a\u6b63\u5f0f\u306a\u30ae\u30e3\u30e9\u30ea\u30fc\u306e\u4f5c\u6210\u306f\u9032\u884c\u4e2d\u3067\u3059\u304c\u3001readme\u3092\u4f5c\u6210\u6642\u70b9\u3067\u306f\u307e\u3060\u5b8c\u6210\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u73fe\u5728\u307e\u3068\u307e\u3063\u3066\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u898b\u308c\u308b\u5834\u6240\u306f2\u304b\u6240\u3042\u308a\u307e\u3059\u3002\u9858\u308f\u304f\u3070\u4eba\u3005\u304c\u4f5c\u3063\u3066\u3044\u308b\u7d20\u6674\u3089\u3057\u3044\u4f5c\u54c1\u3092\u6b63\u5f53\u5316\u3059\u308b\u305f\u3081\u306eWiki\u3084\u305d\u306e\u4ed6\u306e\u4ed5\u7d44\u307f\u304c\u3059\u3050\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002\n\n### \u30e6\u30fc\u30b6\u30fc\u304c\u63d0\u51fa\u3057\u305f\u30ae\u30e3\u30e9\u30ea\u30fc\n\n1\u3064\u76ee\u306f\u3001GitHub\u306b\u3042\u308b[\u30e6\u30fc\u30b6\u30fc\u304c\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u63d0\u51fa\u3057\u305fissue](https://github.com/PySimpleGUI/PySimpleGUI/issues/10)\u3067\u3059\u3002 \u3053\u308c\u306f\u3001\u4eba\u3005\u304c\u4f5c\u3063\u305f\u3082\u306e\u3092\u62ab\u9732\u3059\u308b\u305f\u3081\u306e\u975e\u516c\u5f0f\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u7406\u60f3\u7684\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u30b9\u30bf\u30fc\u30c8\u3067\u3057\u305f\u3002\n\n### \u5927\u91cf\u306b\u30b9\u30af\u30e9\u30c3\u30d7\u3055\u308c\u305fGitHub\u306e\u753b\u50cf\n\n2\u3064\u76ee\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u4f1d\u3048\u3089\u308c\u3066\u3044\u308bGitHub\u306e1,000\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u96c6\u3081\u305f[3,000\u4ee5\u4e0a\u306e\u753b\u50cf\u306e\u5927\u898f\u6a21\u306a\u30ae\u30e3\u30e9\u30ea\u30fc ](https://www.dropbox.com/sh/g67ms0darox0i2p/AAAMrkIM6C64nwHLDkboCWnaa?dl=0)\u3067\u3059\u3002 \u624b\u4f5c\u696d\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u3066\u304a\u308a\u521d\u671f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u305f\u53e4\u3044\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u3053\u306b\u3042\u306a\u305f\u306e\u60f3\u50cf\u529b\u3092\u5f15\u304d\u8d77\u3053\u3059\u4f55\u304b\u304c\u898b\u3064\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n<hr>\n\n# PySimpleGUI \u306e\u7528\u9014\u3067\u3059 :hammer:\n\n\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001PySimpleGUI\u306e\u7528\u9014\u306e\u4e00\u90e8\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 GitHub \u3060\u3051\u3067\u30821,000 \u4ee5\u4e0a\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067PySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u672c\u5f53\u306b\u3053\u308c\u3060\u3051\u306e\u591a\u304f\u306e\u4eba\u3005\u306e\u53ef\u80fd\u6027\u304c\u5e83\u304c\u3063\u305f\u3053\u3068\u306f\u306f\u672c\u5f53\u306b\u9a5a\u304f\u3079\u304d\u3053\u3068\u3067\u3059\u3002 \u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u4ee5\u524d\u306bPython\u3067GUI\u3092\u4f5c\u6210\u3057\u3088\u3046\u3068\u3057\u3066\u5931\u6557\u3057\u3057\u305f\u3068\u8a71\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5f7c\u3089\u304cPySimpleGUI\u3092\u8a66\u3057\u3066\u307f\u305f\u3068\u304d\u306b\u6700\u7d42\u7684\u306b\u81ea\u5206\u306e\u5922\u3092\u9054\u6210\u3057\u305f\u3068\u8a71\u3092\u3057\u307e\u3057\u305f\u3002\n\n## \u6700\u521d\u306eGUI\n\n\u3082\u3061\u308d\u3093\u3001PySimpleGUI\u306e\u6700\u3082\u512a\u308c\u305f\u4f7f\u3044\u65b9\u306e\u4e00\u3064\u306fPython\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u3060\u3051\u306e\u5c0f\u3055\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u3089\u958b\u59cb\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u305f\u3081\u306b\u306f\u3001`popup`\u3068\u547c\u3070\u308c\u308b\u300c\u30cf\u30a4\u30ec\u30d9\u30eb\u95a2\u6570\u300d\u306e1\u3064\u30921\u56de\u547c\u3073\u51fa\u3059\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002 \u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u306b\u306f\u3042\u3089\u3086\u308b\u7a2e\u985e\u304c\u3042\u308a\u3001\u4e00\u90e8\u306f\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\n\n`popup`\u81ea\u4f53\u3067\u60c5\u5831\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002print\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6e21\u305b\u307e\u3059\u3002\u60c5\u5831\u3092\u53d6\u5f97\u3057\u305f\u3044\u5834\u5408\u306f\u3001`popup_get_filename`\u306e\u3088\u3046\u306b`popup_get_\u3067`\u59cb\u307e\u308b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3057\u307e\u3059\u3002\n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e1\u884c\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u300c\u666e\u901a\u306e\u4eba\u300d\u304c\u5feb\u9069\u306b\u4f7f\u7528\u3067\u304d\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5909\u8eab\u3057\u307e\u3059\u3002\n\n\n```python\nimport PySimpleGUI as sg\n\nfilename = sg.popup_get_file('\u51e6\u7406\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044')\nsg.popup('\u5165\u529b\u3057\u305f', filename)\n```\n\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u30012\u3064\u306e\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u307e\u3059\u3002 1\u3064\u306f\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u3067\u3001\u5165\u529b\u30dc\u30c3\u30af\u30b9\u306e\u95b2\u89a7\u3084\u30da\u30fc\u30b9\u30c8\u304c\u3067\u304d\u307e\u3059\u3002  \n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\u3082\u3046\u4e00\u65b9\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u53ce\u96c6\u3055\u308c\u305f\u5185\u5bb9\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"  alt=\"img\" width=\"175px\"></a>\n\n</p>\n\n\n<br>\n\n## Rainmeter\u98a8\u30b9\u30bf\u30a4\u30eb\u30a6\u30a3\u30f3\u30c9\u30a6\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u898b\u6804\u3048\u306e\u826f\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u4f5c\u6210\u3067\u304d\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u7d30\u90e8\u306b\u6ce8\u610f\u3059\u308b\u3053\u3068\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9b45\u529b\u7684\u306b\u898b\u305b\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u3053\u3068\u3092\u304a\u3053\u306a\u3048\u307e\u3059\u3002 PySimpleGUI\u306f\u3001\u8272\u3084\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u306a\u3069\u306e\u6a5f\u80fd\u3092\u3088\u308a\u7c21\u5358\u306b\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002 \u305d\u306e\u7d50\u679c\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u3092Windows\u3067\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u3053\u306e\u4f8b\u3067\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30bf\u30a4\u30c8\u30eb \u30d0\u30fc\u304c\u524a\u9664\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u7d50\u679c\u3068\u3057\u3066\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u306eRainmeter\u3088\u3046\u306b\u898b\u3048\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002\n\n<br><br>\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u900f\u660e\u5ea6\u3082\u7c21\u5358\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002 \u540c\u3058Rainmeter\u30b9\u30bf\u30a4\u30eb\u306e\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u4ed6\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u534a\u900f\u660e\u306a\u306e\u3067\u3001\u8584\u6697\u304f\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\n\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u3068\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u534a\u900f\u660e\u5316\u306e\u4e21\u65b9\u306e\u52b9\u679c\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306fPySimpleGUI\u304c\u3044\u304b\u306b\u6a5f\u80fd\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u304b\u3092\u793a\u3059\u4f8b\u3067\u3059\u3002 \u307e\u305f\u3001PySimpleGUI \u306e\u30b3\u30fc\u30c9\u306fGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u3067\u79fb\u690d\u53ef\u80fd\u306a\u306e\u3067\u3001Qt\u306e\u3088\u3046\u306a\u4ed6\u306e\u30dd\u30fc\u30c8\u3067\u3082\u540c\u3058\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002\n\n\n\u4f8b\uff11\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u306e\u547c\u3073\u51fa\u3057\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u5909\u66f4\u3059\u308b\u3068\u540c\u69d8\u306e\u534a\u900f\u660e\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\n```python\nwindow = sg.Window('My window', layout, no_titlebar=True, alpha_channel=0.5)\n```\n\n## \u30b2\u30fc\u30e0\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306e SDK \u3068\u3057\u3066\u306f\u7279\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u3053\u306e\u30c1\u30a7\u30b9\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30c1\u30a7\u30b9\u3092\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u30c1\u30a7\u30b9AI\u300cStockfish\u300d\u3092\u7d71\u5408\u3057\u307e\u3059\u3002\n<br><br><br><br><br><br><br><br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u306e\u3044\u304f\u3064\u304b\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\n\n<br><br><br><br>\n<br><br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br><br><br><br><br><br><br><br><br>\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br>\n\nPySimpleGUI\u306e`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u3068\u753b\u50cf\u306e\u64cd\u4f5c\u304c\u7c21\u5358\u306a\u306e\u3067\u3001\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306eSDK\u3068\u3057\u3066\u66f8\u304b\u308c\u305f\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002<br><br>\n<br><br>\n<br><br><br>\n\n\n## \u30e1\u30c7\u30a3\u30a2\u306e\u30ad\u30e3\u30d7\u30c1\u30e3\u3068\u518d\u751f\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"  alt=\"img\" align=\"right\" width=\"400px\"></a>\n\n\nWEB\u30ab\u30e1\u30e9\u304b\u3089\u30d3\u30c7\u30aa\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3057\u3066GUI\u3067\u8868\u793a\u3059\u308b\u306e\u306b\u306f\u3001PySimpleGUI\u306e\u30b3\u30fc\u30c9\u3067\u306f4\u884c\u3067\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u5370\u8c61\u7684\u306a\u306e\u306f\u3053\u3089\u306e4\u884c\u306e\u30b3\u30fc\u30c9\u304c tkinter\u3001Qt\u3001\u304a\u3088\u3073 Web \u30dd\u30fc\u30c8\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002  tkinter\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u8868\u793a\u3059\u308b\u306e\u3068\u540c\u3058\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30d6\u30e9\u30a6\u30b6\u3067Web\u30ab\u30e1\u30e9\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u304c\u8868\u793a\u3067\u304d\u307e\u3059\u3002\n\n\u307e\u305f\u3001VLC\u30d7\u30ec\u30fc\u30e4\u30fc\u3092\u4f7f\u3063\u3066\u3001\u30aa\u30fc\u30c7\u30a3\u30aa\u3084\u30d3\u30c7\u30aa\u306a\u3069\u306e\u30e1\u30c7\u30a3\u30a2\u518d\u751f\u3082\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30e2\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5b9f\u969b\u306e\u4f5c\u696d\u4f8b\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u306ereadme\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u306f\u5168\u3066\u3001\u3042\u306a\u305f\u81ea\u8eab\u306e\u5275\u4f5c\u306e\u51fa\u767a\u70b9\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u307e\u3059\u3002\n<br><br><br><br><br>\n<br><br><br><br><br>\n<br><br>\n## \u4eba\u5de5\u77e5\u80fd\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\nAI\u3068Python\u306f\u9577\u3044\u9593\u3001\u3053\u306e2\u3064\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u305f\u3068\u304d\u306e\u30b9\u30fc\u30d1\u30fc\u30d1\u30ef\u30fc\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u591a\u304f\u306e\u5834\u5408\u3001\u30e6\u30fc\u30b6\u30fc\u304cGUI\u3092\u4f7f\u7528\u3057\u3066\u3053\u308c\u3089\u306eAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8eab\u8fd1\u306b\u64cd\u4f5c\u3059\u308b\u65b9\u6cd5\u304c\u6b20\u3051\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u308c\u3089\u306eYOLO\u306e\u30c7\u30e2\u306f\u3001GUI\u304cAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u306e\u5bfe\u8a71\u306b\u304a\u3044\u3066\u3044\u304b\u306b\u5927\u304d\u306a\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u304b\u306e\u7d20\u6674\u3089\u3057\u3044\u4f8b\u3067\u3059\u3002 \u3053\u308c\u3089\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e0b\u90e8\u306b\u3042\u308b2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306f\u3001YOLO\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u4f7f\u7528\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066YOLO\u30c7\u30e2\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5834\u5408\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u8d77\u52d5\u3059\u308b\u3068\u304d\u306b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8a2d\u5b9a\u3057\u3001\u305d\u306e\u5b9f\u884c\u65b9\u6cd5\u3092\u78ba\u8a8d\u3057\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u505c\u6b62\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5909\u66f4\u3057\u3001\u6700\u5f8c\u306b\u65b0\u3057\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n<br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u3053\u308c\u3089\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3001GUI\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u884c\u3067\u304d\u308b\u64cd\u4f5c\u3068\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\u3002 GUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u3001\u3059\u3050\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\n\n<br><br><br><br><br>\n<br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u516c\u958b\u3055\u308c\u3066\u3044\u308bAI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u52d5\u304b\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u975e\u5e38\u306b\u591a\u304f\u5b58\u5728\u3057\u307e\u3059\u3002 \u3053\u308c\u81ea\u4f53\u306f\u5927\u304d\u306a\u30cf\u30fc\u30c9\u30eb\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5165\u529b/\u8cbc\u308a\u4ed8\u3051\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u3066\u3001\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30d3\u30e5\u30fc\u30a2\u3067\u958b\u304f\u306b\u306f\u5341\u5206\u300c\u9762\u5012\u304f\u3055\u3044\u300d\u3067\u3059\u3002\n\n\nGUI \u306b\u306f\u3001**\u30e6\u30fc\u30b6\u30fc\u30a8\u30af\u30b9\u30da\u30ea\u30a8\u30f3\u30b9\u3092\u5909\u66f4\u3059\u308b**\u3092\u300cGUI\u30ae\u30e3\u30c3\u30d7\u300d\u306b\u5909\u5316\u3055\u305b\u308b\u529b\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306f\u753b\u50cf\u304c\u683c\u7d0d\u3055\u308c\u3066\u305f\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3057\u3066\u3001\u753b\u50cf\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3068\u7d50\u679c\u8868\u793a\u306e\u4e21\u65b9\u3092\u884c\u3048\u307e\u3059\u3002  \n\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0/\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u81ea\u7531\u306b\u5229\u7528\u53ef\u80fd\u3067\u3001\u4f7f\u7528\u53ef\u80fd\u3067\u3057\u305f\u3002 \u4e0d\u8db3\u3057\u3066\u3044\u305f\u306e\u306fGUI\u304c\u3082\u305f\u3089\u3059\u4f7f\u3044\u3084\u3059\u3055\u3067\u3059\u3002\n\n\n<hr>\n\n## \u30b0\u30e9\u30d5\u5316\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\nGUI\u3067\u306e\u30c7\u30fc\u30bf\u306e\u8868\u793a\u3068\u64cd\u4f5c\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\u3044\u304f\u3064\u304b\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002\n\u7d44\u307f\u8fbc\u307f\u306e\u63cf\u753b/\u30b0\u30e9\u30d5\u4f5c\u6210\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30ab\u30b9\u30bf\u30e0\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u3053\u306eCPU\u4f7f\u7528\u7387\u30e2\u30cb\u30bf\u306f`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nMatplotlib\u306fPython\u30e6\u30fc\u30b6\u30fc\u306b\u4eba\u6c17\u304c\u3042\u308a\u307e\u3059\u3002 PySimpleGUI\u306f\u3001Matplotlib\u306e\u30b0\u30e9\u30d5\u3092GUI\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u57cb\u3081\u8fbc\u3081\u307e\u3059\u3002 Matplotlib\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u6a5f\u80fd\u3092\u4fdd\u6301\u3057\u305f\u3044\u5834\u5408\u306f\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u57cb\u3081\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nPySimpleGUI\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u306e\u4eba\u304cMatplotlib\u3067\u4f5c\u6210\u3059\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u3088\u308a\u3082\u4e00\u6bb5\u4e0a\u306e\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<hr>\n\n## \u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u524d\u8ff0\u306e\u300cGUI \u30ae\u30e3\u30c3\u30d7\u300d\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002 GUI\u3092\u8ffd\u52a0\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u7528\u610f\u3059\u308b\u5fc5\u8981\u3082\u3042\u308a\u307e\u305b\u3093\u3002 \u300c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u300dGUI \u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u6e21\u3059\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308bGUI\u3067\u3059\u3002\n\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9GUI \u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3068\u3063\u3066\u30e6\u30fc\u30b6\u30fc\u304c\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30fb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u4f7f\u3044\u5fc3\u5730\u3088\u304f\u611f\u3058\u306a\u304b\u3063\u305f\u305f\u3081\u306b\u3001\u4ee5\u524d\u306f\u4f7f\u3044\u305f\u304c\u3089\u306a\u304b\u3063\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u914d\u5e03\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u3089\u306eGUI\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u305f\u3081\u306e\u552f\u4e00\u306e\u9078\u629e\u80a2\u3067\u3059\u3002\n\u3053\u306e\u4f8b\u306f\u3001\u300cJump Cutter\u300d\u3068\u3044\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u3067\u3059\u3002 \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306fGUI\u3092\u3068\u304a\u3057\u3066\u53ce\u96c6\u3055\u308c\u3066\u3001\u305d\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304c\u69cb\u7bc9\u3055\u308c\u3066\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u304cGUI\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3055\u308c\u3066\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001\u5b9f\u884c\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u304c\u9ec4\u8272\u3067\u8868\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n<br><br>\n<hr>\n\n## Raspberry Pi\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nPySimpleGUI\u306fPython 3.4\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u305f\u3081\u3001Raspberry Pi\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u30bf\u30c3\u30c1\u30b9\u30af\u30ea\u30fc\u30f3\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u7279\u306b\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u3002 \u30e2\u30cb\u30bf\u30fc\u304c\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001PySimpleGUIWeb\u3092\u4f7f\u7528\u3057\u3066Pi\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br><br>\n<hr>\n\n\n## \u9ad8\u5ea6\u306a\u6a5f\u80fd\u3078\u306e\u7c21\u5358\u306a\u30a2\u30af\u30bb\u30b9\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u57fa\u790e\u3068\u306a\u308b GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u591a\u304f\u306b\u975e\u5e38\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u305f\u3081\u3001GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u76f4\u63a5\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u307e\u3059\u3002\n\n\u305f\u3068\u3048\u3070\u3001tkinter\u3084\u305d\u306e\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3057\u3066\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u8272\u3084\u5916\u898b\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u304c\u3001PySimpleGUI \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30ab\u30b9\u30bf\u30e0\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u3092\u6301\u3063\u3066\u3044\u308b\u304b\u306e\u3088\u3046\u306b\u8868\u793a\u3055\u308c\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n<br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u4fe1\u3058\u3089\u308c\u306a\u3044\u3053\u3068\u306b\u3001\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u30b9\u30af\u30ea\u30fc\u30f3\u30bb\u30fc\u30d0\u30fc\u306e\u3088\u3046\u306b\u898b\u3048\u308b\u3082\u306e\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306btkinter\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306ftkinter \u306f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u80cc\u666f\u3092\u5b8c\u5168\u306b\u53d6\u308a\u9664\u3051\u307e\u3059\u3002 \u7e70\u308a\u8fd4\u3057\u307e\u3059\u304cPySimpleGUI\u306f\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3092\u7c21\u5358\u306b\u3057\u307e\u3059\u3002 \u900f\u660e\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001`Window`\u3092\u4f5c\u6210\u3059\u308b\u547c\u3073\u51fa\u3057\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u30921\u3064\u8ffd\u52a0\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 1\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u66f4\u3060\u3051\u3067\u3001\u6b21\u306e\u52b9\u679c\u3092\u6301\u3064\u5358\u7d14\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u4e0a\u306e\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u30d5\u30eb\u30b9\u30af\u30ea\u30fc\u30f3\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002\n<hr>\n\n# \u30c6\u30fc\u30de\n\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30ec\u30fc\u306eGUI\u306b\u3046\u3093\u3056\u308a\u3057\u307e\u3057\u305f\u304b?  PySimpleGUI \u306f`theme`\u95a2\u6570\u306e\u547c\u3073\u51fa\u3057\u3092\u884c\u3046\u3053\u3060\u3051\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u898b\u305f\u76ee\u3092\u7d20\u6575\u306b\u3057\u307e\u3059\u3002 150\u7a2e\u985e\u4ee5\u4e0a\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u9078\u629e\u3067\u304d\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ThemePreview.jpg\"  alt=\"img\" width=\"900px\"></a>\n</p>\n\n\n\u307b\u3068\u3093\u3069\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u306f\u3001\u4f5c\u6210\u3059\u308b\u3059\u3079\u3066\u306e\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002  PySimpleGUI\u306f\u3001\u3053\u306e\u96d1\u7528\u3092\u4ee3\u308f\u308a\u306b\u884c\u3044\u81ea\u52d5\u7684\u306b\u9078\u629e\u3057\u305f\u30c6\u30fc\u30de\u306b\u5408\u308f\u305b\u3066\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8272\u4ed8\u3051\u3057\u307e\u3059\u3002\n\n\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u524d\u306b\u30c6\u30fc\u30de\u540d\u3092\u6307\u5b9a\u3057\u3066`theme`\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u307e\u3059\u3002\u8aad\u307f\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c6\u30fc\u30de\u3092\u300cdark grey 9\u300d\u306b\u8a2d\u5b9a\u3059\u308b\u306b\u306f\n```python\nimport PySimpleGUI as sg\n\nsg.theme('dark grey 9')\n```\n\n\u3053\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5916\u89b3\u3092\u5b8c\u5168\u306b\u5909\u66f4\u3057\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/DarkGreyJapanese.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\n\u30c6\u30fc\u30de\u306f\u3001\u80cc\u666f\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u5165\u529b\u80cc\u666f\u3001\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3001\u304a\u3088\u3073\u30dc\u30bf\u30f3\u306e\u8272\u3092\u5909\u66f4\u3057\u307e\u3057\u305f\u3002 \u3053\u306e\u3088\u3046\u306a\u914d\u8272\u3092\u5909\u66f4\u3059\u308b\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u306f\u3001\u5404\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u500b\u5225\u306b\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u30b3\u30fc\u30c9\u3092\u4f55\u5ea6\u3082\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n<hr>\n\n# \u30b5\u30dd\u30fc\u30c8:muscle:\n\n\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3](http://www.PySimpleGUI.org)\u3068[\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0](http://Demos.PySimpleGUI.org)\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u3067\u3059\u3002\u3082\u3057\u307e\u3060\u8cea\u554f\u304c\u3042\u3063\u305f\u308a\u3001\u30d8\u30eb\u30d7\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f...\u554f\u984c\u3042\u308a\u307e\u305b\u3093...\u30d8\u30eb\u30d7\u306f\u7121\u6599\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3067[Issue\u3092\u63d0\u51fa](http://Issues.PySimpleGUI.org)\u3059\u308b\u3060\u3051\u3067\u3001\u52a9\u3051\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\u307b\u3068\u3093\u3069\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4f1a\u793e\u306f\u3001\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306b\u4ed8\u968f\u3059\u308b\u30d5\u30a9\u30fc\u30e0\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u306f\u60aa\u3044\u53d6\u5f15\u3067\u306f\u3042\u308a\u307e\u305b\u3093.\u30d5\u30a9\u30fc\u30e0\u306b\u5fc5\u8981\u4e8b\u9805\u8a18\u5165\u3059\u308c\u3070\u3001\u7121\u6599\u3067\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u60c5\u5831\u306f\u52b9\u7387\u7684\u306b\u56de\u7b54\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\nPySimpleGUI\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u756a\u53f7\u3084\u57fa\u306b\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3069\u306e\u60c5\u5831\u3092\u8981\u6c42\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u554f\u984c\u306e\u89e3\u6c7a\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u30ea\u30b9\u30c8\u3082\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\n***\u30d5\u30a9\u30fc\u30e0\u306b\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044 \u3002*** \u3000\u3042\u306a\u305f\u306b\u306f\u7121\u610f\u5473\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u307b\u3093\u306e\u4e00\u77ac\u3067\u3059\u304c\u82e6\u75db\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u8a18\u5165\u306f\u3042\u306a\u305f\u304c\u3088\u308a\u65e9\u304f\u89e3\u6c7a\u7b56\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u3082\u3057\u3042\u306a\u305f\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u30fc\u306a\u56de\u7b54\u3068\u89e3\u6c7a\u3092\u5f97\u308b\u305f\u3081\u306b\u5f79\u7acb\u3064\u5fc5\u8981\u306a\u60c5\u5831\u3067\u306a\u3051\u308c\u3070\u3001\u8a18\u5165\u306f\u5fc5\u8981\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u79c1\u306f\u3042\u306a\u305f\u3092\u52a9\u3051\u308b\u305f\u3081\u306b\u79c1\u3092\u52a9\u3051\u308b\u300d\u3002\n\n\n# \u30b5\u30dd\u30fc\u30c8\t<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"  alt=\"img\"  width=\"90px\"></a>\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8ca1\u653f\u7684\u652f\u63f4\u306f\u975e\u5e38\u306b\u9ad8\u304f\u8a55\u4fa1\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u6b63\u76f4\u306b\u8a00\u3046\u3068\u3001\u7d4c\u6e08\u7684\u306a\u63f4\u52a9\u304c\u5fc5\u8981\u3067\u3059\u3002 \u30e9\u30a4\u30c8\u3092\u3064\u3051\u7d9a\u3051\u308b\u3060\u3051\u3067\u9ad8\u4fa1\u3067\u3059\u3002 \u30c9\u30e1\u30a4\u30f3\u540d\u767b\u9332\u3001\u30c8\u30ea\u30f3\u30b1\u30c3\u30c8\u3001\u30b3\u30f3\u30b5\u30eb\u30c6\u30a3\u30f3\u30b0\u30d8\u30eb\u30d7\u306a\u3069\u306e\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u9577\u3044\u30ea\u30b9\u30c8\u306f\u3001\u3059\u3050\u306b\u304b\u306a\u308a\u306e\u7e70\u308a\u8fd4\u3057\u30b3\u30b9\u30c8\u306b\u52a0\u7b97\u3055\u308c\u307e\u3059\u3002\n\nPySimpleGUI \u306f\u4f5c\u6210\u3059\u308b\u306e\u306b\u5b89\u4fa1\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u611b\u306e\u52b4\u50cd\u306f\u3001\u975e\u5e38\u306b\u9762\u5012\u3067\u3057\u305f\u3002\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3068\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306b\u5230\u9054\u3059\u308b\u306b\u306f\u3001\u90317\u65e5\u306e\u4f5c\u696d\u306b2\u5e74\u4ee5\u4e0a\u5fc5\u8981\u3067\u3059\u3002\n\nPySimpleGUI\u306b\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u3001\u305d\u306e\u307e\u307e\u6b8b\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\u7d20\u6674\u3089\u3057\u3044\u3053\u3068\u3067\u3059\u3002 \u304a\u5ba2\u69d8\u307e\u305f\u306f\u304a\u5ba2\u69d8\u306e\u4f1a\u793e (\u7279\u306b\u4f01\u696d\u3067PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u5834\u5408) \u304c\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7d4c\u6e08\u7684\u306b\u5229\u76ca\u3092\u5f97\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5bff\u547d\u3092\u5ef6\u9577\u3059\u308b\u6a5f\u80fd\u3092\u6301\u3061\u307e\u3059\u3002\n\n###\u3000Buy Me a Coffee\n\n\u300cBuy Me a Coffee\u300d\u306f\u3001\u958b\u767a\u8005\u3092\u516c\u7684\u306b\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002 \u7d20\u65e9\u304f\u3001\u7c21\u5358\u306b\u3001\u8ca2\u732e\u306f\u8a18\u9332\u3055\u308c\u308b\u306e\u3067\u3001\u3042\u306a\u305f\u304cPySimpleGUI \u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308b\u3053\u3068\u3092\u4ed6\u306e\u4eba\u306b\u898b\u305b\u3089\u308c\u307e\u3059\u3002\u5bc4\u4ed8\u3092\u975e\u516c\u958b\u306b\u3082\u3067\u304d\u307e\u3059\u3002\n\n<a href=\"https://www.buymeacoffee.com/PySimpleGUI\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" width=\"217px\" ></a>\n\n\n\n### GitHub\u30b9\u30dd\u30f3\u30b5\u30fc\n\n<a href=\"https://github.com/sponsors/PySimpleGUI\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=%3Curl%3E&color=f88379\"></a>\n\n[GitHub\u5b9a\u671f\u7684\u306a\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7](https://github.com/sponsors\u30fc/PySimpleGUI)\u306f\u3001\u7d99\u7d9a\u7684\u306b\u3055\u307e\u3056\u307e\u306a\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u30b9\u30dd\u30f3\u30b5\u30fc\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u591a\u304f\u306e\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u4f01\u696d\u30ec\u30d9\u30eb\u306e\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u91d1\u92ad\u7684\u306b\u8ca2\u732e\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3001\u975e\u5e38\u306b\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u958b\u767a\u8005\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u7d4c\u6e08\u7684\u306b\u56f0\u96e3\u3067\u3059\u3002YouTube\u52d5\u753b\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306f\u3001\u52d5\u753b\u4f5c\u6210\u3067\u751f\u8a08\u3092\u7acb\u3066\u3066\u3044\u307e\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u306f\u307e\u3060\u305d\u308c\u307b\u3069\u7c21\u5358\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n# \u8ca2\u732e:construction_worker:\n\nPySimpleGUI \u306f\u73fe\u5728\u3001\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u81ea\u4f53\u306f\u88fd\u54c1\u306e\u3088\u3046\u306b\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u306f\u53d7\u3051\u4ed8\u3051\u3089\u308c\u307e\u305b\u3093\u3002\n\n\u8ca2\u732e\u3059\u308b\u6700\u3082\u826f\u3044\u65b9\u6cd5\u306e1\u3064\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u3068\u516c\u958b\u3067\u3059\u3002 \u30e6\u30fc\u30b6\u30fc\u306f\u3001\u4ed6\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u69cb\u7bc9\u3059\u308b\u3082\u306e\u3092\u898b\u3066\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3044\u307e\u3059\u3002 GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u30b3\u30fc\u30c9\u3092\u6295\u7a3f\u3057\u3066\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u306eReadme\u30d5\u30a1\u30a4\u30eb\u306b\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\u3002  \n\n\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a5f\u80fd\u304c\u3042\u3063\u305f\u308a\u3001\u6a5f\u80fd\u5f37\u5316\u3092\u63d0\u6848\u3057\u305f\u3044\u5834\u5408\u306f\u3001[issue\u3092\u958b\u3044\u3066\u304f\u3060\u3055\u3044](https://github.com/PySimpleGUI/PySimpleGUI/issues/new?assignees=&labels=&template=issue-form---must-fill-in-this-form-with-every-new-issue-submitted.md&title=%5B+Enhancement%2FBug%2FQuestion%5D+My+problem+is.)) \u3002\n\n\n# \u7279\u5225\u306a\u611f\u8b1d :pray:\n\n\nPySimpleGUI\u306e\u3053\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306ereadme\u306f[@M4cs](https://github.com/M4cs)\u306e\u52a9\u3051\u306a\u3057\u3067\u306f\u5b9f\u73fe\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5f7c\u306f\u7d20\u6674\u3089\u3057\u3044\u958b\u767a\u8005\u3067\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u7acb\u3061\u4e0a\u3052\u304b\u3089\u305a\u3063\u3068PySimpleGUI\u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3059\u3002 [@Israel Dryer](https://github.com/israel-dryer)\u3082\u307e\u305f\u9577\u671f\u7684\u306a\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308a\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u6a5f\u80fd\u306e\u9650\u754c\u3092\u62bc\u3057\u5e83\u3052\u305f\u3044\u304f\u3064\u304b\u306ePySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u3044\u3066\u3044\u307e\u3059\u3002 \u30dc\u30fc\u30c9\u306e\u753b\u50cf\u3092\u4f7f\u7528\u3057\u305f\u30e6\u30cb\u30fc\u30af\u306a\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30fc\u306fIsrael\u306b\u304c\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 [@jason990420](https://github.com/jason990420)\u306f\u4e0a\u306e\u5199\u771f\u306e\u3088\u3046\u306a\u3001PySimpleGUI \u3092\u4f7f\u3063\u305f\u6700\u521d\u306e\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u3068\u3001PySimpleGUI \u3067\u4f5c\u3089\u308c\u305f\u6700\u521d\u306e\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30b2\u30fc\u30e0\u3092\u516c\u958b\u3057\u3066\u3001\u591a\u304f\u306e\u4eba\u3092\u9a5a\u304b\u305b\u307e\u3057\u305f\u3002\n\u65e5\u672c\u8a9e\u7248\u306e readme \u306f[@okajun35](https://github.com/okajun35) \u3055\u3093\u306e\u5354\u529b\u3067\u5927\u5e45\u306b\u6539\u5584\u3055\u308c\u307e\u3057\u305f\u3002\n\nPySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b 1,200 \u4ee5\u4e0a\u306e GitHub \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3082\u300c\u3042\u308a\u304c\u3068\u3046\u300d\u306e\u8a00\u8449\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30a8\u30f3\u30b8\u30f3\u3092\u52d5\u304b\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u3042\u306a\u305f\u306e\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u304a\u304b\u3052\u3067\u3059\u3002\n\n\u4e00\u6669\u4e2dTwitter \u306b\u6295\u7a3f\u3057\u3066\u304f\u308c\u308b\u6d77\u5916\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001PySimpleGUI\u306e\u4e00\u65e5\u306e\u4f5c\u696d\u3092\u59cb\u3081\u308b\u304d\u3063\u304b\u3051\u3068\u306a\u308a\u307e\u3059\u3002\u5f7c\u3089\u306f\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u6e90\u3067\u3042\u308a\u3001\u958b\u767a\u30a8\u30f3\u30b8\u30f3\u3092\u59cb\u52d5\u3055\u305b\u3001\u6bce\u65e5\u7a3c\u50cd\u3055\u305b\u308b\u6e96\u5099\u3092\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\u611f\u8b1d\u306e\u610f\u3092\u8fbc\u3081\u3066\u3001\u3053\u306ereadme\u30d5\u30a1\u30a4\u30eb\u3092(\u65e5\u672c\u8a9e)[https://github.com/PySimpleGUI/PySimpleGUI/blob/master/readme.ja.md]\u306b\u7ffb\u8a33\u3057\u307e\u3057\u305f\u3002\n\n\u7686\u3055\u3093\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u671b\u3080\u6700\u9ad8\u306e\u30e6\u30fc\u30b6\u30fc\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3067\u3059\u3002\n\n\n\n&copy; Copyright 2020 PySimpleGUI.org \n"
 },
 {
  "repo": "CMU-Perceptual-Computing-Lab/openpose",
  "language": "C++",
  "readme_contents": "<div align=\"center\">\n    <img src=\".github/Logo_main_black.png\", width=\"300\">\n</div>\n\n-----------------\n\n|                  |`Default Config`  |`CUDA (+Python)`  |`CPU (+Python)`   |`OpenCL (+Python)`| `Debug`          | `Unity`          |\n| :---:            | :---:            | :---:            | :---:            | :---:            | :---:            | :---:            |\n| **`Linux`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/1)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/2)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/3)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/4)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/5)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/6)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`MacOS`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/8)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/9)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/10)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/11)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`Windows`** | [![Status](https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true)](https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master) | | | | |\n<!--\nNote: Currently using [travis-matrix-badges](https://github.com/bjfish/travis-matrix-badges) vs. traditional [![Build Status](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose.svg?branch=master)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose)\n-->\n\n[**OpenPose**](https://github.com/CMU-Perceptual-Computing-Lab/openpose) represents the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**.\n\nIt is **authored by [Gines Hidalgo](https://www.gineshidalgo.com), [Zhe Cao](https://people.eecs.berkeley.edu/~zhecao), [Tomas Simon](http://www.cs.cmu.edu/~tsimon), [Shih-En Wei](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [Hanbyul Joo](https://jhugestar.github.io), and [Yaser Sheikh](http://www.cs.cmu.edu/~yaser)**. Currently, it is being **maintained by [Gines Hidalgo](https://www.gineshidalgo.com) and [Yaadhav Raaj](https://www.raaj.tech)**. In addition, OpenPose would not be possible without the [**CMU Panoptic Studio dataset**](http://domedb.perception.cs.cmu.edu). We would also like to thank all the people who helped OpenPose in any way. The main contributors are listed in [doc/contributors.md](doc/contributors.md).\n\n<!-- The [original CVPR 2017 repo](https://github.com/ZheC/Multi-Person-Pose-Estimation) includes Matlab and Python versions, as well as the training code. The body pose estimation work is based on [the original ECCV 2016 demo](https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose). -->\n\n\n<p align=\"center\">\n    <img src=\"doc/media/pose_face_hands.gif\", width=\"480\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left) and <a href=\"https://jhugestar.github.io\" target=\"_blank\">Hanbyul Joo</a> (right) in front of the <a href=\"http://domedb.perception.cs.cmu.edu\" target=\"_blank\">CMU Panoptic Studio</a></sup>\n</p>\n\n## Features\n- **Functionality**:\n    - **2D real-time multi-person keypoint detection**:\n        - 15 or 18 or **25-keypoint body/foot keypoint estimation**. **Running time invariant to number of detected people**.\n        - **6-keypoint foot keypoint estimation**. Integrated together with the 25-keypoint body/foot keypoint detector.\n        - **2x21-keypoint hand keypoint estimation**. Currently, **running time depends** on **number of detected people**.\n        - **70-keypoint face keypoint estimation**. Currently, **running time depends** on **number of detected people**.\n    - **3D real-time single-person keypoint detection**:\n        - 3-D triangulation from multiple single views.\n        - Synchronization of Flir cameras handled.\n        - Compatible with Flir/Point Grey cameras, but provided C++ demos to add your custom input.\n    - **Calibration toolbox**:\n        - Easy estimation of distortion, intrinsic, and extrinsic camera parameters.\n    - **Single-person tracking** for further speed up or visual smoothing.\n- **Input**: Image, video, webcam, Flir/Point Grey and IP camera. Included C++ demos to add your custom input.\n- **Output**: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), and/or keypoints as array class.\n- **OS**: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\n- **Training and datasets**:\n    - [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train).\n    - [**Foot dataset website**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/).\n- **Others**:\n    - Available: command-line demo, C++ wrapper, and C++ API.\n    - [**Python API**](doc/python_module.md).\n    - [**Unity Plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin).\n    - CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only (no GPU) versions.\n\nFor further details, check [all released features](doc/released_features.md) and [release notes](doc/release_notes.md).\n\n\n\n## Related Work\n- Since Sep 2019: [**Training code**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)!\n- Since Jan 2019: [**Unity plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin)!\n- Since Dec 2018: [**Foot dataset**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset) and [**new paper released**](https://arxiv.org/abs/1812.08008)!\n\n\n\n## Results\n### Body and Foot Estimation\n<p align=\"center\">\n    <img src=\"doc/media/dance_foot.gif\", width=\"360\">\n    <br>\n    <sup>Testing the <a href=\"https://www.youtube.com/watch?v=2DiQUX11YaY\" target=\"_blank\"><i>Crazy Uptown Funk flashmob in Sydney</i></a> video sequence with OpenPose</sup>\n</p>\n\n### 3-D Reconstruction Module (Body, Foot, Face, and Hands)\n<p align=\"center\">\n    <img src=\"doc/media/openpose3d.gif\", width=\"360\">\n    <br>\n    <sup>Testing the 3D Reconstruction Module of OpenPose</sup>\n</p>\n\n### Body, Foot, Face, and Hands Estimation\n<p align=\"center\">\n    <img src=\"doc/media/pose_face.gif\", width=\"360\">\n    <img src=\"doc/media/pose_hands.gif\", width=\"360\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left image) and <a href=\"http://www.cs.cmu.edu/~tsimon\" target=\"_blank\">Tomas Simon</a> (right image) testing OpenPose</sup>\n</p>\n\n### Unity Plugin\n<p align=\"center\">\n    <img src=\"doc/media/unity_main.png\", width=\"240\">\n    <img src=\"doc/media/unity_body_foot.png\", width=\"240\">\n    <img src=\"doc/media/unity_hand_face.png\", width=\"240\">\n    <br>\n    <sup><a href=\"http://tianyizhao.com\" target=\"_blank\">Tianyi Zhao</a> and <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> testing their <a href=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin\" target=\"_blank\">OpenPose Unity Plugin</a></sup>\n</p>\n\n### Runtime Analysis\nInference time comparison between the 3 available pose estimation libraries: OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN:\n<p align=\"center\">\n    <img src=\"doc/media/openpose_vs_competition.png\", width=\"360\">\n</p>\nThis analysis was performed using the same images for each algorithm and a batch size of 1. Each analysis was repeated 1000 times and then averaged. This was all performed on a system with a Nvidia 1080 Ti and CUDA 8. Megvii (Face++) and MSRA GitHub repositories were excluded because they only provide pose estimation results given a cropped person. However, they suffer the same problem than Alpha-Pose and Mask R-CNN, their runtimes grow linearly with the number of people.\n\n\n\n## Contents\n1. [Features](#features)\n2. [Related Work](#related-work)\n3. [Results](#results)\n4. [Installation, Reinstallation and Uninstallation](#installation-reinstallation-and-uninstallation)\n5. [Quick Start](#quick-start)\n6. [Output](#output)\n7. [Speeding Up OpenPose and Benchmark](#speeding-up-openpose-and-benchmark)\n8. [Training Code and Foot Dataset](#training-code-and-foot-dataset)\n9. [Send Us Failure Cases and Feedback!](#send-us-failure-cases-and-feedback)\n10. [Citation](#citation)\n11. [License](#license)\n\n\n\n## Installation, Reinstallation and Uninstallation\n**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.\n\nOtherwise, check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.\n\n\n\n## Quick Start\nMost users do not need the OpenPose C++/Python API, but can simply use the OpenPose Demo:\n\n- **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g., run OpenPose in a video with:\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi\n```\n\n- **OpenPose C++ API**: If you want to read a specific input, and/or add your custom post-processing function, and/or implement your own display/saving, check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/). You can easily **create your custom code** on [examples/user_code/](examples/user_code/) and CMake will automatically compile it together with the whole OpenPose project. See [examples/user_code/README.md](examples/user_code/README.md) for more details.\n\n- **OpenPose Python API**: Analogously to the C++ API, find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).\n\n- **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/calibration/README.md](doc/calibration/README.md).\n\n- **Standalone face or hand detector**:\n    - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces), check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).\n    - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors, rather than using the body detector. E.g., useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).\n\n\n\n## Output\nOutput (format, keypoint index ordering, etc.) in [doc/output.md](doc/output.md).\n\n\n\n## Speeding Up OpenPose and Benchmark\nCheck the OpenPose Benchmark as well as some hints to speed up and/or reduce the memory requirements for OpenPose on [doc/speed_up_openpose.md](doc/speed_up_openpose.md).\n\n\n\n## Training Code and Foot Dataset\nFor training OpenPose, check [github.com/CMU-Perceptual-Computing-Lab/openpose_train](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train).\n\nFor the foot dataset, check the [foot dataset website](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/) and new [OpenPose paper](https://arxiv.org/abs/1812.08008) for more information.\n\n\n\n## Send Us Failure Cases and Feedback!\nOur library is open source for research purposes, and we want to continuously improve it! So please, let us know if...\n\n1. ... you find videos or images where OpenPose does not seems to work well. Feel free to send them to openposecmu@gmail.com (email only for failure cases!), we will use them to improve the quality of the algorithm!\n2. ... you find any bug (in functionality or speed).\n3. ... you added some functionality to some class or some new Worker<T> subclass which we might potentially incorporate.\n4. ... you know how to speed up or improve any part of the library.\n5. ... you have a request about possible functionality.\n6. ... etc.\n\nJust comment on GitHub or make a pull request and we will answer as soon as possible! Send us an email if you use the library to make a cool demo or YouTube video!\n\n\n\n## Citation\nPlease cite these papers in your publications if it helps your research. Most of OpenPose is based on `[8765346]`. In addition, the hand and face keypoint detectors are a combination of `[8765346]` and `[Simon et al. 2017]` (the face detector was trained using the same procedure than the hand detector).\n\n    @article{8765346,\n      author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2019}\n    }\n\n    @inproceedings{simon2017hand,\n      author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\n      year = {2017}\n    }\n\n    @inproceedings{cao2017realtime,\n      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2017}\n    }\n\n    @inproceedings{wei2016cpm,\n      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Convolutional pose machines},\n      year = {2016}\n    }\n\nLinks to the papers:\n\n- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\n    - [IEEE TPAMI](https://ieeexplore.ieee.org/document/8765346)\n    - [ArXiv](https://arxiv.org/abs/1812.08008)\n- [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809)\n- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)\n- [Convolutional Pose Machines](https://arxiv.org/abs/1602.00134)\n\n\n\n## License\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the [license](LICENSE) for further details. Interested in a commercial license? Check this [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740). For commercial queries, use the `Contact` section from the [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740) and also send a copy of that message to [Yaser Sheikh](mailto:yaser@cs.cmu.edu).\n"
 },
 {
  "repo": "oarriaga/face_classification",
  "language": "Python",
  "readme_contents": "# This repository is deprecated for at TF-2.0 rewrite visit:\n# https://github.com/oarriaga/paz\n------------------------------------------------\n# Face classification and detection.\nReal-time face detection and emotion/gender classification using fer2013/IMDB datasets with a keras CNN model and openCV.\n* IMDB gender classification test accuracy: 96%.\n* fer2013 emotion classification test accuracy: 66%.\n\nFor more information please consult the [publication](https://github.com/oarriaga/face_classification/blob/master/report.pdf)\n\n# Emotion/gender examples:\n\n![alt tag](images/demo_results.png)\n\nGuided back-prop\n![alt tag](images/gradcam_results.png)\n\nReal-time demo:\n<div align='center'>\n  <img src='images/color_demo.gif' width='400px'>\n</div>\n\n[B-IT-BOTS](https://mas-group.inf.h-brs.de/?page_id=622) robotics team :)\n![alt tag](images/robocup_team.png)\n\n## Instructions\n\n### Run real-time emotion demo:\n> python3 video_emotion_color_demo.py\n\n### Run real-time guided back-prop demo:\n> python3 image_gradcam_demo.py\n\n### Make inference on single images:\n> python3 image_emotion_gender_demo.py <image_path>\n\ne.g.\n\n> python3 image_emotion_gender_demo.py ../images/test_image.jpg\n\n### Running with Docker\n\nWith a few steps one can get its own face classification and detection running. Follow the commands below:\n\n* ```docker pull ekholabs/face-classifier```\n* ```docker run -d -p 8084:8084 --name=face-classifier ekholabs/face-classifier```\n* ```curl -v -F image=@[path_to_image]  http://localhost:8084/classifyImage > image.png```\n\n### To train previous/new models for emotion classification:\n\n\n* Download the fer2013.tar.gz file from [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xzf fer2013.tar\n\n* Run the train_emotion_classification.py file\n> python3 train_emotion_classifier.py\n\n### To train previous/new models for gender classification:\n\n* Download the imdb_crop.tar file from [here](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) (It's the 7GB button with the tittle Download faces only).\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xfv imdb_crop.tar\n\n* Run the train_gender_classification.py file\n> python3 train_gender_classifier.py\n\n"
 },
 {
  "repo": "openframeworks/openFrameworks",
  "language": "C++",
  "readme_contents": "[openFrameworks](http://openframeworks.cc/)\n================\n\nopenFrameworks is a C++ toolkit for creative coding.  If you are new to OF, welcome!\n\n[![Slack Status](https://ofslack.herokuapp.com/badge.svg)](https://ofslack.herokuapp.com)\n\n## Build status\n\n* The **master** branch contains the newest, most recently updated code. This code is packaged and available for download in the \"Nightly Builds\" section of [openframeworks.cc/download](https://openframeworks.cc/download/).\n* The **stable** branch contains the code corresponding to the last stable openFrameworks release. This stable code is packaged and available for download at [openframeworks.cc/download](https://openframeworks.cc/download/).\n\nPlatform                     | Master branch  | Stable branch\n-----------------------------|:---------|:---------\nWindows MSYS2 32bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows MSYS2 64bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | N/A\nWindows Visual Studio 32bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows Visual Studio 64bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/4)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nLinux 64                     | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv6l                 | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv7l                 | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nEmscripten                   | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos                        | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos makefiles              | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\niOS                          | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\ntvos                         | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid Arm7                 | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid X86                  | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\n\n\n## folder structure\n\nThis release of OF comes with several folders:\n\n* addons\n* apps\n* docs\n* examples\n* export (on some systems)\n* libs\n* other\n* scripts\n* project generator\n\n\n`docs` has some documentation around OF usage, per platform things to consider, etc. You should definitely take a look in there; for example, if you are on OSX, read the osx.md.   `apps` and `examples` are where projects go -- `examples` contains a variety of projects that show you how to use OF, and `apps` is where your own projects will go.  `libs` contains the libraries that OF uses, including the openframeworks core itself.  `addons` are for additional functionality that's not part of the core.  `export` is for DLLs and dylibs that need to be put in each compiled project.  The `scripts` folder has the templates and small scripts for automating OF per platform. `project generator` is a GUI based tool for making new projects - this folder is only there in packaged releases.  \n\nOne idea that's important is that OF releases are designed to be self-contained.  You can put them anywhere on your hard drive, but it's not possible to mix different releases of OF together, so please keep each release (0.8.0, 0.8.1) separate.  Projects may generally work from release to release, but this is not guaranteed.  Because OF is self-contained, there's extensive use of local file paths (ie, ../../../) throughout OF.  It's important to be aware of how directories are structured.  A common error is to take a project and move it so that it's a level below or above where it used to be compared to the root of OF.  This means that links such as ../../../libs will break.  \n\n## Get involved\n\nThe openframeworks forum:\n\n[http://forum.openframeworks.cc/](http://forum.openframeworks.cc/)\n\nis a warm and friendly place.  Please ask or answer a question.  The most important part of this project is that it's a community, more than just a tool, so please join us!  Also, this is free software, and we learn so much about what is hard, what doesn't make sense, what is useful, etc. The most basic questions are acceptable here!  Don't worry, just join the conversation.  Learning in OF is social, it's hard to do it alone, but together we can get far!\n\nOur GitHub site is active:\n\n[https://github.com/openframeworks/openFrameworks](https://github.com/openframeworks/openFrameworks)\n\nif you have bugs or feature requests, consider opening an issue.  If you are a developer and want to help, pull requests are warmly welcome.  Please read the contributing guide for guidelines:\n\n[https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md](https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md\n)\n\nWe also have a developer's mailing list, which is useful for discussing issues around the development and future of OF.\n\n## Developers\n\nTo grab a copy of openFrameworks for your platform, check the [download page](http://openframeworks.cc/download) on the main site.  \n\nIf you are working with the Git repository, the `stable` branch of the OF repository corresponds to the most recent release, with a few important differences:  \n\n1. The release includes a simple openFrameworks project generator.\n2. This GitHub repository contains code and libs for all the platforms, but the releases are done on a per-platform basis.\n3. This GitHub repository has no project files for the different examples. They are generated automatically for each release using a tool in `apps/projectGenerator/`.\n4. There are no external dependencies in this repository, you can download them using the download_libs.sh script for each platform in the particular platform folder inside scripts.\n\nIf you want to work with the openFrameworks GitHub repository, you need to download the external dependencies and you should use the project generator to create project files for all the code in `examples/`.  To generate the project files with the project generator enable the 'Advanced Options' in the settings tab, then use 'Update Multiple' to update the projects for the `examples/` folder path in the repo.\n\nTo set up the project generator submodule within the OF repo, use the command `git submodule init` then `git submodule update` whilst inside the openFrameworks repo.\n\nFor more info on working with the Project Generator, for per-platform readmes, and more information, see the [documentation](docs/table_of_contents.md).\n\n## Versioning\n\nopenFrameworks uses [Semantic Versioning](http://semver.org/), although strict adherence will only come into effect at version 1.0.0.\n"
 },
 {
  "repo": "spmallick/learnopencv",
  "language": "Jupyter Notebook",
  "readme_contents": "# LearnOpenCV\nThis repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog [LearnOpenCV.com](https://www.LearnOpenCV.com).\n\nWant to become an expert in AI? [AI Courses by OpenCV](https://opencv.org/courses/) is a great place to start.\n\n<a href=\"https://opencv.org/courses/\">\n<p align=\"center\">\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/04/AI-Courses-By-OpenCV-Github.png\">\n</p>\n</a>\n\n## List of Blog Posts\n\n| Blog Post | |\n| ------------- |:-------------|\n|[Image Classification with OpenCV for Android](https://www.learnopencv.com/image-classification-with-opencv-for-android/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android) |\n|[Image Classification with OpenCV Java](https://www.learnopencv.com/image-classification-with-opencv-java)|[Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java) |\n|[PyTorch to Tensorflow Model Conversion](https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion) |\n|[Snake Game with OpenCV Python](https://www.learnopencv.com/snake-game-with-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SnakeGame) |\n|[Stanford MRNet Challenge: Classifying Knee MRIs](https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/)|[Code](https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model) |\n|[Experiment Logging with TensorBoard and wandb](https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging) |\n|[Understanding Lens Distortion](https://www.learnopencv.com/understanding-lens-distortion/)|[Code](https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion) |\n|[Image Matting with state-of-the-art Method \u201cF, B, Alpha Matting\u201d](https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FBAMatting) |\n|[Bag Of Tricks For Image Classification - Let's check if it is working or not](https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification) |\n|[Getting Started with OpenCV CUDA Module](https://www.learnopencv.com/getting-started-opencv-cuda-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module) |\n|[Training a Custom Object Detector with DLIB & Making Gesture Controlled Applications](https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib) |\n|[How To Run Inference Using TensorRT C++ API](https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP) |\n|[Using Facial Landmarks for Overlaying Faces with Medical Masks](https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay) |\n|[Tensorboard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning) |\n|[Otsu's Thresholding with OpenCV](https://www.learnopencv.com/otsu-thresholding-with-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/otsu-method) |\n|[PyTorch-to-CoreML-model-conversion](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion) |\n|[Playing Rock, Paper, Scissors with AI](https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI) |\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[CNN Fully Convolutional Image Classification with TensorFlow](https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow) | [Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification) |\n|[How to convert a model from PyTorch to TensorRT and speed up inference](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT) |\n|[Efficient image loading](https://www.learnopencv.com/efficient-image-loading/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading) |\n|[Graph Convolutional Networks: Model Relations In Data](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data)|\n|[Getting Started with Federated Learning with PyTorch and PySyft](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro)|\n|[Creating a Virtual Pen & Eraser](http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser) |\n|[Getting Started with PyTorch Lightning](https://www.learnopencv.com/getting-started-with-pytorch-lightning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning)|\n|[Multi-Label Image Classification with PyTorch: Image Tagging](https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging)|\n|[Funny Mirrors Using OpenCV](https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/)|[code](https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors)|\n|[t-SNE for ResNet feature visualization](https://www.learnopencv.com/t-sne-for-resnet-feature-visualization/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TSNE)|\n|[Multi-Label Image Classification with Pytorch](https://www.learnopencv.com/multi-label-image-classification-with-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification)|\n|[CNN Receptive Field Computation Using Backprop](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop)|\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[Augmented Reality using AruCo Markers in OpenCV(C++ and Python)](https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/) |[Code](https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers)|\n|[Fully Convolutional Image Classification on Arbitrary Sized Image](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification)|\n|[Camera Calibration using OpenCV](https://www.learnopencv.com/camera-calibration-using-opencv/) |[Code](https://github.com/spmallick/learnopencv/tree/master/CameraCalibration)|\n|[Geometry of Image Formation](https://www.learnopencv.com/geometry-of-image-formation/) ||\n|[Ensuring Training Reproducibility in Pytorch](https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch) ||\n|[Gaze Tracking](https://www.learnopencv.com/gaze-tracking/) ||\n|[Simple Background Estimation in Videos Using OpenCV](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation)|\n|[Applications of Foreground-Background separation with Semantic Segmentation](https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg) |\n|[EfficientNet: Theory + Code](https://www.learnopencv.com/efficientnet-theory-code) | [Code](https://github.com/spmallick/learnopencv/tree/master/EfficientNet) |\n|[PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch](https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/) | [Code](./PyTorch-Mask-RCNN) |\n|[PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN) |\n|[PyTorch for Beginners: Semantic Segmentation using torchvision](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision) |\n|[PyTorch for Beginners: Comparison of pre-trained models for Image Classification](https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb) |\n|[PyTorch for Beginners: Basics](https://www.learnopencv.com/pytorch-for-beginners-basics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb) |\n|[PyTorch Model Inference using ONNX and Caffe2](https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2) |\n|[Image Classification Using Transfer Learning in PyTorch](https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch) |\n|[Hangman: Creating games in OpenCV](https://www.learnopencv.com/hangman-creating-games-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hangman) |\n|[Image Inpainting with OpenCV (C++/Python)](https://www.learnopencv.com/image-inpainting-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting) |\n|[Hough Transform with OpenCV (C++/Python)](https://www.learnopencv.com/hough-transform-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hough-Transform) |\n|[Xeus-Cling: Run C++ code in Jupyter Notebook](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/) | [Code](https://github.com/spmallick/learnopencv/tree/master/XeusCling) |\n|[Gender & Age Classification using OpenCV Deep Learning ( C++/Python )](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AgeGender) |\n|[Invisibility Cloak using Color Detection and Segmentation with OpenCV](https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak) |\n|[Fast Image Downloader for Open Images V4 (Python)](https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/) | [Code](https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages) |\n|[Deep Learning based Text Detection Using OpenCV (C++/Python)](https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST) |\n|[Video Stabilization Using Point Feature Matching in OpenCV](https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoStabilization) |\n|[Training YOLOv3 : Deep Learning based Custom Object Detector](https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector\t) |\n|[Using OpenVINO with OpenCV](https://www.learnopencv.com/using-openvino-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV) |\n|[Duplicate Search on Quora Dataset](https://www.learnopencv.com/duplicate-search-on-quora-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search) |\n|[Shape Matching using Hu Moments (C++/Python)](https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HuMoments) |\n|[Install OpenCV 4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh) |\n|[Install OpenCV 4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh) |\n|[Install OpenCV 4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh) |\n|[Install OpenCV 3.4.4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh) |\n|[Install OpenCV 3.4.4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh) |\n|[OpenCV QR Code Scanner (C++ and Python)](https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV) |\n|[Install OpenCV 3.4.4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3) |\n|[Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh) |\n|[Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh) |\n|[Universal Sentence Encoder](https://www.learnopencv.com/universal-sentence-encoder) | [Code](https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder) |\n|[Install OpenCV 4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh) |\n|[Install OpenCV 4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4) |\n|[Hand Keypoint Detection using Deep Learning and OpenCV](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HandPose)|\n|[Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN) |\n|[Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh) |\n|[Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh) |\n|[Multi-Person Pose Estimation in OpenCV using OpenPose](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person) |\n|[Heatmap for Logo Detection using OpenCV (Python)](https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/heatmap)|\n|[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO)|\n|[Convex Hull using OpenCV in Python and C++](https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ConvexHull)|\n|[MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)](https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker) |\n|[Convolutional Neural Network based Image Colorization using OpenCV](https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colorization)|\n|[SVM using scikit-learn](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[GOTURN: Deep Learning based Object Tracking](https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/) | [Code](https://github.com/spmallick/learnopencv/tree/master/GOTURN)|\n|[Find the Center of a Blob (Centroid) using OpenCV (C++/Python)](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CenterofBlob)|\n|[Support Vector Machines (SVM)](https://www.learnopencv.com/support-vector-machines-svm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[Batch Normalization in Deep Networks](https://www.learnopencv.com/batch-normalization-in-deep-networks/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BatchNormalization)|\n|[Deep Learning based Character Classification using Synthetic Dataset](https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CharClassification)|\n|[Image Quality Assessment : BRISQUE](https://www.learnopencv.com/image-quality-assessment-brisque/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageMetrics)|\n|[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)|\n|[Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/OCR)|\n|[Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose)|\n|[Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)](https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/)| |\n|[How to convert your OpenCV C++ code into a Python module](https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/pymodule)|\n|[CV4Faces : Best Project Award 2018](https://www.learnopencv.com/cv4faces-best-project-award-2018/)| |\n|[Facemark : Facial Landmark Detection using OpenCV](https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection)|\n|[Image Alignment (Feature Based) using OpenCV (C++/Python)](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased)|\n|[Barcode and QR code Scanner using ZBar and OpenCV](https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner)|\n|[Keras Tutorial : Fine-tuning using pre-trained models](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning)|\n|[OpenCV Transparent API](https://www.learnopencv.com/opencv-transparent-api/)| |\n|[Face Reconstruction using EigenFaces (C++/Python)](https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces) |\n|[Eigenface using OpenCV (C++/Python)](https://www.learnopencv.com/eigenface-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/EigenFace)|\n|[Principal Component Analysis](https://www.learnopencv.com/principal-component-analysis/)| |\n|[Keras Tutorial : Transfer Learning using pre-trained models](https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning) |\n|[Keras Tutorial : Using pre-trained Imagenet models](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models) |\n|[Technical Aspects of a Digital SLR](https://www.learnopencv.com/technical-aspects-of-a-digital-slr/) | |\n|[Using Harry Potter interactive wand with OpenCV to create magic](https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/)| |\n|[Install OpenCV 3 and Dlib on Windows ( Python only )](https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/)| |\n|[Image Classification using Convolutional Neural Networks in Keras](https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR)|\n|[Understanding Autoencoders using Tensorflow (Python)](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder)|\n|[Best Project Award : Computer Vision for Faces](https://www.learnopencv.com/best-project-award-computer-vision-for-faces/) | |\n|[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)      | |\n|[Image Classification using Feedforward Neural Network in Keras](https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasMLP-MNIST)|\n|[Exposure Fusion using OpenCV (C++/Python)](https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/ExposureFusion)|\n|[Understanding Feedforward Neural Networks](https://www.learnopencv.com/understanding-feedforward-neural-networks/)      | |\n|[High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)](http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python)      | [Code](https://github.com/spmallick/learnopencv/tree/master/hdr)|\n|[Deep learning using Keras \u2013 The Basics](http://www.learnopencv.com/deep-learning-using-keras-the-basics)      | [Code](https://github.com/spmallick/learnopencv/tree/master/keras-linear-regression)|\n|[Selective Search for Object Detection (C++ / Python)](http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch) |\n|[Installing Deep Learning Frameworks on Ubuntu with CUDA support](http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/) | |\n|[Parallel Pixel Access in OpenCV using forEach](http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/) | [Code](https://github.com/spmallick/learnopencv/tree/master/forEach) |\n|[cvui: A GUI lib built on top of OpenCV drawing primitives](http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/) | [Code](https://github.com/spmallick/learnopencv/tree/master/UI-cvui) |\n|[Install Dlib on Windows](http://www.learnopencv.com/install-dlib-on-windows/) | |\n|[Install Dlib on Ubuntu](http://www.learnopencv.com/install-dlib-on-ubuntu/) | |\n|[Install OpenCV3 on Ubuntu](http://www.learnopencv.com/install-opencv3-on-ubuntu/) | |\n|[Read, Write and Display a video using OpenCV ( C++/ Python )](http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay) |\n|[Install Dlib on MacOS](http://www.learnopencv.com/install-dlib-on-macos/) | |\n|[Install OpenCV 3 on MacOS](http://www.learnopencv.com/install-opencv3-on-macos/) | |\n|[Install OpenCV 3 on Windows](http://www.learnopencv.com/install-opencv3-on-windows/) | |\n|[Get OpenCV Build Information ( getBuildInformation )](http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/) | |\n|[Color spaces in OpenCV (C++ / Python)](http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ColorSpaces)|\n|[Neural Networks : A 30,000 Feet View for Beginners](http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/) | |\n|[Alpha Blending using OpenCV (C++ / Python)](http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AlphaBlending) |\n|[User stories : How readers of this blog are applying their knowledge to build applications](http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/) | |\n|[How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?](http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/) | |\n|[Automatic Red Eye Remover using OpenCV (C++ / Python)](http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover) |\n|[Bias-Variance Tradeoff in Machine Learning](http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/) | |\n|[Embedded Computer Vision: Which device should you choose?](http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/) | |\n|[Object Tracking using OpenCV (C++/Python)](http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/tracking) |\n|[Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/digits-classification) |\n|[Training a better Haar and LBP cascade based Eye Detector using OpenCV](http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/) | |\n|[Deep Learning Book Gift Recipients](http://www.learnopencv.com/deep-learning-book-gift-recipients/) | |\n|[Minified OpenCV Haar and LBP Cascades](http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector)|\n|[Deep Learning Book Gift](http://www.learnopencv.com/deep-learning-book-gift/) | |\n|[Histogram of Oriented Gradients](http://www.learnopencv.com/histogram-of-oriented-gradients/) | |\n|[Image Recognition and Object Detection : Part 1](http://www.learnopencv.com/image-recognition-and-object-detection-part1/) | |\n|[Head Pose Estimation using OpenCV and Dlib](http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HeadPose) |\n|[Live CV : A Computer Vision Coding Application](http://www.learnopencv.com/live-cv/) | |\n|[Approximate Focal Length for Webcams and Cell Phone Cameras](http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) | |\n|[Configuring Qt for OpenCV on OSX](http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qt-test) |\n|[Rotation Matrix To Euler Angles](http://www.learnopencv.com/rotation-matrix-to-euler-angles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles) |\n|[Speeding up Dlib\u2019s Facial Landmark Detector](http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/) | |\n|[Warp one triangle to another using OpenCV ( C++ / Python )](http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/WarpTriangle) |\n|[Average Face : OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/average-face-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceAverage) |\n|[Face Swap using OpenCV ( C++ / Python )](http://www.learnopencv.com/face-swap-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceSwap) |\n|[Face Morph Using OpenCV \u2014 C++ / Python](http://www.learnopencv.com/face-morph-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceMorph) |\n|[Deep Learning Example using NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/) | |\n|[NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/nvidia-digits-3-on-ec2/) | |\n|[Homography Examples using OpenCV ( Python / C ++ )](http://www.learnopencv.com/homography-examples-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Homography) |\n|[Filling holes in an image using OpenCV ( Python / C++ )](http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Holes) |\n|[How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?](http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FPS) |\n|[Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) ](http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Delaunay) |\n|[OpenCV (C++ vs Python) vs MATLAB for Computer Vision](http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/) | |\n|[Facial Landmark Detection](http://www.learnopencv.com/facial-landmark-detection/) | |\n|[Why does OpenCV use BGR color format ?](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/) | |\n|[Computer Vision for Predicting Facial Attractiveness](http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness) |\n|[applyColorMap for pseudocoloring in OpenCV ( C++ / Python )](http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colormap) |\n|[Image Alignment (ECC) in OpenCV ( C++ / Python )](http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment) |\n|[How to find OpenCV version in Python and C++ ?](http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/) | |\n|[Baidu banned from ILSVRC 2015](http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/) | |\n|[OpenCV Transparent API](http://www.learnopencv.com/opencv-transparent-api/) | |\n|[How Computer Vision Solved the Greatest Soccer Mystery of All Time](http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/) | |\n|[Embedded Vision Summit 2015](http://www.learnopencv.com/embedded-vision-summit-2015/) | |\n|[Read an Image in OpenCV ( Python, C++ )](http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/imread) |\n|[Non-Photorealistic Rendering using OpenCV ( Python, C++ )](http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering) |\n|[Seamless Cloning using OpenCV ( Python , C++ )](http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning) |\n|[OpenCV Threshold ( Python , C++ )](http://www.learnopencv.com/opencv-threshold-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Threshold) |\n|[Blob Detection Using OpenCV ( Python, C++ )](http://www.learnopencv.com/blob-detection-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BlobDetector) |\n|[Turn your OpenCV Code into a Web API in under 10 minutes \u2014 Part 1](http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/) | |\n|[How to compile OpenCV sample Code ?](http://www.learnopencv.com/how-to-compile-opencv-sample-Code/) | |\n|[Install OpenCV 3 on Yosemite ( OSX 10.10.x )](http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/) | |\n"
 },
 {
  "repo": "vipstone/faceai",
  "language": "Python",
  "readme_contents": "[English Doc](README_en.md)\n# \u529f\u80fd #\n\n1. \u4eba\u8138\u68c0\u6d4b\u3001\u8bc6\u522b\uff08\u56fe\u7247\u3001\u89c6\u9891\uff09\n2. \u8f6e\u5ed3\u6807\u8bc6\n3. \u5934\u50cf\u5408\u6210\uff08\u7ed9\u4eba\u6234\u5e3d\u5b50\uff09\n4. \u6570\u5b57\u5316\u5986\uff08\u753b\u53e3\u7ea2\u3001\u7709\u6bdb\u3001\u773c\u775b\u7b49\uff09\n5. \u6027\u522b\u8bc6\u522b\n6. \u8868\u60c5\u8bc6\u522b\uff08\u751f\u6c14\u3001\u538c\u6076\u3001\u6050\u60e7\u3001\u5f00\u5fc3\u3001\u96be\u8fc7\u3001\u60ca\u559c\u3001\u5e73\u9759\u7b49\u4e03\u79cd\u60c5\u7eea\uff09\n7. \u89c6\u9891\u5bf9\u8c61\u63d0\u53d6\n8. \u56fe\u7247\u4fee\u590d\uff08\u53ef\u7528\u4e8e\u6c34\u5370\u53bb\u9664\uff09\n9. \u56fe\u7247\u81ea\u52a8\u4e0a\u8272\n10. \u773c\u52a8\u8ffd\u8e2a\uff08\u5f85\u5b8c\u5584\uff09\n11. \u6362\u8138\uff08\u5f85\u5b8c\u5584\uff09\n\n**\u67e5\u770b\u529f\u80fd\u9884\u89c8\u2193\u2193\u2193**\n\n# \u5f00\u53d1\u73af\u5883 #\n\n- Windows 10\uff08x64\uff09\n- Python 3.6.4\n- OpenCV 3.4.1\n- Dlib 19.8.1\n- face_recognition 1.2.2\n- keras 2.1.6\n- tensorflow 1.8.0\n- Tesseract OCR 4.0.0-beta.1\n\n\n# \u6559\u7a0b #\n\n[OpenCV\u73af\u5883\u642d\u5efa](doc/settingup.md)\n\n[Tesseract OCR\u6587\u5b57\u8bc6\u522b](doc/tesseractOCR.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/detectionOpenCV.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/detectionDlib.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/videoOpenCV.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/videoDlib.md)\n\n[\u8138\u90e8\u8f6e\u5ed3\u7ed8\u5236](doc/faceRecognitionOutline.md)\n\n[\u6570\u5b57\u5316\u5986](doc/faceRecognitionMakeup.md)\n\n[\u89c6\u9891\u4eba\u8138\u8bc6\u522b](doc/faceRecognition.md)\n\n[\u5934\u50cf\u7279\u6548\u5408\u6210](doc/compose.md)\n\n[\u6027\u522b\u8bc6\u522b](doc/gender.md)\n\n[\u8868\u60c5\u8bc6\u522b](doc/emotion.md)\n\n[\u89c6\u9891\u5bf9\u8c61\u63d0\u53d6](https://github.com/vipstone/faceai/blob/master/doc/hsv-opencv.md)\n\n[\u56fe\u7247\u4fee\u590d](https://github.com/vipstone/faceai/blob/master/doc/inpaint.md)\n\n\n# \u5176\u4ed6\u6559\u7a0b #\n\n[Ubuntu apt-get\u548cpip\u6e90\u66f4\u6362](doc/ubuntuChange.md)\n\n[pip/pip3\u66f4\u6362\u56fd\u5185\u6e90\u2014\u2014Windows\u7248](doc/pipChange.md)\n\n[OpenCV\u6dfb\u52a0\u4e2d\u6587](doc/chinese.md)\n\n[\u4f7f\u7528\u9f20\u6807\u7ed8\u56fe\u2014\u2014OpenCV](https://github.com/vipstone/faceai/blob/master/doc/opencv/mouse.md)\n\n\n# \u529f\u80fd\u9884\u89c8 #\n\n**\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/face_recognition-outline.png\" width = \"250\" height = \"300\" alt=\"\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3\" />\n\n----------\n\n**\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/dlib68.png\" width = \"230\" height = \"300\" alt=\"\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6\" />\n\n----------\n\n**\u5934\u50cf\u7279\u6548\u5408\u6210**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/compose.png\" width = \"200\" height = \"300\" alt=\"\u5934\u50cf\u7279\u6548\u5408\u6210\"  />\n\n----------\n\n**\u6027\u522b\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/gender.png\" width = \"430\" height = \"220\" alt=\"\u6027\u522b\u8bc6\u522b\"  />\n\n----------\n\n**\u8868\u60c5\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/emotion.png\" width = \"250\" height = \"300\" alt=\"\u8868\u60c5\u8bc6\u522b\"  />\n\n----------\n\n**\u6570\u5b57\u5316\u5986**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognitionMakeup-1.png\" width = \"450\" height = \"300\" alt=\"\u89c6\u9891\u4eba\u8138\u8bc6\u522b\"  />\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u68c0\u6d4b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/video-jiance.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognition.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](http://icdn.apigo.cn/opencv-hsv.gif)\n\n----------\n\n**\u56fe\u7247\u4fee\u590d**\n\n![](http://icdn.apigo.cn/inpaint.png?2)\n\n----------\n\n**\u56fe\u7247\u81ea\u52a8\u4e0a\u8272**\n\n![](http://icdn.apigo.cn/colorize-faceai.png)\n\n----------\n\n# \u6280\u672f\u65b9\u6848 #\n\n\u6280\u672f\u5b9e\u73b0\u65b9\u6848\u4ecb\u7ecd\n\n\t\u4eba\u8138\u8bc6\u522b\uff1aOpenCV / Dlib\n\t\n\t\u4eba\u8138\u68c0\u6d4b\uff1aface_recognition\n\t\n\t\u6027\u522b\u8bc6\u522b\uff1akeras + tensorflow\n\t\n\t\u6587\u5b57\u8bc6\u522b\uff1aTesseract OCR\n\n\n### TODO ###\n\n\u6362\u8138\u2014\u2014\u5f85\u5b8c\u5584\n\n\u773c\u775b\u79fb\u52a8\u65b9\u5411\u68c0\u6d4b\u2014\u2014\u5f85\u5b8c\u5584\n\nDlib\u6027\u80fd\u4f18\u5316\u65b9\u6848\n\nDlib\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\nTesseract\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\n# \u8d21\u732e\u8005\u540d\u5355\uff08\u7279\u522b\u611f\u8c22\uff09\t\n\n[archersmind](https://github.com/archersmind)\t\n\t\n[rishab-sharma](https://github.com/rishab-sharma)\n\n# \u5fae\u4fe1\u6253\u8d4f\n\n![\u5fae\u4fe1\u6253\u8d4f](http://icdn.apigo.cn/myinfo/wchat-pay.png)"
 },
 {
  "repo": "opencv/opencv_contrib",
  "language": "C++",
  "readme_contents": "## Repository for OpenCV's extra modules\n\nThis repository is intended for the development of so-called \"extra\" modules,\ncontributed functionality. New modules quite often do not have stable API,\nand they are not well-tested. Thus, they shouldn't be released as a part of\nofficial OpenCV distribution, since the library maintains binary compatibility,\nand tries to provide decent performance and stability.\n\nSo, all the new modules should be developed separately, and published in the\n`opencv_contrib` repository at first. Later, when the module matures and gains\npopularity, it is moved to the central OpenCV repository, and the development team\nprovides production-quality support for this module.\n\n### How to build OpenCV with extra modules\n\nYou can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV.\n\nHere is the CMake command for you:\n\n```\n$ cd <opencv_build_directory>\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory>\n$ make -j5\n```\n\nAs the result, OpenCV will be built in the `<opencv_build_directory>` with all\nmodules from `opencv_contrib` repository. If you don't want all of the modules,\nuse CMake's `BUILD_opencv_*` options. Like in this example:\n\n```\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory>\n```\n\nIf you also want to build the samples from the \"samples\" folder of each module, also include the \"-DBUILD_EXAMPLES=ON\" option.\n\nIf you prefer using the gui version of cmake (cmake-gui), then, you can add `opencv_contrib` modules within `opencv` core by doing the following:\n\n1. Start cmake-gui.\n\n2. Select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface).\n\n3. Press the `configure` button. You will see all the opencv build parameters in the central interface.\n\n4. Browse the parameters and look for the form called `OPENCV_EXTRA_MODULES_PATH` (use the search form to focus rapidly on it).\n\n5. Complete this `OPENCV_EXTRA_MODULES_PATH` by the proper pathname to the `<opencv_contrib>/modules` value using its browse button.\n\n6. Press the `configure` button followed by the `generate` button (the first time, you will be asked which makefile style to use).\n\n7. Build the `opencv` core with the method you chose (make and make install if you chose Unix makefile at step 6).\n\n8. To run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, \"-lopencv_aruco\" flag will be added.\n\n### Update the repository documentation\n\nIn order to keep a clean overview containing all contributed modules, the following files need to be created/adapted:\n\n1. Update the README.md file under the modules folder. Here, you add your model with a single line description.\n\n2. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also.\n"
 },
 {
  "repo": "justadudewhohacks/opencv4nodejs",
  "language": "C++",
  "readme_contents": "opencv4nodejs\n=============\n\n![opencv4nodejs](https://user-images.githubusercontent.com/31125521/37272906-67187fdc-25d8-11e8-9704-40e9e94c1e80.jpg)\n\n[![Build Status](https://travis-ci.org/justadudewhohacks/opencv4nodejs.svg?branch=master)](http://travis-ci.org/justadudewhohacks/opencv4nodejs)\n[![Build status](https://ci.appveyor.com/api/projects/status/cv3o65nrosh1udbb/branch/master?svg=true)](https://ci.appveyor.com/project/justadudewhohacks/opencv4nodejs/branch/master)\n[![Coverage](https://codecov.io/github/justadudewhohacks/opencv4nodejs/coverage.svg?branch=master)](https://codecov.io/gh/justadudewhohacks/opencv4nodejs)\n[![npm download](https://img.shields.io/npm/dm/opencv4nodejs.svg?style=flat)](https://www.npmjs.com/package/opencv4nodejs)\n[![node version](https://img.shields.io/badge/node.js-%3E=_6-green.svg?style=flat)](http://nodejs.org/download/)\n[![Slack](https://slack.bri.im/badge.svg)](https://slack.bri.im/)\n\n**opencv4nodejs allows you to use the native OpenCV library in nodejs. Besides a synchronous API the package provides an asynchronous API, which allows you to build non-blocking and multithreaded computer vision tasks. opencv4nodejs supports OpenCV 3 and OpenCV 4.**\n\n**The ultimate goal of this project is to provide a comprehensive collection of nodejs bindings to the API of OpenCV and the OpenCV-contrib modules. To get an overview of the currently implemented bindings, have a look at the [type declarations](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/lib/typings) of this package. Furthermore, contribution is highly appreciated. If you want to add missing bindings check out the <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/CONTRIBUTING.md\"><b>contribution guide</b>.**\n\n* **[Examples](#examples)**\n* **[How to install](#how-to-install)**\n* **[Usage with Docker](#usage-with-docker)**\n* **[Usage with Electron](#usage-with-electron)**\n* **[Usage with NW.js](#usage-with-nwjs)**\n* **[Quick Start](#quick-start)**\n* **[Async API](#async-api)**\n* **[With TypeScript](#with-typescript)**\n* **[External Memory Tracking (v4.0.0)](#external-mem-tracking)**\n<a name=\"examples\"></a>\n\n# Examples\n\nSee <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples\"><b>examples</b></a> for implementation.\n\n### Face Detection\n\n![face0](https://user-images.githubusercontent.com/31125521/29702727-c796acc4-8972-11e7-8043-117dd2761833.jpg)\n![face1](https://user-images.githubusercontent.com/31125521/29702730-c79d3904-8972-11e7-8ccb-e8c467244ad8.jpg)\n\n### Face Recognition with the OpenCV face module\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-opencv-for-face-recognition-37fa7cb860e8\"><b>Node.js + OpenCV for Face Recognition</b></a>.\n\n![facerec](https://user-images.githubusercontent.com/31125521/35453007-eac9d516-02c8-11e8-9c4d-a77c01ae1f77.jpg)\n\n### Face Landmarks with the OpenCV face module\n\n![facelandmarks](https://user-images.githubusercontent.com/31125521/39297394-af14ae26-4943-11e8-845a-a06cbfa28d5a.jpg)\n\n### Face Recognition with <a href=\"https://github.com/justadudewhohacks/face-recognition.js\"><b>face-recognition.js</b></a>\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-face-recognition-js-simple-and-robust-face-recognition-using-deep-learning-ea5ba8e852\"><b>Node.js + face-recognition.js : Simple and Robust Face Recognition using Deep Learning</b></a>.\n\n[![IMAGE ALT TEXT](https://user-images.githubusercontent.com/31125521/35453884-055f3bde-02cc-11e8-8fa6-945f320652c3.jpg)](https://www.youtube.com/watch?v=ArcFHpX-usQ \"Nodejs Face Recognition using face-recognition.js and opencv4nodejs\")\n\n### Hand Gesture Recognition\nCheck out <a href=\"https://medium.com/@muehler.v/simple-hand-gesture-recognition-using-opencv-and-javascript-eb3d6ced28a0\"><b>Simple Hand Gesture Recognition using OpenCV and JavaScript</b></a>.\n\n![gesture-rec_sm](https://user-images.githubusercontent.com/31125521/30052864-41bd5680-9227-11e7-8a62-6205f3d99d5c.gif)\n\n### Object Recognition with Deep Neural Networks\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-meets-opencvs-deep-neural-networks-fun-with-tensorflow-and-caffe-ff8d52a0f072\"><b>Node.js meets OpenCV\u2019s Deep Neural Networks\u200a\u2014\u200aFun with Tensorflow and Caffe</b></a>.\n\n#### Tensorflow Inception\n\n![husky](https://user-images.githubusercontent.com/31125521/32703295-f6b0e7ee-c7f3-11e7-8039-b3ada21810a0.jpg)\n![car](https://user-images.githubusercontent.com/31125521/32703296-f6cea892-c7f3-11e7-8aaa-9fe48b88fe05.jpeg)\n![banana](https://user-images.githubusercontent.com/31125521/32703297-f6e932ca-c7f3-11e7-9a66-bbc826ebf007.jpg)\n\n\n#### Single Shot Multibox Detector with COCO\n\n![dishes-detection](https://user-images.githubusercontent.com/31125521/32703228-eae787d4-c7f2-11e7-8323-ea0265deccb3.jpg)\n![car-detection](https://user-images.githubusercontent.com/31125521/32703229-eb081e36-c7f2-11e7-8b26-4d253b4702b4.jpg)\n\n### Machine Learning\nCheck out <a href=\"https://medium.com/@muehler.v/machine-learning-with-opencv-and-javascript-part-1-recognizing-handwritten-letters-using-hog-and-88719b70efaa\"><b>Machine Learning with OpenCV and JavaScript: Recognizing Handwritten Letters using HOG and SVM</b></a>.\n\n![resulttable](https://user-images.githubusercontent.com/31125521/30635645-5a466ea8-9df3-11e7-8498-527e1293c4fa.png)\n\n### Object Tracking\n\n![trackbgsubtract](https://user-images.githubusercontent.com/31125521/29702733-c7b59864-8972-11e7-996b-d28cb508f3b8.gif)\n![trackbycolor](https://user-images.githubusercontent.com/31125521/29702735-c8057686-8972-11e7-9c8d-13e30ab74628.gif)\n\n### Feature Matching\n\n![matchsift](https://user-images.githubusercontent.com/31125521/29702731-c79e3142-8972-11e7-947e-db109d415469.jpg)\n\n### Image Histogram\n\n![plotbgr](https://user-images.githubusercontent.com/31125521/29995016-1b847970-8fdf-11e7-9316-4eb0fd550adc.jpg)\n![plotgray](https://user-images.githubusercontent.com/31125521/29995015-1b83e06e-8fdf-11e7-8fa8-5d18326b9cd3.jpg)\n\n### Boiler plate for combination of opencv4nodejs, express and websockets.\n\n[opencv4nodejs-express-websockets](https://github.com/Mudassir-23/opencv4nodejs-express-websockets) - Boilerplate express app for getting started on opencv with nodejs and to live stream the video through websockets.\n\n### Automating lights by people detection through classifier\n\nCheck out <a href=\"https://medium.com/softway-blog/automating-lights-with-computer-vision-nodejs-fb9b614b75b2\"><b>Automating lights with Computer Vision & NodeJS</b></a>.\n\n![user-presence](https://user-images.githubusercontent.com/34403479/70385871-8d62e680-19b7-11ea-855c-3b2febfdbd72.png)\n\n<a name=\"how-to-install\"></a>\n\n# How to install\n\n``` bash\nnpm install --save opencv4nodejs\n```\n\nNative node modules are built via node-gyp, which already comes with npm by default. However, node-gyp requires you to have python installed. If you are running into node-gyp specific issues have a look at known issues with [node-gyp](https://github.com/nodejs/node-gyp) first.\n\n**Important note:** node-gyp won't handle whitespaces properly, thus make sure, that the path to your project directory does **not contain any whitespaces**. Installing opencv4nodejs under \"C:\\Program Files\\some_dir\" or similar will not work and will fail with: \"fatal error C1083: Cannot open include file: 'opencv2/core.hpp'\"!**\n\nOn Windows you will furthermore need Windows Build Tools to compile OpenCV and opencv4nodejs. If you don't have Visual Studio or Windows Build Tools installed, you can easily install the VS2015 build tools:\n\n``` bash\nnpm install --global windows-build-tools\n```\n\n## Installing OpenCV Manually\n\nSetting up OpenCV on your own will require you to set an environment variable to prevent the auto build script to run:\n\n``` bash\n# linux and osx:\nexport OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n# on windows:\nset OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n```\n\n### Windows\n\nYou can install any of the OpenCV 3 or OpenCV 4 <a href=\"https://github.com/opencv/opencv/releases/\"><b>releases</b></a> manually or via the [Chocolatey](https://chocolatey.org/) package manager:\n\n``` bash\n# to install OpenCV 4.1.0\nchoco install OpenCV -y -version 4.1.0\n```\n\nNote, this will come without contrib modules. To install OpenCV under windows with contrib modules you have to build the library from source or you can use the auto build script.\n\nBefore installing opencv4nodejs with an own installation of OpenCV you need to expose the following environment variables:\n- *OPENCV_INCLUDE_DIR* pointing to the directory with the subfolder *opencv2* containing the header files\n- *OPENCV_LIB_DIR* pointing to the lib directory containing the OpenCV .lib files\n\nAlso you will need to add the OpenCV binaries to your system path:\n- add an environment variable *OPENCV_BIN_DIR* pointing to the binary directory containing the OpenCV .dll files\n- append `;%OPENCV_BIN_DIR%;` to your system path variable\n\nNote: Restart your current console session after making changes to your environment.\n\n### MacOSX\n\nUnder OSX we can simply install OpenCV via brew:\n\n``` bash\nbrew update\nbrew install opencv@4\nbrew link --force opencv@4\n```\n\n### Linux\n\nUnder Linux we have to build OpenCV from source manually or using the auto build script.\n\n## Installing OpenCV via Auto Build Script\n\nThe auto build script comes in form of the [opencv-build](https://github.com/justadudewhohacks/npm-opencv-build) npm package, which will run by default when installing opencv4nodejs. The script requires you to have git and a recent version of cmake installed.\n\n### Auto Build Flags\n\nYou can customize the autobuild flags using *OPENCV4NODEJS_AUTOBUILD_FLAGS=<flags>*.\nFlags must be space-separated.\n\nThis is an advanced customization and you should have knowledge regarding the OpenCV compilation flags. Flags added by default are listed [here](https://github.com/justadudewhohacks/npm-opencv-build/blob/master/src/constants.ts#L44-L82).\n\n### Installing a Specific Version of OpenCV\n\nYou can specify the Version of OpenCV you want to install via the script by setting an environment variable:\n`export OPENCV4NODEJS_AUTOBUILD_OPENCV_VERSION=4.1.0`\n\n### Installing only a Subset of OpenCV modules\n\nIf you only want to build a subset of the OpenCV modules you can pass the *-DBUILD_LIST* cmake flag via the *OPENCV4NODEJS_AUTOBUILD_FLAGS* environment variable. For example `export OPENCV4NODEJS_AUTOBUILD_FLAGS=-DBUILD_LIST=dnn` will build only modules required for `dnn` and reduces the size and compilation time of the OpenCV package.\n\n## Configuring Environments via package.json\n\nIt's possible to specify build environment variables by inserting them into the `package.json` as follows:\n\n```json\n{\n  \"name\": \"my-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"opencv4nodejs\": \"^X.X.X\"\n  },\n  \"opencv4nodejs\": {\n    \"disableAutoBuild\": 1,\n    \"opencvIncludeDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\include\",\n    \"opencvLibDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\lib\",\n    \"opencvBinDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\bin\"\n  }\n}\n```\n\nThe following environment variables can be passed:\n\n- autoBuildBuildCuda\n- autoBuildFlags\n- autoBuildOpencvVersion\n- autoBuildWithoutContrib\n- disableAutoBuild\n- opencvIncludeDir\n- opencvLibDir\n- opencvBinDir\n\n<a name=\"usage-with-docker\"></a>\n\n# Usage with Docker\n\n### [opencv-express](https://github.com/justadudewhohacks/opencv-express) - example for opencv4nodejs with express.js and docker\n\nOr simply pull from [justadudewhohacks/opencv-nodejs](https://hub.docker.com/r/justadudewhohacks/opencv-nodejs/) for opencv-3.2 + contrib-3.2 with opencv4nodejs globally installed:\n\n``` docker\nFROM justadudewhohacks/opencv-nodejs\n```\n\n**Note**: The aforementioned Docker image already has ```opencv4nodejs``` installed globally. In order to prevent build errors during an ```npm install```, your ```package.json``` should not include ```opencv4nodejs```, and instead should include/require the global package either by requiring it by absolute path or setting the ```NODE_PATH``` environment variable to ```/usr/lib/node_modules``` in your Dockerfile and requiring the package as you normally would.\n\nDifferent OpenCV 3.x base images can be found here: https://hub.docker.com/r/justadudewhohacks/.\n\n<a name=\"usage-with-electron\"></a>\n\n# Usage with Electron\n\n### [opencv-electron](https://github.com/justadudewhohacks/opencv-electron) - example for opencv4nodejs with electron\n\nAdd the following script to your package.json:\n``` python\n\"electron-rebuild\": \"electron-rebuild -w opencv4nodejs\"\n```\n\nRun the script:\n``` bash\n$ npm run electron-rebuild\n```\n\nRequire it in the application:\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"usage-with-nwjs\"></a>\n\n# Usage with NW.js\n\nAny native modules, including opencv4nodejs, must be recompiled to be used with [NW.js](https://nwjs.io/). Instructions on how to do this are available in the **[Use Native Modules](http://docs.nwjs.io/en/latest/For%20Users/Advanced/Use%20Native%20Node%20Modules/)** section of the the NW.js documentation.\n\nOnce recompiled, the module can be installed and required as usual:\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"quick-start\"></a>\n\n# Quick Start\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n### Initializing Mat (image matrix), Vec, Point\n\n``` javascript\nconst rows = 100; // height\nconst cols = 100; // width\n\n// empty Mat\nconst emptyMat = new cv.Mat(rows, cols, cv.CV_8UC3);\n\n// fill the Mat with default value\nconst whiteMat = new cv.Mat(rows, cols, cv.CV_8UC1, 255);\nconst blueMat = new cv.Mat(rows, cols, cv.CV_8UC3, [255, 0, 0]);\n\n// from array (3x3 Matrix, 3 channels)\nconst matData = [\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]],\n  [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]]\n];\nconst matFromArray = new cv.Mat(matData, cv.CV_8UC3);\n\n// from node buffer\nconst charData = [255, 0, ...];\nconst matFromArray = new cv.Mat(Buffer.from(charData), rows, cols, cv.CV_8UC3);\n\n// Point\nconst pt2 = new cv.Point(100, 100);\nconst pt3 = new cv.Point(100, 100, 0.5);\n\n// Vector\nconst vec2 = new cv.Vec(100, 100);\nconst vec3 = new cv.Vec(100, 100, 0.5);\nconst vec4 = new cv.Vec(100, 100, 0.5, 0.5);\n```\n\n### Mat and Vec operations\n\n``` javascript\nconst mat0 = new cv.Mat(...);\nconst mat1 = new cv.Mat(...);\n\n// arithmetic operations for Mats and Vecs\nconst matMultipliedByScalar = mat0.mul(0.5);  // scalar multiplication\nconst matDividedByScalar = mat0.div(2);       // scalar division\nconst mat0PlusMat1 = mat0.add(mat1);          // addition\nconst mat0MinusMat1 = mat0.sub(mat1);         // subtraction\nconst mat0MulMat1 = mat0.hMul(mat1);          // elementwise multiplication\nconst mat0DivMat1 = mat0.hDiv(mat1);          // elementwise division\n\n// logical operations Mat only\nconst mat0AndMat1 = mat0.and(mat1);\nconst mat0OrMat1 = mat0.or(mat1);\nconst mat0bwAndMat1 = mat0.bitwiseAnd(mat1);\nconst mat0bwOrMat1 = mat0.bitwiseOr(mat1);\nconst mat0bwXorMat1 = mat0.bitwiseXor(mat1);\nconst mat0bwNot = mat0.bitwiseNot();\n```\n\n### Accessing Mat data\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\nconst matGray = new cv.Mat(..., cv.CV_8UC1);\n\n// get pixel value as vector or number value\nconst vec3 = matBGR.at(200, 100);\nconst grayVal = matGray.at(200, 100);\n\n// get raw pixel value as array\nconst [b, g, r] = matBGR.atRaw(200, 100);\n\n// set single pixel values\nmatBGR.set(50, 50, [255, 0, 0]);\nmatBGR.set(50, 50, new Vec(255, 0, 0));\nmatGray.set(50, 50, 255);\n\n// get a 25x25 sub region of the Mat at offset (50, 50)\nconst width = 25;\nconst height = 25;\nconst region = matBGR.getRegion(new cv.Rect(50, 50, width, height));\n\n// get a node buffer with raw Mat data\nconst matAsBuffer = matBGR.getData();\n\n// get entire Mat data as JS array\nconst matAsArray = matBGR.getDataAsArray();\n```\n\n### IO\n\n``` javascript\n// load image from file\nconst mat = cv.imread('./path/img.jpg');\ncv.imreadAsync('./path/img.jpg', (err, mat) => {\n  ...\n})\n\n// save image\ncv.imwrite('./path/img.png', mat);\ncv.imwriteAsync('./path/img.jpg', mat,(err) => {\n  ...\n})\n\n// show image\ncv.imshow('a window name', mat);\ncv.waitKey();\n\n// load base64 encoded image\nconst base64text='data:image/png;base64,R0lGO..';//Base64 encoded string\nconst base64data =base64text.replace('data:image/jpeg;base64','')\n                            .replace('data:image/png;base64','');//Strip image type prefix\nconst buffer = Buffer.from(base64data,'base64');\nconst image = cv.imdecode(buffer); //Image is now represented as Mat\n\n// convert Mat to base64 encoded jpg image\nconst outBase64 =  cv.imencode('.jpg', croppedImage).toString('base64'); // Perform base64 encoding\nconst htmlImg='<img src=data:image/jpeg;base64,'+outBase64 + '>'; //Create insert into HTML compatible <img> tag\n\n// open capture from webcam\nconst devicePort = 0;\nconst wCap = new cv.VideoCapture(devicePort);\n\n// open video capture\nconst vCap = new cv.VideoCapture('./path/video.mp4');\n\n// read frames from capture\nconst frame = vCap.read();\nvCap.readAsync((err, frame) => {\n  ...\n});\n\n// loop through the capture\nconst delay = 10;\nlet done = false;\nwhile (!done) {\n  let frame = vCap.read();\n  // loop back to start on end of stream reached\n  if (frame.empty) {\n    vCap.reset();\n    frame = vCap.read();\n  }\n\n  // ...\n\n  const key = cv.waitKey(delay);\n  done = key !== 255;\n}\n```\n\n### Useful Mat methods\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\n\n// convert types\nconst matSignedInt = matBGR.convertTo(cv.CV_32SC3);\nconst matDoublePrecision = matBGR.convertTo(cv.CV_64FC3);\n\n// convert color space\nconst matGray = matBGR.bgrToGray();\nconst matHSV = matBGR.cvtColor(cv.COLOR_BGR2HSV);\nconst matLab = matBGR.cvtColor(cv.COLOR_BGR2Lab);\n\n// resize\nconst matHalfSize = matBGR.rescale(0.5);\nconst mat100x100 = matBGR.resize(100, 100);\nconst matMaxDimIs100 = matBGR.resizeToMax(100);\n\n// extract channels and create Mat from channels\nconst [matB, matG, matR] = matBGR.splitChannels();\nconst matRGB = new cv.Mat([matR, matB, matG]);\n```\n\n### Drawing a Mat into HTML Canvas\n\n``` javascript\nconst img = ...\n\n// convert your image to rgba color space\nconst matRGBA = img.channels === 1\n  ? img.cvtColor(cv.COLOR_GRAY2RGBA)\n  : img.cvtColor(cv.COLOR_BGR2RGBA);\n\n// create new ImageData from raw mat data\nconst imgData = new ImageData(\n  new Uint8ClampedArray(matRGBA.getData()),\n  img.cols,\n  img.rows\n);\n\n// set canvas dimensions\nconst canvas = document.getElementById('myCanvas');\ncanvas.height = img.rows;\ncanvas.width = img.cols;\n\n// set image data\nconst ctx = canvas.getContext('2d');\nctx.putImageData(imgData, 0, 0);\n```\n\n### Method Interface\n\nOpenCV method interface from official docs or src:\n``` c++\nvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT);\n```\n\ntranslates to:\n\n``` javascript\nconst src = new cv.Mat(...);\n// invoke with required arguments\nconst dst0 = src.gaussianBlur(new cv.Size(5, 5), 1.2);\n// with optional paramaters\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, 0.8, cv.BORDER_REFLECT);\n// or pass specific optional parameters\nconst optionalArgs = {\n  borderType: cv.BORDER_CONSTANT\n};\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, optionalArgs);\n```\n\n<a name=\"async-api\"></a>\n\n# Async API\n\nThe async API can be consumed by passing a callback as the last argument of the function call. By default, if an async method is called without passing a callback, the function call will yield a Promise.\n\n### Async Face Detection\n\n``` javascript\nconst classifier = new cv.CascadeClassifier(cv.HAAR_FRONTALFACE_ALT2);\n\n// by nesting callbacks\ncv.imreadAsync('./faceimg.jpg', (err, img) => {\n  if (err) { return console.error(err); }\n\n  const grayImg = img.bgrToGray();\n  classifier.detectMultiScaleAsync(grayImg, (err, res) => {\n    if (err) { return console.error(err); }\n\n    const { objects, numDetections } = res;\n    ...\n  });\n});\n\n// via Promise\ncv.imreadAsync('./faceimg.jpg')\n  .then(img =>\n    img.bgrToGrayAsync()\n      .then(grayImg => classifier.detectMultiScaleAsync(grayImg))\n      .then((res) => {\n        const { objects, numDetections } = res;\n        ...\n      })\n  )\n  .catch(err => console.error(err));\n\n// using async await\ntry {\n  const img = await cv.imreadAsync('./faceimg.jpg');\n  const grayImg = await img.bgrToGrayAsync();\n  const { objects, numDetections } = await classifier.detectMultiScaleAsync(grayImg);\n  ...\n} catch (err) {\n  console.error(err);\n}\n```\n\n<a name=\"with-typescript\"></a>\n\n# With TypeScript\n\n``` javascript\nimport * as cv from 'opencv4nodejs'\n```\n\nCheck out the TypeScript [examples](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples/typed).\n\n<a name=\"external-mem-tracking\"></a>\n\n# External Memory Tracking (v4.0.0)\n\nSince version 4.0.0 was released, external memory tracking has been enabled by default. Simply put, the memory allocated for Matrices (cv.Mat) will be manually reported to the node process. This solves the issue of inconsistent Garbage Collection, which could have resulted in spiking memory usage of the node process eventually leading to overflowing the RAM of your system, prior to version 4.0.0.\n\nNote, that in doubt this feature can be **disabled** by setting an environment variable `OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING` before requiring the module:\n\n``` bash\nexport OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // linux\nset OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // windows\n```\n\nOr directly in your code:\n``` javascript\nprocess.env.OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING = 1\nconst cv = require('opencv4nodejs')\n```\n\n\n"
 },
 {
  "repo": "kelaberetiv/TagUI",
  "language": "JavaScript",
  "readme_contents": "<img src=\"https://raw.githubusercontent.com/kelaberetiv/TagUI/master/src/media/tagui_logo.png\" height=\"111\" align=\"right\">\n\n# TagUI\n\n### TagUI is a command-line tool for digital process automation (RPA)\n\n### [Download TagUI v6](https://tagui.readthedocs.io/en/latest/setup.html)&ensp;|&ensp;[Visit documentation](https://tagui.readthedocs.io/en/latest/index.html)&ensp;|&ensp;[User feedback](https://forms.gle/mieY66xTN4NNm5Gq5)&ensp;|&ensp;[We're hiring](https://nuscareers.taleo.net/careersection/2/jobdetail.ftl?job=00CMO)\n\nWrite flows in simple TagUI language and automate your web, mouse and keyboard interactions on the screen.\n\nTagUI is free to use and open-source. It's easy to setup and use, and works on Windows, macOS and Linux.\n\nIn TagUI language, you use steps like `click` and `type` to interact with identifiers, which include web identifiers, image snapshots, screen coordinates, or even text using OCR. Below is a sample flow to download a report:\n\n```\nhttps://www.typeform.com\n\nclick login\ntype username as user@gmail.com\ntype password as 12345678\nclick btnlogin\n\ndownload https://admin.typeform.com/xxx to report.csv\n```\n```\n// besides web identifiers, images of UI elements can be used\nclick login_button.png\ntype username_box.png as user@gmail.com\n```\n```\n// (x,y) coordinates of user-interface elements can also be used\nclick (1200,200)\ntype (800,400) as user@gmail.com\n```\n\n# v6 Features\n\n### TagUI live mode\nYou can run live mode directly for faster development by running `tagui live` on the command line.\n\n### Click text using OCR\nTagUI can now click on the screen with visual automation just using text input, by using OCR technology.\n\n```\nclick v6 Features using ocr\n```\n\n### Deploy flows to run when double clicked\nYou can now create a shortcut for a flow, which can be moved to your desktop and double-clicked to run the flow. The flow will be run with all the options used when creating the shortcut.\n\n```\n$ tagui my_flow.tag -deploy\nOR\n$ tagui my_flow.tag -d\n```\n\n### Running flows with options can be done with abbreviations\nFor example, you can now do ``tagui my_flow.tag -h`` instead of ``tagui my_flow.tag -headless``.\n\n# Migrating to v6\n\n### Mandatory .tag file extension\nAll flow files must have a .tag extension.\n\n### Options must be used with a leading hyphen (-)\nWhen running a flow with options, prefix a - to the options.\n\nBefore v6:\n```\n$ tagui my_flow.tag headless\n```\n\nAfter v6:\n```\n$ tagui my_flow.tag -headless\nOR\n$ tagui my_flow.tag -h\n```\n\n### Change in syntax for echo, dump, write steps\nThe echo, dump and write steps are now consistent with the other steps. They no longer require quotes surrounding the string input. Instead, variables now need to be surrounded by backticks.\n\nBefore v6:\n```\necho 'This works!' some_text_variable\n```\n\nAfter v6:\n```\necho This works! `some_text_variable`\n```\n\n### if and loop code blocks can use indentation instead of curly braces {}\nThis increases readability and ease of use. Just indent your code within the if and loop code blocks. \n\nBefore v6:\n```\nif some_condition\n{\ndo_some_step_A\ndo_some_step_B\n}\n```\n\nAfter v6:\n```\nif some_condition\n  do_some_step_A\n  do_some_step_B\n```\n\n# TagUI v5.11\n\n### [Visit TagUI v5.11 homepage & documentation](https://github.com/kelaberetiv/TagUI/tree/pre_v6)\n\n# Credits\n- [TagUI v3](https://github.com/kensoh/TagUI/tree/before_aisg) - Ken Soh from Singapore\n- [SikuliX](http://sikulix.com) - Raimund Hocke from Germany\n- [CasperJS](http://casperjs.org) - Nicolas Perriault from France\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia\n- [SlimerJS](https://slimerjs.org) - Laurent Jouanneau from France\n\nThis project  is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG-RP-2019-050). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n"
 },
 {
  "repo": "hybridgroup/gocv",
  "language": "Go",
  "readme_contents": "# GoCV\n\n[![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/gocvlogo.jpg)](http://gocv.io/)\n\n[![GoDoc](https://godoc.org/gocv.io/x/gocv?status.svg)](https://godoc.org/github.com/hybridgroup/gocv)\n[![CircleCI Build status](https://circleci.com/gh/hybridgroup/gocv/tree/dev.svg?style=svg)](https://circleci.com/gh/hybridgroup/gocv/tree/dev)\n[![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/9asd5foet54ru69q/branch/dev?svg=true)](https://ci.appveyor.com/project/deadprogram/gocv/branch/dev)\n[![codecov](https://codecov.io/gh/hybridgroup/gocv/branch/dev/graph/badge.svg)](https://codecov.io/gh/hybridgroup/gocv)\n[![Go Report Card](https://goreportcard.com/badge/github.com/hybridgroup/gocv)](https://goreportcard.com/report/github.com/hybridgroup/gocv)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/hybridgroup/gocv/blob/release/LICENSE.txt)\n\nThe GoCV package provides Go language bindings for the [OpenCV 4](http://opencv.org/) computer vision library.\n\nThe GoCV package supports the latest releases of Go and OpenCV (v4.5.0) on Linux, macOS, and Windows. We intend to make the Go language a \"first-class\" client compatible with the latest developments in the OpenCV ecosystem.\n\nGoCV supports [CUDA](https://en.wikipedia.org/wiki/CUDA) for hardware acceleration using Nvidia GPUs. Check out the [CUDA README](./cuda/README.md) for more info on how to use GoCV with OpenCV/CUDA.\n\nGoCV also supports [Intel OpenVINO](https://software.intel.com/en-us/openvino-toolkit). Check out the [OpenVINO README](./openvino/README.md) for more info on how to use GoCV with the Intel OpenVINO toolkit.\n\n## How to use\n\n### Hello, video\n\nThis example opens a video capture device using device \"0\", reads frames, and shows the video in a GUI window:\n\n```go\npackage main\n\nimport (\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n\twebcam, _ := gocv.OpenVideoCapture(0)\n\twindow := gocv.NewWindow(\"Hello\")\n\timg := gocv.NewMat()\n\n\tfor {\n\t\twebcam.Read(&img)\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### Face detect\n\n![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/face-detect.jpg)\n\nThis is a more complete example that opens a video capture device using device \"0\". It also uses the CascadeClassifier class to load an external data file containing the classifier data. The program grabs each frame from the video, then uses the classifier to detect faces. If any faces are found, it draws a green rectangle around each one, then displays the video in an output window:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"image/color\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n    // set to use a video capture device 0\n    deviceID := 0\n\n\t// open webcam\n\twebcam, err := gocv.OpenVideoCapture(deviceID)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tdefer webcam.Close()\n\n\t// open display window\n\twindow := gocv.NewWindow(\"Face Detect\")\n\tdefer window.Close()\n\n\t// prepare image matrix\n\timg := gocv.NewMat()\n\tdefer img.Close()\n\n\t// color for the rect when faces detected\n\tblue := color.RGBA{0, 0, 255, 0}\n\n\t// load classifier to recognize faces\n\tclassifier := gocv.NewCascadeClassifier()\n\tdefer classifier.Close()\n\n\tif !classifier.Load(\"data/haarcascade_frontalface_default.xml\") {\n\t\tfmt.Println(\"Error reading cascade file: data/haarcascade_frontalface_default.xml\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"start reading camera device: %v\\n\", deviceID)\n\tfor {\n\t\tif ok := webcam.Read(&img); !ok {\n\t\t\tfmt.Printf(\"cannot read device %v\\n\", deviceID)\n\t\t\treturn\n\t\t}\n\t\tif img.Empty() {\n\t\t\tcontinue\n\t\t}\n\n\t\t// detect faces\n\t\trects := classifier.DetectMultiScale(img)\n\t\tfmt.Printf(\"found %d faces\\n\", len(rects))\n\n\t\t// draw a rectangle around each face on the original image\n\t\tfor _, r := range rects {\n\t\t\tgocv.Rectangle(&img, r, blue, 3)\n\t\t}\n\n\t\t// show the image in the window, and wait 1 millisecond\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### More examples\n\nThere are examples in the [cmd directory](./cmd) of this repo in the form of various useful command line utilities, such as [capturing an image file](./cmd/saveimage), [streaming mjpeg video](./cmd/mjpeg-streamer), [counting objects that cross a line](./cmd/counter), and [using OpenCV with Tensorflow for object classification](./cmd/tf-classifier).\n\n## How to install\n\nTo install GoCV, run the following command:\n\n```\ngo get -u -d gocv.io/x/gocv\n```\n\nTo run code that uses the GoCV package, you must also install OpenCV 4.5.0 on your system. Here are instructions for Ubuntu, Raspian, macOS, and Windows.\n\n## Ubuntu/Linux\n\n### Installation\n\nYou can use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install\n\nIf you need static opencv libraries\n\n\tmake install BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n#### Install Cuda\n\n\t[cuda directory](./cuda)\n\n#### Install OpenVINO\n\n\t[openvino directory](./openvino)\n\t\n#### Install OpenVINO and Cuda\n\n\tThe following commands should do everything to download and install OpenCV 4.5.0 with Cuda and OpenVINO on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_all\n\nIf you need static opencv libraries\n\n\tmake install_all BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0-openvino\n    cuda information:\n      Device 0:  \"GeForce MX150\"  2003Mb, sm_61, Driver/Runtime ver.10.0/10.0\n\n#### Complete Install\n\nIf you have already done the \"Quick Install\" as described above, you do not need to run any further commands. For the curious, or for custom installations, here are the details for each of the steps that are performed when you run `make install`.\n\n##### Install required packages\n\nFirst, you need to change the current directory to the location of the GoCV repo, so you can access the `Makefile`:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNext, you need to update the system, and install any required packages:\n\n\tmake deps\n\n#### Download source\n\nNow, download the OpenCV 4.5.0 and OpenCV Contrib source code:\n\n\tmake download\n\n#### Build\n\nBuild everything. This will take quite a while:\n\n\tmake build\n\nIf you need static opencv libraries\n\n\tmake build BUILD_SHARED_LIBS=OFF\n\n#### Install\n\nOnce the code is built, you are ready to install:\n\n\tmake sudo_install\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n#### Cleanup extra files\n\nAfter the installation is complete, you can remove the extra files and folders:\n\n\tmake clean\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CPPFLAGS=\"-I/usr/local/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/lib -lopencv_core -lopencv_face -lopencv_videoio -lopencv_imgproc -lopencv_highgui -lopencv_imgcodecs -lopencv_objdetect -lopencv_features2d -lopencv_video -lopencv_dnn -lopencv_xfeatures2d\"\n\nPlease note that you will need to run these 2 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n### Docker\n\nThe project now provides `Dockerfile` which lets you build [GoCV](https://gocv.io/) Docker image which you can then use to build and run `GoCV` applications in Docker containers. The `Makefile` contains `docker` target which lets you build Docker image with a single command:\n\n```\nmake docker\n```\n\nBy default Docker image built by running the command above ships [Go](https://golang.org/) version `1.13.5`, but if you would like to build an image which uses different version of `Go` you can override the default value when running the target command:\n\n```\nmake docker GOVERSION='1.13.5'\n```\n\n#### Running GUI programs in Docker on macOS\n\nSometimes your `GoCV` programs create graphical interfaces like windows eg. when you use `gocv.Window` type when you display an image or video stream. Running the programs which create graphical interfaces in Docker container on macOS is unfortunately a bit elaborate, but not impossible. First you need to satisfy the following prerequisites:\n* install [xquartz](https://www.xquartz.org/). You can also install xquartz using [homebrew](https://brew.sh/) by running `brew cask install xquartz`\n* install [socat](https://linux.die.net/man/1/socat) `brew install socat`\n\nNote, you will have to log out and log back in to your machine once you have installed `xquartz`. This is so the X window system is reloaded.\n\nOnce you have installed all the prerequisites you need to allow connections from network clients to `xquartz`. Here is how you do that. First run the following command to open `xquart` so you can configure it:\n\n```shell\nopen -a xquartz\n```\nClick on *Security* tab in preferences and check the \"Allow connections\" box:\n\n![app image](./images/xquartz.png)\n\nNext, you need to create a TCP proxy using `socat` which will stream [X Window](https://en.wikipedia.org/wiki/X_Window_System) data into `xquart`. Before you start the proxy you need to make sure that there is no process listening in port `6000`. The following command should **not** return any results:\n\n```shell\nlsof -i TCP:6000\n```\nNow you can start a local proxy which will proxy the X Window traffic into xquartz which acts a your local X server:\n\n```shell\nsocat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\\"$DISPLAY\\\"\n```\n\nYou are now finally ready to run your `GoCV` GUI programs in Docker containers. In order to make everything work you must set `DISPLAY` environment variables as shown in a sample command below:\n\n```shell\ndocker run -it --rm -e DISPLAY=docker.for.mac.host.internal:0 your-gocv-app\n```\n\n**Note, since Docker for MacOS does not provide any video device support, you won't be able run GoCV apps which require camera.**\n\n### Alpine 3.7 Docker image\n\nThere is a Docker image with Alpine 3.7 that has been created by project contributor [@denismakogon](https://github.com/denismakogon). You can find it located at [https://github.com/denismakogon/gocv-alpine](https://github.com/denismakogon/gocv-alpine).\n\n## Raspbian\n\n### Installation\n\nWe have a special installation for the Raspberry Pi that includes some hardware optimizations. You use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Raspbian:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_raspi\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n## macOS\n\n### Installation\n\nYou can install OpenCV 4.5.0 using Homebrew.\n\nIf you already have an earlier version of OpenCV (3.4.x) installed, you should probably remove it before installing the new version:\n\n\tbrew uninstall opencv\n\nYou can then install OpenCV 4.5.0:\n\n\tbrew install opencv\n\n### pkgconfig Installation\npkg-config is used to determine the correct flags for compiling and linking OpenCV.\nYou can install it by using Homebrew:\n\n    brew install pkgconfig\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CXXFLAGS=\"--std=c++11\"\n\texport CGO_CPPFLAGS=\"-I/usr/local/Cellar/opencv/4.5.0/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/Cellar/opencv/4.5.0/lib -lopencv_stitching -lopencv_superres -lopencv_videostab -lopencv_aruco -lopencv_bgsegm -lopencv_bioinspired -lopencv_ccalib -lopencv_dnn_objdetect -lopencv_dpm -lopencv_face -lopencv_photo -lopencv_fuzzy -lopencv_hfs -lopencv_img_hash -lopencv_line_descriptor -lopencv_optflow -lopencv_reg -lopencv_rgbd -lopencv_saliency -lopencv_stereo -lopencv_structured_light -lopencv_phase_unwrapping -lopencv_surface_matching -lopencv_tracking -lopencv_datasets -lopencv_dnn -lopencv_plot -lopencv_xfeatures2d -lopencv_shape -lopencv_video -lopencv_ml -lopencv_ximgproc -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_flann -lopencv_xobjdetect -lopencv_imgcodecs -lopencv_objdetect -lopencv_xphoto -lopencv_imgproc -lopencv_core\"\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n## Windows\n\n### Installation\n\nThe following assumes that you are running a 64-bit version of Windows 10.\n\nIn order to build and install OpenCV 4.5.0 on Windows, you must first download and install MinGW-W64 and CMake, as follows.\n\n#### MinGW-W64\n\nDownload and run the MinGW-W64 compiler installer from [https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/).\n\nThe latest version of the MinGW-W64 toolchain is `7.3.0`, but any version from `7.X` on should work.\n\nChoose the options for \"posix\" threads, and for \"seh\" exceptions handling, then install to the default location `c:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2`.\n\nAdd the `C:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2\\mingw64\\bin` path to your System Path.\n\n#### CMake\n\nDownload and install CMake [https://cmake.org/download/](https://cmake.org/download/) to the default location. CMake installer will add CMake to your system path.\n\n#### OpenCV 4.5.0 and OpenCV Contrib Modules\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Windows:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\twin_build_opencv.cmd\n\nIt might take up to one hour.\n\nLast, add `C:\\opencv\\build\\install\\x64\\mingw\\bin` to your System Path.\n\n### Verifying the installation\n\nChange the current directory to the location of the GoCV repo:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\nNow you should be able to build or run any of the command examples:\n\n\tgo run cmd\\version\\main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, OpenCV is expected to be in `C:\\opencv\\build\\install\\include`. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nDue to the way OpenCV produces DLLs, including the version in the name, using this method is required if you're using a different version of OpenCV.\n\nFor example:\n\n\tset CGO_CXXFLAGS=\"--std=c++11\"\n\tset CGO_CPPFLAGS=-IC:\\opencv\\build\\install\\include\n\tset CGO_LDFLAGS=-LC:\\opencv\\build\\install\\x64\\mingw\\lib -lopencv_core412 -lopencv_face412 -lopencv_videoio412 -lopencv_imgproc412 -lopencv_highgui412 -lopencv_imgcodecs412 -lopencv_objdetect412 -lopencv_features2d412 -lopencv_video412 -lopencv_dnn412 -lopencv_xfeatures2d412 -lopencv_plot412 -lopencv_tracking412 -lopencv_img_hash412\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv cmd\\version\\main.go\n\n## Android\n\nThere is some work in progress for running GoCV on Android using Gomobile. For information on how to install OpenCV/GoCV for Android, please see:\nhttps://gist.github.com/ogero/c19458cf64bd3e91faae85c3ac887481\n\nSee original discussion here:\nhttps://github.com/hybridgroup/gocv/issues/235\n\n## Profiling\n\nSince memory allocations for images in GoCV are done through C based code, the go garbage collector will not clean all resources associated with a `Mat`.  As a result, any `Mat` created *must* be closed to avoid memory leaks.\n\nTo ease the detection and repair of the resource leaks, GoCV provides a `Mat` profiler that records when each `Mat` is created and closed.  Each time a `Mat` is allocated, the stack trace is added to the profile.  When it is closed, the stack trace is removed. See the [runtime/pprof documentation](https://golang.org/pkg/runtime/pprof/#Profile).\n\nIn order to include the MatProfile custom profiler, you MUST build or run your application or tests using the `-tags matprofile` build tag. For example:\n\n\tgo run -tags matprofile cmd/version/main.go\n\nYou can get the profile's count at any time using:\n\n```go\ngocv.MatProfile.Count()\n```\n\nYou can display the current entries (the stack traces) with:\n\n```go\nvar b bytes.Buffer\ngocv.MatProfile.WriteTo(&b, 1)\nfmt.Print(b.String())\n```\n\nThis can be very helpful to track down a leak.  For example, suppose you have\nthe following nonsense program:\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tfmt.Printf(\"initial MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tleak()\n\n\tfmt.Printf(\"final MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tvar b bytes.Buffer\n\tgocv.MatProfile.WriteTo(&b, 1)\n\tfmt.Print(b.String())\n}\n```\n\nRunning this program produces the following output:\n\n```\ninitial MatProfile count: 0\nfinal MatProfile count: 1\ngocv.io/x/gocv.Mat profile: total 1\n1 @ 0x40b936c 0x40b93b7 0x40b94e2 0x40b95af 0x402cd87 0x40558e1\n#\t0x40b936b\tgocv.io/x/gocv.newMat+0x4b\t/go/src/gocv.io/x/gocv/core.go:153\n#\t0x40b93b6\tgocv.io/x/gocv.NewMat+0x26\t/go/src/gocv.io/x/gocv/core.go:159\n#\t0x40b94e1\tmain.leak+0x21\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:11\n#\t0x40b95ae\tmain.main+0xae\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:16\n#\t0x402cd86\truntime.main+0x206\t\t/usr/local/Cellar/go/1.11.1/libexec/src/runtime/proc.go:201\n```\n\nWe can see that this program would leak memory.  As it exited, it had one `Mat` that was never closed.  The stack trace points to exactly which line the allocation happened on (line 11, the `gocv.NewMat()`).\n\nFurthermore, if the program is a long running process or if GoCV is being used on a web server, it may be helpful to install the HTTP interface )). For example:\n\n```go\npackage main\n\nimport (\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"time\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tgo func() {\n\t\tticker := time.NewTicker(time.Second)\n\t\tfor {\n\t\t\t<-ticker.C\n\t\t\tleak()\n\t\t}\n\t}()\n\n\thttp.ListenAndServe(\"localhost:6060\", nil)\n}\n\n```\n\nThis will leak a `Mat` once per second.  You can see the current profile count and stack traces by going to the installed HTTP debug interface: [http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat](http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat?debug=1).\n\n\n## How to contribute\n\nPlease take a look at our [CONTRIBUTING.md](./CONTRIBUTING.md) document to understand our contribution guidelines.\n\nThen check out our [ROADMAP.md](./ROADMAP.md) document to know what to work on next.\n\n## Why this project exists\n\nThe [https://github.com/go-opencv/go-opencv](https://github.com/go-opencv/go-opencv) package for Go and OpenCV does not support any version above OpenCV 2.x, and work on adding support for OpenCV 3 had stalled for over a year, mostly due to the complexity of [SWIG](http://swig.org/). That is why we started this project.\n\nThe GoCV package uses a C-style wrapper around the OpenCV 4 C++ classes to avoid having to deal with applying SWIG to a huge existing codebase. The mappings are intended to match as closely as possible to the original OpenCV project structure, to make it easier to find things, and to be able to figure out where to add support to GoCV for additional OpenCV image filters, algorithms, and other features.\n\nFor example, the [OpenCV `videoio` module](https://github.com/opencv/opencv/tree/master/modules/videoio) wrappers can be found in the GoCV package in the `videoio.*` files.\n\nThis package was inspired by the original https://github.com/go-opencv/go-opencv project, the blog post https://medium.com/@peterleyssens/using-opencv-3-from-golang-5510c312a3c and the repo at https://github.com/sensorbee/opencv thank you all!\n\n## License\n\nLicensed under the Apache 2.0 license. Copyright (c) 2017-2020 The Hybrid Group.\n\nLogo generated by GopherizeMe - https://gopherize.me\n"
 },
 {
  "repo": "nuno-faria/tiler",
  "language": "Python",
  "readme_contents": "![title](images/title_stripes.png)\n\n\ud83d\udc77 Build images with images.\n\n\n## About\n\nTiler is a tool to create an image using all kinds of other smaller images (tiles). It is different from other mosaic tools since it can adapt to tiles with multiple shapes and sizes (i.e. not limited to squares).\n\nAn image can be built out of circles, lines, waves, cross stitches, legos, minecraft blocks, paper clips, letters, ... The possibilities are endless!\n\n\n## Installation\n\n- Clone the repo: `git clone https://github.com/nuno-faria/tiler.git`;\n- Install Python 3;\n- Install pip (optional, to install the dependencies);\n- Install dependencies: `pip install -r requirements.txt`\n\n## Usage\n\n- Make a folder with the tiles (and only the tiles) to build the image;\n    - The script `gen_tiles.py` can help in this task; it builds tiles with multiple colors based on the source tile (note: its recommended for the source file to have an RGB color of (240,240,240)). It is used as `python gen_tiles.py path/to/image` and creates a folder with a 'gen_' prefix in the same path as the base image.\n- Run `python tiler.py path/to/image path/to/tiles_folder/`.\n\n## Configuration\n\nAll configurations can be changed in the `conf.py` file.\n\n#### `gen_tiles.py`\n\n- `DEPTH` - number of divisions in each color channel (ex: DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `ROTATIONS` - list of rotations, in degrees, to apply over the original image (ex: [0, 90]).\n\n#### `tiler.py`\n\n- `COLOR_DEPTH` - number of divisions in each color channel (ex: COLOR_DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `RESIZING_SCALES` - scale to apply to each tile (ex: [1, 0.75, 0.5, 0.25]);\n- `PIXEL_SHIFT` - number of pixels shifted to create each box (ex: (5,5)); if None, shift will be the same as the tile dimension);\n    <img src=\"images/pixel_shift.png\" width=\"100%\">\n- `OVERLAP_TILES` - if tiles can overlap;\n- `RENDER` - render image as its being built;\n- `POOL_SIZE` - multiprocessing pool size;\n- `IMAGE_TO_TILE` - image to tile (ignored if passed as the 1st arg);\n- `TILES_FOLDER` - folder with tiles (ignored if passed as the 2nd arg);\n- `OUT` - result image filename.\n\n\n## Examples\n\n### Circles\n\n#### Various sizes\n\n<img src=\"images/cake_circles.png\" width=\"40%\">\n\n[Original](https://www.flaticon.com/free-icon/cake_1102780) cake image by [pongsakornred](https://www.flaticon.com/authors/pongsakornred) from [FLATICON](https://www.flaticon.com).\n\n#### Fixed\n\n- 10x10\n<img src=\"images/cake_circles_simple.png\" width=\"40%\">\n<img src=\"images/starry_night_circles_10x10.png\" width=\"100%\">\n- 25x25\n<img src=\"images/starry_night_circles_25x25.png\" width=\"100%\">\n- 50x50\n<img src=\"images/starry_night_circles_50x50.png\" width=\"100%\">\n\n\n### Paper clips\n\n<img src=\"images/cake_clips.png\" width=\"40%\">\n\n\n### Cross stitch (times)\n\n<img src=\"images/cake_times.png\" width=\"40%\">\n\n<img src=\"images/starry_night_times.png\" width=\"100%\">\n\n\n### Hearts\n\n<img src=\"images/heart_hearts.png\" width=\"40%\">\n\n\n### Legos\n\n<img src=\"images/cake_lego.png\" width=\"40%\">\n<img src=\"images/starry_night_lego.png\" width=\"100%\">\n\n\n### Minecraft blocks\n\n<img src=\"images/cake_minecraft.png\" width=\"40%\">\n<img src=\"images/starry_night_minecraft.png\" width=\"100%\">\n\n\n### Stripes (lines)\n\n<img src=\"images/cake_stripes.png\" width=\"50%\">\n\n\n### At\n\n<img src=\"images/github_logo_at.png\" width=\"50%\">\n"
 },
 {
  "repo": "hamuchiwa/AutoRCCar",
  "language": "Python",
  "readme_contents": "## AutoRCCar\n### Python3 + OpenCV3\n\nSee self-driving in action  \n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=BBwEF6WBUQs\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/BBwEF6WBUQs/0.jpg\" width=\"360\" height=\"240\" border=\"10\" /></a>\n\nThis project builds a self-driving RC car using Raspberry Pi, Arduino and open source software. Raspberry Pi collects inputs from a camera module and an ultrasonic sensor, and sends data to a computer wirelessly. The computer processes input images and sensor data for object detection (stop sign and traffic light) and collision avoidance respectively. A neural network model runs on computer and makes predictions for steering based on input images. Predictions are then sent to the Arduino for RC car control. \n  \n### Setting up environment with Anaconda\n  1. Install [`miniconda(Python3)`](https://conda.io/miniconda.html) on your computer\n  2. Create `auto-rccar` environment with all necessary libraries for this project  \n     ```conda env create -f environment.yml```\n     \n  3. Activate `auto-rccar` environment  \n     ```source activate auto-rccar```\n  \n  &ensp; To exit, simply close the terminal window. More info about managing Anaconda environment, please see [here](https://conda.io/docs/user-guide/tasks/manage-environments.html).\n  \n### About the files\n**test/**  \n  &emsp; &emsp; `rc_control_test.py`: RC car control with keyboard  \n  &emsp; &emsp;  `stream_server_test.py`: video streaming from Pi to computer  \n  &emsp; &emsp;  `ultrasonic_server_test.py`: sensor data streaming from Pi to computer  \n  &emsp; &emsp;  **model_train_test/**  \n      &emsp; &emsp;  &emsp; &emsp; `data_test.npz`: sample data  \n      &emsp; &emsp;  &emsp; &emsp; `train_predict_test.ipynb`: a jupyter notebook that goes through neural network model in OpenCV3  \n  \n**raspberryPi/**    \n  &emsp; &emsp;  `stream_client.py`:        stream video frames in jpeg format to the host computer  \n  &emsp; &emsp;  `ultrasonic_client.py`:    send distance data measured by sensor to the host computer  \n  \n**arduino/**  \n  &emsp; &emsp;  `rc_keyboard_control.ino`: control RC car controller  \n  \n**computer/**    \n  &emsp; &emsp;  **cascade_xml/**  \n      &emsp; &emsp;  &emsp; &emsp;  trained cascade classifiers  \n  &emsp; &emsp;  **chess_board/**   \n      &emsp; &emsp;  &emsp; &emsp;  images for calibration, captured by pi camera  \n      \n  &emsp; &emsp;  `picam_calibration.py`:     pi camera calibration  \n  &emsp; &emsp;  `collect_training_data.py`: collect images in grayscale, data saved as `*.npz`  \n  &emsp; &emsp;  `model.py`:                 neural network model  \n  &emsp; &emsp;  `model_training.py`:        model training and validation  \n  &emsp; &emsp;  `rc_driver_helper.py`:      helper classes/functions for `rc_driver.py`  \n  &emsp; &emsp;  `rc_driver.py`:             receive data from raspberry pi and drive the RC car based on model prediction  \n  &emsp; &emsp;  `rc_driver_nn_only.py`:     simplified `rc_driver.py` without object detection  \n  \n  \n**Traffic_signal**  \n  &emsp; &emsp;  trafic signal sketch contributed by [@geek111](https://github.com/geek1111)\n\n\n### How to drive\n1. **Testing:** Flash `rc_keyboard_control.ino` to Arduino and run `rc_control_test.py` to drive the RC car with keyboard. Run `stream_server_test.py` on computer and then run `stream_client.py` on raspberry pi to test video streaming. Similarly, `ultrasonic_server_test.py` and `ultrasonic_client.py` can be used for sensor data streaming testing.   \n\n2. **Pi Camera calibration (optional):** Take multiple chess board images using pi camera module at various angles and put them into **`chess_board`** folder, run `picam_calibration.py` and returned parameters from the camera matrix will be used in `rc_driver.py`.\n\n3. **Collect training/validation data:** First run `collect_training_data.py` and then run `stream_client.py` on raspberry pi. Press arrow keys to drive the RC car, press `q` to exit. Frames are saved only when there is a key press action. Once exit, data will be saved into newly created **`training_data`** folder.\n\n4. **Neural network training:** Run `model_training.py` to train a neural network model. Please feel free to tune the model architecture/parameters to achieve a better result. After training, model will be saved into newly created **`saved_model`** folder.\n\n5. **Cascade classifiers training (optional):** Trained stop sign and traffic light classifiers are included in the **`cascade_xml`** folder, if you are interested in training your own classifiers, please refer to [OpenCV doc](http://docs.opencv.org/doc/user_guide/ug_traincascade.html) and this great [tutorial](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n6. **Self-driving in action**: First run `rc_driver.py` to start the server on the computer (for simplified no object detection version, run `rc_driver_nn_only.py` instead), and then run `stream_client.py` and `ultrasonic_client.py` on raspberry pi. \n\n[\u4e2d\u6587\u6587\u6863](https://github.com/zhaoying9105/AutoRCCar) (\u611f\u8c22[zhaoying9105](https://github.com/zhaoying9105))\n"
 },
 {
  "repo": "bijection/sistine",
  "language": "Python",
  "readme_contents": "# Project Sistine\n\n![Sistine * 3/2](splash.png)\n\nWe turned a MacBook into a touchscreen using only $1 of hardware and a little bit of computer vision. The proof-of-concept, dubbed \u201cProject Sistine\u201d after our [recreation](https://www.anishathalye.com/media/2018/04/03/thumbnail.jpg) of the famous [painting](https://en.wikipedia.org/wiki/The_Creation_of_Adam) in the Sistine Chapel, was prototyped by [Anish Athalye](https://www.anishathalye.com/), [Kevin Kwok](https://twitter.com/antimatter15), [Guillermo Webster](https://twitter.com/biject), and [Logan Engstrom](https://github.com/lengstrom) in about 16 hours.\n\n## Basic Principle\n\nThe basic principle behind Sistine is simple. Surfaces viewed from an angle tend to look shiny, and you can tell if a finger is touching the surface by checking if it\u2019s touching its own reflection.\n\n![Hover versus touch](https://www.anishathalye.com/media/2018/04/03/explanation.png)\n\nKevin, back in middle school, noticed this phenomenon and built [ShinyTouch](https://antimatter15.com/project/shinytouch/), utilizing an external webcam to build a touch input system requiring virtually no setup. We wanted to see if we could miniaturize the idea and make it work without an external webcam. Our idea was to retrofit a small mirror in front of a MacBook\u2019s built-in webcam, so that the webcam would be looking down at the computer screen at a sharp angle. The camera would be able to see fingers hovering over or touching the screen, and we\u2019d be able to translate the video feed into touch events using computer vision.\n\n(Read the rest of our blog post, including a video demo and a high-level explanation of the algorithm, [here](https://www.anishathalye.com/2018/04/03/macbook-touchscreen/))\n\n## Installation (with Homebrew Python)\n\n* First, make sure you have [Mac Homebrew](https://brew.sh/) installed on your computer. If not, you can install it by running `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\n* Install Python 2 via Homebrew with `brew install python2`\n\n* Install OpenCV 3 via Homebrew with `brew install opencv3`\n\n* Install PyObjC via Pip with `pip2 install pyobjc`\n\n## Running\n\nRun `python2 sistine.py`\n\n## License\n\nCopyright (c) 2016-2018 Anish Athalye, Kevin Kwok, Guillermo Webster, and Logan\nEngstrom. Released under the MIT License. See [LICENSE.md][license] for\ndetails.\n\n[license]: LICENSE.md\n"
 },
 {
  "repo": "jrosebr1/imutils",
  "language": "Python",
  "readme_contents": "# imutils\nA series of convenience functions to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and ***both*** Python 2.7 and Python 3.\n\nFor more information, along with a detailed code review check out the following posts on the [PyImageSearch.com](http://www.pyimagesearch.com) blog:\n\n- [http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/](http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/)\n- [http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/)\n- [http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/](http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/)\n\n## Installation\nProvided you already have NumPy, SciPy, Matplotlib, and OpenCV already installed, the `imutils` package is completely `pip`-installable:\n\n<pre>$ pip install imutils</pre>\n\n## Finding function OpenCV functions by name\nOpenCV can be a big, hard to navigate library, especially if you are just getting started learning computer vision and image processing. The `find_function` method allows you to quickly search function names across modules (and optionally sub-modules) to find the function you are looking for.\n\n#### Example:\nLet's find all function names that contain the text `contour`:\n\n<pre>import imutils\nimutils.find_function(\"contour\")</pre>\n\n#### Output:\n<pre>1. contourArea\n2. drawContours\n3. findContours\n4. isContourConvex</pre>\n\nThe `contourArea` function could therefore be accessed via: `cv2.contourArea`\n\n\n## Translation\nTranslation is the shifting of an image in either the *x* or *y* direction. To translate an image in OpenCV you would need to supply the *(x, y)*-shift, denoted as *(t<sub>x</sub>, t<sub>y</sub>)* to construct the translation matrix *M*:\n\n![Translation equation](docs/images/translation_eq.png?raw=true)\n\nAnd from there, you would need to apply the `cv2.warpAffine` function.\n\nInstead of manually constructing the translation matrix *M* and calling `cv2.warpAffine`, you can simply make a call to the `translate` function of `imutils`.\n\n#### Example:\n<pre># translate the image x=25 pixels to the right and y=75 pixels up\ntranslated = imutils.translate(workspace, 25, -75)</pre>\n\n#### Output:\n\n<img src=\"docs/images/translation.png?raw=true\" alt=\"Translation example\" style=\"max-width: 500px;\">\n\n## Rotation\nRotating an image in OpenCV is accomplished by making a call to `cv2.getRotationMatrix2D` and `cv2.warpAffine`. Further care has to be taken to supply the *(x, y)*-coordinate of the point the image is to be rotated about. These calculation calls can quickly add up and make your code bulky and less readable. The `rotate` function in `imutils` helps resolve this problem.\n\n#### Example:\n<pre># loop over the angles to rotate the image\nfor angle in xrange(0, 360, 90):\n\t# rotate the image and display it\n\trotated = imutils.rotate(bridge, angle=angle)\n\tcv2.imshow(\"Angle=%d\" % (angle), rotated)</pre>\n\n#### Output:\n\n<img src=\"docs/images/rotation.png?raw=true\" alt=\"Rotation example\" style=\"max-width: 500px;\">\n\n## Resizing\nResizing an image in OpenCV is accomplished by calling the `cv2.resize` function. However, special care needs to be taken to ensure that the aspect ratio is maintained.  This `resize` function of `imutils` maintains the aspect ratio and provides the keyword arguments `width` and `height` so the image can be resized to the intended width/height while (1) maintaining aspect ratio and (2) ensuring the dimensions of the image do not have to be explicitly computed by the developer.\n\nAnother optional keyword argument, `inter`, can be used to specify interpolation method as well.\n\n#### Example:\n<pre># loop over varying widths to resize the image to\nfor width in (400, 300, 200, 100):\n\t# resize the image and display it\n\tresized = imutils.resize(workspace, width=width)\n\tcv2.imshow(\"Width=%dpx\" % (width), resized)</pre>\n\n#### Output:\n\n<img src=\"docs/images/resizing.png?raw=true\" alt=\"Resizing example\" style=\"max-width: 500px;\">\n\n## Skeletonization\nSkeletonization is the process of constructing the \"topological skeleton\" of an object in an image, where the object is presumed to be white on a black background. OpenCV does not provide a function to explicitly construct the skeleton, but does provide the morphological and binary functions to do so.\n\nFor convenience, the `skeletonize` function of `imutils` can be used to construct the topological skeleton of the image.\n\nThe first argument, `size` is the size of the structuring element kernel. An optional argument, `structuring`, can be used to control the structuring element -- it defaults to `cv2.MORPH_RECT`\t, but can be any valid structuring element.\n\n#### Example:\n<pre># skeletonize the image\ngray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nskeleton = imutils.skeletonize(gray, size=(3, 3))\ncv2.imshow(\"Skeleton\", skeleton)</pre>\n\n#### Output:\n\n<img src=\"docs/images/skeletonization.png?raw=true\" alt=\"Skeletonization example\" style=\"max-width: 500px;\">\n\n## Displaying with Matplotlib\nIn the Python bindings of OpenCV, images are represented as NumPy arrays in BGR order. This works fine when using the `cv2.imshow` function. However, if you intend on using Matplotlib, the `plt.imshow` function assumes the image is in RGB order. A simple call to `cv2.cvtColor` will resolve this problem, or you can use the `opencv2matplotlib` convenience function.\n\n#### Example:\n<pre># INCORRECT: show the image without converting color spaces\nplt.figure(\"Incorrect\")\nplt.imshow(cactus)\n\n# CORRECT: convert color spaces before using plt.imshow\nplt.figure(\"Correct\")\nplt.imshow(imutils.opencv2matplotlib(cactus))\nplt.show()</pre>\n\n#### Output:\n\n<img src=\"docs/images/matplotlib.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## URL to Image\nThis the `url_to_image` function accepts a single parameter: the `url` of the image we want to download and convert to a NumPy array in OpenCV format. This function performs the download in-memory. The `url_to_image` function has been detailed [here](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/) on the PyImageSearch blog.\n\n#### Example:\n<pre>url = \"http://pyimagesearch.com/static/pyimagesearch_logo_github.png\"\nlogo = imutils.url_to_image(url)\ncv2.imshow(\"URL to Image\", logo)\ncv2.waitKey(0)</pre>\n\n#### Output:\n\n<img src=\"docs/images/url_to_image.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Checking OpenCV Versions\nOpenCV 3 has finally been released! But with the major release becomes backward compatibility issues (such as with the `cv2.findContours` and `cv2.normalize` functions). If you want your OpenCV 3 code to be backwards compatible with OpenCV 2.4.X, you'll need to take special care to check which version of OpenCV is currently being used and then take appropriate action. The `is_cv2()` and `is_cv3()` are simple functions that can be used to automatically determine the OpenCV version of the current environment.\n\n#### Example:\n<pre>print(\"Your OpenCV version: {}\".format(cv2.__version__))\nprint(\"Are you using OpenCV 2.X? {}\".format(imutils.is_cv2()))\nprint(\"Are you using OpenCV 3.X? {}\".format(imutils.is_cv3()))</pre>\n\n#### Output:\n<pre>Your OpenCV version: 3.0.0\nAre you using OpenCV 2.X? False\nAre you using OpenCV 3.X? True</pre>\n\n## Automatic Canny Edge Detection\nThe Canny edge detector requires two parameters when performing hysteresis. However, tuning these two parameters to obtain an optimal edge map is non-trivial, especially when working with a dataset of images. Instead, we can use the `auto_canny` function which uses the median of the grayscale pixel intensities to derive the upper and lower thresholds. You can read more about the `auto_canny` function [here](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/).\n\n#### Example:\n<pre>gray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nedgeMap = imutils.auto_canny(gray)\ncv2.imshow(\"Original\", logo)\ncv2.imshow(\"Automatic Edge Map\", edgeMap)</pre>\n\n#### Output:\n\n<img src=\"docs/images/auto_canny.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## 4-point Perspective Transform\nA common task in computer vision and image processing is to perform a 4-point perspective transform of a ROI in an image and obtain a top-down, \"birds eye view\" of the ROI. The `perspective` module takes care of this for you. A real-world example of applying a 4-point perspective transform can be bound in this blog on on [building a kick-ass mobile document scanner](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/).\n\n#### Example\nSee the contents of `demos/perspective_transform.py`\n\n#### Output:\n\n<img src=\"docs/images/perspective_transform.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Sorting Contours\nThe contours returned from `cv2.findContours` are unsorted. By using the `contours` module the the `sort_contours` function we can sort a list of contours from left-to-right, right-to-left, top-to-bottom, and bottom-to-top, respectively.\n\n#### Example:\nSee the contents of `demos/sorting_contours.py`\n\n#### Output:\n\n<img src=\"docs/images/sorting_contours.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## (Recursively) Listing Paths to Images\nThe `paths` sub-module of `imutils` includes a function to recursively find images based on a root directory.\n\n#### Example:\nAssuming we are in the `demos` directory, let's list the contents of the `../demo_images`:\n\n<pre>from imutils import paths\nfor imagePath in paths.list_images(\"../demo_images\"):\n\tprint imagePath</pre>\n\n#### Output:\n<pre>../demo_images/bridge.jpg\n../demo_images/cactus.jpg\n../demo_images/notecard.png\n../demo_images/pyimagesearch_logo.jpg\n../demo_images/shapes.png\n../demo_images/workspace.jpg</pre>\n"
 },
 {
  "repo": "peterbraden/node-opencv",
  "language": "C++",
  "readme_contents": "# node-opencv\n\n[![Build Status](https://secure.travis-ci.org/peterbraden/node-opencv.svg)](http://travis-ci.org/peterbraden/node-opencv)\n\n[OpenCV](http://opencv.org) bindings for Node.js. OpenCV is\nthe defacto computer vision library - by interfacing with it natively in node,\nwe get powerful real time vision in js.\n\nPeople are using node-opencv to fly control quadrocoptors, detect faces from\nwebcam images and annotate video streams. If you're using it for something\ncool, I'd love to hear about it!\n\n## Install\n\nYou'll need OpenCV 2.3.1 or newer installed before installing node-opencv.\n\n## Specific for macOS\nInstall OpenCV using brew\n```bash\nbrew install pkg-config\nbrew install opencv@2\nbrew link --force opencv@2\n```\n\n\n## Specific for Windows\n1. Download and install OpenCV (Be sure to use a 2.4 version) @\nhttp://opencv.org/releases.html\nFor these instructions we will assume OpenCV is put at C:\\OpenCV, but you can\nadjust accordingly.\n\n2. If you haven't already, create a system variable called OPENCV_DIR and set it\n   to C:\\OpenCV\\build\\x64\\vc12\n\n   Make sure the \"x64\" part matches the version of NodeJS you are using.\n\n   Also add the following to your system PATH\n        ;%OPENCV_DIR%\\bin\n\n3. Install Visual Studio 2013. Make sure to get the C++ components.\n   You can use a different edition, just make sure OpenCV supports it, and you\n   set the \"vcxx\" part of the variables above to match.\n\n4. Download peterbraden/node-opencv fork\ngit clone https://github.com/peterbraden/node-opencv\n\n5. run npm install\n\n```bash\n$ npm install opencv\n```\n\n## Examples\nRun the examples from the parent directory.\n\n### Face Detection\n\n```javascript\ncv.readImage(\"./examples/files/mona.png\", function(err, im){\n  im.detectObject(cv.FACE_CASCADE, {}, function(err, faces){\n    for (var i=0;i<faces.length; i++){\n      var x = faces[i]\n      im.ellipse(x.x + x.width/2, x.y + x.height/2, x.width/2, x.height/2);\n    }\n    im.save('./out.jpg');\n  });\n})\n```\n\n\n## API Documentation\n\n### Matrix\n\nThe [matrix](http://opencv.jp/opencv-2svn_org/cpp/core_basic_structures.html#mat) is the most useful\nbase data structure in OpenCV. Things like images are just matrices of pixels.\n\n#### Creation\n\n```javascript\nnew Matrix(rows, cols)\n```\n\nOr if you're thinking of a Matrix as an image:\n\n```javascript\nnew Matrix(height, width)\n```\n\nOr you can use opencv to read in image files. Supported formats are in the OpenCV docs, but jpgs etc are supported.\n\n```javascript\ncv.readImage(filename, function(err, mat){\n  ...\n})\n\ncv.readImage(buffer, function(err, mat){\n  ...\n})\n```\n\nIf you need to pipe data into an image, you can use an ImageDataStream:\n\n```javascript\nvar s = new cv.ImageDataStream()\n\ns.on('load', function(matrix){\n  ...\n})\n\nfs.createReadStream('./examples/files/mona.png').pipe(s);\n```\n\nIf however, you have a series of images, and you wish to stream them into a\nstream of Matrices, you can use an ImageStream. Thus:\n\n```javascript\nvar s = new cv.ImageStream()\n\ns.on('data', function(matrix){\n   ...\n})\n\nardrone.createPngStream().pipe(s);\n```\n\nNote: Each 'data' event into the ImageStream should be a complete image buffer.\n\n\n\n#### Accessing Data\n\n```javascript\nvar mat = new cv.Matrix.Eye(4,4); // Create identity matrix\n\nmat.get(0,0) // 1\n\nmat.row(0)  // [1,0,0,0]\nmat.col(3)  // [0,0,0,1]\n```\n\n##### Save\n\n```javascript\nmat.save('./pic.jpg')\n```\n\nor:\n\n```javascript\nvar buff = mat.toBuffer()\n```\n\n#### Image Processing\n\n```javascript\nim.convertGrayscale()\nim.canny(5, 300)\nim.houghLinesP()\n```\n\n\n#### Simple Drawing\n\n```javascript\nim.ellipse(x, y)\nim.line([x1,y1], [x2, y2])\n```\n\n#### Object Detection\n\nThere is a shortcut method for\n[Viola-Jones Haar Cascade](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) object\ndetection. This can be used for face detection etc.\n\n```javascript\nmat.detectObject(haar_cascade_xml, opts, function(err, matches){})\n```\n\nFor convenience in face detection, cv.FACE_CASCADE is a cascade that can be used for frontal face detection.\n\nAlso:\n\n```javascript\nmat.goodFeaturesToTrack\n```\n\n#### Contours\n\n```javascript\nmat.findCountours\nmat.drawContour\nmat.drawAllContours\n```\n\n### Using Contours\n\n`findContours` returns a `Contours` collection object, not a native array. This object provides\nfunctions for accessing, computing with, and altering the contours contained in it.\nSee [relevant source code](src/Contours.cc) and [examples](examples/)\n\n```javascript\nvar contours = im.findContours();\n\n// Count of contours in the Contours object\ncontours.size();\n\n// Count of corners(verticies) of contour `index`\ncontours.cornerCount(index);\n\n// Access vertex data of contours\nfor(var c = 0; c < contours.size(); ++c) {\n  console.log(\"Contour \" + c);\n  for(var i = 0; i < contours.cornerCount(c); ++i) {\n    var point = contours.point(c, i);\n    console.log(\"(\" + point.x + \",\" + point.y + \")\");\n  }\n}\n\n// Computations of contour `index`\ncontours.area(index);\ncontours.arcLength(index, isClosed);\ncontours.boundingRect(index);\ncontours.minAreaRect(index);\ncontours.isConvex(index);\ncontours.fitEllipse(index);\n\n// Destructively alter contour `index`\ncontours.approxPolyDP(index, epsilon, isClosed);\ncontours.convexHull(index, clockwise);\n```\n\n#### Face Recognization\n\nIt requires to `train` then `predict`. For acceptable result, the face should be cropped, grayscaled and aligned, I ignore this part so that we may focus on the api usage.\n\n** Please ensure your OpenCV 3.2+ is configured with contrib. MacPorts user may `port install opencv +contrib` **\n\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst cv = require('opencv');\n\nfunction forEachFileInDir(dir, cb) {\n  let f = fs.readdirSync(dir);\n  f.forEach(function (fpath, index, array) {\n    if (fpath != '.DS_Store')\n     cb(path.join(dir, fpath));\n  });\n}\n\nlet dataDir = \"./_training\";\nfunction trainIt (fr) {\n  // if model existe, load it\n  if ( fs.existsSync('./trained.xml') ) {\n    fr.loadSync('./trained.xml');\n    return;\n  }\n\n  // else train a model\n  let samples = [];\n  forEachFileInDir(dataDir, (f)=>{\n      cv.readImage(f, function (err, im) {\n          // Assume all training photo are named as id_xxx.jpg\n          let labelNumber = parseInt(path.basename(f).substring(3));\n          samples.push([labelNumber, im]);\n      })\n  })\n\n  if ( samples.length > 3 ) {\n    // There are async and sync version of training method:\n    // .train(info, cb)\n    //     cb : standard Nan::Callback\n    //     info : [[intLabel,matrixImage],...])\n    // .trainSync(info)\n    fr.trainSync(samples);\n    fr.saveSync('./trained.xml');\n  }else {\n    console.log('Not enough images uploaded yet', cvImages)\n  }\n}\n\nfunction predictIt(fr, f){\n  cv.readImage(f, function (err, im) {\n    let result = fr.predictSync(im);\n    console.log(`recognize result:(${f}) id=${result.id} conf=${100.0-result.confidence}`);\n  });\n}\n\n//using defaults: .createLBPHFaceRecognizer(radius=1, neighbors=8, grid_x=8, grid_y=8, threshold=80)\nconst fr = new cv.FaceRecognizer();\ntrainIt(fr);\nforEachFileInDir('./_bench', (f) => predictIt(fr, f));\n```\n\n## Test\n\nUsing [tape](https://github.com/substack/tape). Run with command:\n\n`npm test`.\n\n## Contributing\n\nI (@peterbraden) don't spend much time maintaining this library, it runs\nprimarily on contributor support. I'm happy to accept most PR's if the tests run\ngreen, all new functionality is tested, and there are no objections in the PR.\n\nBecause I haven't got much time for maintenance, I'd prefer to keep an absolute\nminimum of dependencies.\n\n\n## MIT License\nThe library is distributed under the MIT License - if for some reason that\ndoesn't work for you please get in touch.\n"
 },
 {
  "repo": "bytedeco/javacv",
  "language": "Java",
  "readme_contents": "JavaCV\r\n======\r\n\r\n[![Gitter](https://badges.gitter.im/bytedeco/javacv.svg)](https://gitter.im/bytedeco/javacv) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform) [![Sonatype Nexus (Snapshots)](https://img.shields.io/nexus/s/https/oss.sonatype.org/org.bytedeco/javacv.svg)](http://bytedeco.org/builds/) [![Build Status](https://travis-ci.org/bytedeco/javacv.svg?branch=master)](https://travis-ci.org/bytedeco/javacv) <sup>Commercial support:</sup> [![xscode](https://img.shields.io/badge/Available%20on-xs%3Acode-blue?style=?style=plastic&logo=appveyor&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////////VXz1bAAAAAJ0Uk5T/wDltzBKAAAAlUlEQVR42uzXSwqAMAwE0Mn9L+3Ggtgkk35QwcnSJo9S+yGwM9DCooCbgn4YrJ4CIPUcQF7/XSBbx2TEz4sAZ2q1RAECBAiYBlCtvwN+KiYAlG7UDGj59MViT9hOwEqAhYCtAsUZvL6I6W8c2wcbd+LIWSCHSTeSAAECngN4xxIDSK9f4B9t377Wd7H5Nt7/Xz8eAgwAvesLRjYYPuUAAAAASUVORK5CYII=)](https://xscode.com/bytedeco/javacv)\r\n\r\n\r\nIntroduction\r\n------------\r\nJavaCV uses wrappers from the [JavaCPP Presets](https://github.com/bytedeco/javacpp-presets) of commonly used libraries by researchers in the field of computer vision ([OpenCV](http://opencv.org/), [FFmpeg](http://ffmpeg.org/), [libdc1394](http://damien.douxchamps.net/ieee1394/libdc1394/), [PGR FlyCapture](https://www.ptgrey.com/flycapture-sdk), [OpenKinect](http://openkinect.org/), [librealsense](https://github.com/IntelRealSense/librealsense), [CL PS3 Eye Driver](https://codelaboratories.com/downloads/), [videoInput](http://muonics.net/school/spring05/videoInput/), [ARToolKitPlus](https://launchpad.net/artoolkitplus), [flandmark](http://cmp.felk.cvut.cz/~uricamic/flandmark/), [Leptonica](http://www.leptonica.org/), and [Tesseract](https://github.com/tesseract-ocr/tesseract)) and provides utility classes to make their functionality easier to use on the Java platform, including Android.\r\n\r\nJavaCV also comes with hardware accelerated full-screen image display (`CanvasFrame` and `GLCanvasFrame`), easy-to-use methods to execute code in parallel on multiple cores (`Parallel`), user-friendly geometric and color calibration of cameras and projectors (`GeometricCalibrator`, `ProCamGeometricCalibrator`, `ProCamColorCalibrator`), detection and matching of feature points (`ObjectFinder`), a set of classes that implement direct image alignment of projector-camera systems (mainly `GNImageAligner`, `ProjectiveTransformer`, `ProjectiveColorTransformer`, `ProCamTransformer`, and `ReflectanceInitializer`), a blob analysis package (`Blobs`), as well as miscellaneous functionality in the `JavaCV` class. Some of these classes also have an OpenCL and OpenGL counterpart, their names ending with `CL` or starting with `GL`, i.e.: `JavaCVCL`, `GLCanvasFrame`, etc.\r\n\r\nTo learn how to use the API, since documentation currently lacks, please refer to the [Sample Usage](#sample-usage) section below as well as the [sample programs](https://github.com/bytedeco/javacv/tree/master/samples/), including two for Android (`FacePreview.java` and `RecordActivity.java`), also found in the `samples` directory. You may also find it useful to refer to the source code of [ProCamCalib](https://github.com/bytedeco/procamcalib) and [ProCamTracker](https://github.com/bytedeco/procamtracker) as well as [examples ported from OpenCV2 Cookbook](https://github.com/bytedeco/javacv-examples/) and the associated [wiki pages](https://github.com/bytedeco/javacv-examples/tree/master/OpenCV_Cookbook).\r\n\r\nPlease keep me informed of any updates or fixes you make to the code so that I may integrate them into the next release. Thank you! And feel free to ask questions on [the mailing list](http://groups.google.com/group/javacv) if you encounter any problems with the software! I am sure it is far from perfect...\r\n\r\n\r\nDownloads\r\n---------\r\nArchives containing JAR files are available as [releases](https://github.com/bytedeco/javacv/releases). The binary archive contains builds for Android, iOS, Linux, Mac OS X, and Windows. The JAR files for specific child modules or platforms can also be obtained individually from the [Maven Central Repository](http://search.maven.org/#search|ga|1|bytedeco).\r\n\r\nTo install manually the JAR files, follow the instructions in the [Manual Installation](#manual-installation) section below.\r\n\r\nWe can also have everything downloaded and installed automatically with:\r\n\r\n * Maven (inside the `pom.xml` file)\r\n```xml\r\n  <dependency>\r\n    <groupId>org.bytedeco</groupId>\r\n    <artifactId>javacv-platform</artifactId>\r\n    <version>1.5.4</version>\r\n  </dependency>\r\n```\r\n\r\n * Gradle (inside the `build.gradle` file)\r\n```groovy\r\n  dependencies {\r\n    implementation group: 'org.bytedeco', name: 'javacv-platform', version: '1.5.4'\r\n  }\r\n```\r\n\r\n * Leiningen (inside the `project.clj` file)\r\n```clojure\r\n  :dependencies [\r\n    [org.bytedeco/javacv-platform \"1.5.4\"]\r\n  ]\r\n```\r\n\r\n * sbt (inside the `build.sbt` file)\r\n```scala\r\n  libraryDependencies += \"org.bytedeco\" % \"javacv-platform\" % \"1.5.4\"\r\n```\r\n\r\nThis downloads binaries for all platforms, but to get binaries for only one platform we can set the `javacpp.platform` system property (via the `-D` command line option) to something like `android-arm`, `linux-x86_64`, `macosx-x86_64`, `windows-x86_64`, etc. Please refer to the [README.md file of the JavaCPP Presets](https://github.com/bytedeco/javacpp-presets#downloads) for details. Another option available to Gradle users is [Gradle JavaCPP](https://github.com/bytedeco/gradle-javacpp), and similarly for Scala users there is [SBT-JavaCV](https://github.com/bytedeco/sbt-javacv).\r\n\r\n\r\nRequired Software\r\n-----------------\r\nTo use JavaCV, you will first need to download and install the following software:\r\n\r\n * An implementation of Java SE 7 or newer:\r\n   * OpenJDK  http://openjdk.java.net/install/  or\r\n   * Oracle JDK  http://www.oracle.com/technetwork/java/javase/downloads/  or\r\n   * IBM JDK  http://www.ibm.com/developerworks/java/jdk/\r\n\r\nFurther, although not always required, some functionality of JavaCV also relies on:\r\n\r\n * CL Eye Platform SDK (Windows only)  http://codelaboratories.com/downloads/\r\n * Android SDK API 21 or newer  http://developer.android.com/sdk/\r\n * JOCL and JOGL from JogAmp  http://jogamp.org/\r\n\r\nFinally, please make sure everything has the same bitness: **32-bit and 64-bit modules do not mix under any circumstances**.\r\n\r\n\r\nManual Installation\r\n-------------------\r\nSimply put all the desired JAR files (`opencv*.jar`, `ffmpeg*.jar`, etc.), in addition to `javacpp.jar` and `javacv.jar`, somewhere in your class path. Here are some more specific instructions for common cases:\r\n\r\nNetBeans (Java SE 7 or newer):\r\n\r\n 1. In the Projects window, right-click the Libraries node of your project, and select \"Add JAR/Folder...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nEclipse (Java SE 7 or newer):\r\n\r\n 1. Navigate to Project > Properties > Java Build Path > Libraries and click \"Add External JARs...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nIntelliJ IDEA (Android 5.0 or newer):\r\n\r\n 1. Follow the instructions on this page: http://developer.android.com/training/basics/firstapp/\r\n 2. Copy all the JAR files into the `app/libs` subdirectory.\r\n 3. Navigate to File > Project Structure > app > Dependencies, click `+`, and select \"2 File dependency\".\r\n 4. Select all the JAR files from the `libs` subdirectory.\r\n\r\nAfter that, the wrapper classes for OpenCV and FFmpeg, for example, can automatically access all of their C/C++ APIs:\r\n\r\n * [OpenCV documentation](http://docs.opencv.org/master/)\r\n * [FFmpeg documentation](http://ffmpeg.org/doxygen/trunk/)\r\n\r\n\r\nSample Usage\r\n------------\r\nThe class definitions are basically ports to Java of the original header files in C/C++, and I deliberately decided to keep as much of the original syntax as possible. For example, here is a method that tries to load an image file, smooth it, and save it back to disk:\r\n\r\n```java\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgcodecs.*;\r\n\r\npublic class Smoother {\r\n    public static void smooth(String filename) {\r\n        Mat image = imread(filename);\r\n        if (image != null) {\r\n            GaussianBlur(image, image, new Size(3, 3), 0);\r\n            imwrite(filename, image);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nJavaCV also comes with helper classes and methods on top of OpenCV and FFmpeg to facilitate their integration to the Java platform. Here is a small demo program demonstrating the most frequently useful parts:\r\n\r\n```java\r\nimport java.io.File;\r\nimport java.net.URL;\r\nimport org.bytedeco.javacv.*;\r\nimport org.bytedeco.javacpp.*;\r\nimport org.bytedeco.javacpp.indexer.*;\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport org.bytedeco.opencv.opencv_calib3d.*;\r\nimport org.bytedeco.opencv.opencv_objdetect.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_calib3d.*;\r\nimport static org.bytedeco.opencv.global.opencv_objdetect.*;\r\n\r\npublic class Demo {\r\n    public static void main(String[] args) throws Exception {\r\n        String classifierName = null;\r\n        if (args.length > 0) {\r\n            classifierName = args[0];\r\n        } else {\r\n            URL url = new URL(\"https://raw.github.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\");\r\n            File file = Loader.cacheResource(url);\r\n            classifierName = file.getAbsolutePath();\r\n        }\r\n\r\n        // We can \"cast\" Pointer objects by instantiating a new object of the desired class.\r\n        CascadeClassifier classifier = new CascadeClassifier(classifierName);\r\n        if (classifier == null) {\r\n            System.err.println(\"Error loading classifier file \\\"\" + classifierName + \"\\\".\");\r\n            System.exit(1);\r\n        }\r\n\r\n        // The available FrameGrabber classes include OpenCVFrameGrabber (opencv_videoio),\r\n        // DC1394FrameGrabber, FlyCapture2FrameGrabber, OpenKinectFrameGrabber, OpenKinect2FrameGrabber,\r\n        // RealSenseFrameGrabber, RealSense2FrameGrabber, PS3EyeFrameGrabber, VideoInputFrameGrabber, and FFmpegFrameGrabber.\r\n        FrameGrabber grabber = FrameGrabber.createDefault(0);\r\n        grabber.start();\r\n\r\n        // CanvasFrame, FrameGrabber, and FrameRecorder use Frame objects to communicate image data.\r\n        // We need a FrameConverter to interface with other APIs (Android, Java 2D, JavaFX, Tesseract, OpenCV, etc).\r\n        OpenCVFrameConverter.ToMat converter = new OpenCVFrameConverter.ToMat();\r\n\r\n        // FAQ about IplImage and Mat objects from OpenCV:\r\n        // - For custom raw processing of data, createBuffer() returns an NIO direct\r\n        //   buffer wrapped around the memory pointed by imageData, and under Android we can\r\n        //   also use that Buffer with Bitmap.copyPixelsFromBuffer() and copyPixelsToBuffer().\r\n        // - To get a BufferedImage from an IplImage, or vice versa, we can chain calls to\r\n        //   Java2DFrameConverter and OpenCVFrameConverter, one after the other.\r\n        // - Java2DFrameConverter also has static copy() methods that we can use to transfer\r\n        //   data more directly between BufferedImage and IplImage or Mat via Frame objects.\r\n        Mat grabbedImage = converter.convert(grabber.grab());\r\n        int height = grabbedImage.rows();\r\n        int width = grabbedImage.cols();\r\n\r\n        // Objects allocated with `new`, clone(), or a create*() factory method are automatically released\r\n        // by the garbage collector, but may still be explicitly released by calling deallocate().\r\n        // You shall NOT call cvReleaseImage(), cvReleaseMemStorage(), etc. on objects allocated this way.\r\n        Mat grayImage = new Mat(height, width, CV_8UC1);\r\n        Mat rotatedImage = grabbedImage.clone();\r\n\r\n        // The OpenCVFrameRecorder class simply uses the VideoWriter of opencv_videoio,\r\n        // but FFmpegFrameRecorder also exists as a more versatile alternative.\r\n        FrameRecorder recorder = FrameRecorder.createDefault(\"output.avi\", width, height);\r\n        recorder.start();\r\n\r\n        // CanvasFrame is a JFrame containing a Canvas component, which is hardware accelerated.\r\n        // It can also switch into full-screen mode when called with a screenNumber.\r\n        // We should also specify the relative monitor/camera response for proper gamma correction.\r\n        CanvasFrame frame = new CanvasFrame(\"Some Title\", CanvasFrame.getDefaultGamma()/grabber.getGamma());\r\n\r\n        // Let's create some random 3D rotation...\r\n        Mat randomR    = new Mat(3, 3, CV_64FC1),\r\n            randomAxis = new Mat(3, 1, CV_64FC1);\r\n        // We can easily and efficiently access the elements of matrices and images\r\n        // through an Indexer object with the set of get() and put() methods.\r\n        DoubleIndexer Ridx = randomR.createIndexer(),\r\n                   axisIdx = randomAxis.createIndexer();\r\n        axisIdx.put(0, (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4);\r\n        Rodrigues(randomAxis, randomR);\r\n        double f = (width + height) / 2.0;  Ridx.put(0, 2, Ridx.get(0, 2) * f);\r\n                                            Ridx.put(1, 2, Ridx.get(1, 2) * f);\r\n        Ridx.put(2, 0, Ridx.get(2, 0) / f); Ridx.put(2, 1, Ridx.get(2, 1) / f);\r\n        System.out.println(Ridx);\r\n\r\n        // We can allocate native arrays using constructors taking an integer as argument.\r\n        Point hatPoints = new Point(3);\r\n\r\n        while (frame.isVisible() && (grabbedImage = converter.convert(grabber.grab())) != null) {\r\n            // Let's try to detect some faces! but we need a grayscale image...\r\n            cvtColor(grabbedImage, grayImage, CV_BGR2GRAY);\r\n            RectVector faces = new RectVector();\r\n            classifier.detectMultiScale(grayImage, faces);\r\n            long total = faces.size();\r\n            for (long i = 0; i < total; i++) {\r\n                Rect r = faces.get(i);\r\n                int x = r.x(), y = r.y(), w = r.width(), h = r.height();\r\n                rectangle(grabbedImage, new Point(x, y), new Point(x + w, y + h), Scalar.RED, 1, CV_AA, 0);\r\n\r\n                // To access or pass as argument the elements of a native array, call position() before.\r\n                hatPoints.position(0).x(x - w / 10     ).y(y - h / 10);\r\n                hatPoints.position(1).x(x + w * 11 / 10).y(y - h / 10);\r\n                hatPoints.position(2).x(x + w / 2      ).y(y - h / 2 );\r\n                fillConvexPoly(grabbedImage, hatPoints.position(0), 3, Scalar.GREEN, CV_AA, 0);\r\n            }\r\n\r\n            // Let's find some contours! but first some thresholding...\r\n            threshold(grayImage, grayImage, 64, 255, CV_THRESH_BINARY);\r\n\r\n            // To check if an output argument is null we may call either isNull() or equals(null).\r\n            MatVector contours = new MatVector();\r\n            findContours(grayImage, contours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);\r\n            long n = contours.size();\r\n            for (long i = 0; i < n; i++) {\r\n                Mat contour = contours.get(i);\r\n                Mat points = new Mat();\r\n                approxPolyDP(contour, points, arcLength(contour, true) * 0.02, true);\r\n                drawContours(grabbedImage, new MatVector(points), -1, Scalar.BLUE);\r\n            }\r\n\r\n            warpPerspective(grabbedImage, rotatedImage, randomR, rotatedImage.size());\r\n\r\n            Frame rotatedFrame = converter.convert(rotatedImage);\r\n            frame.showImage(rotatedFrame);\r\n            recorder.record(rotatedFrame);\r\n        }\r\n        frame.dispose();\r\n        recorder.stop();\r\n        grabber.stop();\r\n    }\r\n}\r\n```\r\n\r\nFurthermore, after creating a `pom.xml` file with the following content:\r\n```xml\r\n<project>\r\n    <modelVersion>4.0.0</modelVersion>\r\n    <groupId>org.bytedeco.javacv</groupId>\r\n    <artifactId>demo</artifactId>\r\n    <version>1.5.4</version>\r\n    <properties>\r\n        <maven.compiler.source>1.7</maven.compiler.source>\r\n        <maven.compiler.target>1.7</maven.compiler.target>\r\n    </properties>\r\n    <dependencies>\r\n        <dependency>\r\n            <groupId>org.bytedeco</groupId>\r\n            <artifactId>javacv-platform</artifactId>\r\n            <version>1.5.4</version>\r\n        </dependency>\r\n    </dependencies>\r\n    <build>\r\n        <sourceDirectory>.</sourceDirectory>\r\n    </build>\r\n</project>\r\n```\r\n\r\nAnd by placing the source code above in `Demo.java`, or similarly for other classes found in the [`samples`](samples), we can use the following command to have everything first installed automatically and then executed by Maven:\r\n```bash\r\n $ mvn compile exec:java -Dexec.mainClass=Demo\r\n```\r\n\r\n**Note**: In case of errors, please make sure that the `artifactId` in the `pom.xml` file reads `javacv-platform`, not `javacv` only, for example. The artifact `javacv-platform` adds all the necessary binary dependencies.\r\n\r\n\r\nBuild Instructions\r\n------------------\r\nIf the binary files available above are not enough for your needs, you might need to rebuild them from the source code. To this end, the project files were created for:\r\n\r\n * Maven 3.x  http://maven.apache.org/download.html\r\n * JavaCPP 1.5.4  https://github.com/bytedeco/javacpp\r\n * JavaCPP Presets 1.5.4  https://github.com/bytedeco/javacpp-presets\r\n\r\nOnce installed, simply call the usual `mvn install` command for JavaCPP, its Presets, and JavaCV. By default, no other dependencies than a C++ compiler for JavaCPP are required. Please refer to the comments inside the `pom.xml` files for further details.\r\n\r\nInstead of building the native libraries manually, we can run `mvn install` for JavaCV only and rely on the snapshot artifacts from the CI builds:\r\n\r\n * http://bytedeco.org/builds/\r\n\r\n\r\n----\r\nProject lead: Samuel Audet [samuel.audet `at` gmail.com](mailto:samuel.audet&nbsp;at&nbsp;gmail.com)  \r\nDeveloper site: https://github.com/bytedeco/javacv  \r\nDiscussion group: http://groups.google.com/group/javacv\r\n"
 },
 {
  "repo": "esimov/pigo",
  "language": "Go",
  "readme_contents": "<h1 align=\"center\"><img alt=\"pigo-logo\" src=\"https://user-images.githubusercontent.com/883386/55795932-8787cf00-5ad1-11e9-8c3e-8211ba9427d8.png\" height=240/></h1>\r\n\r\n[![Build Status](https://travis-ci.org/esimov/pigo.svg?branch=master)](https://travis-ci.org/esimov/pigo)\r\n[![GoDoc](https://godoc.org/github.com/golang/gddo?status.svg)](https://godoc.org/github.com/esimov/pigo/core)\r\n[![license](https://img.shields.io/github/license/esimov/pigo)](./LICENSE)\r\n[![release](https://img.shields.io/badge/release-v1.4.2-blue.svg)](https://github.com/esimov/pigo/releases/tag/v1.4.2)\r\n[![snapcraft](https://img.shields.io/badge/snapcraft-v1.3.0-green.svg)](https://snapcraft.io/pigo)\r\n\r\nPigo is a pure Go face detection, pupil/eyes localization and facial landmark points detection library based on ***Pixel Intensity Comparison-based Object detection*** paper (https://arxiv.org/pdf/1305.4537.pdf).\r\n\r\n| Rectangle face marker | Circle face marker\r\n|:--:|:--:\r\n| ![rectangle](https://user-images.githubusercontent.com/883386/40916662-2fbbae1a-6809-11e8-8afd-d4ed40c7d4e9.png) | ![circle](https://user-images.githubusercontent.com/883386/40916683-447088a8-6809-11e8-942f-3112c10bede3.png) |\r\n\r\n### Motivation\r\nI've intended to implement this face detection method because all of the existing solutions for face detection in the Go ecosystem are only bindings to some C/C++ libraries like OpenCV, but installing OpenCV on various platforms is cumbersome.\r\n\r\nThe library does not require any third party modules or applications to be installed. However in case you wish to try the real time, webcam based face detection you might need to have Python2 and OpenCV installed, but **the core API does not require any third party module or external dependency**.\r\n\r\n### Key features\r\n- [x] Does not require OpenCV or any 3rd party modules to be installed\r\n- [x] High processing speed\r\n- [x] There is no need for image preprocessing prior detection\r\n- [x] There is no need for the computation of integral images, image pyramid, HOG pyramid or any other similar data structure\r\n- [x] The face detection is based on pixel intensity comparison encoded in the binary file tree structure\r\n- [x] Fast detection of in-plane rotated faces\r\n- [x] The library can detect even faces with eyeglasses\r\n- [x] [Pupils/eyes localization](#pupils--eyes-localization)\r\n- [x] [Facial landmark points detection](#facial-landmark-points-detection)\r\n- [x] **[Webassembly support \ud83c\udf89](#wasm-webassembly-support)**\r\n\r\n### Todo\r\n- [ ] Features detection and description\r\n\r\n**The library can also detect in plane rotated faces.** For this reason a new `-angle` parameter have been included into the command line utility. The command below will generate the following result (see the table below for all the supported options).\r\n\r\n```bash\r\n$ pigo -in input.jpg -out output.jpg -cf cascade/facefinder -angle=0.8 -iou=0.01\r\n```\r\n\r\n| Input file | Output file\r\n|:--:|:--:\r\n| ![input](https://user-images.githubusercontent.com/883386/50761018-015db180-1272-11e9-93d9-d3693cae9d66.jpg) | ![output](https://user-images.githubusercontent.com/883386/50761024-03277500-1272-11e9-9c20-2568b87a2344.png) |\r\n\r\n\r\nNote: In case of in plane rotated faces the angle value should be adapted to the provided image.\r\n\r\n### Pupils / eyes localization\r\n\r\nStarting from **v1.2.0** Pigo offer pupils/eyes localization capabilites. The implementation is based on [Eye pupil localization with an ensemble of randomized trees](https://www.sciencedirect.com/science/article/abs/pii/S0031320313003294).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/puploc\r\n\r\n![puploc](https://user-images.githubusercontent.com/883386/62784340-f5b3c100-bac6-11e9-865e-a2b4b9520b08.png)\r\n\r\n### Facial landmark points detection\r\n\r\n**v1.3.0** marks a new milestone in the library evolution, Pigo being able for facial landmark points detection. The implementation is based on [Fast Localization of Facial Landmark Points](https://arxiv.org/pdf/1403.6888.pdf).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/facial_landmark\r\n\r\n![flp_example](https://user-images.githubusercontent.com/883386/66802771-3b0cc880-ef26-11e9-9ee3-7e9e981ef3f7.png)\r\n\r\n## Install\r\n\r\n**Important note: for the Webassembly demo at least Go 1.13 is required!**\r\n\r\nInstall Go, set your `GOPATH`, and make sure `$GOPATH/bin` is on your `PATH`.\r\n\r\n```bash\r\n$ export GOPATH=\"$HOME/go\"\r\n$ export PATH=\"$PATH:$GOPATH/bin\"\r\n```\r\nNext download the project and build the binary file.\r\n\r\n```bash\r\n$ go get -u -f github.com/esimov/pigo/cmd/pigo\r\n$ go install\r\n```\r\n\r\n### Binary releases\r\nIn case you do not have installed or do not wish to install Go, you can obtain the binary file from the [releases](https://github.com/esimov/pigo/releases) folder.\r\n\r\nThe library can be accessed as a snapcraft function too.\r\n\r\n<a href=\"https://snapcraft.io/pigo\"><img src=\"https://raw.githubusercontent.com/snapcore/snap-store-badges/master/EN/%5BEN%5D-snap-store-white-uneditable.png\" alt=\"snapcraft pigo\"></a>\r\n\r\n## API\r\nBelow is a minimal example of using the face detection API.\r\n\r\nFirst you need to load and parse the binary classifier, then convert the image to grayscale mode,\r\nand finally to run the cascade function which returns a slice containing the row, column, scale and the detection score.\r\n\r\n```Go\r\ncascadeFile, err := ioutil.ReadFile(\"/path/to/cascade/file\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %v\", err)\r\n}\r\n\r\nsrc, err := pigo.GetImage(\"/path/to/image\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Cannot open the image file: %v\", err)\r\n}\r\n\r\npixels := pigo.RgbToGrayscale(src)\r\ncols, rows := src.Bounds().Max.X, src.Bounds().Max.Y\r\n\r\ncParams := pigo.CascadeParams{\r\n\tMinSize:     20,\r\n\tMaxSize:     1000,\r\n\tShiftFactor: 0.1,\r\n\tScaleFactor: 1.1,\r\n\r\n\tImageParams: pigo.ImageParams{\r\n\t\tPixels: pixels,\r\n\t\tRows:   rows,\r\n\t\tCols:   cols,\r\n\t\tDim:    cols,\r\n\t},\r\n}\r\n\r\npigo := pigo.NewPigo()\r\n// Unpack the binary file. This will return the number of cascade trees,\r\n// the tree depth, the threshold and the prediction from tree's leaf nodes.\r\nclassifier, err := pigo.Unpack(cascadeFile)\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %s\", err)\r\n}\r\n\r\nangle := 0.0 // cascade rotation angle. 0.0 is 0 radians and 1.0 is 2*pi radians\r\n\r\n// Run the classifier over the obtained leaf nodes and return the detection results.\r\n// The result contains quadruplets representing the row, column, scale and detection score.\r\ndets := classifier.RunCascade(cParams, angle)\r\n\r\n// Calculate the intersection over union (IoU) of two clusters.\r\ndets = classifier.ClusterDetections(dets, 0.2)\r\n```\r\n\r\n**A note about imports**:  in order to decode the image you will need to import `image/jpeg` or `image/png` (depending on the provided image type) and the Pigo library as well, otherwise you will get a `\"Image: Unkown format\"` error. See the following example:\r\n```Go\r\nimport (\r\n    _ \"image/jpeg\"\r\n    pigo \"github.com/esimov/pigo/core\"\r\n)\r\n```\r\n\r\n## Usage\r\nA command line utility is bundled into the library to detect faces in static images.\r\n\r\n```bash\r\n$ pigo -in input.jpg -out out.jpg -cf cascade/facefinder\r\n```\r\n\r\n### Supported flags:\r\n\r\n```bash\r\n$ pigo --help\r\n\r\n\u250c\u2500\u2510\u252c\u250c\u2500\u2510\u250c\u2500\u2510\r\n\u251c\u2500\u2518\u2502\u2502 \u252c\u2502 \u2502\r\n\u2534  \u2534\u2514\u2500\u2518\u2514\u2500\u2518\r\n\r\nGo (Golang) Face detection library.\r\n    Version: 1.4.2\r\n\r\n  -angle float\r\n    \t0.0 is 0 radians and 1.0 is 2*pi radians\r\n  -cf string\r\n    \tCascade binary file\r\n  -flpc string\r\n    \tFacial landmark points cascade directory\r\n  -in string\r\n    \tSource image (default \"-\")\r\n  -iou float\r\n    \tIntersection over union (IoU) threshold (default 0.2)\r\n  -json string\r\n    \tOutput the detection points into a json file\r\n  -mark\r\n    \tMark detected eyes (default true)\r\n  -marker string\r\n    \tDetection marker: rect|circle|ellipse (default \"rect\")\r\n  -max int\r\n    \tMaximum size of face (default 1000)\r\n  -min int\r\n    \tMinimum size of face (default 20)\r\n  -out string\r\n    \tDestination image (default \"-\")\r\n  -plc string\r\n    \tPupils/eyes localization cascade file\r\n  -scale float\r\n    \tScale detection window by percentage (default 1.1)\r\n  -shift float\r\n    \tShift detection window by percentage (default 0.1)\r\n```\r\n\r\n**Important notice:** In case the `plc` flag is not empty and the provided path is a valid file it will run the pupil/eyes detection method. The same is true for the `flpc` flag, only that in this case you need to provide the directory to the landmark point cascades found under `cascades/lps`.\r\n\r\n### CLI command examples\r\nYou can also use the `stdin` and `stdout` pipe commands:\r\n\r\n```bash\r\n$ cat input/source.jpg | pigo > -in - -out - >out.jpg -cf=/path/to/cascade\r\n```\r\n\r\n`in` and `out` default to `-` so you can also use:\r\n```bash\r\n$ cat input/source.jpg | pigo >out.jpg -cf=/path/to/cascade\r\n$ pigo -out out.jpg < input/source.jpg -cf=/path/to/cascade\r\n```\r\nUsing the `empty` string as value for the `-out` flag will skip the image generation part. This, combined with the `-json` flag will encode the detection results into the specified json file. You can also use the pipe `-` value for the `-json` flag to output the detection coordinates to the standard (`stdout`) output.\r\n\r\n## Real time face detection (running as a shared object)\r\n\r\nIn case you wish to test the library real time face detection capabilities using a webcam, the `examples` folder contains a  web and a few Python examples. Prior running it you need to have Python2 and OpenCV2 installed.\r\n\r\nSelect one of the few Python files provided in the `examples` folder and simply run them. Each of them will execute the exported Go binary file as a shared library. This is also a proof of concept how Pigo can be integrated into different programming languages. I have provided examples only for Python, since this was the only viable way to access the webcam, the Go ecosystem suffering badly from a comprehensive, cross platform and widely available library for accessing the webcam.\r\n\r\n## WASM (Webassembly) support \ud83c\udf89\r\n\r\nStarting from version **v1.4.0** the library has been ported to [**WASM**](http://webassembly.org/). This gives the library a huge performance gain in terms of real time face detection capabilities. Form more details check the subpage description: https://github.com/esimov/pigo/tree/master/wasm.\r\n\r\n## Benchmark results\r\n\r\nBelow are the benchmark results obtained running Pigo against [GoCV](https://github.com/hybridgroup/gocv) using the same conditions.\r\n\r\n```\r\n    BenchmarkGoCV-4   \t       3\t 414122553 ns/op\t     704 B/op\t       1 allocs/op\r\n    BenchmarkPIGO-4   \t      10\t 173664832 ns/op\t       0 B/op\t       0 allocs/op\r\n    PASS\r\n    ok  \tgithub.com/esimov/gocv-test\t4.530s\r\n```\r\nThe code used for the above test can be found under the following link: https://github.com/esimov/pigo-gocv-benchmark\r\n\r\n## Author\r\n\r\n* Endre Simo ([@simo_endre](https://twitter.com/simo_endre))\r\n\r\n## License\r\n\r\nCopyright \u00a9 2019 Endre Simo\r\n\r\nThis software is distributed under the MIT license. See the [LICENSE](https://github.com/esimov/pigo/blob/master/LICENSE) file for the full license text.\r\n"
 },
 {
  "repo": "mapillary/OpenSfM",
  "language": "JavaScript",
  "readme_contents": "OpenSfM ![Docker workflow](https://github.com/mapillary/opensfm/workflows/Docker%20CI/badge.svg)\n=======\n\n## Overview\nOpenSfM is a Structure from Motion library written in Python. The library serves as a processing pipeline for reconstructing camera poses and 3D scenes from multiple images. It consists of basic modules for Structure from Motion (feature detection/matching, minimal solvers) with a focus on building a robust and scalable reconstruction pipeline. It also integrates external sensor (e.g. GPS, accelerometer) measurements for geographical alignment and robustness. A JavaScript viewer is provided to preview the models and debug the pipeline.\n\n<p align=\"center\">\n  <img src=\"https://docs.opensfm.org/_images/berlin_viewer.jpg\" />\n</p>\n\nCheckout this [blog post with more demos](http://blog.mapillary.com/update/2014/12/15/sfm-preview.html)\n\n\n## Getting Started\n\n* [Building the library][]\n* [Running a reconstruction][]\n* [Documentation][]\n\n\n[Building the library]: https://docs.opensfm.org/building.html (OpenSfM building instructions)\n[Running a reconstruction]: https://docs.opensfm.org/using.html (OpenSfM usage)\n[Documentation]: https://docs.opensfm.org  (OpenSfM documentation)\n\n## License\nOpenSfM is BSD-style licensed, as found in the LICENSE file.  See also the Facebook Open Source [Terms of Use][] and [Privacy Policy][]\n\n[Terms of Use]: https://opensource.facebook.com/legal/terms (Facebook Open Source - Terms of Use)\n[Privacy Policy]: https://opensource.facebook.com/legal/privacy (Facebook Open Source - Privacy Policy)\n"
 },
 {
  "repo": "amusi/AI-Job-Notes",
  "language": null,
  "readme_contents": "# AI-Job-Notes\nAI\u7b97\u6cd5\u5c97\u6c42\u804c\u653b\u7565\uff1a\u6db5\u76d6\u6821\u62db\u65f6\u95f4\u8868\u3001\u51c6\u5907\u653b\u7565\u3001\u5237\u9898\u6307\u5357\u3001\u5185\u63a8\u3001AI\u516c\u53f8\u6e05\u5355\u548c\u7b54\u7591\u7b49\u8d44\u6599\n\nAI\u7b97\u6cd5\u5c97\u65b9\u5411\uff1a\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u50cf\u5904\u7406\u548cSLAM\u7b49\n\n>  \u6ce8\uff1a\u5982\u679c\u4f60\u770b\u5230\u8fd9\u7bc7\u6587\u7ae0\uff0c\u4e14\u6709\u4e00\u4e9b\u7591\u95ee\u6216\u8005\u60f3\u63d0\u4f9b\u4e00\u4e9b\u8d44\u6e90\uff0c\u6b22\u8fce\u63d0\u4ea4issues\uff01\n\n# \u76ee\u5f55\n\n<!-- MarkdownTOC depth=4 -->\n\n- [1 \u6821\u62db\u65f6\u95f4\u8868](#Scheduled)\n- [2 \u51c6\u5907\u653b\u7565](#Strategy)\n- [3 AI \u9762\u7ecf\u548c\u5237\u9898\u6307\u5357](#Coding)\n- [4 \u5185\u63a8](#Recommend)\n- [5 \u7b80\u5386\u6a21\u677f](#Resume)\n- [6 AI \u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09](#Company)\n- [7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5](#Salary)\n- [8 \u7b54\u7591\uff08\u542b130\u4e2a\u95ee\u7b54\uff09](#Q&A)\n\n<a name=\"Scheduled\"></a>\n\n## 1 \u6821\u62db\u65f6\u95f4\u8868\n\n![](imgs/\u6821\u62db\u65f6\u95f4\u8868.png)\n\n\u4ee5\u4eca\u5e74(2020)\u4e3a\u4f8b\uff0c\u9ed8\u8ba4\u4e3a2021\u5c4a\u5b66\u751f\uff082020\u5c4a\u5b66\u751f\u79f0\u4e3a\u4e0a\u5c4a\uff09\n\n| \u65f6\u95f4           | \u4efb\u52a1                                |\n| -------------- | ----------------------------------- |\n| 2020\u5e743\u6708~6\u6708  | \u627e\u6691\u671f\u5b9e\u4e60/\u4e0a\u5c4a\u6625\u62db\uff08\u8865\u62db\uff09         |\n| 2020\u5e746\u6708~8\u6708  | \u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09              |\n| 2020\u5e749\u6708~11\u6708 | \u79cb\u62db\u6b63\u5f0f\u6279\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09 |\n\n### 1.1 \u6691\u671f\u5b9e\u4e60\n\n2020\u5e743\u6708~6\u6708\uff1a\u6691\u671f\u5b9e\u4e60\u3002\n\n\u5b9e\u4e60\u4e00\u822c\u5206\u6210\u4e24\u79cd\uff1a\n\n- \u65e5\u5e38\u5b9e\u4e60\n- \u6691\u671f\u5b9e\u4e60\n\n![](imgs/\u5b9e\u4e60.png)\n\n**\u65e5\u5e38\u5b9e\u4e60**\uff1a\u65e5\u5e38\u5b9e\u4e60\u662f\u4efb\u4f55\u65f6\u5019\u90fd\u53ef\u4ee5\u627e\u7684\uff0c\u901a\u5e38\u662f\u6839\u636e\u5177\u4f53\u90e8\u95e8\u7684\u9700\u6c42\uff0c\u7531\u516c\u53f8HR\u3001\u90e8\u95e8\u4e3b\u7ba1\u6216\u8005\u90e8\u95e8\u5458\u5de5\u53d1\u5e03\u62db\u8058\u6d88\u606f\uff0c\u76f8\u5bf9\u8f83\u4e3a\u96f6\u6563\u4e5f\u6bd4\u8f83\u7075\u6d3b\u3002\n\n**\u6691\u671f\u5b9e\u4e60**\uff1a\u5f88\u591a\u516c\u53f8\uff0c\u7279\u522b\u662f\u5927\u516c\u53f8\uff08\u5982BAT\u7b49\u5927\u5382\uff09\uff0c\u90fd\u4f1a\u7ec4\u7ec7\u4e13\u9879\u7684**\u6691\u671f\u5b9e\u4e60\u751f**\u62db\u8058\u6d3b\u52a8\u3002\u4e00\u65b9\u9762\u662f\u9488\u5bf9\u5728\u6821\u5b66\u751f\u7684\u60c5\u51b5\uff08\u5f88\u591a\u5b66\u751f\u53ea\u6709\u6691\u671f\u624d\u6709\u5047\u671f\uff0c\u6216\u8005\u5bfc\u5e08\u6691\u5047\u624d\u653e\u4eba\uff09\uff0c\u53e6\u4e00\u65b9\u9762\u5c31\u662f\u4e3a\u4e86\u79cb\u5b63\u6821\u62db\uff08\u5927\u89c4\u6a21\u62db\u8058\uff09\u5438\u5f15\u4eba\u624d\u3002\u6691\u671f\u5b9e\u4e60\u5177\u6709\u5f88\u5927\u7684\u610f\u4e49\uff0c\u5bf9\u5b66\u751f\u6765\u8bf4\uff0c\u6700\u76f4\u63a5\u7684\u597d\u5904\u5c31\u662f\u8f6c\u6b63\u673a\u4f1a\u3002\u6691\u671f\u5b9e\u4e60\uff0c\u4e00\u822c6\u6708\u5e95\u5de6\u53f3\u5b9e\u4e60\u5165\u804c\uff08\u4e5f\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u7684\u65f6\u95f4\uff0c\u63d0\u524d\u5165\u804c\uff09\uff0c\u4e00\u822c8\u6708\u5e95\u62169\u6708\u4efd\u4f1a\u6709\u4e13\u9879\u6691\u671f\u5b9e\u4e60\u7b54\u8fa9\uff0c\u6839\u636e\u7efc\u5408\u8868\u73b0\uff0c\u7b54\u8fa9\u901a\u8fc7\u540e\u5c31\u53ef\u4ee5\u57fa\u672c\u7ed3\u675f\u79cb\u62db\u4e86\u3002\n\n\u6ce8\uff1a\u8fd9\u91cc\u5efa\u8bae\u5728\u8fdb\u5165\u516c\u53f8\u53c2\u52a0\u6691\u671f\u5b9e\u4e60\u7684\u671f\u95f4\uff0c\u4e5f\u8981\u53c2\u52a0\u79cb\u62db\u63d0\u524d\u6279\u548c\u79cb\u62db\u6b63\u5f0f\u6279\uff0c\u5e76\u591a\u6295\u9012\u4e00\u4e9b\u516c\u53f8\uff0c\u5373\u4f7f\u5728\u5b9e\u4e60\uff0c\u6240\u8c13\u7684\u5f88\u5fd9\uff0c\u6ca1\u65f6\u95f4\u51c6\u5907\u79cb\u62db\u4e86\uff0c\u90a3\u4e5f\u8981\u591a\u6295\u3002\u6691\u671f\u5b9e\u4e60\u7684\u53e6\u4e00\u4e2a\u597d\u5904\u662f\u589e\u52a0\u53ef\u8d35\u7684\u5b9e\u4e60\u7ecf\u9a8c\uff0c\u7b80\u5386\u4f1a\u597d\u770b\u5f88\u591a\u3002\n\n### 1.2 \u79cb\u62db\u63d0\u524d\u6279\n\n**2020\u5e746\u6708~8\u6708\uff1a\u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09**\n\n\u636e\u6211\u4e86\u89e3\u4e0a\u5c4a\u6253\u54cd\u79cb\u62db\u7b2c\u4e00\u67aa\u7684\u662f\u5927\u7586(DJI)\u79d1\u6280\uff0c\u5176\u57286\u6708\u5e95\u5c31\u5df2\u7ecf\u7ed3\u675f\u7b80\u5386\u6295\u9012\u4e86\uff0c\u7136\u540eBAT\u7b49\u5927\u5382\u90fd\u662f7\u6708\u4efd\u5f00\u59cb\u3002\u8fd9\u65f6\u5019\u7684\u6821\u62db\uff0c\u7edd\u5927\u90e8\u5206\u90fd\u662f\u5185\u63a8/\u63d0\u524d\u6279\uff0c\u800c\u4e0d\u662f\u6b63\u5f0f\u6279\uff0c\u5927\u5bb6\u4e00\u5b9a\u8981\u73cd\u60dc\u8fd9\u4e2a\u65f6\u95f4\u70b9\uff1a6\u6708~8\u6708\u3002\u867d\u7136\u6211\u8c03\u4f83\u7740\u8bf4\u795e\u4ed9\u6253\u67b6\uff0c\u4f46\u8fd8\u662f\u8981\u6ce8\u610f\u8fd9\u65f6\u5019\u6027\u4ef7\u6bd4\u7279\u522b\u9ad8\u3002\u4e00\u65b9\u9762\u662f\u85aa\u8d44\u666e\u904d\u9ad8\uff0c\u901a\u5e38\u4e00\u4e9bSP/SSP Offer\u90fd\u662f\u8fd9\u4e2a\u8282\u70b9\u53d1\u51fa\u6765\u7684\uff0c\u53e6\u4e00\u65b9\u9762\u662f\u6295\u9012\u7684\u4eba\u6570\u8fd8\u4e0d\u662f\u5f88\u591a\uff0c\u56e0\u4e3a\u6709\u4e9b\u4eba\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e2a\u63d0\u524d\u6279\u7684\u91cd\u8981\u6027\uff0c\u8001\u60f3\u7740\u591a\u51c6\u5907\u4e00\u70b9\uff0c\u5230\u79cb\u62db\u6b63\u5f0f\u6279\u518d\u5927\u5e72\u4e00\u573a\u3002\n\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a\u53c2\u4e0e\u79cb\u62db\u63d0\u524d\u6279\u7684\u5927\u4f6c\u7279\u522b\u591a\uff0c\u540c\u65f6\u5c97\u4f4dhc\u5e76\u4e0d\u591a\uff08\u56e0\u4e3a\u4f01\u4e1a\u8981\u8003\u8651\u6b63\u5f0f\u6279\u7684\u60c5\u51b5\uff0c\u4f1a\u63a7\u5236\u62db\u8058\u4eba\u6570\uff09\uff0c\u6240\u4ee5\u6211\u628a\u79cb\u62db\u63d0\u524d\u6279\u6bd4\u4f5c\uff1a\u795e\u4ed9\u6253\u67b6\u3002\u53e6\u5916\uff0c\u79cb\u62db\u63d0\u524d\u6279\u5927\u591a\u4ee5\u5185\u63a8\u4e3a\u4e3b\uff0c\u540e\u9762\u7ae0\u8282\u4e2d\u6211\u4f1a\u8bf4\u5230\u5982\u4f55\u83b7\u53d6\u62db\u8058\u4fe1\u606f\u4ee5\u53ca\u5982\u4f55\u5185\u63a8\u3002\n\n\u6ce8\uff1a\u63d0\u53d6\u6279\u6302\u4e86\uff0c\u6b63\u5f0f\u6279\u53ef\u4ee5\u518d\u7ee7\u7eed\u6295\uff08\u5177\u4f53\u770b\u4e0d\u540c\u516c\u53f8\u7684\u62db\u8058\u4ecb\u7ecd\uff09\u3002\n\n### 1.3 \u79cb\u62db\u6b63\u5f0f\u6279\n\n**2020\u5e749\u6708~11\u6708\uff1a\u79cb\u62db\u6b63\u5f0f\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09**\n\n\u6709\u53e5\u8bdd\u53eb\u505a\u91d1\u4e5d\u94f6\u5341\uff0c\u4e5f\u5c31\u662f9\u6708\u4efd\u7684 Offer \u6bd410\u6708\u4efd\u7684 Offer \u66f4\u53ef\u8d35\uff0c\u8fd9\u8bdd\u5176\u5b9e\u5f88\u6709\u9053\u7406\uff0c\u6240\u4ee5\u5927\u5bb6\u53ef\u4ee5\u8111\u8865\u52307\u30018\u6708\u4efd\u7684 Offer \u5c5e\u4e8e\u4ec0\u4e48 level \u4e86\u3002\u8fd9\u65f6\u5019\u4e5f\u5f88\u8003\u9a8c\u5927\u5bb6\u7684\u5fc3\u6001\uff0c\u6bd4\u59829\u6708\u4efd\u621610\u6708\u4efd\u4e86\uff0c\u5982\u679c\u4f60\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u518d\u770b\u770b\u8eab\u8fb9\u5df2\u7ecf\u62ff\u5230Offer\u7684\u540c\u5b66\uff0c\u4e00\u5b9a\u53d8\u6210\u67e0\u6aac\u7cbe\u3002\n\n\u6240\u4ee5 Amusi \u8fd9\u91cc\u5f3a\u70c8\u5efa\u8bae\u4e00\u5b9a\u8981\u628a\u63e1\u4f4f 1.2\u8282\u4e2d\u7684**\u79cb\u62db\u63d0\u524d\u6279 **\u3002\u5f53\u7136\u4e86\uff0c\u5982\u679c9\u6708\u4efd\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u5fc3\u6001\u5343\u4e07\u522b\u5d29\uff0c\u7ee7\u7eed\u6295\u7ee7\u7eed\u5e72\uff0c\u8bb0\u4f4f\u4e00\u53e5\u8bdd\uff1a\u591a\u6295\u51c6\u6ca1\u9519\uff01\u5176\u5b9e\u5927\u90e8\u5206\u540c\u5b66\u90fd\u662f9\u6708\u300110\u6708\u624d\u9646\u7eed\u6536\u5230Offer\u7684\uff0c\u6240\u4ee5\u4f60\u591a\u6295\u7ee7\u7eed\u52aa\u529b\uff0c\u6536\u83b7\u80af\u5b9a\u4f1a\u6709\u7684\u3002\n\n<a name=\"Strategy\"></a>\n\n## 2 \u51c6\u5907\u653b\u7565\n\n\u51c6\u5907\u653b\u7565\uff0c\u6211\u6ca1\u6709\u5177\u4f53\u7684\u65b9\u6848\uff0c\u56e0\u4e3a\u8fd9\u5c31\u597d\u50cf\u662f\u5b66\u4e60\u8ba1\u5212\u4e00\u6837\uff0c\u6bcf\u4e2a\u4eba\u90fd\u8981\u81ea\u5df1\u7684\u4e60\u60ef\uff0c\u6211\u7684\u4f60\u5e76\u4e0d\u4e00\u5b9a\u9002\u7528\u3002\u6240\u4ee5\u6211\u5c31\u7528\u4e00\u4e2a\u7cbe\u7b80\u7684\u516c\u53f8\u6765\u4ecb\u7ecd\u3002\n~~\u516c\u5f0f1.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60(\u53ef\u9009)+\u7ade\u8d5b(\u53ef\u9009)+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)~~\n\n\u516c\u5f0f2.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60+\u7ade\u8d5b+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)\n\n<a name=\"Coding\"></a>\n\n## 3 AI\u9762\u7ecf\u548c\u5237\u9898\u6307\u5357\n\n### 3.1 \u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\n\n\u8be6\u89c1\uff1a[\u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\uff08\u542b\u6570\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cSLAM\u7b49\u65b9\u5411\uff09](<https://github.com/amusi/Deep-Learning-Interview-Book>)\n\n**Deep Learning Interview Book** \u90e8\u5206\u5185\u5bb9\u5982\u4e0b\uff1a\n\n- \ud83d\ude03 [\u81ea\u6211\u4ecb\u7ecd](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D.md)\n- \ud83d\udd22 [\u6570\u5b66](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E5%AD%A6.md)\n- \ud83c\udf93 [\u673a\u5668\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd5 [\u6df1\u5ea6\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd7 [\u5f3a\u5316\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udc40 [\u8ba1\u7b97\u673a\u89c6\u89c9](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.md)\n- \ud83d\udcf7 [\u4f20\u7edf\u56fe\u50cf\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.md)\n- \ud83c\udc04\ufe0f [\u81ea\u7136\u8bed\u8a00\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.md)\n- \ud83c\udfc4 [SLAM](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/SLAM.md)\n- \ud83d\udc65 [\u63a8\u8350\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.md)\n- \ud83d\udcca [\u6570\u636e\u7ed3\u6784\u4e0e\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.md)\n- \ud83d\udc0d [\u7f16\u7a0b\u8bed\u8a00\uff1aC/C++/Python](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.md)\n- \ud83c\udf86 [\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6.md)\n- \u270f\ufe0f [\u9762\u8bd5\u7ecf\u9a8c](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C.md)\n- \ud83d\udca1 [\u9762\u8bd5\u6280\u5de7](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E6%8A%80%E5%B7%A7.md)\n- \ud83d\udce3 [\u5176\u5b83\uff08\u8ba1\u7b97\u673a\u7f51\u7edc/Linux\u7b49\uff09](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%85%B6%E5%AE%83.md)\n\n### 3.2 \u5237\u9898\u6307\u5357\n\n\u5237\u9898\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b66\u4e60\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\uff0c\u953b\u70bc\u7f16\u7a0b\u80fd\u529b\u548c\u719f\u6089\u5237\u9898\u6280\u5de7\n\n\u5237\u9898\u5efa\u8bae\uff1a\u5148\u5237[\u300a\u5251\u6307Offer\u300b](https://www.nowcoder.com/ta/coding-interviews)\uff0866\u9898\uff09\uff0c\u518d\u5237 [LeetCode](https://leetcode.com/)\uff08\u76ee\u524dLeetCode\u5df2\u7ecf\u6709900+\u9898\uff0c\u53ef\u4ee5\u6839\u636e\u7c7b\u522b\u6765\u5237\uff0c\u4f46\u5f3a\u70c8\u5efa\u8bae\u5148\u5237\u5b8c [LeetCode \u9762\u8bd5\u9ad8\u9891\u9898](https://leetcode.com/problemset/top-interview-questions/)\uff09\n\n> \u6ce8\uff1a\u6839\u636e 2020 \u5e74\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0cLeetCode \u5efa\u8bae\u81f3\u5c11\u5237200-300\u9898\uff0c\u6240\u4ee52020\u5e74\uff082021\u5c4a\uff09\u627e\u5de5\u4f5c\u7684\u540c\u5b66\u4e00\u5b9a\u8981\u52aa\u529b\u5237\u8d77\u6765\u4e86\uff01\n\n#### 3.2.1 \u5237\u9898\u7f16\u7a0b\u8bed\u8a00\n\n- C/C++\n- Python\n- JAVA\uff08\u4e0d\u63a8\u8350\uff09\n\n> \u6ce8\uff1a\u5982\u679c\u65f6\u95f4\u5145\u88d5\uff0c\u800c\u4e14\u6709 C++ \u57fa\u7840\uff0c\u90a3\u4e48\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 C++\u548c Python \u540c\u65f6\u5237\u9898\u3002\n>\n> \u6839\u636e 2019 \u5e74(2020\u5c4a)\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0c\u4f1a C++ \u7684\u540c\u5b66\u5177\u6709\u6709\u4e00\u5b9a\u4f18\u52bf\u3002\n\n#### 3.2.2 \u4e66\u7c4d\u63a8\u8350\n\n| \u4e66\u7c4d                                                         | \u8c46\u74e3\u8bc4\u5206 | \u63a8\u8350\u6307\u6570 |\n| ------------------------------------------------------------ | -------- | -------- |\n| [\u300a\u5251\u6307Offer\u300b](https://book.douban.com/subject/25910559/)   | 9.1      | \u2606\u2606\u2606\u2606\u2606    |\n| [\u300a\u6570\u636e\u7ed3\u6784(C++\u8bed\u8a00\u7248)\u300b](https://book.douban.com/subject/25859528/) | 9.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u7b97\u6cd5\u56fe\u89e3\u300b](https://book.douban.com/subject/26979890/)    | 8.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u5927\u8bdd\u6570\u636e\u7ed3\u6784\u300b](https://book.douban.com/subject/6424904/) | 7.9      | \u2606\u2606\u2606      |\n| [\u300a\u7b97\u6cd5\u300b(\u7b2c\u56db\u7248)](https://book.douban.com/subject/19952400/) | 9.4      | \u2606\u2606\u2606      |\n\n>  \u6ce8\uff1a\u5176\u5b9e\u8fd8\u6709\u5f88\u591a\u65b9\u5411\u6ca1\u6709\u6d89\u53ca\uff0c\u5982linux\u3001\u6570\u636e\u5e93\uff0c\u4f46\u6682\u65f6\u5148\u63a8\u8350\u8fd9\u4e9b\uff0c\u540e\u9762\u518d\u8865\u5145\n\n#### 3.2.3 \u5728\u7ebf\u5237\u9898\u7f51\u7ad9\n\n- [LeetCode(\u82f1\u6587)](https://leetcode.com/)\n- [LeetCode(\u4e2d\u6587)](https://leetcode-cn.com/)\n- [\u725b\u5ba2\u7f51](https://www.nowcoder.com/)\uff1a\u63a8\u8350\u5251\u6307Offer\u548c\u5404\u5927\u516c\u53f8\u5f80\u5e74\u9898\u5e93\uff0c\u725b\u5ba2\u7f51\u7684\u4f18\u52bf\u5728\u4e8e\u5f88\u591a\u516c\u53f8\u90fd\u4f1a\u4f7f\u7528\u5176\u4f5c\u4e3a\u5728\u7ebf\u5237\u9898\u5e73\u53f0\uff0c\u6240\u4ee5\u5728\u8fd9\u4e0a\u9762\u5237\u9898\uff0c\u6709\u5229\u4e8e\u61c2\u5f97\u8f93\u5165\u8f93\u51fa\u7b49\"\u5957\u8def\"\n\n#### 3.2.4 \u5237\u9898\u65b9\u6cd5\n\n- \u300a\u5251\u6307Offer\u300b\u5168\u5237\u5b8c\n- LeetCode\u9009\u62e9\u6027\u5237\uff1a\u53ef\u4ee5\u7c7b\u522b\u6765\u5237\u9898\uff0c\u5982\u6570\u7ec4\u7c7b\u3001\u94fe\u8868\u7c7b\uff0c\u6216\u8005\u9762\u8bd5\u9ad8\u9891\u7c7b\n\n#### 3.2.5 \u5237\u9898\u65f6\u95f4\n\n\u73b0\u5728\u8d77~2020-10-15\n\n#### 3.2.6 \u5237\u9898\u91cd\u8981\u6027\n\n\u6b63\u5e38\u6821\u62db\u6d41\u7a0b\u90fd\u8981\u8fdb\u884c\u5728\u7ebf\u7b14\u8bd5\uff0c\u9762\u8bd5\u4e2d\u4e5f\u53ef\u80fd\u4f1a\u624b\u6495\u4ee3\u7801\uff0c\u6240\u4ee5\u5237\u9898\u5341\u5206\u5f71\u54cd\u9762\u8bd5\u7ed3\u679c\u3002\n\n<a name=\"Recommend\"></a>\n\n## 4 \u5185\u63a8\n\n\u56fd\u5185\u516c\u53f8\u4eba\u5de5\u667a\u80fd\u65b9\u5411\u5c97\u4f4d\u7684\u5185\u63a8\u673a\u4f1a\uff0c\u542b\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u65b9\u5411\u3002\n\n[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4e3b\u8981\u4ee5\u5168\u804c\u3001\u5b9e\u4e60\u548c\u6821\u62db\u4e3a\u4e3b\uff0c\u5e76\u4e14\u5168\u90fd\u662f\u5185\u63a8\u65b9\u5f0f\uff01\n\n- [\u5b9e\u4e60\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%AE%9E%E4%B9%A0/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u3001\u4eac\u4e1c\u548c\u5546\u6c64\u7b49\u516c\u53f8\n\n- [\u5168\u804c\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%85%A8%E8%81%8C/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u7b49\u516c\u53f8\n\n\u6ce8\uff1a2020\u5e746\u6708\u5f00\u59cb\uff0c[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4f1a\u63a8\u51fa\u5927\u91cf\u6821\u62db\u5185\u63a8\u8d44\u6e90\uff0c\u6b22\u8fcestar/fork/watching\u3002\n\n### 4.1 \u5185\u63a8\u7684\u91cd\u8981\u6027\n\n\u5185\u63a8\uff0c\u771f\u7684\u592a\u91cd\u8981\u4e86\u3002\u5176\u5b9e\u73b0\u5728\u627e\u5b9e\u4e60\u4e5f\u4e00\u6837\uff0c\u5185\u63a8\u7684\u91cd\u8981\u6027\u5c31\u63d0\u9192\u51fa\u6765\u4e86\uff0c\u6bd4\u5982\u6211\u8fd9\u8fb9\u7684\u8d44\u6e90\u5c31\u53ef\u4ee5\u5185\u63a8\u5230BAT\u3001\u5546\u6c64\u3001\u65f7\u89c6\u7b49\u516c\u53f8\uff0c\u4e00\u822c\u5e38\u89c4\u64cd\u4f5c\u662f\u7f51\u4e0a\u6295\u9012\u7b80\u5386\uff0c\u800c\u5feb\u901f\u76f4\u63a5\u7684\u5c31\u662f\u5c06\u7b80\u5386\u9001\u5230leader/\u4e3b\u7ba1\u90a3\u91cc\u3002\u800c\u4e14\u5185\u63a8\u662f\u5efa\u7acb\u5728\u4e00\u79cd\u4e92\u4fe1\u7684\u57fa\u7840\u4e0a(\u867d\u7136\u4e0d\u5927)\uff0c\u8be5\u8d70\u7684\u6d41\u7a0b\u8fd8\u662f\u8981\u8d70\uff0c\u4f46\u65e0\u5f62\u4e2d\u589e\u5927\u4e86\u9762\u8bd5\u901a\u8fc7\u6982\u7387\u3002\u4f60\u8981\u77e5\u9053\uff0c\u5f88\u591a\u4eba\u7684\u7b80\u5386\u5728\u5b98\u7f51\u6216\u8005\u5176\u4ed6\u7b2c\u4e09\u65b9\u62db\u8058\u7f51\u7ad9\u4e0a\u5c31\u76f4\u63a5\u5361\u6b7b\u4e86\u3002\n\n### 4.2 \u5982\u4f55\u5185\u63a8\uff1f\n\n\u5185\u63a8\u7684\u65b9\u5f0f\u5f88\u591a\uff0c\u6bd4\u5982\uff1a\n\n1. \u5f3a\u5173\u8054\uff1a\u76f4\u63a5\u627e\u5df2\u7ecf\u6bd5\u4e1a\u7684\u5e08\u5144\u5e08\u59d0\u6216\u670b\u53cb\u5185\u63a8\uff08\u7f3a\u70b9\u662f\u8eab\u8fb9\u670b\u53cb\u53bb\u7684\u4f01\u4e1a\u6709\u9650\uff0c\u5f88\u591a\u4eba\u662f\u7b2c\u4e00\u6279\u4ece\u4e8b\u7b97\u6cd5\u5c97\u7684\uff0c\u53ef\u80fd\u90fd\u6ca1\u6709\u5e08\u5144\u5e08\u59d0\u641e\u8fd9\u4e2a\uff09\n2. \u5e38\u89c4\u64cd\u4f5c\uff1a\u4e0a\u725b\u5ba2\u7f51\u8bba\u575b\u770b\u4f01\u4e1a\u4eba\u5458\u53d1\u5185\u63a8\u5e16\u5b50\u3001\u5173\u6ce8\u4e00\u4e9b\u62db\u8058\u516c\u4f17\u53f7\uff08\u8fd9\u91cc\u6211\u5c31\u4e0d\u63a8\u8350\uff0c\u56e0\u4e3a\u5f88\u591a\u516c\u4f17\u53f7\u90fd\u5f88\u6709\u5957\u8def\uff0c\u5185\u63a8\u4e00\u4e2a\u4f01\u4e1a\uff0c\u8fd8\u8981\u8f6c\u53d1\u6587\u7ae0\u5230\u5176\u5b83\u7fa4\u91cc\uff0c\u7136\u540e\u622a\u56fe\u7ed9\u4ed6\u4eec\uff0c\u53ef\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u4eba\uff0c\u4e3a\u4e86\u5185\u63a8\uff0c\u53ea\u80fd\u8fd9\u4e48\u5e72\uff09\n3. Amusi \u5185\u63a8\u3002\u8fd9\u91cc\u611f\u89c9\u50cf\u4f3c\u6253\u5e7f\u544a\u4e00\u6837\uff0c\u4f46\u786e\u5b9e\u662f\u4e00\u4e2a\u65b9\u5f0f\uff0c\u56e0\u4e3a\u6211\u624b\u91cc\u8d44\u6e90\u633a\u591a\u7684\uff0c\u5f88\u591a\u516c\u53f8\u7684\u4eba\u90fd\u8ba4\u8bc6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5185\u63a8\u3002\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u5173\u6ce8\u4e00\u4e0b\u8fd9\u4e2a\u6c42\u804c\u7fa4[\u300c2020AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4\u300d](https://t.zsxq.com/VFUZR3n) \u6216\u8005 [AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend)\n\n![](imgs/2020\u5e74AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4.png)\n\n<a name=\"Resume\"></a>\n\n## 5 \u7b80\u5386\u6a21\u677f\n\n\u63d0\u4f9b\u4e86\u4e09\u4efd\u7b80\u5386\u6a21\u677f\uff0c\u8be6\u89c1\uff1a[AI \u7b97\u6cd5\u5c97\u7b80\u5386\u6a21\u677f](https://github.com/amusi/AI-Job-Resume)\n\n![](imgs/Resume-Demo.png)\n\n<a name=\"Company\"></a>\n\n## 6 AI\u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09\n\n\u9996\u5148 AI > CV\uff0c\u6240\u4ee5\u63d0\u4f9bCV\u5c97\u7684\u516c\u53f8\u80af\u5b9a\u5c31\u63d0\u4f9b AI\u5c97\u3002\u4f46\u81f3\u4e8e\u8fd9\u4e9b\u516c\u53f8\u662f\u5426\u8fd8\u6709 NLP\u3001\u673a\u5668\u5b66\u4e60\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u63a8\u8350\u7b97\u6cd5\u548c SLAM\u7b49\u5c97\u4f4d\uff0c\u8fd9\u4e2a\u9700\u8981\u5927\u5bb6\u81ea\u884c\u53bb\u5b98\u7f51\u8fdb\u884c\u4e86\u89e3\u3002\n\n\u8350\u8bfb\uff1a[\u56fd\u5185\u63d0\u4f9b\u8ba1\u7b97\u673a\u89c6\u89c9(CV)\u7b97\u6cd5\u5c97\u4f4d\u7684\u516c\u53f8\u540d\u5355(\u542b\u5916\u4f01\u548c\u56fd\u5185\u516c\u53f8)](https://github.com/amusi/CV-Company-List )\n\n**\u5317\u4eac\u63d0\u4f9bCV\u7b97\u6cd5\u5c97\u7684\u516c\u53f8\u540d\u5355**\n\n![](/imgs/Beijing.png)\n\n\u66f4\u591a\u57ce\u5e02\u4fe1\u606f\uff08\u4e0a\u6d77\uff0c\u6df1\u5733\uff0c\u676d\u5dde\uff0c\u5357\u4eac\uff0c\u5e7f\u5dde\u548c\u6210\u90fd\u7b49\uff09\u8be6\u89c1\uff1ahttps://github.com/amusi/CV-Jobs\n\n<a name=\"Salary\"></a>\n\n## 7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5\n\n\u4eca\u5e74\u662f19\u5e74\uff0c\u6240\u4ee5\u8fd9\u91cc\u4ee52020\u5c4a\u4e3a\u4f8b\u3002\u6211\u662f18\u5e74\u627e\u7684\u5de5\u4f5c\uff0c\u4f46\u5e94\u8be5\u662f2019\u5c4a\u7684\uff0c\u518d\u6b21\u5f3a\u8c03\u4e00\u4e0b\u65f6\u95f4\u4e0d\u8981\u641e\u6df7\u4e86\uff0c\u6240\u4ee5\u6211\u8fd9\u91cc\u8bf4\u8bf42019\u5c4aAI\u7b97\u6cd5\u5c97\u7684\u85aa\u8d44\u60c5\u51b5\u3002\n\n\u6211\u53ea\u4ee5**\u7855\u58eb\u53ca\u4e00\u7ebf\u5de6\u53f3\u57ce\u5e02**\u4e3a\u4f8b\uff08\u5317\u4e0a\u5e7f\u6df1\u3001\u5357\u4eac\u3001\u676d\u5dde\u7b49\uff09\uff0c\u56e0\u4e3a\u50cf\u6b66\u6c49\u3001\u6210\u90fd\uff0c\u4f60\u5373\u4f7f\u627e\u7684AI\u7b97\u6cd5\u5c97\uff0c\u4f46\u57ce\u5e02\u4e0d\u4e00\u6837\uff0c\u85aa\u8d44\u8fd8\u662f\u591a\u5c11\u6709\u533a\u522b\uff0c\u660e\u663e\u4e0d\u80fd\u53ea\u770bMoney\uff0c\u4e0d\u8003\u8651\u57ce\u5e02\u5927\u73af\u5883\u3002\n\n- **\u767d\u83dc\u4ef7\uff1a25w~30w**\n\n- **SP\uff1a30w~40w**\n\n- **SSP\uff1a40w+**\n\n\u8bf4\u5e74\u85aa\u6709\u70b9\u7b3c\u7edf\uff0c\u6211\u518d\u8bf4\u7ec6\u4e00\u70b9\uff0c\u5927\u5bb6\u4e5f\u53ef\u4ee5\u63d0\u53d6\u719f\u6089\u4e00\u4e0b\u3002\n\n\u4e00\u822c\u4f01\u4e1a\u85aa\u8d44\u6784\u6210\u662f\uff1a\n\n- \u5e74\u85aa = \u6708\u85aa*12 + \u5e74\u7ec8\u5956\n\n\u5e74\u7ec8\u5956\u4e00\u822c\u662f2~5\u4e2a\u6708\u7684\u85aa\u8d44\uff0c\u5927\u6982\u662f3\u4e2a\u6708\n\n\u6240\u4ee5\uff0c\u5e74\u85aa=\u6708\u85aa*15\n\n\u5982\u679c\u4f60\u6708\u85aa2w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f30w=2*15\uff08\u767d\u83dc\u7684Top\uff0cSP\u7684Down\uff09\n\n\u5982\u679c\u4f60\u6708\u85aa2.7w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f40.5w=2.7*15\uff08SP\u7684Top\uff0cSSP\u7684Down\uff09\n\n\u8fd9\u91ccpo\u4e00\u5f20\u5f88\u5168\u5f88\u5168\u7684\u9ad8\u85aa\u56fe\uff0c\u6765\u81eaOfferShow\n\n\u6ce8\uff1a\u8ddfhr\u8c08\u85aa\u8d44\u7684\u65f6\u5019\uff0c\u5982\u679c\u5979/\u4ed6\u95ee\u4f60\uff1a\u4f60\u7684\u5e0c\u671b\u85aa\u8d44\u662f\u591a\u5c11\uff1f\uff01\u8fd9\u65f6\u5019\u4f60\u4e00\u5b9a\u8981\u5f80\u9ad8\u4e86\u8981\uff0c\u81f3\u5c11\u6bd4\u4f60\u60f3\u8981\u7684\u9ad830%\u3002\u542c\u6211\u7684\uff0c\u6ca1\u6709\u9519\uff0c\u4e0d\u7136...\n\n![](imgs/salary.png)\n\n<a name=\"Q&A\"></a>\n\n## 8 \u7b54\u7591\n\n130\u4e2a\u95ee\u7b54\u8bf7\u6233\u2014> [Q&A](Q&A.md)"
 },
 {
  "repo": "soruly/trace.moe",
  "language": "PHP",
  "readme_contents": "# trace.moe\n\n[![License](https://img.shields.io/github/license/soruly/trace.moe.svg)](https://github.com/soruly/trace.moe/blob/master/LICENSE)\n[![Discord](https://img.shields.io/discord/437578425767559188.svg)](https://discord.gg/K9jn6Kj)\n[![Donate](https://img.shields.io/badge/donate-patreon-orange.svg)](https://www.patreon.com/soruly)\n\nThe website of trace.moe (whatanime.ga)\n\nImage Reverse Search for Anime Scenes\n\nUse anime screenshots to search where this scene is taken from.\n\nIt tells you which anime, which episode, and exactly which moment this scene appears in Japanese Anime.\n\n## Demo\n\nDemo image\n\n![](https://images.plurk.com/2FKxneXP64qiKwjlUA7sKj.jpg)\n\nSearch result tells you which moment it appears.\n\n![](https://addons.cdn.mozilla.net/user-media/previews/full/209/209947.png)\n\n## How does it work\n\ntrace.moe uses [sola](https://github.com/soruly/sola) to index video and work with liresolr. This repo only include the webapp for trace.moe, which demonstrate how to integrate anilist info, and how thumbnail/video previews are generated. If you want to make your own video scene search engine, please refer to sola instead.\n\nTo learn more, read the presentation slides below\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga.slide) May 2016\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2017.slide) Jun 2017\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2018.slide) Jun 2018\n\n[Presentation slides](https://github.com/soruly/slides/blob/master/2019-COSCUP-trace.moe.md) Aug 2019\n\nSystem Overview\n\n![](https://pbs.twimg.com/media/CstZmrxUIAAi8La.jpg)\n\nYou may find some other related repo here\n\n- [sola](https://github.com/soruly/sola)\n- [LireSolr](https://github.com/soruly/liresolr)\n- [anilist-crawler](https://github.com/soruly/anilist-crawler)\n- [trace.moe-WebExtension](https://github.com/soruly/trace.moe-WebExtension)\n- [trace.moe-telegram-bot](https://github.com/soruly/trace.moe-telegram-bot)\n\n## Official API Docs (Beta)\n\nhttps://soruly.github.io/trace.moe/\n\n## Mobile Apps (3rd party)\n\nWhatAnime by Andr\u00e9e Torres\nhttps://play.google.com/store/apps/details?id=com.maddog05.whatanime\nSource: https://github.com/maddog05/whatanime-android\n\nWhatAnime - \u4ee5\u56fe\u641c\u756a by Mystery0 (Simplified Chinese)\nhttps://play.google.com/store/apps/details?id=pw.janyo.whatanime\nSource: https://github.com/JanYoStudio/WhatAnime\n\n## Integrating search with trace.moe\n\nTo add trace.moe as a search option for your site, pass the image URL via query string like this\n\n```\nhttps://trace.moe/?url=http://searchimageurl\n```\n\nYou can also specify playback options like this\n\n```\nhttps://trace.moe/?autoplay=0&loop&mute=1&url=http://searchimageurl\n```\n\nPlayback URL params:\n\n| param    | value  | default (not set in URL param) | set with empty or other value |\n| -------- | ------ | ------------------------------ | ----------------------------- |\n| autoplay | 0 or 1 | 1                              | 1                             |\n| mute     | 0 or 1 | 0                              | 1                             |\n| loop     | 0 or 1 | 0                              | 1                             |\n\nThe `auto` URL parameter is no longer used, it would always search automatically when there is `?url=` param.\n\nNote that the server cannot access private image URLs.\nIn that case, users has to copy and paste (Ctrl+V/Cmd+V) the image directly, or save and upload the file.\n"
 },
 {
  "repo": "makelove/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "- \u6211\u5728B\u7ad9\u505a\u89c6\u9891\u535a\u5ba2VLoger\uff0c\u6b22\u8fce\u5927\u5bb6\u6765\u6367\u573a\u3002\u4e0d\u53ea\u662fOpenCV\n    - \u7a0b\u5e8f\u5458\u8d5a\u94b1\u6307\u5357 https://space.bilibili.com/180948619\n\n# [OpenCV-Python-Tutorial](https://github.com/makelove/OpenCV-Python-Tutorial)\n- \u6709\u670b\u53cb\u53cd\u6620\u8bf4\u4e0b\u8f7drepo\u6162\uff0c\u56e0\u4e3adata\u6709\u4e00\u4e9b\u89c6\u9891sample\n    - \u73b0\u57282020-8-15\u628arepo\u538b\u7f29\uff0c\u4e0a\u4f20\u5230\u767e\u5ea6\u4e91\u76d8\n        - \u94fe\u63a5: https://pan.baidu.com/s/1jpjpfum5EMpNrZoEHGvn1g \u63d0\u53d6\u7801: 8cab\n\n## [OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf](OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf)\n- \u8fd9\u4e2arepo\u662f\u8fd9\u672c\u4e66PDF\u7684\u6240\u6709\u6e90\u4ee3\u7801\uff0c\u51e0\u4e4e\u90fd\u88ab\u6d4b\u8bd5\u8fc7\uff0c\u80fd\u6b63\u5e38\u8fd0\u884c\u3002\u7a0b\u5e8f\u4f7f\u7528\u7684\u56fe\u7247\u548c\u89c6\u9891\uff0c\u90fd\u5728data\u6587\u4ef6\u5185\u3002\n\n### \u5e73\u65f6\u4f1a\u6dfb\u52a0\u4e00\u4e9b\u6709\u8da3\u7684\u4ee3\u7801\uff0c\u5b9e\u73b0\u67d0\u79cd\u529f\u80fd\u3002\n- \u5b98\u7f51 https://opencv.org/\n- \u5b98\u65b9\u6587\u6863api https://docs.opencv.org/4.0.0/\n- \u5b98\u65b9\u82f1\u6587\u6559\u7a0b http://docs.opencv.org/3.2.0/d6/d00/tutorial_py_root.html\n\n## \u8fd0\u884c:\u5b98\u65b9samples/demo.py \u4f1a\u6709\u5f88\u591a\u6709\u8da3\u7684\u4f8b\u5b50\uff0c\u4ecb\u7ecd\u4f60\u53bb\u4e86\u89e3OpenCV\u7684\u529f\u80fd\u3002\n\n\n~~python 2.7 \u5206\u652f\u88ab\u5e9f\u5f03\u4e86\uff0c\u4e0d\u518d\u66f4\u65b0~~\n\n~~# \u6dfb\u52a0\u4e86 Python3.6\u5206\u652f,\n\u8be5\u5206\u652f\u662f\u4f7f\u7528 opencv3.2+Python3.6~~\n\n## \u628a\u539f\u6765\u7684master\u5206\u652f\u6539\u4e3apython2.7\u5206\u652f\uff0cpython3.6\u5206\u652f\u6539\u4e3amaster\u5206\u652f\n* git clone https://github.com/makelove/OpenCV-Python-Tutorial.git\n* ~~git checkout python3.6~~\n\n##### \u5efa\u8bae\u4f7f\u7528PyCharm\u6765\u7f16\u5199/\u8c03\u8bd5Python\u4ee3\u7801\n\n## \u5f00\u53d1\u73af\u5883\n* macOS Mojave 10.14\n* Python 3.6.1\n* OpenCV 3.2.0\n* PyCharm 2018.3\n\n\n### VMware \u865a\u62df\u673a\n\u5982\u679c\u5b89\u88c5OpenCV\u6709\u95ee\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528VMware \u865a\u62df\u673a\u5b89\u88c5Ubuntu\u7cfb\u7edf\uff0c\u672c\u4eba\u53ef\u4ee5\u5e2e\u4f60\u4eec\u5b89\u88c5\u4e00\u4e2a\uff0c\u518d\u5171\u4eab\u5230\u767e\u5ea6\u4e91\n\n### \u6811\u8393\u6d3e3b\n\u672c\u4eba\u6709\u4e00\u5757\u3010\u6811\u8393\u6d3e3b\u3011\u5f00\u53d1\u677f\uff0c\u4e5f\u5b89\u88c5\u4e86OpenCV3\uff0c\u5f88\u597d\u7528\uff0c\u5efa\u8bae\u4f60\u4eec\u4e5f\u4e70\u4e00\u5757\u6765\u73a9\u4e00\u73a9\u3002\n\n### \u6444\u50cf\u5934\n* MacBook pro\u81ea\u5e26\n* \u6dd8\u5b9d\uff0c[130W\u50cf\u7d20\u9ad8\u6e05\u6444\u50cf\u5934\u6a21\u7ec4 720P 1280x720 USB2.0\u514d\u9a71 \u5fae\u8ddd\u6a21\u5757](https://s.click.taobao.com/gOB3ACw)\n* \u6dd8\u5b9d\uff0c[\u6811\u8393\u6d3e3\u4ee3B Raspberry Pi USB\u6444\u50cf\u5934\uff0c\u514d\u9a71\u52a8](https://s.click.taobao.com/kTu2ACw) \u4e0d\u597d\u7528\uff0c\u53ef\u89c6\u89d2\u5ea6\u592a\u5c0f\uff01\n* Kinect for Xbox360 Slim\uff0c AUX\u63a5\u53e3\u4e0d\u80fd\u76f4\u63a5\u63d2\u5165\u7535\u8111\uff0c\u9700\u8981\u8d2d\u4e70\u7535\u6e90\u9002\u914d\u5668 [\u6dd8\u5b9d](https://s.click.taobao.com/t?e=m%3D2%26s%3DuOhQTZaHKEQcQipKwQzePOeEDrYVVa64LKpWJ%2Bin0XLjf2vlNIV67rEUhWAGPPKrYFMBzHxYoCOlldgrEKAMDfvtTsPa%2Bvw8FDXjhIkoffd7RTQd3LKg2nJi6DFpZGNc%2Bht3wBcxEogkdIkZMKiRbrUG0ypJDuSgXlTpbZcV4j5YC7K2OdchcA%3D%3D&scm=null&pvid=null&app_pvid=59590_11.9.33.73_524_1585572680125&ptl=floorId%3A17741&originalFloorId%3A17741&app_pvid%3A59590_11.9.33.73_524_1585572680125&union_lens=lensId%3APUB%401585572666%400b1a25a5_48ac_1712b7ede03_179a%40023mXY9mmpUNuNySUoJofoOt)\n\n## \u6559\u7a0b\u8d44\u6e90\n- http://www.learnopencv.com/\n- http://www.pyimagesearch.com/\n- [YouTube\u4e0asentex\u7684OpenCV\u89c6\u9891\u6559\u7a0b](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq)\n- B\u7ad9 [OpenCV YouTube](https://search.bilibili.com/all?keyword=OpenCV%20YouTube)\n- [\u5b98\u65b9\u6559\u7a0b](https://opencv.org/courses/)\n\n## \u65b0\u95fbNews https://opencv.org/news.html\n- \u4e2d\u6587\u8bba\u575b http://www.opencv.org.cn/\n- [OpenCV 3.3\u53d1\u5e03\u4e86](http://opencv.org/opencv-3-3.html) \n    1. \u4e3b\u8981\u6d88\u606f\u662f\u6211\u4eec\u5c06DNN\u6a21\u5757\u4eceopencv_contrib\u63a8\u5e7f\u5230\u4e3b\u5b58\u50a8\u5e93\uff0c\u6539\u8fdb\u548c\u52a0\u901f\u4e86\u5f88\u591a\u3002\u4e0d\u518d\u9700\u8981\u5916\u90e8BLAS\u5b9e\u73b0\u3002\u5bf9\u4e8eGPU\uff0c\u4f7f\u7528Halide\uff08http://halide-lang.org\uff09\u8fdb\u884c\u5b9e\u9a8c\u6027DNN\u52a0\u901f\u3002\u6709\u5173\u8be5\u6a21\u5757\u7684\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728\u6211\u4eec\u7684wiki\u4e2d\u627e\u5230\uff1a[OpenCV\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV)\u3002\n    2. OpenCV\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u6807\u5fd7ENABLE_CXX11\u6784\u5efa\u4e3aC ++ 11\u5e93\u3002\u6dfb\u52a0\u4e86C ++ 11\u7a0b\u5e8f\u5458\u7684\u4e00\u4e9b\u5f88\u9177\u7684\u529f\u80fd\u3002\n    3. \u7531\u4e8e\u201c\u52a8\u6001\u8c03\u5ea6\u201d\u529f\u80fd\uff0c\u6211\u4eec\u8fd8\u5728OpenCV\u7684\u9ed8\u8ba4\u7248\u672c\u4e2d\u542f\u7528\u4e86\u4e0d\u5c11AVX / AVX2\u548cSSE4.x\u4f18\u5316\u3002DNN\u6a21\u5757\u8fd8\u5177\u6709\u4e00\u4e9bAVX / AVX2\u4f18\u5316\u3002\nIntel Media SDK\u73b0\u5728\u53ef\u4ee5\u88ab\u6211\u4eec\u7684videoio\u6a21\u5757\u7528\u6765\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u7684\u89c6\u9891\u7f16\u7801/\u89e3\u7801\u3002\u652f\u6301MPEG1 / 2\uff0c\u4ee5\u53caH.264\u3002\n    4. \u5d4c\u5165OpenCV Intel IPP\u5b50\u96c6\u5df2\u4ece2015.12\u5347\u7ea7\u52302017.2\u7248\u672c\uff0c\u4ece\u800c\u5728\u6211\u4eec\u7684\u6838\u5fc3\u548cimgproc perf\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e8615\uff05\u7684\u901f\u5ea6\u3002\n    5. 716\u62c9\u8bf7\u6c42\u5df2\u7ecf\u5408\u5e76\uff0c588\u6211\u4eec\u7684\u9519\u8bef\u8ddf\u8e2a\u5668\u4e2d\u7684\u95ee\u9898\u5df2\u7ecf\u5173\u95ed\uff0c\u56e0\u4e3aOpenCV 3.2\u3002\u53e6\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u4e25\u683c\u7684\u9759\u6001\u5206\u6790\u4eea\u5de5\u5177\u8fd0\u884cOpenCV\uff0c\u5e76\u4fee\u590d\u4e86\u68c0\u6d4b\u5230\u7684\u95ee\u9898\u3002\u6240\u4ee5OpenCV 3.3\u5e94\u8be5\u662f\u975e\u5e38\u7a33\u5b9a\u548c\u53ef\u9760\u7684\u91ca\u653e\u3002\n    6. \u6709\u5173OpenCV 3.3\u7684\u66f4\u6539\u548c\u65b0\u529f\u80fd\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95eehttps://github.com/opencv/opencv/wiki/ChangeLog\u3002\n    7. [\u4e0b\u8f7dOpenCV 3.3](https://github.com/opencv/opencv/releases/tag/3.3.0)\n    8. [\u5b89\u88c5OpenCV 3.3](http://www.linuxfromscratch.org/blfs/view/cvs/general/opencv.html)\n- OpenCV 4.0\u53d1\u5e03\u4e86 https://opencv.org/opencv-4-0-0.html\n\n## \u600e\u6837\u7ffb\u5899\uff1f\u4f7f\u7528Google\u641c\u7d22\u5f15\u64ce\uff0c\u89c2\u770bYouTube\u89c6\u9891\u6559\u7a0b\n- shadowsocks\n    - \u65b9\u4fbf\uff0c\u968f\u5730\u968f\u65f6\u7ffb\u5899\n    - \u624b\u673a\u4f7f\u75284G\u4fe1\u53f7\u4e0a\u7f51\uff0c\u4e5f\u53ef\u4ee5\u3002\n    - \u5f3a\u70c8\u63a8\u8350\uff01\n    - \u8d2d\u7269\u56fd\u5916\u670d\u52a1\u5668\uff0c\u642d\u5efa\u4e5f\u5f88\u5bb9\u6613\n        - \u53c2\u8003 https://isweic.com/build-shadowsocks-python-server/\n        - pip install shadowsocks\n        - \u8fd0\u884c\n            - shell\u7a97\u53e3\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb\n                - 8388\u662f\u7aef\u53e3\u53f7\uff0cpassword\u662f\u5bc6\u7801\uff0caes-256-cfb\u662f\u52a0\u5bc6\u7c7b\u578b\uff0c\u901a\u8fc7Ctrl+C\u7ed3\u675f\n            - \u540e\u53f0\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb --user nobody -d start\n                - \u7ed3\u675f\u540e\u53f0\u8fd0\u884c\n                    - ssserver -d stop\n            - \u68c0\u67e5\u8fd0\u884c\u65e5\u5fd7\n                - less /var/log/shadowsocks.log\n- [Lantern\u84dd\u706f](https://github.com/getlantern/lantern/releases/tag/latest)\n    - \u672c\u4eba\u4e0d\u4f7f\u7528\u84dd\u706f\u4e86\u3002\n    1. \u53ef\u4ee5\u514d\u8d39\u4f7f\u7528\uff0c\u4f46\u7528\u5b8c800m\u6d41\u91cf\u540e\u4f1a\u9650\u901f\uff0c\u8fd8\u80fd\u6b63\u5e38\u4f7f\u7528\uff0c\u5c31\u662f\u6709\u70b9\u6162\n    2. \u4e13\u4e1a\u7248\u4e0d\u8d35\uff0c2\u5e74336\u5143\uff0c\u6bcf\u59290.46\u5143\u3002[Lantern\u84dd\u706f\u4e13\u4e1a\u7248\u8d2d\u4e70\u6d41\u7a0b](https://github.com/getlantern/forum/issues/3863)\n    3. \u9080\u8bf7\u597d\u53cb\u6765\u83b7\u5f97\u66f4\u591a\u7684\u4e13\u4e1a\u7248\u4f7f\u7528\u65f6\u95f4\u3002\u6211\u7684\u9080\u8bf7\u7801\uff1aGW2362\n    \n## \u66f4\u65b0\n- [\u7834\u89e3\u9a8c\u8bc1\u7801](my06-\u9a8c\u8bc1\u7801\u8bc6\u522b/solving_captchas_code_examples/README.md)\n    \n## \u6350\u8d60\u6253\u8d4f  \n- OpenCV\u95ee\u7b54\u7fa41,QQ\u7fa4\u53f7:187436093\n- \u5fae\u4fe1  \n    - <img src=\"data/wechat_donate.jpg\" width = \"200\" height = \"200\" alt=\"wechat_donate\"  />\n\n\n- \u652f\u4ed8\u5b9d\n    - <img src=\"data/alipay_donate.jpg\" width = \"200\" height = \"200\" alt=\"alipay_donate\"  />\n \n- \u798f\u5229\n    - \u514d\u8d39\u56fd\u5185\u670d\u52a1\u5668\uff0c\u4f46\u9700\u8981\u4ea4\u62bc\u91d1\uff0c\u968f\u65f6\u5168\u989d\u539f\u8def\u9000\u8fd8\n        - \u6709\u9700\u8981\u7684\u670b\u53cb\u8bf7\u52a0\u5165QQ\u7fa4\uff0c\u53d1\u3010\u624b\u673a\u53f7\u3011\u7ed9\u7fa4\u4e3b\n        - ![free_server](data/free_server.jpeg)"
 },
 {
  "repo": "CodecWang/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "# \u9762\u5411\u521d\u5b66\u8005\u7684OpenCV-Python\u6559\u7a0b\n\n- \u6559\u7a0b\u5730\u5740: [http://codec.wang/#/opencv/](http://codec.wang/#/opencv/)\n- \u672c\u4ed3\u5e93\u4e3a\u6559\u7a0b\u4e2d\u6240\u7528\u5230\u7684\u6e90\u7801\u3001\u56fe\u7247\u548c\u97f3\u89c6\u9891\u7d20\u6750\u7b49\n\n![](http://cos.codec.wang/opencv-python-tutorial-amend-new-cover.png)\n\n## \u76ee\u5f55\n\n### \u5165\u95e8\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u7b80\u4ecb\u4e0e\u5b89\u88c5](http://codec.wang/#/opencv/start/01-introduction-and-installation) | \u4e86\u89e3\u548c\u5b89\u88c5OpenCV-Python |\n| [\u756a\u5916\u7bc7: \u4ee3\u7801\u6027\u80fd\u4f18\u5316](http://codec.wang/#/opencv/start/extra-01-code-optimization) | \u5ea6\u91cf\u8fd0\u884c\u65f6\u95f4/\u63d0\u5347\u6548\u7387\u7684\u51e0\u79cd\u65b9\u5f0f |\n| [\u57fa\u672c\u5143\u7d20: \u56fe\u7247](http://codec.wang/#/opencv/start/02-basic-element-image) | \u56fe\u7247\u7684\u8f7d\u5165/\u663e\u793a\u548c\u4fdd\u5b58 |\n| [\u756a\u5916\u7bc7: \u65e0\u635f\u4fdd\u5b58\u548cMatplotlib\u4f7f\u7528](http://codec.wang/#/opencv/start/extra-02-high-quality-save-and-matplotlib) | \u9ad8\u4fdd\u771f\u4fdd\u5b58\u56fe\u7247\u3001Matplotlib\u5e93\u7684\u7b80\u5355\u4f7f\u7528 |\n| [\u6253\u5f00\u6444\u50cf\u5934](http://codec.wang/#/opencv/start/03-open-camera) | \u6253\u5f00\u6444\u50cf\u5934\u6355\u83b7\u56fe\u7247/\u5f55\u5236\u89c6\u9891/\u64ad\u653e\u672c\u5730\u89c6\u9891 |\n| [\u756a\u5916\u7bc7: \u6ed1\u52a8\u6761](http://codec.wang/#/opencv/start/extra-03-trackbar) | \u6ed1\u52a8\u6761\u7684\u4f7f\u7528 |\n| [\u56fe\u50cf\u57fa\u672c\u64cd\u4f5c](http://codec.wang/#/opencv/start/04-basic-operations) | \u8bbf\u95ee\u50cf\u7d20\u70b9/ROI/\u901a\u9053\u5206\u79bb\u5408\u5e76/\u56fe\u7247\u5c5e\u6027 |\n| [\u989c\u8272\u7a7a\u95f4\u8f6c\u6362](http://codec.wang/#/opencv/start/05-changing-colorspaces) | \u989c\u8272\u7a7a\u95f4\u8f6c\u6362/\u8ffd\u8e2a\u7279\u5b9a\u989c\u8272\u7269\u4f53 |\n| [\u9608\u503c\u5206\u5272](http://codec.wang/#/opencv/start/06-image-thresholding) | \u9608\u503c\u5206\u5272/\u4e8c\u503c\u5316 |\n| [\u756a\u5916\u7bc7: Otsu\u9608\u503c\u6cd5](http://codec.wang/#/opencv/start/extra-04-otsu-thresholding) | \u53cc\u5cf0\u56fe\u7247/Otsu\u81ea\u52a8\u9608\u503c\u6cd5 |\n| [\u56fe\u50cf\u51e0\u4f55\u53d8\u6362](http://codec.wang/#/opencv/start/07-image-geometric-transformation) | \u65cb\u8f6c/\u5e73\u79fb/\u7f29\u653e/\u7ffb\u8f6c |\n| [\u756a\u5916\u7bc7: \u4eff\u5c04\u53d8\u6362\u4e0e\u900f\u89c6\u53d8\u6362](http://codec.wang/#/opencv/start/extra-05-warpaffine-warpperspective) | \u57fa\u4e8e2\u00d73\u7684\u4eff\u5c04\u53d8\u6362/\u57fa\u4e8e3\u00d73\u7684\u900f\u89c6\u53d8\u6362 |\n| [\u7ed8\u56fe\u529f\u80fd](http://codec.wang/#/opencv/start/08-drawing-function) | \u753b\u7ebf/\u753b\u5706/\u753b\u77e9\u5f62/\u6dfb\u52a0\u6587\u5b57 |\n| [\u756a\u5916\u7bc7: \u9f20\u6807\u7ed8\u56fe](http://codec.wang/#/opencv/start/extra-06-drawing-with-mouse) | \u7528\u9f20\u6807\u5b9e\u65f6\u7ed8\u56fe |\n| [\u6311\u6218\u7bc7: \u753b\u52a8\u6001\u65f6\u949f](http://codec.wang/#/opencv/start/challenge-01-draw-dynamic-clock) | / |\n| [\u6311\u6218\u7bc7: PyQt5\u7f16\u5199GUI\u754c\u9762](http://codec.wang/#/opencv/start/challenge-02-create-gui-with-pyqt5) | / |\n\n### \u57fa\u7840\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u56fe\u50cf\u6df7\u5408](http://codec.wang/#/opencv/basic/09-image-blending) | \u7b97\u6570\u8fd0\u7b97/\u6df7\u5408/\u6309\u4f4d\u8fd0\u7b97 |\n| [\u756a\u5916\u7bc7: \u4eae\u5ea6\u4e0e\u5bf9\u6bd4\u5ea6](http://codec.wang/#/opencv/basic/extra-07-contrast-and-brightness) | \u8c03\u6574\u56fe\u7247\u7684\u4eae\u5ea6\u548c\u5bf9\u6bd4\u5ea6 |\n| [\u5e73\u6ed1\u56fe\u50cf](http://codec.wang/#/opencv/basic/10-smoothing-images) | \u5377\u79ef/\u6ee4\u6ce2/\u6a21\u7cca/\u964d\u566a |\n| [\u756a\u5916\u7bc7: \u5377\u79ef\u57fa\u7840-\u56fe\u7247\u8fb9\u6846](http://codec.wang/#/opencv/basic/extra-08-padding-and-convolution) | \u4e86\u89e3\u5377\u79ef/\u6ee4\u6ce2\u7684\u57fa\u7840\u77e5\u8bc6/\u7ed9\u56fe\u7247\u6dfb\u52a0\u8fb9\u6846 |\n| [\u8fb9\u7f18\u68c0\u6d4b](http://codec.wang/#/opencv/basic/11-edge-detection) | Canny/Sobel\u7b97\u5b50 |\n| [\u756a\u5916\u7bc7: \u56fe\u50cf\u68af\u5ea6](http://codec.wang/#/opencv/basic/extra-09-image-gradients) | \u4e86\u89e3\u56fe\u50cf\u68af\u5ea6\u548c\u8fb9\u7f18\u68c0\u6d4b\u7684\u76f8\u5173\u6982\u5ff5 |\n| [\u8150\u8680\u4e0e\u81a8\u80c0](http://codec.wang/#/opencv/basic/12-erode-and-dilate) | \u5f62\u6001\u5b66\u64cd\u4f5c/\u8150\u8680/\u81a8\u80c0/\u5f00\u8fd0\u7b97/\u95ed\u8fd0\u7b97 |\n| [\u8f6e\u5ed3](http://codec.wang/#/opencv/basic/13-contours) | \u5bfb\u627e/\u7ed8\u5236\u8f6e\u5ed3 |\n| [\u756a\u5916\u7bc7: \u8f6e\u5ed3\u5c42\u7ea7](http://codec.wang/#/opencv/basic/extra-10-contours-hierarchy) | \u4e86\u89e3\u8f6e\u5ed3\u95f4\u7684\u5c42\u7ea7\u5173\u7cfb |\n| [\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/14-contour-features) | \u9762\u79ef/\u5468\u957f/\u6700\u5c0f\u5916\u63a5\u77e9\\(\u5706\\)/\u5f62\u72b6\u5339\u914d |\n| [\u756a\u5916\u7bc7: \u51f8\u5305\u53ca\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/extra-11-convex-hull) | \u8ba1\u7b97\u51f8\u5305/\u4e86\u89e3\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81 |\n| [\u76f4\u65b9\u56fe](http://codec.wang/#/opencv/basic/15-histograms) | \u8ba1\u7b97\u7ed8\u5236\u76f4\u65b9\u56fe/\u5747\u8861\u5316 |\n| [\u6a21\u677f\u5339\u914d](http://codec.wang/#/opencv/basic/16-template-matching) | \u56fe\u4e2d\u627e\u5c0f\u56fe |\n| [\u970d\u592b\u53d8\u6362](http://codec.wang/#/opencv/basic/17-hough-transform) | \u63d0\u53d6\u76f4\u7ebf/\u5706 |\n| [\u6311\u6218\u4efb\u52a1: \u8f66\u9053\u68c0\u6d4b](http://codec.wang/#/opencv/basic/challenge-03-lane-road-detection) | / |\n\n> \u5982\u679c\u60a8\u89c9\u5f97\u5199\u7684\u4e0d\u9519\u7684\u8bdd\uff0c\u6b22\u8fce\u6253\u8d4f\uff0c\u6211\u4f1a\u52aa\u529b\u5199\u51fa\u66f4\u597d\u7684\u5185\u5bb9\uff01\u270a\ud83e\udd1f\n\n![](http://cos.codec.wang/wechat_alipay_pay_pic.png)\n\n"
 },
 {
  "repo": "HuTianQi/SmartOpenCV",
  "language": "C++",
  "readme_contents": "# SmartOpenCV\n\n![SmartOpenCV](art/logo.png)  \n### \u524d\u8a00\n\n:fire: :fire: :fire: \u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u4ee5\u53ca\u7ec8\u7aef\u8bbe\u5907\u786c\u4ef6\u6c34\u5e73\u7684\u4e0d\u65ad\u63d0\u5347\uff0c\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u76f4\u63a5\u8fd0\u884c\u667a\u80fd\u7cfb\u7edf\u6210\u4e3a\u53ef\u80fd\uff0c\u7aef\u4fa7\u667a\u80fd\u5177\u5907\u4f4e\u5ef6\u65f6\uff0c\u9690\u79c1\u5b89\u5168\u7b49\u7279\u70b9\u3002\u540c\u65f6\u964d\u4f4e\u4e86\u4e91\u7aef\u667a\u80fd\u5b58\u5728\u7684\u7f51\u7edc\u4f20\u8f93\u4e0d\u53ef\u9760\u98ce\u9669\uff0c\u4f7f\u5f97\u7aef\u4fa7\u667a\u80fd\u8d8a\u6765\u8d8a\u5f97\u5230\u91cd\u89c6\u3002\u7aef\u4fa7\u667a\u80fd\u6bd4\u8f83\u6210\u719f\u7684\u9886\u57df\u5c31\u662fNLP\u4ee5\u53caCV\u3002\u5728CV\u9886\u57dfOpenCV\u4f5c\u4e3a\u5f00\u6e90\u4e14\u5f3a\u5927\u7684\u8de8\u5e73\u53f0\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u5728\u56fe\u50cf\u5904\u7406\u4ee5\u53ca\u56fe\u50cf\u8bc6\u522b\u65b9\u5411\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u4f46\u662f\u5728Android\u5e73\u53f0OpenCV\u5b98\u65b9SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u8bf8\u591a\u7f3a\u9677\u3002\n\n### SmartOpenCV\u662f\u4ec0\u4e48\nSmartOpenCV\u662f\u4e00\u4e2aOpenCV\u5728Android\u7aef\u7684\u589e\u5f3a\u5e93\uff0c\u89e3\u51b3\u4e86OpenCV Android SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u7684\u8bf8\u591a\u95ee\u9898\uff0c\u800c\u4e14\u65e0\u9700\u4fee\u6539OpenCV SDK\u6e90\u7801\uff0c\u4e0eOpenCV\u7684SDK\u89e3\u8026\uff0c\u53ea\u9700\u66ff\u6362xml\u4e2d\u539fOpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\u8fbe\u5230\u5177\u5907OpenCV\u5b98\u65b9SDK\u7684\u539f\u529f\u80fd\u4ee5\u53caSmartOpenCV\u7684\u589e\u5f3a\u529f\u80fd\u3002\n\n### OpenCV\u5b98\u65b9SDK\u5b58\u5728\u7684\u95ee\u9898\n\nOpenCV Android\u7aefSDK\u867d\u7136\u5f88\u5bb9\u6613\u4e0a\u624b\u548c\u4f7f\u7528\uff0c\u4f46\u662f\u9884\u89c8\u5b58\u5728\u5f88\u591a\u95ee\u9898\uff0c\u5e38\u89c1\u95ee\u9898\u5982\u4e0b\uff1a\n\n- **\u9ed8\u8ba4\u6a2a\u5c4f\u663e\u793a\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u63a5\u53e3\u4fee\u6539\u9884\u89c8\u65b9\u5411**\n\n- **\u9884\u89c8\u7ed8\u5236\u5b58\u5728\u9ed1\u8fb9**\uff1aOpenCV\u9ed8\u8ba4\u7ed8\u5236\u7b97\u6cd5\u5728\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\u5230Canvas\u65f6\u5b58\u5728\u4e00\u5b9a\u7684\u504f\u79fb\uff0c\u5728\u89c6\u89c9\u4e0a\u8868\u73b0\u5c31\u662f\u9884\u89c8\u5e27\u53ea\u4f1a\u5360SurfaceView\u63a7\u4ef6\u7684\u4e00\u90e8\u5206\u533a\u57df\uff0c\u504f\u79fb\u90e8\u5206\u533a\u57df\u4f1a\u663e\u793a\u4e3a\u9ed1\u8272\n\n  ```java\n  if (mScale != 0) {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2),\n                      (int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2 + mScale * mCacheBitmap.getWidth()),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2 + mScale * mCacheBitmap.getHeight())), null);\n  } else {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((canvas.getWidth() - mCacheBitmap.getWidth()) / 2,\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2,\n                      (canvas.getWidth() - mCacheBitmap.getWidth()) / 2 + mCacheBitmap.getWidth(),\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2 + mCacheBitmap.getHeight()), null);\n  }\n  ```\n\n  \n\n- **\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\u4e0d\u7b26\u5408\u5b9e\u9645\u573a\u666f\u8981\u6c42**\uff1a\u5bf9\u4e8e\u9884\u89c8\u5e27\u5927\u5c0f\u7684\u9009\u62e9\uff0cOpenCV\u9ed8\u8ba4\u7b97\u6cd5\u662f\u9009\u62e9**\u5c0f\u4e8e**\u9884\u89c8\u63a7\u4ef6(\u6216\u8bbe\u7f6e\u7684\u6700\u5927\u5e27\u5927\u5c0f)\u7684\u6700\u5927\u9884\u89c8\uff0c\u8fd9\u5c06\u5bfc\u81f4\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u9884\u89c8\u56fe\u50cf\u7684\u663e\u793a\u4e0d\u80fd\u94fa\u6ee1\u6574\u4e2a\u63a7\u4ef6\u751a\u81f3\u8fdc\u5c0f\u4e8e\u63a7\u4ef6\u5927\u5c0f\uff0c \u5728\u7edd\u5927\u90e8\u5206\u4e1a\u52a1\u573a\u666f\u4e0b\uff0c\u8fd9\u79cd\u7b97\u6cd5\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\n\n  ```java\n  protected Size calculateCameraFrameSize(List<?> supportedSizes, ListItemAccessor accessor, int surfaceWidth, int surfaceHeight) {\n          int calcWidth = 0;\n          int calcHeight = 0;\n  \n          int maxAllowedWidth = (mMaxWidth != MAX_UNSPECIFIED && mMaxWidth < surfaceWidth)? mMaxWidth : surfaceWidth;\n          int maxAllowedHeight = (mMaxHeight != MAX_UNSPECIFIED && mMaxHeight < surfaceHeight)? mMaxHeight : surfaceHeight;\n  \n          for (Object size : supportedSizes) {\n              int width = accessor.getWidth(size);\n              int height = accessor.getHeight(size);\n              Log.d(TAG, \"trying size: \" + width + \"x\" + height);\n  \n              if (width <= maxAllowedWidth && height <= maxAllowedHeight) {\n                  if (width >= calcWidth && height >= calcHeight) {\n                      calcWidth = (int) width;\n                      calcHeight = (int) height;\n                  }\n              }\n          }\n          if ((calcWidth == 0 || calcHeight == 0) && supportedSizes.size() > 0)\n          {\n              Log.i(TAG, \"fallback to the first frame size\");\n              Object size = supportedSizes.get(0);\n              calcWidth = accessor.getWidth(size);\n              calcHeight = accessor.getHeight(size);\n          }\n  \n          return new Size(calcWidth, calcHeight);\n      }\n  ```\n\n  \n\n\n### SmartOpenCV\u7684\u7279\u70b9\n\n- **\u6613\u4f7f\u7528**\uff1a\u5982\u679c\u4f60\u9879\u76ee\u4e2d\u4e4b\u524d\u4f7f\u7528\u7684\u662fOpenCV\u7684\u5b98\u65b9SDK\uff0c\u90a3\u4e48\u5f15\u5165SmartOpenCV\u540e\u53ea\u9700\u5c06xml\u6587\u4ef6\u4e2d\u7684`JavaCameraView`/`JavaCamera2View`\u66ff\u6362\u4e3aSmartOpenCV\u7684`CamerPreview`/`Camera2Preview`\u5373\u53ef\u8fbe\u5230\u4e0e\u4f7f\u7528\u5b98\u65b9SDK\u76f8\u540c\u7684\u6548\u679c\n\n- **\u529f\u80fd\u589e\u5f3a**\uff1a\n  1. \u9884\u89c8\u81ea\u9002\u5e94\uff1a\u81ea\u52a8\u6839\u636e\u524d\u540e\u6444\u50cf\u5934\uff0c\u6a2a\u7ad6\u5c4f\u4ee5\u53ca\u4e0d\u540c\u6444\u50cf\u5934\u53c2\u6570\u6765\u8c03\u6574\u4e0e\u9002\u914d\u9884\u89c8\u65b9\u5411\u4ee5\u53ca\u5927\u5c0f\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5199\u4efb\u4f55\u989d\u5916\u4ee3\u7801\n  2. \u53ef\u6269\u5c55\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u7ed8\u5236\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u573a\u666f\u81ea\u5b9a\u4e49\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\n  3. \u53ef\u6269\u5c55\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\n  4. \u652f\u6301**USB\u6444\u50cf\u5934**\uff1aUSB\u6444\u50cf\u5934\u4f5c\u4e3a\u5916\u8bbe\u63a5\u5165\u8bbe\u5907\uff0c\u548c\u624b\u673a/\u5e73\u677f\u7b49\u79fb\u52a8\u8bbe\u5907\u5185\u7f6e\u6444\u50cf\u5934\u5b58\u5728\u5dee\u5f02\uff0cSDK\u5185\u90e8\u5728\u5904\u7406\u79fb\u52a8\u8bbe\u5907\u6444\u50cf\u5934\u7684\u903b\u8f91\u65f6\u4e5f\u517c\u5bb9\u4e86\u5bf9\u95f8\u673a\u7b49\u7684USB\u6444\u50cf\u5934\u7684\u5904\u7406\n  \n- **\u63d0\u4f9b\u66f4\u53cb\u597d\u7684API\u63a5\u53e3**\uff1a\u5728\u7ee7\u627fOpenCV\u5b98\u65b9\u63a5\u53e3\u7684\u540c\u65f6\uff0cSmartOpenCV\u5c06\u4f17\u591a\u7e41\u6742\u64cd\u4f5c\u7edf\u4e00\u901a\u8fc7CameraConfiguration\u6765\u914d\u7f6e\uff0c\u63d0\u4f9b\u66f4\u53cb\u597d\u7684Fluent API\u63a5\u53e3\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\n\n- **\u4e0d\u76f4\u63a5\u4f9d\u8d56\u5b98\u65b9SDK\uff0c\u65b9\u4fbf\u5347\u7ea7\u5b98\u65b9SDK**\uff1a\u4e0eOpenCV\u5b98\u65b9SDK\u89e3\u8026\uff0c\u53ea\u8981\u5b98\u65b9SDK\u5185\u90e8\u6838\u5fc3\u903b\u8f91\u672a\u505a\u4fee\u6539\uff0c\u90a3\u4e48SmartOpenCV\u53ef\u4ee5\u517c\u5bb9\u6240\u6709\u7248\u672c\u7684\u5b98\u65b9SDK\uff0c\u4f7f\u7528SmartOpenCV\u540e\u5982\u679c\u4ee5\u540e\u6253\u7b97\u5347\u7ea7\u4f9d\u8d56\u7684OpenCV\u4e3a\u66f4\u65b0\u7248\u672c\uff0c\u53ea\u9700\u5c06OpenCV\u7684\u4f9d\u8d56\u66f4\u65b0\u4e3a\u65b0\u7248\u672c\u5373\u53ef\uff0c\u4ee3\u7801\u65e0\u9700\u505a\u4efb\u4f55\u6539\u52a8\n\n### \u6548\u679c\u5bf9\u6bd4\n#### \u4ee5\u4eba\u8138\u8bc6\u522b\u4e3a\u4f8b\n\n|            | \u6a2a\u5c4f                                                         | \u7ad6\u5c4f   |\n| ---------- | ------------------------------------------------------------ | ------ |\n| OpenCV | <div align=center>**\u5373\u4f7f\u5bbd\u4e0e\u9ad8\u90fd\u8bbe\u7f6e\u4e3amatch_parent\u4e5f\u65e0\u6cd5\u5168\u5c4f\uff0c\u5b58\u5728\u9ed1\u8fb9**  <img src=\"./art/screenshort/opencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\"/></div>  |<div align=center> **\u5b58\u5728\u9ed1\u8fb9\uff0c\u4e14\u9ed8\u8ba4\u4e0d\u652f\u6301\u7ad6\u5c4f**  <img src=\"./art/screenshort/opencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\" /></div>  |\n| SmartOpenCV | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\" /></div> | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\"/></div> |\n\n### Demo\u5bf9\u6bd4\u4f53\u9a8c\n[smartopencv-app-debug.apk](demo/smartopencv-app-debug.apk)  \n[opencv-app-debug.apk](demo/opencv-app-debug.apk)  \n\n### Integration\n\nStep1\uff1a\u5728\u9879\u76ee\u6839\u76ee\u5f55\u7684build.gradle\u4e2d\u6dfb\u52a0\u5bf9jitpack\u4ed3\u5e93\u7684\u914d\u7f6e\n\n```\nallprojects {\n    repositories {\n        ...\n        maven { url 'https://jitpack.io' }\n    }\n}\n```\n\nStep2\uff1a\u5728\u9700\u8981\u4f7f\u7528`SmartOpenCV`\u5e93\u7684\u6a21\u5757\u4e2d\u6dfb\u52a0\u4f9d\u8d56\n\n```\ndependencies {\n\timplementation('com.github.HuTianQi:SmartOpenCV:1.0.1') { // \u7248\u672c\u53f7\u5efa\u8bae\u4f7f\u7528\u5df2release\u7684\u6700\u65b0\u7248\u672c\n        exclude module: 'openCVLibrary411' // \u7531\u4e8e\u76ee\u524d\u591a\u6a21\u5757\u4f9d\u8d56\u65f6jitpack\u6253\u5305\u5b58\u5728bug\uff0c\u6392\u9664\u6253\u5305\u65f6\u4f9d\u8d56\u7684\u8be5\u6a21\u5757\n    }\n}\n```\n\n\n### Usage\n\n#### \u57fa\u7840\u7528\u6cd5\n\n\u5728\u9879\u76ee\u4e2d\u9700\u8981\u4f7f\u7528\u9884\u89c8\u7684xml\u4e2d\u7528SmartOpenCV\u7684`CameraPreview`/`Camera2Preview`\u66ff\u6362OpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\uff0c\u5c31\u8fd9\u4e48\u7b80\u5355\uff0c\u5176\u4f59\u7684\u4ec0\u4e48\u90fd\u4e0d\u7528\u505a\n\n```xml\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <!--<org.opencv.android.JavaCameraView-->\n    <!--android:id=\"@+id/fd_activity_surface_view\"-->\n    <!--android:layout_width=\"match_parent\"-->\n    <!--android:layout_height=\"match_parent\" />-->\n\n    <tech.huqi.smartopencv.core.preview.CameraPreview\n        android:id=\"@+id/fd_activity_surface_view\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</LinearLayout>\n```\n\n#### \u9ad8\u7ea7\u7528\u6cd5\n\n\u5982\u679c\u6253\u7b97\u901a\u8fc7SmartOpenCV\u63d0\u4f9b\u7684\u63a5\u53e3\u6765\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\uff0c\u90a3\u4e48\u8c03\u7528`SmartOpenCV.getInstance().init()`\u4f20\u5165\u524d\u9762\u83b7\u53d6\u7684\u9884\u89c8\u63a7\u4ef6\u5bf9\u8c61\u5373\u53ef\uff0c\u7528\u6cd5\u5982\u4e0b\uff1a\n\n```java\nSmartOpenCV.getInstance().init(mOpenCvCameraView, new CameraConfiguration.Builder()\n    .debug(true)\n    .cameraIndex(0)      // \u8bbe\u7f6e\u6444\u50cf\u5934\u7d22\u5f15,\u4e3b\u8981\u7528\u4e8e\u591a\u6444\u50cf\u5934\u8bbe\u5907\uff0c\u4f18\u5148\u7ea7\u4f4e\u4e8efrontCamera\n    .keepScreenOn(false) // \u662f\u5426\u4fdd\u6301\u5c4f\u5e55\u5e38\u4eae\n    .frontCamera(true)   // \u662f\u5426\u4f7f\u7528\u524d\u7f6e\u6444\u50cf\u5934\n    .openCvDefaultDrawStrategy(false)      // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u56fe\u50cf\u7ed8\u5236\u7b56\u7565\n    .openCvDefaultPreviewCalculator(false) // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n    .landscape(false)     // \u662f\u5426\u6a2a\u5c4f\u663e\u793a\n    .enableFpsMeter(true) // \u5f00\u542f\u9884\u89c8\u5e27\u7387\u7684\u663e\u793a\n    .usbCamera(false)     // \u662f\u5426\u4f7f\u7528USB\u6444\u50cf\u5934\uff0c\u5f53\u8bbe\u5907\u63a5\u5165\u7684\u662fUSB\u6444\u50cf\u5934\u65f6\u5c06\u5176\u8bbe\u7f6e\u4e3atrue\n    .maxFrameSize(400, 300)     // \u8bbe\u7f6e\u9884\u89c8\u5e27\u7684\u6700\u5927\u5927\u5c0f\n    .cvCameraViewListener(this) // \u8bbe\u7f6eOpenCV\u56de\u8c03\u76d1\u542c\u5668\n    .previewSizeCalculator(new IPreviewSizeCalculator() { // \u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n        @Override\n        public Size calculateCameraFrameSize(List<Size> supportedSizes, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u6539\u5199\u89c8\u5e27\u5927\u5c0f\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n            return new Size(1080,1920); \n        }\n    })\n    .drawStrategy(new IDrawStrategy() { // \u81ea\u5b9a\u4e49\u7ed8\u5236\u7b56\u7565\n        @Override\n        public void drawBitmap(Canvas canvas, Bitmap frameBitmap, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n        }\n    })\n    .build());\n```\n\n\n\n### LICENSE\n[LICENSE](LICENSE)  \n\n### \u516c\u4f17\u53f7\n![\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\u4ea4\u6d41\u53cd\u9988](art/wx_gzh.jpg)  \n"
 },
 {
  "repo": "MasteringOpenCV/code",
  "language": "C++",
  "readme_contents": "==============================================================================\r\nMastering OpenCV with Practical Computer Vision Projects\r\n==============================================================================\r\nFull source-code for the book.\r\n--------------------------------------------------------------------------------\r\n\r\n    Source-Code:    https://github.com/MasteringOpenCV/code\r\n    Book:           http://www.packtpub.com/cool-projects-with-opencv/book\r\n    Copyright:      Packt Publishing 2012.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nTo build & run the projects for the book:\r\n--------------------------------------------------------------------------------\r\n- Install OpenCV (versions between 2.4.2 to 2.4.11 are supported, whereas OpenCV 3.0 is not yet supported). eg: go to \"http://opencv.org/\", click on\r\n  Downloads, download the latest OpenCV 2.4 version (including prebuilt library), and extract\r\n  it to \"C:\\OpenCV\" for Windows or \"~/OpenCV\" for Linux. In OpenCV v2.4.3, the\r\n  prebuilt OpenCV library is in \"C:\\OpenCV\\build\" or \"~/OpenCV/build\", such as\r\n  \"C:\\OpenCV\\build\\x64\\vc9\" for MS Visual Studio 2008 (or \"vs10\" folder for MS \r\n  Visual Studio 2010, or the \"x86\" parent folder for 32-bit Windows).\r\n\r\n- Install all the source code of the book. eg: extract the code to\r\n  \"C:\\MasteringOpenCV\" for Windows or \"~/MasteringOpenCV\" for Linux.\r\n  \r\n- Install CMake v2.8 or later from \"http://www.cmake.org/\".\r\n\r\nEach chapter of the book is for a separate project. Therefore there are 9\r\nprojects for the 9 chapters (remember that Chapter 9 is an online chapter that\r\ncan be downloaded from \"http://www.packtpub.com/cool-projects-with-opencv/book\").\r\nYou can run each project separately, they each contain a README.md text file\r\ndescribing how to build that project, using CMake in most cases, because CMake\r\ncan be used with many compilers and many operating systems.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nChapters:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android, by Shervin Emami.\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad, by Khvedchenia Ievgen.\r\n- Ch3) Marker-less Augmented Reality, by Khvedchenia Ievgen.\r\n- Ch4) Exploring Structure from Motion using OpenCV, by Roy Shilkrot.\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks, by David Escriv\u00e1.\r\n- Ch6) Non-rigid Face Tracking, by Jason Saragih.\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT, by Daniel L\u00e9lis Baggio.\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces, by Shervin Emami.\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect, by Naureen Mahmood.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nWhat you need for this book:\r\n--------------------------------------------------------------------------------\r\nYou don't need to have special knowledge in computer vision to read this book,\r\nbut you should have good C/C++ programming skills and basic experience with\r\nOpenCV before reading this book. Readers without experience in OpenCV may wish to\r\nread the book Learning OpenCV for an introduction to the OpenCV features, or read\r\n\"OpenCV 2 Cookbook\" for examples on how to use OpenCV with recommended C/C++\r\npatterns, because \"Mastering OpenCV with Practical Computer Vision Projects\" will\r\nshow you how to solve real problems, assuming you are already familiar with the\r\nbasics of OpenCV and C/C++ development.\r\n\r\nIn addition to C/C++ and OpenCV experience, you will also need a computer, and an\r\nIDE of your choice (such as Visual Studio, XCode, Eclipse, or QtCreator, running\r\non Windows, Mac or Linux). Some chapters have further requirements, particularly:\r\n\r\n- To develop the Android app, you will need an Android device, Android\r\n  development tools, and basic Android development experience.\r\n- To develop the iOS app, you will need an iPhone, iPad, or iPod Touch device,\r\n  iOS development tools (including an Apple computer, XCode IDE, and an Apple\r\n  Developer Certificate), and basic iOS and Objective-C development experience.\r\n- Several desktop projects require a webcam connected to your computer. Any\r\n  common USB webcam should suffice, but a webcam of at least 1 megapixel may be\r\n  desirable.\r\n- CMake is used in most projects, including OpenCV itself, to build across\r\n  operating systems and compilers. A basic understanding of build systems is\r\n  required, and knowledge of cross-platform building is recommended.\r\n- An understanding of linear algebra is expected, such as basic vector and matrix\r\n  operations and eigen decomposition.\r\n\r\nPer-chapter Requirements:\r\n- Ch1: webcam (for desktop app), or Android development system (for Android app).\r\n- Ch2: iOS development system (to build an iOS app).\r\n- Ch3: OpenGL built into OpenCV.\r\n- Ch4: PCL (http://pointclouds.org/) and SSBA (http://www.inf.ethz.ch/personal/chzach/opensource.html).\r\n- Ch5: nothing.\r\n- Ch6: nothing, but requires training data for execution.\r\n- Ch7: nothing.\r\n- Ch8: webcam.\r\n- Ch9: Kinect depth sensor.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nScreenshots:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android:\r\n![Ch1) Cartoonifier and Skin Changer for Android](https://raw.github.com/MasteringOpenCV/code/master/Chapter1_AndroidCartoonifier/screenshot.png)\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad:\r\n![Ch2) Marker-based Augmented Reality on iPhone or iPad](https://raw.github.com/MasteringOpenCV/code/master/Chapter2_iPhoneAR/screenshot.png)\r\n- Ch3) Marker-less Augmented Reality:\r\n![Ch3) Marker-less Augmented Reality](https://raw.github.com/MasteringOpenCV/code/master/Chapter3_MarkerlessAR/screenshot.png)\r\n- Ch4) Exploring Structure from Motion using OpenCV:\r\n![Ch4) Exploring Structure from Motion using OpenCV](https://raw.github.com/MasteringOpenCV/code/master/Chapter4_StructureFromMotion/screenshot.png)\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks:\r\n![Ch5) Number Plate Recognition using SVM and Neural Networks](https://raw.github.com/MasteringOpenCV/code/master/Chapter5_NumberPlateRecognition/screenshot.png)\r\n- Ch6) Non-rigid Face Tracking:\r\n![Ch6) Non-rigid Face Tracking](https://raw.github.com/MasteringOpenCV/code/master/Chapter6_NonRigidFaceTracking/screenshot.png)\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT:\r\n![Ch7) 3D Head Pose Estimation using AAM and POSIT](https://raw.github.com/MasteringOpenCV/code/master/Chapter7_HeadPoseEstimation/screenshot.png)\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces:\r\n![Ch8) Face Recognition using Eigenfaces or Fisherfaces](https://raw.github.com/MasteringOpenCV/code/master/Chapter8_FaceRecognition/screenshot.png)\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect:\r\n![Ch9) Developing Fluid Wall using the Microsoft Kinect](https://raw.github.com/MasteringOpenCV/code/master/Chapter9_FluidInteractionUsingKinect/screenshot.png)\r\n\r\n\r\n"
 },
 {
  "repo": "Roujack/mathAI",
  "language": "Python",
  "readme_contents": "# mathAI\n\n\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c\u3002\n**\u8bf7\u67e5\u770b\u7cfb\u7edf\u6587\u6863\u8bf4\u660e\u6765\u8fd0\u884c\u7a0b\u5e8f\u3002\u6ce8\u610f\uff0c\u8fd9\u662f\u4e00\u4e2a\u534a\u5f00\u6e90\u7684\u9879\u76ee\uff0c\u76ee\u524d\u4e0a\u4f20\u7684\u7248\u672c\u53ea\u80fd\u5904\u7406\u7b80\u5355\u7684\u4e00\u7ef4\u52a0\u51cf\u4e58\u9664\u7b97\u672f\u8868\u8fbe\u5f0f\uff08\u5982\u679c\u60f3\u8981\u8bc6\u522b\u66f4\u52a0\u590d\u6742\u7684\u8868\u8fbe\u5f0f\uff0c\u53ef\u4ee5\u53c2\u8003\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u7684\u8bba\u6587\uff09\u3002\u53ef\u4ee5\u53c2\u8003\u7684\u4ee3\u7801\u662f\u524d\u9762\u5b57\u7b26\u8bc6\u522b\u90e8\u5206\u4ee5\u53ca\u6574\u4e2a\u7b97\u6cd5\u5904\u7406\u6846\u67b6\u3002**\n![image](https://github.com/Roujack/mathAI/blob/master/test.png)\n\n\u6574\u4e2a\u7a0b\u5e8f\u4f7f\u7528python\u5b9e\u73b0\uff0c\u5177\u4f53\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u4e86\u56fe\u50cf\u9884\u5904\u7406\u3001\u5b57\u7b26\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u679c\u8f93\u51fa\u3002\n\n\u672c\u7a0b\u5e8f\u4f7f\u7528opencv\u5bf9\u8f93\u5165\u7684\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5e76\u5c06\u5b57\u7b26\u88c1\u526a\u51fa\u6765\u518d\u5f52\u4e00\u5316\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u77e9\u9635\u3002\u6211\u5728TensorFlow\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2alenet5\n\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u8bc6\u522b\u6570\u5b66\u5b57\u7b26\uff0c\u8bad\u7ec3\u4f7f\u7528CHROME\u6570\u636e\u96c6\u3002\u5bf9\u4e8e\u6570\u5b66\u516c\u5f0f\u7684\u8bc6\u522b\uff0c\u4e3b\u8981\u662f\u5c06\u8bc6\u522b\u51fa\u7684\u72ec\u7acb\u7684\u5b57\u7b26\u7ec4\u7ec7\u6210\u8ba1\u7b97\u673a\u80fd\u591f\n\u7406\u89e3\u7684\u6570\u5b66\u516c\u5f0f\uff08\u8fd9\u91cc\u7684\u6570\u5b66\u516c\u5f0f\u5c31\u662f\u7eaf\u5b57\u7b26\u7684\u53ef\u6c42\u89e3\u7684\u6570\u5b66\u8ba1\u7b97\u9898\uff09\u3002\u5927\u6982\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u7f16\u8bd1\u539f\u7406\u7684\u7b97\u7b26\u4f18\u5148\u6cd5\u548c\u9012\u5f52\u4e0b\u964d\u6cd5\u8fdb\u884c\u5b9e\u73b0\u3002\n\u7136\u540e\u6839\u636e\u5c5e\u6027\u6587\u6cd5\u7684\u503c\u4f20\u9012\u601d\u60f3\uff0c\u5c06\u6570\u5b66\u516c\u5f0f\u7684\u503c\u8ba1\u7b97\u51fa\u6765\u3002\u6700\u540e\u4f7f\u7528python\u7684matlibplot\u5e93\u628a\u8ba1\u7b97\u8fc7\u7a0b\u548c\u7b54\u6848\u6253\u5370\u51fa\u6765\u3002\n\n\u4f18\u70b9\uff1a\u8fd9\u662f\u4e00\u6574\u5957\u62cd\u7167\u505a\u9898\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u540c\u65f6\u80fd\u591f\u5904\u7406\u591a\u79cd\u591a\u6837\u7684\u8ba1\u7b97\u9898\uff0c\u76ee\u524d\u5e02\u9762\u4e0a\u8fd8\u6ca1\u6709\u770b\u5230\u5b9e\u73b0\u3002OCR\u6280\u672f\u5982\u6b64\u6210\u719f\u7684\u4eca\u5929\u5b57\u7b26\u8bc6\u522b\n\u5df2\u7ecf\u4e0d\u7b97\u6709\u6311\u6218\u7684\u4e1c\u897f\u4e86\u3002\n\u7f3a\u70b9\uff1a\u5b57\u7b26\u7a7a\u95f4\u5173\u7cfb\u5224\u65ad\u53ea\u7528\u4e86\u4eba\u7c7b\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u56fe\u50cf\u9884\u5904\u7406\u4e0d\u591f\u9c81\u68d2\uff0c\u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784\u8bc6\u522b\u7b97\u6cd5\u4e0d\u591f\u5b8c\u7f8e\uff08\u53ef\u4ee5\u8003\u8651\u4f7f\u7528\u4e8c\u7ef4\u6587\u6cd5\u6765\u505a\uff09\u3002\n\u7cfb\u7edf\u8fd8\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002\n"
 },
 {
  "repo": "anandpawara/Real_Time_Image_Animation",
  "language": "Python",
  "readme_contents": "# Real time Image Animation\nThe Project is real time application in opencv using first order model\n\n# Steps to setup\n\n## Step 1: Create virtual environment\n\n**Python version** : python v3.7.3 or higher\n\n**create virual environment** : ```pip install virtualenv```\n\n**activate virtual environment** : ```virtualenv env```\n\n## Step 2: Activate virtual environment\n\n**For windows** : ```env/Script/activate```\n\n**For Linux** : ```source env/bin/activate```\n\n## Step 3 : Install required modules\n\n**Install modules** : ``` pip install -r requirements.txt ```\n\n**Install pytorch and torchvision** : ```pip install torch===1.0.0 torchvision===0.2.1 -f https://download.pytorch.org/whl/cu100/torch_stable.html ```\n\n## Step 4 : Download cascade file ,weights and model and save in folder named extract\n\n```gdown --id 1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK```\nThe file is also availible via direct link on Google's Drive:\nhttps://drive.google.com/uc?id=1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK\n\n**On Linux machine** : ```unzip checkpoints.zip```\n\nIf on windows platfrom unzip checkpoints.zip using unzipping software like 7zip.\n\n**Delete zip file** : ```rm checkpoints.zip```\n\n## Step 5 : Run the project\n\n**Run application from live camera** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar```\n\n**Run application from video file** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint -v path_to_video_file```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar -v .\\video_input\\test1.mp4 ```\n\n![test demo](animate.gif)\n\n### TODO:\nTkinter version\n\nNeed work on face alignments\n\nFuture plans adding deepfake voice and merging with video\n\nCredits\n=======\n```\n@InProceedings{Siarohin_2019_NeurIPS,\n  author={Siarohin, Aliaksandr and Lathuili\u00e8re, St\u00e9phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},\n  title={First Order Motion Model for Image Animation},\n  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},\n  month = {December},\n  year = {2019},\n  url = {https://github.com/AliaksandrSiarohin/first-order-model}\n}\n```\n- Original Project\n    * [AliaksandrSiarohin](https://github.com/AliaksandrSiarohin/first-order-model)\n\n    If you like this project give your support to original author of this project by giving github star to author's project\n\n- video explanation to the project <br/>\n    * [Video explanation by original author](https://www.youtube.com/watch?v=u-0cQ-grXBQ)\n    * [Two min papers](https://www.youtube.com/watch?v=mUfJOQKdtAk)    \n\n- try project on google colab\n    * [youtube link](https://www.youtube.com/watch?v=RsOJJd1q6Bg&feature=youtu.be)\n    * [link to colab version](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)\n\n    For any valueable feedback feel free to contact me on [linkedin](https://www.linkedin.com/in/anand-pawara-8045/)\n\n"
 },
 {
  "repo": "shimat/opencvsharp",
  "language": "C#",
  "readme_contents": "# OpenCvSharp [![CircleCI Status](https://circleci.com/gh/shimat/opencvsharp/tree/master.svg?style=svg)](https://circleci.com/gh/shimat/opencvsharp/tree/master) [![Appveyor Build status](https://ci.appveyor.com/api/projects/status/dvjexft02s6b3re6/branch/master?svg=true)](https://ci.appveyor.com/project/shimat/opencvsharp/branch/master) [![Github Actions Ubuntu Status](https://github.com/shimat/opencvsharp/workflows/Ubuntu%2018.04/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![Github Actions MacOS Status](https://github.com/shimat/opencvsharp/workflows/macOS%2010.15/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![GitHub license](https://img.shields.io/github/license/shimat/opencvsharp.svg)](https://github.com/shimat/opencvsharp/blob/master/LICENSE) \n\nWrapper of OpenCV for .NET\n\nOld versions of OpenCvSharp are stored in [opencvsharp_2410](https://github.com/shimat/opencvsharp_2410).\n\n## NuGet\n\n| Package | Description | Link |\n|---------|-------------|------|\n|**OpenCvSharp4**| OpenCvSharp core libraries | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.svg)](https://badge.fury.io/nu/OpenCvSharp4) |\n|**OpenCvSharp4.WpfExtensions**| WPF Extensions | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions.svg)](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions) |\n|**OpenCvSharp4.Windows**| All-in-one package for Windows (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.Windows.svg)](https://badge.fury.io/nu/OpenCvSharp4.Windows) |\n|**OpenCvSharp4.runtime.win**| Native bindings for Windows x64/x86 (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.win.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.win) |\n|**OpenCvSharp4.runtime.uwp**| Native bindings for UWP (Universal Windows Platform) x64/x86/ARM | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp) |\n|**OpenCvSharp4.runtime.ubuntu.18.04-x64**| Native bindings for Ubuntu 18.04 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64) |\n|**OpenCvSharp4.runtime.osx.10.15-x64**| Native bindings for macOS 10.15 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.osx.10.15-x64.svg)](https://www.nuget.org/packages/OpenCvSharp4.runtime.osx.10.15-x64/) |\n|(beta packages)| Development Build Package    | https://ci.appveyor.com/nuget/opencvsharp |\n\nNative binding (OpenCvSharpExtern.dll / libOpenCvSharpExtern.so) is required to work OpenCvSharp. To use OpenCvSharp, you should add both `OpenCvSharp4` and `OpenCvSharp4.runtime.*` packages to your project. Currently, native bindings for Windows, UWP, Ubuntu 18.04 and macOS are released.\n\nPackages named OpenCvSharp3-* and OpenCvSharp-* are deprecated.\n> [OpenCvSharp3-AnyCPU](https://www.nuget.org/packages/OpenCvSharp3-AnyCPU/) / [OpenCvSharp3-WithoutDll](https://www.nuget.org/packages/OpenCvSharp3-WithoutDll/) / [OpenCvSharp-AnyCPU](https://www.nuget.org/packages/OpenCvSharp-AnyCPU/) /  [OpenCvSharp-WithoutDll](https://www.nuget.org/packages/OpenCvSharp-WithoutDll/)\n\n## Docker images\nhttps://hub.docker.com/u/shimat\n- [shimat/ubuntu18-dotnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/ubuntu18-dotnetcore3.1-opencv4.5.0)\n- [shimat/appengine-aspnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/appengine-aspnetcore3.1-opencv4.5.0)\n\n\n## Installation\n\n### Windows (except UWP)\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.win` NuGet packages to your project. You can use `OpenCvSharp4.Windows` instead.\n\n### UWP\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.uwp` NuGet packages to your project. Note that `OpenCvSharp4.runtime.win` and `OpenCvSharp4.Windows` don't work for UWP. \n\n### Ubuntu 18.04\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.ubuntu.18.04.x64` NuGet packages to your project.\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\ndotnet add package OpenCvSharp4.runtime.ubuntu.18.04-x64\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Google AppEngine Flexible (Ubuntu 16.04)\nSome Docker images are provided to use OpenCvSharp with AppEngine Flexible. The native binding (libOpenCvSharpExtern) is already built in the docker image and you don't need to worry about it.\n```\nFROM shimat/appengine-aspnetcore3.1-opencv4.5.0:20201030\n\nADD ./ /app \nENV ASPNETCORE_URLS=http://*:${PORT} \n\nWORKDIR /app \nENTRYPOINT [ \"dotnet\", \"YourAspNetCoreProject.dll\" ]\n```\n\n### Ubuntu 18.04 Docker image\nYou can use the `shimat/ubuntu18-dotnetcore3.1-opencv4.5.0` docker image.\nThis issue may be helpful: https://github.com/shimat/opencvsharp/issues/920\n\n### Downloads\nIf you do not use NuGet, get DLL files from the [release page](https://github.com/shimat/opencvsharp/releases).\n\n## Target OpenCV\n* [OpenCV 4.5.0](http://opencv.org/) with [opencv_contrib](https://github.com/opencv/opencv_contrib)\n\n## Requirements\n* [.NET Framework 4.6.1](http://www.microsoft.com/ja-jp/download/details.aspx?id=1639) / [.NET Core 2.0](https://www.microsoft.com/net/download) / [Mono](http://www.mono-project.com/Main_Page)\n* (Windows) [Visual C++ 2019 Redistributable Package](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\n* (Windows Server) Media Foundation\n```\nPS1> Install-WindowsFeature Server-Media-Foundation\n```\n* (Ubuntu, Mac) You must pre-install all the dependency packages needed to build OpenCV. Many packages such as libjpeg must be installed in order to work OpenCV. \nhttps://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n\n\n**OpenCvSharp won't work on Unity and Xamarin platform.** For Unity, please consider using [OpenCV for Unity](https://www.assetstore.unity3d.com/en/#!/content/21088) or some other solutions.\n\n**OpenCvSharp does not support CUDA.** If you want to use the CUDA features, you need to customize the native bindings yourself.\n\n## Usage\nFor more details, see **[samples](https://github.com/shimat/opencvsharp_samples/)** and **[Wiki](https://github.com/shimat/opencvsharp/wiki)** pages.\n\n**Always remember to release Mat instances! The `using` syntax is useful.**\n```C#\n// C# 8\n// Edge detection by Canny algorithm\nusing OpenCvSharp;\n\nclass Program \n{\n    static void Main() \n    {\n        using var src = new Mat(\"lenna.png\", ImreadModes.Grayscale);\n \u00a0 \u00a0 \u00a0 \u00a0using var dst = new Mat();\n        \n        Cv2.Canny(src, dst, 50, 200);\n        using (new Window(\"src image\", src)) \n        using (new Window(\"dst image\", dst)) \n        {\n            Cv2.WaitKey();\n        }\n    }\n}\n```\n\n## Features\n* OpenCvSharp is modeled on the native OpenCV C/C++ API style as much as possible.\n* Many classes of OpenCvSharp implement IDisposable. There is no need to manage unsafe resources. \n* OpenCvSharp does not force object-oriented programming style on you. You can also call native-style OpenCV functions.\n* OpenCvSharp provides functions for converting from `Mat` into `Bitmap`(GDI+) or `WriteableBitmap`(WPF).\n\n## Code samples\nhttps://github.com/shimat/opencvsharp_samples/\n\n## Documents\nhttps://shimat.github.io/opencvsharp_docs/index.html\n\n## OpenCvSharp Build Instructions\n### Windows\n- Install Visual Studio 2019 or later\n  - VC++ features are required.\n- Run `download_opencv_windows.ps1` to download OpenCV libs and headers from https://github.com/shimat/opencv_files. Those lib files are precompiled by the owner of OpenCvSharp using AppVeyor CI.\n```\n.\\download_opencv_windows.ps1\n```\n- Build OpenCvSharp\n  - Open `OpenCvSharp.sln` and build\n  \n#### How to customize OpenCV binaries yourself\nIf you want to use some OpenCV features that are not provided by default in OpenCvSharp (e.g. GPU), you will have to build OpenCV yourself. The binary files of OpenCV for OpenCvSharp for Windows are created in the [opencv_files](https://github.com/shimat/opencv_files) repository. See the README.\n\n- `git clone --recursive https://github.com/shimat/opencv_files`\n- Edit `build_windows.ps1` or `build_uwp.ps1` to customize the CMake parameters .\n- Run the PowerShell script.\n\n### Ubuntu 18.04\n- Build OpenCV with opencv_contrib. \n  - https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n- Install .NET Core SDK. https://docs.microsoft.com/ja-jp/dotnet/core/install/linux-package-manager-ubuntu-1804\n- Get OpenCvSharp source files\n```\ngit clone https://github.com/shimat/opencvsharp.git\ncd opencvsharp\ngit fetch --all --tags --prune && git checkout ${OPENCVSHARP_VERSION}\n```\n\n- Build native wrapper `OpenCvSharpExtern`\n```\ncd opencvsharp/src\nmkdir build\ncd build\ncmake -D CMAKE_INSTALL_PREFIX=${YOUR_OPENCV_INSTALL_PATH} ..\nmake -j \nmake install\n```\nYou should add reference to `opencvsharp/src/build/OpenCvSharpExtern/libOpenCvSharpExtern.so`\n```\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/home/shimat/opencvsharp/src/build/OpenCvSharpExtern\"\n```\n\n- Add `OpenCvSharp4` NuGet package to your project\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Older Ubuntu\nRefer to the [Dockerfile](https://github.com/shimat/opencvsharp/blob/master/docker/google-appengine-ubuntu.16.04-x64/Dockerfile) and [Wiki pages](https://github.com/shimat/opencvsharp/wiki).\n\n## Donations\nIf you find the OpenCvSharp library useful and would like to show your gratitude by donating, here are some donation options. Thank you.\n\nhttps://github.com/sponsors/shimat\n"
 },
 {
  "repo": "changwookjun/StudyBook",
  "language": null,
  "readme_contents": "# Study E-Book(ComputerVision DeepLearning MachineLearning Math NLP Python ReinforcementLearning)\n\nContents  \n* [Computer Vision Books](https://github.com/changwookjun/StudyBook/tree/master/ComputerVisionBooks)   \n  + [Machine Learning for OpenCV.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Machine%20Learning%20for%20OpenCV.pdf)   \n  + [Computer Vision- Algorithms and Applications.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Computer%20Vision-%20Algorithms%20and%20Applications.pdf)   \n* [Deep Learning Books](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks)   \n  + [Deep Learning - Josh Patterson & Adam Gibson.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20-%20Josh%20Patterson%20%26%20Adam%20Gibson.pdf)   \n  + [Deep Learning with Python A Hands-on Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20with%20Python%20A%20Hands-on%20Introduction.pdf)   \n  + [Fundamentals of Deep Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Fundamentals%20of%20Deep%20Learning.pdf)   \n  + [Introduction to Deep Learning Using R.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Introduction%20to%20Deep%20Learning%20Using%20R.pdf)   \n  + [Learning TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Learning%20TensorFlow.pdf)   \n  + [deeplearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning.pdf)   \n  + [deeplearningbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook.pdf)   \n  + [deeplearningbook_bookmarked.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook_bookmarked.pdf)   \n  + [oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf)   \n  + [CS 20_Tensorflow for Deep Learning Research](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research)     \n      - [01 _ Lecture slide _ Overview of Tensorflow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/01%20_%20Lecture%20slide%20_%20Overview%20of%20Tensorflow.pdf)     \n      - [02_Lecture slide_TensorFlow Operations.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/02_Lecture%20slide_TensorFlow%20Operations.pdf)     \n      - [03 _ Lecture slide _ Basic Models in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/03%20_%20Lecture%20slide%20_%20Basic%20Models%20in%20TensorFlow.pdf)     \n      - [04 Eager Execution + word2vec.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/04%20Eager%20Execution%20%2B%20word2vec.pdf)     \n      - [05_Slide_Managing your experiment.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/05_Slide_Managing%20your%20experiment.pdf)     \n      - [06_Introduction to Computer Vision and convolutional network.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/06_Introduction%20to%20Computer%20Vision%20and%20convolutional%20network.pdf)     \n      - [07 _ Covnets in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/07%20_%20Covnets%20in%20TensorFlow.pdf)     \n      - [08_Style transfer.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/08_Style%20transfer.pdf)     \n      - [10_Lecture_Slides_VAE in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/10_Lecture_Slides_VAE%20in%20TensorFlow.pdf)     \n      - [11 _ Slides _ Introduction to RNNs.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/11%20_%20Slides%20_%20Introduction%20to%20RNNs.pdf)     \n      - [12_Slides_Machine Translation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/12_Slides_Machine%20Translation.pdf)    \n      - [14_Slides_A TensorFlow Chatbot.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/14_Slides_A%20TensorFlow%20Chatbot.pdf)    \n      - [16_Slides_Tensor2Tensor.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/16_Slides_Tensor2Tensor.pdf)    \n      - [CS20_intro_to_RL.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/CS20_intro_to_RL.pdf)   \n      - [march9guestlecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/march9guestlecture.pdf)    \n  + [DeepLearning_chapter-wise-pdf](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf)     \n      - [table-of-contents.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B1%5Dtable-of-contents.pdf)  \n      - [acknowledgements.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B2%5Dacknowledgements.pdf)  \n      - [notation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B3%5Dnotation.pdf)  \n      - [chapter-1-introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B4%5Dchapter-1-introduction.pdf)  \n      - [part-1-basics.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B5%5Dpart-1-basics.pdf)  \n      - [part-1-chapter-2.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B6%5Dpart-1-chapter-2.pdf)  \n      - [part-1-chapter-3.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B7%5Dpart-1-chapter-3.pdf)  \n      - [part-1-chapter-4.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B8%5Dpart-1-chapter-4.pdf)  \n      - [part-1-chapter-5.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B9%5Dpart-1-chapter-5.pdf)  \n      - [part-2-deep-network-modern-practices.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B10%5Dpart-2-deep-network-modern-practices.pdf)  \n      - [part-2-chapter-6.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B11%5Dpart-2-chapter-6.pdf)  \n      - [part-2-chapter-7.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B12%5Dpart-2-chapter-7.pdf)  \n      - [part-2-chapter-8.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B13%5Dpart-2-chapter-8.pdf)  \n      - [part-2-chapter-9.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B14%5Dpart-2-chapter-9.pdf)        \n      - [part-2-chapter-10.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B15%5Dpart-2-chapter-10.pdf)        \n      - [part-2-chapter-11.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B16%5Dpart-2-chapter-11.pdf)        \n      - [part-2-chapter-12.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B17%5Dpart-2-chapter-12.pdf)  \n      - [part-3-deep-learning-research.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B18%5Dpart-3-deep-learning-research.pdf) \n      - [part-3-chapter-13.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B19%5Dpart-3-chapter-13.pdf) \n      - [part-3-chapter-14.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B20%5Dpart-3-chapter-14.pdf) \n      - [part-3-chapter-15.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B21%5Dpart-3-chapter-15.pdf) \n      - [part-3-chapter-16.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B22%5Dpart-3-chapter-16.pdf) \n      - [part-3-chapter-17.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B23%5Dpart-3-chapter-17.pdf) \n      - [part-3-chapter-18.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B24%5Dpart-3-chapter-18.pdf) \n      - [part-3-chapter-19.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B25%5Dpart-3-chapter-19.pdf) \n      - [part-3-chapter-20.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B26%5Dpart-3-chapter-20.pdf) \n      - [bibliography.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B27%5Dbibliography.pdf) \n      - [index.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B28%5Dindex.pdf) \n  + [d2l-en.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/d2l-en.pdf) \n  + [Dive into DeepLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Dive_into_Deep_Learning.pdf)   \n  + [ee559 Deep learning](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/ee559-Deeplearning)     \n  + [Hands-on-Machine-Learning-with-Scikit-2E.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Hands-on-Machine-Learning-with-Scikit-2E.pdf)  \n  + [deeplearning_2019_spring.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning_2019_spring.pdf)    \n  + [Deep-Learning-with-PyTorch.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep-Learning-with-PyTorch.pdf)  \n\n* [Machine Learning Books](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks)      \n  + [30_03_atelierdatamining.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/30_03_atelierdatamining.pdf)  \n  + [Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)  \n  + [Building Machine Learning Systems with Python, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Building%20Machine%20Learning%20Systems%20with%20Python%2C%202nd%20Edition.pdf)   \n  + [MATLAB Machine Learning by Michael Paluszek.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/MATLAB%20Machine%20Learning%20by%20Michael%20Paluszek.pdf)  \n  + [Machine Learning in Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine%20Learning%20in%20Python.pdf)  \n  + [Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine_Learning.pdf)  \n  + [Mastering Feature Engineering.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Feature%20Engineering.pdf)  \n  + [Mastering Machine Learning with scikit-learn, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Machine%20Learning%20with%20scikit-learn%2C%202nd%20Edition.pdf)  \n  + [NG_MLY.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Ng_MLY.pdf) \n  + [Practical Machine Learning A New Look at Anomaly Detection.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Practical%20Machine%20Learning%20A%20New%20Look%20at%20Anomaly%20Detection.pdf) \n  + [Practical Machine Learning with H2O.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Data%20Analytics.pdf) \n  + [Python Real World Machine Learning - Prateek Joshi.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Real%20World%20Machine%20Learning%20-%20Prateek%20Joshi.pdf) \n  + [Gaussian Processes for Machine Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/RW.pdf) \n  + [The Elements of Statistical Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/ESLII_print12.pdf) \n  + [Foundations of Data Science.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science.pdf) \n  + [cs229-cheatsheet](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks/cs229-cheatsheet) \n  + [Automatic_Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Automatic_Machine_Learning.pdf) \n  + [DataScienceHandbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/DataScienceHandbook.pdf) \n  + [Python Data Science Handbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/pythondatasciencehandbook.pdf) \n  + [Foundations of Data Science(Microsoft).pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science(Microsoft).pdf) \n  + [Bayesian_Data_Analysis_Third_edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bayesian_Data_Analysis_Third_edition.pdf)  \n  + [Joseph K. Blitzstein, Jessica Hwang-Introduction to Probability.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Joseph%20K.%20Blitzstein%2C%20Jessica%20Hwang-Introduction%20to%20Probability.pdf)   \n  \n* [Math Books](https://github.com/changwookjun/StudyBook/tree/master/MathBooks)     \n  + [MIT18_657F15_LecNote.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/MIT18_657F15_LecNote.pdf) \n  + [Mathematics for Machine Learnin.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Mathematics%20for%20Machine%20Learnin.pdf) \n  + [mathandcomp.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mathandcomp.pdf) \n  + [Introduction to Applied Linear Algebra.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Introduction%20to%20Applied%20Linear%20Algebra.pdf) \n  + [matrixcookbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/matrixcookbook.pdf) \n  + [mml-book](https://github.com/changwookjun/StudyBook/tree/master/MathBooks/mml-book)   \n  + [MATHEMATICS FOR MACHINE LEARNING.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mml-book.pdf)  \n  + [LINEAR ALGEBRA.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/LINEAR%20ALGEBRA.pdf)  \n\n* [NLP Books](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks)     \n  + [Applied Text Analysis with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Applied%20Text%20Analysis%20with%20Python.pdf) \n  + [Natural Language Processing with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Natural%20Language%20Processing%20with%20Python.pdf) \n  + [Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from your Data.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Text%20Analytics%20with%20Python%20A%20Practical%20Real-World%20Approach%20to%20Gaining%20Actionable%20Insights%20from%20your%20Data.pdf) \n  + [The Text Mining HandBook.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/The%20Text%20Mining%20HandBook.pdf) \n  + [eisenstein-nlp-notes.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/eisenstein-nlp-notes.pdf)\n  + [oxford-cs-deepnlp-2017](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks/oxford-cs-deepnlp-2017)\n    - [Lecture 1a - Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201a%20-%20Introduction.pdf)    \n    - [Lecture 1b - Deep Neural Networks Are Our Friends.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201b%20-%20Deep%20Neural%20Networks%20Are%20Our%20Friends.pdf)   \n    - [Lecture 2a- Word Level Semantics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202a-%20Word%20Level%20Semantics.pdf) \n    - [Lecture 2b - Overview of the Practicals.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202b%20-%20Overview%20of%20the%20Practicals.pdf) \n    - [Lecture 3 - Language Modelling and RNNs Part 1.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%203%20-%20Language%20Modelling%20and%20RNNs%20Part%201.pdf) \n    - [Lecture 4 - Language Modelling and RNNs Part 2.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%204%20-%20Language%20Modelling%20and%20RNNs%20Part%202.pdf)   \n    - [Lecture 5 - Text Classification.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%205%20-%20Text%20Classification.pdf)   \n    - [Lecture 6 - Nvidia RNNs and GPUs.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%206%20-%20Nvidia%20RNNs%20and%20GPUs.pdf)   \n    - [Lecture 7 - Conditional Language Modeling.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%207%20-%20Conditional%20Language%20Modeling.pdf)   \n    - [Lecture 8 - Conditional Language Modeling with Attention.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)   \n    - [Lecture 9 - Speech Recognition.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%209%20-%20Speech%20Recognition.pdf)   \n    - [Lecture 10 - Text to Speech.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2010%20-%20Text%20to%20Speech.pdf)   \n    - [Lecture 11 - Question Answering.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2011%20-%20Question%20Answering.pdf)   \n    - [Lecture 12- Memory Lecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2012-%20Memory%20Lecture.pdf)    \n    - [Lecture 13 - Linguistics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2013%20-%20Linguistics.pdf)      \n  + [Speech and Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Speech_and_Language_Processing.pdf) \n  + [Embeddings in Natural Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Embeddings%20in%20Natural%20Language%20Processing.pdf)  \n\n\n* [Python Books](https://github.com/changwookjun/StudyBook/tree/master/PythonBooks)   \n  + [Learn Python The Hard Way 3rd Edition free pdf download.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Learn%20Python%20The%20Hard%20Way%203rd%20Edition%20free%20pdf%20download.pdf)\n  + [SciPy and NumPy.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/SciPy%20and%20NumPy.pdf)\n  + [ScipyLectures-simple.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/ScipyLectures-simple.pdf)\n  + [Shaw Z.A. - Learn Python the Hard Way, 2nd Edition [2011, PDF, ENG].pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Shaw%20Z.A.%20-%20Learn%20Python%20the%20Hard%20Way%2C%202nd%20Edition%20%5B2011%2C%20PDF%2C%20ENG%5D.pdf)\n  + [Understanding GIL.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Understanding%20GIL.pdf)\n  + [scipy-ref-0.17.0.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/scipy-ref-0.17.0.pdf)\n  \n* [Reinforcement Learning Books](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks)  \n  + [RLAlgsInMDPs.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLAlgsInMDPs.pdf)\n  + [RLbook2018.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLbook2018.pdf)\n  + [Dissecting Reinforcement Learning](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning)    \n    - [Dissecting Reinforcement Learning-Part1.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part1.pdf)   \n    - [Dissecting Reinforcement Learning-Part2.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part2.pdf) \n    - [Dissecting Reinforcement Learning-Part3.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part3.pdf) \n    - [Dissecting Reinforcement Learning-Part4.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part4.pdf) \n    - [Dissecting Reinforcement Learning-Part5.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part5.pdf)   \n    - [Dissecting Reinforcement Learning-Part6.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part6.pdf)   \n    - [Dissecting Reinforcement Learning-Part7.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part7.pdf)  \n  + [AI_CRASH_COURSE.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AI_CRASH_COURSE.pdf)   \n  \n  + [UCL Course on RL d.silver](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver)\n    - [Lecture 1: Introduction to Reinforcement Learning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/intro_RL.pdf)  \n    - [Lecture 2: Markov Decision Processes](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MDP.pdf)      \n    - [Lecture 3: Planning by Dynamic Programming](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/DP.pdf)  \n    - [Lecture 4: Model-Free Prediction](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MC-TD.pdf)      \n    - [Lecture 5: Model-Free Control](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/control.pdf)      \n    - [Lecture 6: Value Function Approximation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/FA.pdf)      \n    - [Lecture 7: Policy Gradient Methods](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/pg.pdf)      \n    - [Lecture 8: Integrating Learning and Planning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/dyna.pdf)      \n    - [Lecture 9: Exploration and Exploitation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/XX.pdf)      \n    - [Lecture 10: Case Study: RL in Classic Games](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/games.pdf)  \n    - [Lecture 11: Case Study: Deep RL](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/deep_rl_tutorial.pdf)      \n    - [Video-lectures available here](https://www.youtube.com/watch?v=2pWv7GOvuf0)          \n  + [AnIntroductiontoDeepReinforcementLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AnIntroductiontoDeepReinforcementLearning.pdf)  \n\n# Author\nChangWookJun / @changwookjun (changwookjun@gmail.com)\n"
 },
 {
  "repo": "oreillymedia/Learning-OpenCV-3_examples",
  "language": "C++",
  "readme_contents": "# Learning OpenCV 3\n\n## INTRO\n\nThis is the example code that accompanies Learning OpenCV 3 by Adrian Kaehler and Gary Bradski ([9781491937990](http:*shop.oreilly.com/product/0636920044765.do)).\n\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n\n\n## NOTES\n\nFor default suggestions of how the run the code, it assumes you put your build directory under `Learning-OpenCV-3_examples` directory. \n\nThus, from the `Learning-OpenCV-3_examples` directory:\n\n```\t\n  mkdir build\n  cd build\n  cmake ..\n  make -j\n```\n\n#### Docker\nFor your interest, included here is an Ubuntu _Docker_ file that\n* Shares a directory with the host operating system\n* Shares the first camera between both systems\n* Loads Ubuntu 16.04 \n* Loads all dependencies for OpenCV 3.2 and opencv_contrib\n* Loads and builds OpenCV 3.2 and opencv_contrib into a build directory \n  * executable files end up in `opencv-3.2.0/build/bin`\n* Next, it `git clones` the code (and Docker file) for Learning OpenCV 3 and builds it\n  * executable files end up in `Learning_OpenCV-3_examples/build`\n* To get to the top level directory, just type: `cd`\n\n\n## CONTENTS:\n\n### SPECIAL FILES:\n\n* README.md       -- this readme file\n* Dockerfile      -- complete self contained opencv environment using Ubuntu 16-04\n* CMakeLists.txt  -- how to buld everything here \n\n### EXERCISES:\n\n* Exercises at end of Chapter 5\n* Exercises at end of Chapter 7\n* Exercises_8_1.cpp Exercises at end of Chapter 8\n* Exercises_9_1-2-10-11-12-15-16.cpp Exercises at end of Chapter 8\n* Exercises_9_4.cpp Exercises at end of Chapter 9\n* Exercises_9_5.cpp Exercises at end of Chapter 9\n* Exercises at end of Chapter 11\n* Exercises_13_1-2-11.cpp\tExercises for Chapter 13\n* Exercises_13_9.cpp\t\n\n### EXAMPLES:\n\n* Example 2-1. A simple OpenCV program that loads an image from disk and displays it\n* Example 2-2. Same as Example 2-1 but employing the \u201cusing namespace\u201d directive\n* Example 2-3. A simple OpenCV program for playing a video file from disk\n* Example 2-4. Adding a trackbar slider to the basic viewer window for moving around\n* Example 2-5. Loading and then smoothing an image before it is displayed on the screen\n* Example 2-6. Using cv::pyrDown() to create a new image that is half the width and\n* Example 2-7. The Canny edge detector writes its output to a single-channel (grayscale) image\n* Example 2-8. Combining the pyramid down operator (twice) and the Canny\n* Example 2-9. Getting and setting pixels in Example 2-8\n* Example 2-10. The same object can load videos from a camera or a file\n* Example 2-11. A complete program to read in a color video and write out the log-polar-\n* Example 4-1. Summation of a multidimensional array, done plane by plane\n* Example 4-2. Summation of two arrays using the N-ary operator\n* Example 4-3. Printing all of the nonzero elements of a sparse array\n* Example 4-4. A better way to print a matrix\n* Example 5-1. Complete program to alpha-blend the ROI starting at (0,0) in src2 with the ROI starting at (x,y) in src1\n* Example 7-1. Using the default random number generator to generate a pair of integers\n* Example 8-1. Unpacking a four-character code to identify a video codec\n* Example 8-2. Using cv::FileStorage to create a .yml data file\n* Example 8-3. Using cv::FileStorage to read a .yml file\n* Example 9-1. Creating a window and displaying an image in that window\n* Example 9-2. Toy program for using a mouse to draw boxes on the screen\n* Example 9-3. Using a trackbar to create a \u201cswitch\u201d tha t the user can turn on and off;\n* Example 9-4. Slightly modified code from the OpenCV documentation that draws a\n* Example 9-5. An example program ch4_qt.cpp, which takes a single argument\n* Example 9-6. The QMoviePlayer object header file QMoviePlayer.hpp\n* Example 9-7. The QMoviePlayer object source file: QMoviePlayer.cpp\n* Example 9-8. An example program which takes a single argument\n* Example 9-9. The WxMoviePlayer object header file WxMoviePlayer.hpp\n* Example 9-10. The WxMoviePlayer object source file WxMoviePlayer.cpp\n* Example 9-11. An example header file for our custom View class\n* Example 10-1. Using cv::threshold() to sum three channels of an image\n* Example 10-2. Alternative method to combine and threshold image planes\n* Example 10-3. Threshold versus adaptive threshold\n* Example 11-1. An affine transformation.\n* Example 11-2. Code for perspective transformation\n* Example 11-3. Log-polar transform example\n* Example 12-1. Using cv::dft() and cv::idft() to accelerate the computation of\n* Example 12-2. Using cv::HoughCircles() to return a sequence of circles found in a\n* **EXTRA** Example 12-3. Using GrabCut for background removal\n* **EXTRA** Example 12-4. Using GrabCut for background removal\n* Example 13-1. Histogram computation and display\n* Example 13-2. Creating signatures from histograms for EMD; note that this code is the\n* Example 13-3. Template matching\n* Example 14-1. Finding contours based on a trackbar\u2019s location; the contours are\n* Example 14-2. Finding and drawing contours on an input image\n* Example 14-3. Drawing labeled connected components\n* Example 14-4. Using the shape context distance extractor\n* Example 15-1. Reading out the RGB values of all pixels in one row of a video and\n* Example 15-2. Learning a background model to identify foreground pixels\n* Example 15-3. Computing the on and off-diagonal elements of a variance/covariance model\n* Example 15-4. Codebook algorithm implementation\n* Example 15-5. Cleanup using connected components\n* **EXTRA** Example 15-6, using OpenCV's background subtractor class.  Modified by Gary Bradski, 6/4/2017\n* Example 16-1. Pyramid L-K optical flow\n* **EXTRA** Example 16-2. 2D Feature detectors and 2D Extra Features framework\n* Example 17-1. Kalman filter example code\n* Example 17-2. Farneback optical flow example code\n* Example 18-1. Reading a chessboard\u2019s width and height, reading them and calibrating \n* **EXTRA** Example 18-1. From disk. Reading a chessboard\u2019s width and height, reading them and calibrating \n* Example 19-1. Bird\u2019s - eye view\n* Example 19-2. Computing the fundamental matrix using RANSAC\n* Example 19-3. Stereo calibration, rectification, and correspondence\n* Example 19-4. Two-dimensional line fitting\n* Example 20-01. Using K-means\n* Example 20-02. Using the Mahalanobis distance for classification\n* Example 21-1. Creating and training a decision tree\n* Example 22-1. Detecting and drawing faces\n\n### IMAGES:\n\n* box.png\n* box_in_scene.png\n* checkerboard9x6.png\n* example_16-01-imgA.png\n* example_16-01-imgB.png\n* faces.png\n* BlueCup.jpg\n* HandIndoorColor.jpg\n* HandOutdoorColor.jpg\n* HandOutdoorSunColor.jpg\n* adrian.jpg\n* faceScene.jpg\n* faceTemplate.jpg\n* fruits.jpg\n* stuff.jpg\n\n### MOVIES:\n\n* test.avi\n* tree.avi\n\n### CLASSIFIERS:\n\n* haarcascade_frontalcatface.xml           #Cat faces!\n* haarcascade_frontalcatface_extended.xml\n* haarcascade_frontalface_alt.xml\n\n### DIRECTORIES:\n\n* birdseye     -- where the images are of checkerboards on the floor\n* build        -- you will make and build things in this directory\n* calibration  -- checkerboard images to calibrate on\n* muchroom     -- machine learning database\n* shape_sample -- silhoette shapes to recognize\n* stereoData   -- left, right image pairs of checkboards to calibrate and view on\n\n\n## LINKS:\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n"
 },
 {
  "repo": "kongqw/OpenCVForAndroid",
  "language": "Java",
  "readme_contents": "# OpenCV 3.2.0\n\n\u8fd0\u884c\u524d\u8bf7\u5148\u5b89\u88c5[OpenCV Manager(\u5fc5\u987b)](https://github.com/kongqw/FaceDetectLibrary/tree/opencv3.2.0/OpenCVManager)\u3002\n\n\u672c\u793a\u4f8b\u57fa\u4e8eOpenCV 3.2.0 \u7248\u672c\u5728Android\u5e73\u53f0\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u548c\u76ee\u6807\u8ffd\u8e2a\n\n## \u76ee\u6807\u68c0\u6d4b\n\n- \u4eba\u8138\u68c0\u6d4b\n- \u773c\u775b\u68c0\u6d4b\n- \u5fae\u7b11\u68c0\u6d4b\n- \u4e0a\u534a\u8eab\u68c0\u6d4b\n- \u4e0b\u534a\u8eab\u68c0\u6d4b\n- \u5168\u8eab\u68c0\u6d4b\n\n## \u76ee\u6807\u8ffd\u8e2a\n\n- CamShift\u7b97\u6cd5\u5b9e\u73b0\u76ee\u6807\u8ffd\u8e2a\n\n### \u6548\u679c\u56fe\n\n\u76ee\u6807\u68c0\u6d4b\n\n![\u76ee\u6807\u68c0\u6d4b](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectDetecting.gif)\n\n\u76ee\u6807\u8ffd\u8e2a\n\n![\u76ee\u6807\u8ffd\u8e2a](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectTracking.gif)\n\n## \u4eba\u8138\u8bc6\u522b\n\n\u4eba\u8138\u8bc6\u522b\uff08\u5bf9\u6bd4\uff09\u8bf7\u5207\u6362\u5230[master](https://github.com/kongqw/FaceDetectLibrary/tree/master)\u5206\u652f\uff0c\u57fa\u4e8eOpenCV 2.4.11\u3002\n"
 },
 {
  "repo": "opentrack/opentrack",
  "language": "C++",
  "readme_contents": "## Intro\n\n[<img src=\"https://ci.appveyor.com/api/projects/status/n0j9h38jnif5qbe9/branch/unstable?svg=true\"/>](https://ci.appveyor.com/project/sthalik/opentrack/branch/unstable)\n\nopentrack project home is located at <<http://github.com/opentrack/opentrack>>.\n\nFor the latest **downloads** visit <<https://github.com/opentrack/opentrack/releases>> Download an `.exe` installer or a `.7z` archive. Currently installers and portable versions for Windows are available for each release. It supports [USB stick truly \"portable\" installations](https://github.com/opentrack/opentrack/wiki/portable-mode-for-USB-sticks)\n\nPlease first refer to <<https://github.com/opentrack/opentrack/wiki>>\nfor [new user guide](https://github.com/opentrack/opentrack/wiki/Quick-Start-Guide-(WIP)), [frequent answers](https://github.com/opentrack/opentrack/wiki/common-issues), specific tracker/filter\ndocumentation. See also the [gameplay video](https://www.youtube.com/watch?v=XI73ul_FnBI) with opentrack set up.\n\n## Looking for railway planning software?\n\n**Railway planning software** <<http://opentrack.ch>> had the name `opentrack` first. Apologies for the long-standing naming conflict.\n\n## Usage\n\n`opentrack` is an application dedicated to tracking user's head\nmovements and relaying the information to games and flight simulation\nsoftware.\n\n`opentrack` allows for output shaping, filtering, and operating with many input and output devices and protocols; the codebase runs Microsoft Windows, Apple OSX (currently unmaintained), and GNU/Linux.\n\nDon't be afraid to submit an **issue/feature request** if you have any problems! We're a friendly bunch.\n\n## Tracking input\n\n- PointTracker by Patrick Ruoff, freetrack-like light sources\n- Oculus Rift DK1, DK2, CV, and legacy/knockoff versions (Windows only)\n- Paper [marker support](https://github.com/opentrack/opentrack/wiki/Aruco-tracker)\n  via the ArUco library <<https://github.com/opentrack/aruco>>\n- Razer Hydra\n- Relaying via UDP from a different computer\n- Relaying UDP via FreePIE-specific Android app\n- Joystick analog axes (Windows)\n- Windows Phone [tracker](https://github.com/ZanderAdam/OpenTrack.WindowsPhone/wiki) over opentrack UDP protocol\n- Arduino with custom firmware\n- Intel RealSense 3D cameras (Windows)\n- BBC micro:bit, LEGO, sensortag support via Smalltalk<sup>[(1)](https://en.wikipedia.org/wiki/Smalltalk)[(2)](https://en.wikipedia.org/wiki/Alan_Kay)</sup>\n  [S2Bot](http://www.picaxe.com/Teaching/Other-Software/Scratch-Helper-Apps/)\n- Wiimote (Windows)\n\n## Output protocols\n\n- SimConnect for newer Microsoft Flight Simulator (Windows)\n- freetrack implementation (Windows)\n- Relaying UDP to another computer\n- Virtual joystick output (Windows, Linux, OSX)\n- Wine freetrack glue protocol (Linux, OSX)\n- X-Plane plugin (Linux)\n- Tablet-like mouse output (Windows)\n- FlightGear\n- FSUIPC for Microsoft Flight Simulator 2002/2004 (Windows)\n- SteamVR through a bridge (Windows; see <<https://github.com/r57zone/OpenVR-OpenTrack>> by @r57zone)\n\n## Credits, in chronological order\n\n- Stanis\u0142aw Halik (maintainer)\n- Wim Vriend -- author of [FaceTrackNoIR](http://facetracknoir.sourceforge.net/) that served as the initial codebase for `opentrack`. While the  code was almost entirely rewritten, we still hold on to many of `FaceTrackNoIR`'s ideas.\n- Chris Thompson (aka mm0zct, Rift and Razer Hydra author and maintainer)\n- Patrick Ruoff (PT tracker author)\n- Xavier Hallade (Intel RealSense tracker author and maintainer)\n- furax49 (hatire tracker author)\n- Michael Welter (contributor)\n- Alexander Orokhovatskiy (Russian translation; profile repository maintenance; providing hardware; translating reports from the Russian community)\n- Attila Csipa (Micro:Bit author)\n- Eike \"e4z9\" (OSX joystick output driver)\n- Wei Shuai (Wiimote tracker)\n- St\u00e9phane Lenclud (Kinect Face Tracker, Easy Tracker)\n\n## Thanks\n\n- uglyDwarf (high CON)\n- Andrzej Czarnowski (FreePIE tracker and\n  [Google Cardboard](https://github.com/opentrack/opentrack/wiki/VR-HMD-goggles-setup-----google-cardboard,-colorcross,-opendive)\n  assistance, testing)\n- Wim Vriend (original codebase author and maintainer)\n- Ryan Spicer (OSX tester, contributor)\n- Ries van Twisk (OSX tester, OSX Build Fixes, contributor)\n- Donovan Baarda (filtering/control theory expert)\n- Mathijs Groothuis (@MathijsG, dozens of bugs and other issues reported; NL translation)\n- The Russian community from the [IL-2 Sturmovik forums](https://forum.il2sturmovik.ru/) (reporting bugs, requesting important features)\n- OpenCV authors and maintainers <<https://github.com/opencv/opencv/>>.\n\n## Contributing\n\nCode, translations, \n\nPlease see [basic rules for contributing](https://github.com/opentrack/opentrack/blob/unstable/CONTRIBUTING.md). There's also a guide for [working with core code](https://github.com/opentrack/opentrack/wiki/Hacking-opentrack). For writing input and output modules you don't need this guide except for \n\n## License and warranty\n\nAlmost all code is licensed under the [ISC license](https://en.wikipedia.org/wiki/ISC_license). There are very few proprietary dependencies. There is no copyleft code. See individual files for licensing and authorship information.\n\nSee [WARRANTY.txt](WARRANTY.txt) for applying warranty terms (that is, disclaiming possible pre-existing warranty) that are in force unless the software author specifies their own warranty terms. In short, we disclaim all possible warranty and aren't responsible for any possible damage or losses.\n\nThe code is held to a high-quality standard and written with utmost care; consider this a promise without legal value. Despite doing the best we can not to injure users' equipment, software developers don't want to be dragged to courts for imagined or real issues. Disclaiming warranty is a standard practice in the field, even for expensive software like operating systems.\n\n## Building opentrack from source\n\nOn Windows, use either mingw-w64 or MS Visual Studio 2015 Update 3/newer. On other platforms use GNU or LLVM. Refer to [Visual C++ 2015 build instructions](https://github.com/opentrack/opentrack/wiki/Building-under-MS-Visual-C---2017-and-later).\n"
 },
 {
  "repo": "nagadomi/lbpcascade_animeface",
  "language": null,
  "readme_contents": "# lbpcascade_animeface\n\nThe face detector for anime/manga using OpenCV.\n\nOriginal release since 2011 at [OpenCV\u306b\u3088\u308b\u30a2\u30cb\u30e1\u9854\u691c\u51fa\u306a\u3089lbpcascade_animeface.xml](http://ultraist.hatenablog.com/entry/20110718/1310965532) (in Japanese)\n\n## Usage\n\nDownload and place the cascade file into your project directory.\n\n    wget https://raw.githubusercontent.com/nagadomi/lbpcascade_animeface/master/lbpcascade_animeface.xml\n\n### Python Example\n\n```python\nimport cv2\nimport sys\nimport os.path\n\ndef detect(filename, cascade_file = \"../lbpcascade_animeface.xml\"):\n    if not os.path.isfile(cascade_file):\n        raise RuntimeError(\"%s: not found\" % cascade_file)\n\n    cascade = cv2.CascadeClassifier(cascade_file)\n    image = cv2.imread(filename, cv2.IMREAD_COLOR)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.equalizeHist(gray)\n    \n    faces = cascade.detectMultiScale(gray,\n                                     # detector options\n                                     scaleFactor = 1.1,\n                                     minNeighbors = 5,\n                                     minSize = (24, 24))\n    for (x, y, w, h) in faces:\n        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n    cv2.imshow(\"AnimeFaceDetect\", image)\n    cv2.waitKey(0)\n    cv2.imwrite(\"out.png\", image)\n\nif len(sys.argv) != 2:\n    sys.stderr.write(\"usage: detect.py <filename>\\n\")\n    sys.exit(-1)\n    \ndetect(sys.argv[1])\n```\nRun\n\n    python detect.py imas.jpg\n\n![result](https://user-images.githubusercontent.com/287255/43184241-ed3f1af8-9022-11e8-8800-468b002c73d9.png)\n\n## Note\nI am providing similar project at https://github.com/nagadomi/animeface-2009. animeface-2009 is my original work that was made before libcascade_animeface. The detection accuracy is higher than this project. However, installation of that is a bit complicated. Also I am providing a face cropping script using animeface-2009.\n"
 },
 {
  "repo": "andrewssobral/bgslibrary",
  "language": "C++",
  "readme_contents": "## BGSLibrary\nA Background Subtraction Library\n\n[![Release](https://img.shields.io/badge/Release-3.0.0-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![License: GPL v3](https://img.shields.io/badge/License-MIT-blue.svg)](http://www.gnu.org/licenses/gpl-3.0) [![Platform: Windows, Linux, OS X](https://img.shields.io/badge/Platform-Windows%2C%20Linux%2C%20OS%20X-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![OpenCV](https://img.shields.io/badge/OpenCV-2.4.x%2C%203.x%2C%204.x-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Wrapper: Python, MATLAB](https://img.shields.io/badge/Wrapper-Java%2C%20Python%2C%20MATLAB-orange.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Algorithms](https://img.shields.io/badge/Algorithms-43-red.svg)](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n\n<p align=\"center\">\n<a href=\"https://youtu.be/_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgs_giphy2.gif\" border=\"0\" />\n</a>\n</p>\n\nLast page update: **06/08/2019**\n\nLibrary Version: **3.0.0** (see **[Build Status](https://github.com/andrewssobral/bgslibrary/wiki/Build-status)** and **[Release Notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)** for more info)\n\nThe **BGSLibrary** was developed early 2012 by [Andrews Sobral](http://andrewssobral.wixsite.com/home) to provide an easy-to-use C++ framework (wrappers for Python, Java and MATLAB are also available) for foreground-background separation in videos based on [OpenCV](http://www.opencv.org/). The bgslibrary is compatible with OpenCV 2.4.x, 3.x and 4.x, and compiles under Windows, Linux, and Mac OS X. Currently the library contains **43** algorithms. The source code is available under the [MIT license](https://opensource.org/licenses/MIT), the library is available free of charge to all users, academic and commercial.\n\n* [List of available algorithms](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n* [Algorithms benchmark](https://github.com/andrewssobral/bgslibrary/wiki/Algorithms-benchmark)\n* [Which algorithms really matter?](https://github.com/andrewssobral/bgslibrary/wiki/Which-algorithms-really-matter%3F)\n* [Library architecture](https://github.com/andrewssobral/bgslibrary/wiki/Library-architecture)\n\n* Installation instructions\n\nYou can either install BGSLibrary via [pre-built binary package](https://github.com/andrewssobral/bgslibrary/releases) or build it from source via:\n\n`git clone --recursive https://github.com/andrewssobral/bgslibrary.git`\n\n* * [Windows installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions---Windows)\n\n* * [Ubuntu / OS X installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions-Ubuntu-or-OSX)\n\nSupported Compilers are:\n\n    GCC 4.8 and above\n    Clang 3.4 and above\n    MSVC 2015, 2017, 2019\n\nOther compilers might work, but are not officially supported.\nThe bgslibrary requires some features from the ISO C++ 2014 standard.\n\n* Graphical User Interface:\n\n*  * [C++ QT](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-QT) ***(Official)***\n*  * [C++ MFC](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-MFC) ***(Deprecated)***\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-Java) ***(Obsolete)***\n\n* Wrappers:\n\n*  * [Python](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Python)\n*  * [MATLAB](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-MATLAB)\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Java)\n\n* [Docker images](https://github.com/andrewssobral/bgslibrary/wiki/Docker-images)\n* [How to integrate BGSLibrary in your own CPP code](https://github.com/andrewssobral/bgslibrary/wiki/How-to-integrate-BGSLibrary-in-your-own-CPP-code)\n* [How to contribute](https://github.com/andrewssobral/bgslibrary/wiki/How-to-contribute)\n* [List of collaborators](https://github.com/andrewssobral/bgslibrary/wiki/List-of-collaborators)\n* [Release notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)\n\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/andrewssobral/bgslibrary.svg)](https://starchart.cc/andrewssobral/bgslibrary)\n\n\nCitation\n--------\n\nIf you use this library for your publications, please cite it as:\n```\n@inproceedings{bgslibrary,\nauthor    = {Sobral, Andrews},\ntitle     = {{BGSLibrary}: An OpenCV C++ Background Subtraction Library},\nbooktitle = {IX Workshop de Vis\u00e3o Computacional (WVC'2013)},\naddress   = {Rio de Janeiro, Brazil},\nyear      = {2013},\nmonth     = {Jun},\nurl       = {https://github.com/andrewssobral/bgslibrary}\n}\n```\nA chapter about the BGSLibrary has been published in the handbook on [Background Modeling and Foreground Detection for Video Surveillance](https://sites.google.com/site/backgroundmodeling/).\n```\n@incollection{bgslibrarychapter,\nauthor    = {Sobral, Andrews and Bouwmans, Thierry},\ntitle     = {BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation},\nbooktitle = {Background Modeling and Foreground Detection for Video Surveillance},\npublisher = {CRC Press, Taylor and Francis Group.}\nyear      = {2014},\n}\n```\n\n\nDownload PDF:\n* Sobral, Andrews. BGSLibrary: An OpenCV C++ Background Subtraction Library. IX Workshop de Vis\u00e3o Computacional (WVC'2013), Rio de Janeiro, Brazil, Jun. 2013. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in brazilian-portuguese containing an english abstract).\n\n* Sobral, Andrews; Bouwmans, Thierry. \"BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation\". Chapter on the handbook \"Background Modeling and Foreground Detection for Video Surveillance\", CRC Press, Taylor and Francis Group, 2014. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in english).\n\n\nSome references\n---------------\n\nSome algorithms of the BGSLibrary were used successfully in the following papers: \n\n* (2014) Sobral, Andrews; Vacavant, Antoine. A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos. Computer Vision and Image Understanding (CVIU), 2014. ([Online](http://dx.doi.org/10.1016/j.cviu.2013.12.005)) ([PDF](http://www.researchgate.net/publication/259340906_A_comprehensive_review_of_background_subtraction_algorithms_evaluated_with_synthetic_and_real_videos))\n\n* (2013) Sobral, Andrews; Oliveira, Luciano; Schnitman, Leizer; Souza, Felippe. (**Best Paper Award**) Highway Traffic Congestion Classification Using Holistic Properties. In International Conference on Signal Processing, Pattern Recognition and Applications (SPPRA'2013), Innsbruck, Austria, Feb 2013. ([Online](http://dx.doi.org/10.2316/P.2013.798-105)) ([PDF](http://www.researchgate.net/publication/233427564_HIGHWAY_TRAFFIC_CONGESTION_CLASSIFICATION_USING_HOLISTIC_PROPERTIES))\n\n\nVideos\n------\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_qt_gui_video.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=Ccqa9KBO9_U\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_youtube.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n"
 },
 {
  "repo": "tebelorg/RPA-Python",
  "language": "Python",
  "readme_contents": "# RPA for Python :snake:\n\n[**Use Cases**](#use-cases)&ensp;|&ensp;[**API Reference**](#api-reference)&ensp;|&ensp;[**About & Credits**](#about--credits)&ensp;|&ensp;[**PyCon Video**](https://www.youtube.com/watch?v=F2aQKWx_EAE)&ensp;|&ensp;[**v1.27**](https://github.com/tebelorg/RPA-Python/releases)\n\n>_This tool was previously known as TagUI for Python. [More details](https://github.com/tebelorg/RPA-Python/issues/100) on the name change, which is backward compatible so existing scripts written with `import tagui as t` and `t.function()` will continue to work._\n\n![RPA for Python demo in Jupyter notebook](https://raw.githubusercontent.com/tebelorg/Tump/master/tagui_python.gif)\n\nTo install this Python package for RPA (robotic process automation) -\n```\npip install rpa\n```\n\nTo use it in Jupyter notebook, Python script or interactive shell -\n```python\nimport rpa as r\n```\n\nNotes on different operating systems and optional visual automation mode -\n- :rainbow_flag: **Windows -** if visual automation is cranky, try setting your display zoom level to recommended % or 100%\n- :apple: **macOS -** Catalina update introduces tighter app security, see solutions for [PhantomJS](https://github.com/tebelorg/RPA-Python/issues/79) and [Java popups](https://github.com/tebelorg/RPA-Python/issues/78)\n- :penguin: **Linux -** visual automation mode requires special setup on Linux, see how to [install OpenCV and Tesseract](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html)\n\n# Use Cases\n\nRPA for Python's simple and powerful API makes robotic process automation fun! You can use it to quickly automate repetitive time-consuming tasks, whether the tasks involve websites, desktop applications, or the command line.\n\n#### WEB AUTOMATION&ensp;:spider_web:\n```python\nr.init()\nr.url('https://www.google.com')\nr.type('//*[@name=\"q\"]', 'decentralization[enter]')\nprint(r.read('result-stats'))\nr.snap('page', 'results.png')\nr.close()\n```\n\n#### VISUAL AUTOMATION&ensp;:see_no_evil:\n```python\nr.init(visual_automation = True)\nr.dclick('outlook_icon.png')\nr.click('new_mail.png')\n...\nr.type('message_box.png', 'message')\nr.click('send_button.png')\nr.close()\n```\n\n#### OCR AUTOMATION&ensp;\ud83e\uddff\n```python\nr.init(visual_automation = True, chrome_browser = False)\nprint(r.read('pdf_window.png'))\nprint(r.read('image_preview.png'))\nr.hover('anchor_element.png')\nprint(r.read(r.mouse_x(), r.mouse_y(), r.mouse_x() + 400, r.mouse_y() + 200))\nr.close()\n```\n\n#### KEYBOARD AUTOMATION&ensp;:musical_keyboard:\n```python\nr.init(visual_automation = True, chrome_browser = False)\nr.keyboard('[cmd][space]')\nr.keyboard('safari[enter]')\nr.keyboard('[cmd]t')\nr.keyboard('joker[enter]')\nr.wait(2.5)\nr.snap('page.png', 'results.png')\nr.close()\n```\n\n#### MOUSE AUTOMATION&ensp;:mouse:\n```python\nr.init(visual_automation = True)\nr.type(600, 300, 'open source')\nr.click(900, 300)\nr.snap('page.bmp', 'results.bmp')\nr.hover('button_to_drag.bmp')\nr.mouse('down')\nr.hover(r.mouse_x() + 300, r.mouse_y())\nr.mouse('up')\nr.close()\n```\n\n# API Reference\n\nCheck out [sample Python script](https://github.com/tebelorg/RPA-Python/blob/master/sample.py), [RPA Challenge solution](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610518196), and [RedMart groceries example](https://github.com/tebelorg/RPA-Python/issues/24). To automate Chrome browser invisibly, see this [simple hack](https://github.com/tebelorg/RPA-Python/issues/133#issuecomment-634113838). To run 20-30X faster, without normal UI interaction delays, [see this advanced hack](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610532082).\n\n#### ELEMENT IDENTIFIERS\nAn element identifier helps to tell RPA for Python exactly which element on the user interface you want to interact with. For example, //\\*[@id=\"email\"] is an XPath pointing to the webpage element having the id attribute \"email\".\n\n- :globe_with_meridians: For web automation, the web element identifier can be XPath selector, CSS selector, or the following attributes - id, name, class, title, aria-label, text(), href, in decreasing order of priority. Recommend writing XPath manually or simply using attributes. There is automatic waiting for an element to appear before timeout happens, and error is returned that the element cannot be found. To change the default timeout of 10 seconds, use timeout() function.\n\n- :camera_flash: An element identifier can also be a .png or .bmp image snapshot representing the UI element (can be on desktop applications, terminal window or web browser). If the image file specified does not exist, OCR will be used to search for that text on the screen to act on the UI element containing the text, eg r.click('Submit Form.png'). Transparency (0% opacity) is supported in .png images. x, y coordinates of elements on the screen can be used as well.\n\n- :page_facing_up: A further image identifier example is an image of the window (PDF viewer, MS Word, textbox etc) with the center content of the image set as transparent. This allows using read() and snap() to perform OCR and save snapshots of application windows, containers, frames, textboxes with varying content. Also for read() and snap(), x1, y1, x2, y2 coordinates pair can be used to define the region of interest on the screen to perform OCR or capture snapshot.\n\n#### CORE FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\ninit()|visual_automation = False, chrome_browser = True|start TagUI, auto-setup on first run\nclose()||close TagUI, Chrome browser, SikuliX\npack()||for deploying package without internet\nupdate()||for updating package without internet\n\n>_to print and log debug info to rpa_python.log use debug(True), to switch off use debug(False)_\n\n#### BASIC FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nurl()|webpage_url (no parameter to return current URL)|go to web URL\nclick()|element_identifier (or x, y using visual automation)| left-click on element\nrclick()|element_identifier (or x, y using visual automation)|right-click on element\ndclick()|element_identifier (or x, y using visual automation)|double-click on element\nhover()|element_identifier (or x, y using visual automation)|move mouse to element\ntype()|element_identifier (or x, y), text_to_type ('[enter]', '[clear]')|enter text at element\nselect()|element_identifier (or x, y), option_value (or x, y)|choose dropdown option\nread()|element_identifier (page = web page) (or x1, y1, x2, y2)|fetch & return element text\nsnap()|element_identifier (page = web page), filename_to_save|save screenshot to file\nload()|filename_to_load|load & return file content\ndump()|text_to_dump, filename_to_save|save text to file\nwrite()|text_to_write, filename_to_save|append text to file\nask()|text_to_prompt|ask & return user input\n\n>_to wait for an element to appear until timeout() value, use hover(). to drag-and-drop, [you can do this](https://github.com/tebelorg/RPA-Python/issues/58#issuecomment-570778431)_\n\n#### PRO FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nkeyboard()|keys_and_modifiers (using visual automation)|send keystrokes to screen\nmouse()|'down' or 'up' (using visual automation)|send mouse event to screen\nwait()|delay_in_seconds (default 5 seconds)|explicitly wait for some time\ntable()|element_identifier (XPath only), filename_to_save|save basic HTML table to CSV\nupload()|element_identifier (CSS only), filename_to_upload|upload file to web element\ndownload()|download_url, filename_to_save(optional)|download from URL to file\nunzip()|file_to_unzip, unzip_location (optional)|unzip zip file to specified location\nframe()|main_frame id or name, sub_frame (optional)|set web frame, frame() to reset\npopup()|string_in_url (no parameter to reset to main page)|set context to web popup tab\nrun()|command_to_run (use ; between commands)|run OS command & return output\ndom()|statement_to_run (JS code to run in browser)|run code in DOM & return output\nvision()|command_to_run (Python code for SikuliX)|run custom SikuliX commands\ntimeout()|timeout_in_seconds (blank returns current timeout)|change wait timeout (default 10s)\n\nkeyboard() modifiers and special keys -\n>_[shift] [ctrl] [alt] [cmd] [win] [meta] [clear] [space] [enter] [backspace] [tab] [esc] [up] [down] [left] [right] [pageup] [pagedown] [delete] [home] [end] [insert] [f1] .. [f15] [printscreen] [scrolllock] [pause] [capslock] [numlock]_\n\n#### HELPER FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nexist()|element_identifier|return True or False if element exists before timeout\npresent()|element_identifier|return True or False if element is present now\ncount()|element_identifier|return number of web elements as integer\nclipboard()|text_to_put or no parameter|put text or return clipboard text as string\nmouse_xy()||return '(x,y)' coordinates of mouse as string\nmouse_x()||return x coordinate of mouse as integer\nmouse_y()||return y coordinate of mouse as integer\ntitle()||return page title of current web page as string\ntext()||return text content of current web page as string\ntimer()||return time elapsed in sec between calls as float\n\n>_to type large amount of text quickly, use clipboard() and keyboard() to paste instead of type()_\n\n# About & Credits\n\nTagUI is the leading open-source RPA software :robot: with thousands of active users. It was created in 2016-2017 when I left DBS Bank as a test automation engineer, to embark on a one-year sabbatical to Eastern Europe. Most of its code base was written in Novi Sad Serbia. My wife and I also spent a couple of months in Budapest Hungary, as well as Chiang Mai Thailand for visa runs. In 2018, I joined AI Singapore to continue development of TagUI.\n\nOver the past few months I take on a daddy role full-time, taking care of my newborn baby girl and wife :cowboy_hat_face:\ud83e\udd31. In between the nannying, I use my time pockets to create this Python package that's built on TagUI. I hope that RPA for Python and ML frameworks would be good friends, and `pip install rpa` would make life easier for Python users.\n\nLastly, at only ~1k lines of code, it would make my day to see developers of other languages port this project over to their favourite programming language. See ample comments in this [single-file package](https://github.com/tebelorg/RPA-Python/blob/master/tagui.py), and its intuitive architecture -\n\n![RPA for Python architecture](https://raw.githubusercontent.com/tebelorg/Tump/master/TagUI-Python/architecture.png)\n\nI would like to credit and express my appreciation below :bowing_man:, and you are invited to [connect on LinkedIn](https://www.linkedin.com/in/kensoh) -\n\n- [TagUI](https://github.com/kelaberetiv/TagUI/tree/pre_v6) - AI Singapore from Singapore / [@aisingapore](https://www.aisingapore.org)\n- [SikuliX](https://github.com/RaiMan/SikuliX1) - Raimund Hocke from Germany / [@RaiMan](https://github.com/RaiMan)\n- [CasperJS](https://github.com/casperjs/casperjs) - Nicolas Perriault from France / [@n1k0](https://github.com/n1k0)\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia / [@ariya](https://github.com/ariya)\n- [SlimerJS](https://github.com/laurentj/slimerjs) - Laurent Jouanneau from France / [@laurentj](https://github.com/laurentj)\n\n# License\nRPA for Python is open-source software released under Apache 2.0 license\n"
 },
 {
  "repo": "abhiTronix/vidgear",
  "language": "Python",
  "readme_contents": "<!--\r\n===============================================\r\nvidgear library source-code is deployed under the Apache 2.0 License:\r\n\r\nCopyright (c) 2019-2020 Abhishek Thakur(@abhiTronix) <abhi.una12@gmail.com>\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n===============================================\r\n-->\r\n\r\n<h1 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/vidgear.webp\" alt=\"VidGear\" title=\"Logo designed by Abhishek Thakur(@abhiTronix), under CC-BY-NC-SA 4.0 License\" width=\"80%\"/>\r\n</h1>\r\n<h2 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/tagline.svg\" alt=\"VidGear tagline\" width=\"40%\"/>\r\n</h2>\r\n\r\n<div align=\"center\">\r\n\r\n[Releases][release]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Gears][gears]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentation][docs]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Installation][installation]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[License](#license)\r\n\r\n[![Build Status][travis-cli]][travis] [![Codecov branch][codecov]][code] [![Build Status][appveyor]][app]\r\n\r\n[![Glitter chat][gitter-bagde]][gitter] [![PyPi version][pypi-badge]][pypi] [![Twitter][twitter-badge]][twitter-intent]\r\n\r\n[![Code Style][black-badge]][black]\r\n\r\n</div>\r\n\r\n&nbsp;\r\n\r\nVidGear is a high-performance Video Processing python library that provides an easy-to-use, highly extensible, **Multi-Threaded + Asyncio wrapper** around many state-of-the-art specialized libraries like *[OpenCV][opencv], [FFmpeg][ffmpeg], [ZeroMQ][zmq], [picamera][picamera], [starlette][starlette], [pafy][pafy] and [python-mss][mss]* at its backend, and enable us to flexibly exploit their internal parameters and methods, while silently delivering robust error-handling and unparalleled real-time performance.\r\n\r\nVidGear primarily focuses on simplicity, and thereby lets programmers and software developers to easily integrate and perform Complex Video Processing Tasks, in just a few lines of code.\r\n\r\n&nbsp;\r\n\r\nThe following **functional block diagram** clearly depicts the generalized functioning of VidGear APIs:\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/gears_fbd.webp\" alt=\"@Vidgear Functional Block Diagram\" />\r\n</p>\r\n\r\n&nbsp;\r\n\r\n# Table of Contents\r\n\r\n* [**TL;DR**](#tldr)\r\n* [**Getting Started**](#getting-started)\r\n* [**Gears: What are these?**](#gears)\r\n  * [**CamGear**](#camgear)\r\n  * [**PiGear**](#pigear)\r\n  * [**VideoGear**](#videogear)\r\n  * [**ScreenGear**](#screengear)\r\n  * [**WriteGear**](#writegear)\r\n  * [**StreamGear**](#streamgear)\r\n  * [**NetGear**](#netgear)\r\n  * [**WebGear**](#webgear)\r\n  * [**NetGear_Async**](#netgear_async)\r\n* [**Documentation**](#documentation)\r\n* [**Community Channel**](#community-channel)\r\n* [**Contributions & Support**](#contributions--support)\r\n  * [**Support**](#support)\r\n  * [**Contributors**](#contributors)\r\n* [**Citation**](#citation)\r\n* [**Copyright**](#copyright)\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# TL;DR\r\n  \r\n#### What is vidgear?\r\n\r\n> *\"VidGear is a High-Performance Framework that provides an all-in-one complete Video Processing solution for building real-time applications in python.\"*\r\n\r\n#### What does it do?\r\n\r\n> *\"VidGear can read, write, process, send & receive video frames from/to various devices in real-time.\"*\r\n\r\n#### What is its purpose?\r\n\r\n> *\"Built with simplicity in mind, VidGear lets programmers and software developers to easily integrate and perform complex Video Processing tasks in their existing or new applications, in just a [few lines of code][switch_from_cv]. Beneficial for both, if you're new to programming with Python language or already a pro at it.\"*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## Getting Started\r\n\r\nIf this is your first time using VidGear, head straight to the [Installation \u27b6][installation] to install VidGear.\r\n\r\nOnce you have VidGear installed, **checkout its well-documented [Gears \u27b6][gears]**\r\n\r\nAlso, if you're already familar with [OpenCV][opencv] library, then see [Switching from OpenCV \u27b6][switch_from_cv]\r\n\r\nOr, if you're just getting started with OpenCV with Python, then see [here \u27b6](https://abhitronix.github.io/vidgear/help/general_faqs/#im-new-to-python-programming-or-its-usage-in-computer-vision-how-to-use-vidgear-in-my-projects)\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## Gears: What are these?\r\n\r\n> **VidGear is built with multiple [Gears][gears] (APIs), each with some unique functionality.**\r\n\r\nEach of these APIs is exclusively designed to handle/control different device-specific video streams, network streams, and media encoders. These APIs provide an easy-to-use, highly extensible, multi-threaded and asyncio layer above state-of-the-art libraries under the hood to exploit their internal parameters and methods flexibly while providing robust error-handling and unparalleled performance. \r\n\r\n**These Gears can be classified as follows:**\r\n\r\n**A. VideoCapture Gears:**\r\n\r\n  * [**CamGear:**](#camgear) Multi-threaded API targeting various IP-USB-Cameras/Network-Streams/YouTube-Video-URLs.\r\n  * [**PiGear:**](#pigear) Multi-threaded API targeting  various Raspberry Pi Camera Modules.\r\n  * [**ScreenGear:**](#screengear) Multi-threaded ultra-fast Screencasting.    \r\n  * [**VideoGear:**](#videogear) Common API with internal [Video Stabilizer](/gears/stabilizer/overview/) wrapper.\r\n\r\n**B. VideoWriter Gears:**\r\n\r\n  * [**WriteGear:**](#writegear) Handles Flexible Lossless Video Encoding and Compression.\r\n\r\n**C. Streaming Gears:**\r\n\r\n  * [**StreamGear**](#streamgear): Handles Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats.\r\n\r\n\r\n**D. Network Gears:**\r\n\r\n  * [**NetGear:**](#netgear) Handles high-performance video-frames & data transfer between interconnecting systems over the network.\r\n\r\n  * **Asynchronous I/O Network Gears:**\r\n\r\n    * [**WebGear:**](#webgear) ASGI Video Server that can send live video-frames to any web browser on the network.\r\n    * [**NetGear_Async:**](#netgear_async) Immensely Memory-efficient Asyncio video-frames network messaging framework.\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## CamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/camgear.webp\" alt=\"CamGear Functional Block Diagram\" width=\"45%\"/>\r\n</p>\r\n\r\n> *CamGear can grab ultra-fast frames from diverse range of devices/streams, which includes almost any IP/USB Cameras, multimedia video file format ([_upto 4k tested_][test-4k]), various network stream protocols such as `http(s), rtp, rstp, rtmp, mms, etc.`, plus support for live Gstreamer's stream pipeline and YouTube video/live-streams URLs.*\r\n\r\nCamGear provides a flexible, high-level multi-threaded wrapper around `OpenCV's` [VideoCapture class][opencv-vc] with access almost all of its available parameters and also employs [`pafy`][pafy] python APIs for live [YouTube streaming][youtube-doc]. Furthermore, CamGear relies exclusively on [**Threaded Queue mode**][TQM-doc] for ultra-fast, error-free and synchronized frame handling.\r\n\r\n### CamGear API Guide:\r\n\r\n[**>>> Usage Guide**][camgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## VideoGear\r\n\r\n> *VideoGear API provides a special internal wrapper around VidGear's exclusive [**Video Stabilizer**][stablizer-doc] class.*\r\n\r\nVideoGear also act as a Common API, that provides an internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs and their parameters, with a special `enablePiCamera` boolean flag.\r\n\r\nVideoGear is basically ideal when you need to switch to different video sources without changing your code much. Also, it enables easy stabilization for various video-streams _(real-time or not)_  with minimum effort and using way fewer lines of code.\r\n\r\n\r\n**Below is a snapshot of a VideoGear Stabilizer in action  (_See its detailed usage [here][stablizer-doc-ex]_):**\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://github.com/abhiTronix/Imbakup/raw/master/Images/stabilizer.gif\" alt=\"VideoGear Stabilizer in action!\"/>\r\n  <br>\r\n  <sub><i>Original Video Courtesy <a href=\"http://liushuaicheng.org/SIGGRAPH2013/database.html\" title=\"opensourced video samples database\">@SIGGRAPH2013</a></i></sub>\r\n</p>\r\n\r\n**Code to generate above result:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import VideoGear\r\nimport numpy as np\r\nimport cv2\r\n\r\n# open any valid video stream with stabilization enabled(`stabilize = True`)\r\nstream_stab = VideoGear(source = \"test.mp4\", stabilize = True).start()\r\n\r\n# open same stream without stabilization for comparison\r\nstream_org = VideoGear(source = \"test.mp4\").start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read stabilized frames\r\n    frame_stab = stream_stab.read()\r\n\r\n    # check for stabilized frame if Nonetype\r\n    if frame_stab is None:\r\n        break\r\n\r\n    # read un-stabilized frame\r\n    frame_org = stream_org.read()\r\n\r\n    # concatenate both frames\r\n    output_frame = np.concatenate((frame_org, frame_stab), axis=1)\r\n\r\n    # put text over concatenated frame\r\n    cv2.putText(\r\n        output_frame, \"Before\", (10, output_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n    cv2.putText(\r\n        output_frame, \"After\", (output_frame.shape[1] // 2 + 10, output_frame.shape[0] - 10),\r\n        cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Stabilized Frame\", output_frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close both video streams\r\nstream_org.stop()\r\nstream_stab.stop()\r\n```\r\n\r\n### VideoGear API Guide:\r\n\r\n[**>>> Usage Guide**][videogear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## PiGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/picam2.webp\" alt=\"PiGear\" width=\"50%\" />\r\n</p>\r\n\r\n> *PiGear is similar to CamGear but made to support various Raspberry Pi Camera Modules *(such as [OmniVision OV5647 Camera Module][OV5647-picam] and [Sony IMX219 Camera Module][IMX219-picam])*.*\r\n\r\nPiGear provides a flexible multi-threaded wrapper around complete [picamera](https://picamera.readthedocs.io/en/release-1.13/index.html) python library, and also provides us the ability to exploit almost all of its parameters like `brightness, saturation, sensor_mode, iso, exposure, etc.` effortlessly. Furthermore, PiGear supports multiple camera modules, such as in case of Raspberry Pi Compute module IO boards.\r\n\r\nBest of all, PiGear provides excellent error-handling with features like a **Threaded Internal Timer** - that keeps active track of any frozen-threads/hardware-failures robustly, and exit safely if it does occurs, i.e. If you're running PiGear API in your script, and someone accidentally pulls Camera module cable out, instead of going into possible kernel panic, PiGear will exit safely to save resources. \r\n\r\n\r\n**Code to open picamera stream with variable parameters in PiGear API:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import PiGear\r\nimport cv2\r\n\r\n# add various Picamera tweak parameters to dictionary\r\noptions = {\"hflip\": True, \"exposure_mode\": \"auto\", \"iso\": 800, \"exposure_compensation\": 15, \"awb_mode\": \"horizon\", \"sensor_mode\": 0}\r\n\r\n# open pi video stream with defined parameters\r\nstream = PiGear(resolution = (640, 480), framerate = 60, logging = True, **options).start() \r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n### PiGear API Guide:\r\n\r\n[**>>> Usage Guide**][pigear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## ScreenGear\r\n\r\n> *ScreenGear is designed exclusively for ultra-fast Screencasting, that means it can grab frames from your monitor in real-time, either by define an area on the computer screen, or full-screen, at the expense of inconsiderable latency. ScreenGear also seamlessly support frame capturing from multiple monitors.*\r\n\r\nScreenGear implements a multi-threaded wrapper around [**python-mss**][mss] python library API and also supports a easy and flexible direct internal parameter manipulation. \r\n\r\n**Below is a snapshot of a ScreenGear API in action:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/screengear.gif\" alt=\"ScreenGear in action!\"/>\r\n</p>\r\n\r\n**Code to generate the above results:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import ScreenGear\r\nimport cv2\r\n\r\n# open video stream with default parameters\r\nstream = ScreenGear().start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n\r\n### ScreenGear API Guide:\r\n\r\n[**>>> Usage Guide**][screengear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n## WriteGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/writegear.webp\" alt=\"WriteGear Functional Block Diagram\" width=\"70%\" />\r\n</p>\r\n\r\n> *WriteGear handles various powerful Writer Tools that provide us the freedom to do almost anything imagine with multimedia files.*\r\n\r\nWriteGear API provides a complete, flexible and robust wrapper around [**FFmpeg**][ffmpeg], a leading multimedia framework. With WriteGear, we can process real-time frames into a lossless compressed video-file with any suitable specification in just few easy lines of codes. These specifications include setting video/audio properties such as `bitrate, codec, framerate, resolution, subtitles,  etc.`, and also performing complex tasks such as multiplexing video with audio in real-time _(see this [doc][live-audio-doc])_, while handling all errors robustly. \r\n\r\nBest of all, WriteGear grants the complete freedom to play with any FFmpeg parameter with its exclusive **Custom Commands function** _(see this [doc][custom-command-doc])_, without relying on any Third-party library.\r\n\r\nIn addition to this, WriteGear also provides flexible access to [**OpenCV's VideoWriter API**][opencv-writer] which provides some basic tools for video frames encoding but without compression.\r\n\r\n**WriteGear primarily operates in the following two modes:**\r\n\r\n  * **Compression Mode:** In this mode, WriteGear utilizes powerful [**FFmpeg**][ffmpeg] inbuilt encoders to encode lossless multimedia files. This mode provides us the ability to exploit almost any parameter available within FFmpeg, effortlessly and flexibly, and while doing that it robustly handles all errors/warnings quietly. *You can find more about this mode [here \u27b6][cm-writegear-doc]*\r\n\r\n  * **Non-Compression Mode:**  In this mode, WriteGear utilizes basic [**OpenCV's inbuilt VideoWriter API**][opencv-vw] tools. This mode also supports all parameters manipulation available within VideoWriter API, but it lacks the ability to manipulate encoding parameters and other important features like video compression, audio encoding, etc. *You can learn about this mode [here \u27b6][ncm-writegear-doc]*\r\n\r\n### WriteGear API Guide:\r\n\r\n[**>>> Usage Guide**][writegear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## StreamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/streamgear_flow.webp\" alt=\"NetGear API\" width=80%/>\r\n</p>\r\n\r\n\r\n> *StreamGear automates transcoding workflow for generating Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats (such as MPEG-DASH) in just few lines of python code.*\r\n\r\nStreamGear provides a standalone, highly extensible and flexible wrapper around [**FFmpeg**][ffmpeg] - a leading multimedia framework, for generating chunked-encoded media segments of the content.\r\n\r\nSteamGear API automatically transcodes source videos/audio files & real-time frames, and breaks them into a sequence of multiple smaller chunks/segments (typically 2-4 seconds in length) at different quality levels _(i.e. different bitrates or spatial resolutions)_. It also creates a Manifest file _(such as MPD in-case of DASH)_ that describes these segment information _(timing, URL, media characteristics like video resolution and bit rates)_, and is provided to the client prior to the streaming session. Thereby, segments are served on a web server and can be downloaded through HTTP standard compliant GET requests. This makes it possible to stream videos at different quality levels, and to switch in the middle of a video from one quality level to another one \u2013 if bandwidth permits \u2013 on a per segment basis.\r\n\r\n\r\nSteamGear currently only supports [**MPEG-DASH**](https://www.encoding.com/mpeg-dash/) _(Dynamic Adaptive Streaming over HTTP, ISO/IEC 23009-1)_ , but other adaptive streaming technologies such as Apple HLS, Microsoft Smooth Streaming, will be added soon.\r\n\r\n**StreamGear primarily works in two independent modes for transcoding which serves different purposes:**\r\n\r\n  * **Single-Source Mode:** In this mode, StreamGear transcodes entire video/audio file _(as opposed to frames by frame)_ into a sequence of multiple smaller chunks/segments for streaming. This mode works exceptionally well, when you're transcoding lossless long-duration videos(with audio) for streaming and required no extra efforts or interruptions. But on the downside, the provided source cannot be changed or manipulated before sending onto FFmpeg Pipeline for processing.  This mode can be easily activated by assigning suitable video path as input to `-video_source` attribute, during StreamGear initialization. ***Learn more about this mode [here \u27b6][ss-mode-doc]***\r\n\r\n  * **Real-time Frames Mode:** When no valid input is received on `-video_source` attribute, StreamGear API activates this mode where it directly transcodes video-frames _(as opposed to a entire file)_, into a sequence of multiple smaller chunks/segments for streaming. In this mode, StreamGear supports real-time [`numpy.ndarray`](https://numpy.org/doc/1.18/reference/generated/numpy.ndarray.html#numpy-ndarray) frames, and process them over FFmpeg pipeline. But on the downside, audio has to added manually _(as separate source)_ for streams. ***Learn more about this mode [here \u27b6][rtf-mode-doc]***\r\n\r\n\r\n### StreamGear API Guide:\r\n\r\n[**>>> Usage Guide**][streamgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/netgear.webp\" alt=\"NetGear API\" width=65%/>\r\n</p>\r\n\r\n> *NetGear is exclusively designed to transfer video frames synchronously and asynchronously between interconnecting systems over the network in real-time.*\r\n\r\nNetGear implements a high-level wrapper around [**PyZmQ**][pyzmq] python library that contains python bindings for [ZeroMQ][zmq] - a high-performance asynchronous distributed messaging library that provides a message queue, but unlike message-oriented middleware, its system can run without a dedicated message broker. \r\n\r\nNetGear provides seamless support for [*Bidirectional data transmission*][netgear_bidata_doc] between receiver(client) and sender(server) through bi-directional synchronous messaging patterns such as zmq.PAIR _(ZMQ Pair Pattern)_ & zmq.REQ/zmq.REP _(ZMQ Request/Reply Pattern)_. \r\n\r\nNetGear also supports real-time [*Frame Compression capabilities*][netgear_compression_doc] for optimizing performance while sending the frames directly over the network, by encoding the frame before sending it and decoding it on the client's end automatically in real-time. \r\n\r\nFor security, NetGear implements easy access to ZeroMQ's powerful, smart & secure Security Layers, that enables [*Strong encryption on data*][netgear_security_doc], and unbreakable authentication between the Server and the Client with the help of custom certificates/keys and brings easy, standardized privacy and authentication for distributed systems over the network. \r\n\r\nBest of all, NetGear can robustly handle [*Multiple Server-Systems*][netgear_multi_server_doc] and [*Multiple Client-Systems*][netgear_multi_client_doc] and at once, thereby providing access to seamless Live Streaming of the multiple device in a network at the same time.\r\n\r\n\r\n**NetGear as of now seamlessly supports three ZeroMQ messaging patterns:**\r\n\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear API Guide:\r\n\r\n[**>>> Usage Guide**][netgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## WebGear\r\n\r\n> *WebGear is a powerful [ASGI](https://asgi.readthedocs.io/en/latest/) Video-streamer API, that is built upon [Starlette](https://www.starlette.io/) - a lightweight ASGI framework/toolkit, which is ideal for building high-performance asyncio services.*\r\n\r\nWebGear API provides a highly extensible and flexible asyncio wrapper around [Starlette][starlette] ASGI application, and provides easy access to its complete framework. Thereby, WebGear API can flexibly interact with the Starlette's ecosystem of shared middleware and mountable applications, and its various [Response classes](https://www.starlette.io/responses/), [Routing tables](https://www.starlette.io/routing/), [Static Files](https://www.starlette.io/staticfiles/), [Templating engine(with Jinja2)](https://www.starlette.io/templates/), etc. \r\n\r\nIn layman's terms, WebGear can acts as powerful **Video Streaming Server** that transfers live video-frames to any web browser on a network. It addition to this, WebGear API also provides a special internal wrapper around [VideoGear](#videogear), which itself provides internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs thereby granting it exclusive power for streaming frames incoming from any device/source, such as streaming [Stabilization enabled Video][stabilize_webgear_doc] in real-time.\r\n\r\n**Below is a snapshot of a WebGear Video Server in action on the Mozilla Firefox browser:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/webgear.gif\" alt=\"WebGear in action!\" width=\"70%\" />\r\n  <br>\r\n  <sub><i>WebGear Video Server at <a href=\"http://localhost:8000/\" title=\"default address\">http://localhost:8000/</a> address.</i></sub>\r\n</p>\r\n\r\n**Code to generate the above result:**\r\n\r\n```python\r\n# import required libraries\r\nimport uvicorn\r\nfrom vidgear.gears.asyncio import WebGear\r\n\r\n#various performance tweaks\r\noptions = {\"frame_size_reduction\": 40, \"frame_jpeg_quality\": 80, \"frame_jpeg_optimize\": True, \"frame_jpeg_progressive\": False}\r\n\r\n#initialize WebGear app  \r\nweb = WebGear(source = \"foo.mp4\", logging = True, **options)\r\n\r\n#run this app on Uvicorn server at address http://localhost:8000/\r\nuvicorn.run(web(), host='localhost', port=8000)\r\n\r\n#close app safely\r\nweb.shutdown()\r\n```\r\n\r\n### WebGear API Guide:\r\n\r\n[**>>> Usage Guide**][webgear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear_Async \r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/zmq_asyncio.webp\" alt=\"WebGear in action!\" width=\"70%\"/>\r\n</p>\r\n\r\n> _NetGear_Async can generate double performance as compared to [NetGear API](#netgear) at about 1/3rd of memory consumption, and also provide complete server-client handling with various options to use variable protocols/patterns similar to NetGear, but it doesn't support any [NetGear's Exclusive Modes][netgear-exm] yet._\r\n\r\nNetGear_Async is an asyncio videoframe messaging framework, built on [`zmq.asyncio`][asyncio-zmq], and powered by high-performance asyncio event loop called [**`uvloop`**][uvloop] to achieve unmatchable high-speed and lag-free video streaming over the network with minimal resource constraints. Basically, this API is able to transfer thousands of frames in just a few seconds without causing any significant load on your system. \r\n\r\nNetGear_Async provides complete server-client handling and options to use variable protocols/patterns similar to [NetGear API](#netgear) but doesn't support any [NetGear Exclusive modes][netgear-exm] yet. Furthermore, NetGear_Async allows us to  define our own custom Server Source to manipulate frames easily before sending them across the network(see this [doc][netgear_Async-cs] example).\r\n\r\nNetGear_Async as of now supports [all four ZeroMQ messaging patterns](#attributes-and-parameters-wrench):\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_ \r\n* [**`zmq.PUSH/zmq.PULL`**][zmq-pull-push] _(ZMQ Push/Pull Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear_Async API Guide:\r\n\r\n[**>>> Usage Guide**][netgear_async-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Documentation\r\n\r\nThe complete documentation for all VidGear APIs can be found in the link below:\r\n\r\n* [**Documentation - English**][docs]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n# Contributions & Support\r\n\r\nContributions are welcome, We'd love your contribution to VidGear in order to fix bugs or to implement new features!  \r\n\r\nPlease see our **[Contribution Guidelines](contributing.md)** for more details.\r\n\r\n### Support\r\n\r\n**VidGear is free, but rely on your support.** \r\n\r\nSending a donation using link below is **extremely** helpful in keeping VidGear development alive:\r\n\r\n[![ko-fi][kofi-badge]][kofi]\r\n\r\n### Contributors\r\n\r\n<a href=\"https://github.com/abhiTronix/vidgear/graphs/contributors\">\r\n  <img src=\"https://contributors-img.web.app/image?repo=abhiTronix/vidgear\" />\r\n</a>\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Community Channel\r\n\r\nIf you've come up with some new idea, or looking for the fastest way troubleshoot your problems, then *join our [Gitter community channel \u27b6][gitter]*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# Citation\r\n\r\nHere is a Bibtex entry you can use to cite this project in a publication:\r\n\r\n\r\n```BibTeX\r\n@misc{vidgear,\r\n    author = {Abhishek Thakur},\r\n    title = {vidgear},\r\n    howpublished = {\\url{https://github.com/abhiTronix/vidgear}},\r\n    year = {2019-2020}\r\n  }\r\n```\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Copyright\r\n\r\n**Copyright \u00a9 abhiTronix 2019-2020**\r\n\r\nThis library is released under the **[Apache 2.0 License][license]**.\r\n\r\n\r\n\r\n\r\n<!--\r\nBadges\r\n-->\r\n\r\n[appveyor]:https://img.shields.io/appveyor/ci/abhitronix/vidgear.svg?style=for-the-badge&logo=appveyor\r\n[codecov]:https://img.shields.io/codecov/c/github/abhiTronix/vidgear/testing?style=for-the-badge&logo=codecov\r\n[travis-cli]:https://img.shields.io/travis/com/abhiTronix/vidgear/testing?logo=travis&style=for-the-badge\r\n[prs-badge]:https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAABC0lEQVRYhdWVPQoCMRCFX6HY2ghaiZUXsLW0EDyBrbWtN/EUHsHTWFnYyCL4gxibVZZlZzKTnWz0QZpk5r0vIdkF/kBPAMOKeddE+CQPKoc5Yt5cTjBMdQSwDQToWgBJAn3jmhqgltapAV6E6b5U17MGGAUaUj07TficMfIBZDV6vxowBm1BP9WbSQE4o5h9IjPJmy73TEPDDxVmoZdQrQ5jRhly9Q8tgMUXkIIWn0oG4GYQfAXQzz1PGoCiQndM7b4RgJay/h7zBLT3hASgoKjamQJMreKf0gfuAGyYtXEIAKcL/Dss15iq6ohXghozLYiAMxPuACwtIT4yeQUxAaLrZwAoqGRKGk7qDSYTfYQ8LuYnAAAAAElFTkSuQmCC\r\n[twitter-badge]:https://img.shields.io/badge/Tweet-Now-blue.svg?style=for-the-badge&logo=twitter\r\n[pypi-badge]:https://img.shields.io/pypi/v/vidgear.svg?style=for-the-badge&logo=pypi\r\n[gitter-bagde]:https://img.shields.io/badge/Chat-Gitter-blue.svg?style=for-the-badge&logo=gitter\r\n[Coffee-badge]:https://abhitronix.github.io/img/vidgear/orange_img.png\r\n[kofi-badge]:https://www.ko-fi.com/img/githubbutton_sm.svg\r\n[black-badge]:https://img.shields.io/badge/code%20style-black-000000.svg?style=for-the-badge&logo=github\r\n\r\n\r\n<!--\r\nInternal URLs\r\n-->\r\n\r\n[release]:https://github.com/abhiTronix/vidgear/releases/latest\r\n[pypi]:https://pypi.org/project/vidgear/\r\n[gitter]:https://gitter.im/vidgear/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\r\n[twitter-intent]:https://twitter.com/intent/tweet?url=https%3A%2F%2Fabhitronix.github.io%2Fvidgear&via%20%40abhi_una12&text=Checkout%20VidGear%20-%20A%20High-Performance%20Video-Processing%20Python%20Framework.&hashtags=vidgear%20%23videoprocessing%20%23python%20%23threaded%20%23asyncio\r\n[coffee]:https://www.buymeacoffee.com/2twOXFvlA\r\n[kofi]: https://ko-fi.com/W7W8WTYO\r\n[license]:https://github.com/abhiTronix/vidgear/blob/master/LICENSE\r\n[travis]:https://travis-ci.com/github/abhiTronix/vidgear\r\n[app]:https://ci.appveyor.com/project/abhiTronix/vidgear\r\n[code]:https://codecov.io/gh/abhiTronix/vidgear\r\n\r\n[test-4k]:https://github.com/abhiTronix/vidgear/blob/e0843720202b0921d1c26e2ce5b11fadefbec892/vidgear/tests/benchmark_tests/test_benchmark_playback.py#L65\r\n[bs_script_dataset]:https://github.com/abhiTronix/vidgear/blob/testing/scripts/bash/prepare_dataset.sh\r\n\r\n[faq]:https://abhitronix.github.io/vidgear/help/get_help/#frequently-asked-questions\r\n[doc-vidgear-purpose]:https://abhitronix.github.io/vidgear/help/motivation/#why-is-vidgear-a-thing\r\n[live-audio-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/usage/#using-compression-mode-with-live-audio-input\r\n[ffmpeg-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/ffmpeg_install/\r\n[youtube-doc]:https://abhitronix.github.io/vidgear/gears/camgear/usage/#using-camgear-with-youtube-videos\r\n[TQM-doc]:https://abhitronix.github.io/vidgear/bonus/TQM/#threaded-queue-mode\r\n[camgear-doc]:https://abhitronix.github.io/vidgear/gears/camgear/overview/\r\n[stablizer-doc]:https://abhitronix.github.io/vidgear/gears/stabilizer/overview/\r\n[stablizer-doc-ex]:https://abhitronix.github.io/vidgear/gears/videogear/usage/#using-videogear-with-video-stabilizer-backend\r\n[videogear-doc]:https://abhitronix.github.io/vidgear/gears/videogear/overview/\r\n[pigear-doc]:https://abhitronix.github.io/vidgear/gears/pigear/overview/\r\n[cm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/overview/\r\n[ncm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/non_compression/overview/\r\n[screengear-doc]:https://abhitronix.github.io/vidgear/gears/screengear/overview/\r\n[streamgear-doc]:https://abhitronix.github.io/vidgear/gears/streamgear/overview/\r\n[writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/introduction/\r\n[netgear-doc]:https://abhitronix.github.io/vidgear/gears/netgear/overview/\r\n[webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/overview/\r\n[netgear_async-doc]:https://abhitronix.github.io/vidgear/gears/netgear_async/overview/\r\n[drop35]:https://github.com/abhiTronix/vidgear/issues/99\r\n[custom-command-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/cciw/\r\n[advanced-webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/\r\n[netgear_bidata_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/bidirectional_mode/\r\n[netgear_compression_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/compression/\r\n[netgear_security_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/secure_mode/\r\n[netgear_multi_server_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_server/\r\n[netgear_multi_client_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_client/\r\n[netgear-exm]: https://abhitronix.github.io/vidgear/gears/netgear/overview/#modes-of-operation\r\n[stabilize_webgear_doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/#using-webgear-with-real-time-video-stabilization-enabled\r\n[netgear_Async-cs]: https://abhitronix.github.io/vidgear/gears/netgear_async/usage/#using-netgear_async-with-a-custom-sourceopencv\r\n[installation]:https://abhitronix.github.io/vidgear/installation/\r\n[gears]:https://abhitronix.github.io/vidgear/gears\r\n[switch_from_cv]:https://abhitronix.github.io/vidgear/switch_from_cv/\r\n[ss-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#a-single-source-mode\r\n[rtf-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#b-real-time-frames-mode\r\n[docs]: https://abhitronix.github.io/vidgear\r\n\r\n<!--\r\nExternal URLs\r\n-->\r\n[asyncio-zmq]:https://pyzmq.readthedocs.io/en/latest/api/zmq.asyncio.html\r\n[uvloop]: https://github.com/MagicStack/uvloop\r\n[uvloop-ns]: https://github.com/MagicStack/uvloop/issues/14\r\n[ffmpeg]:https://www.ffmpeg.org/\r\n[flake8]: https://flake8.pycqa.org/en/latest/\r\n[black]: https://github.com/psf/black\r\n[pytest]:https://docs.pytest.org/en/latest/\r\n[opencv-writer]:https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5\r\n[OpenCV-windows]:https://www.learnopencv.com/install-opencv3-on-windows/\r\n[OpenCV-linux]:https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/\r\n[OpenCV-pi]:https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/\r\n[starlette]:https://www.starlette.io/\r\n[uvicorn]:http://www.uvicorn.org/\r\n[daphne]:https://github.com/django/daphne/\r\n[hypercorn]:https://pgjones.gitlab.io/hypercorn/\r\n[prs]:http://makeapullrequest.com\r\n[opencv]:https://github.com/opencv/opencv\r\n[picamera]:https://github.com/waveform80/picamera\r\n[pafy]:https://github.com/mps-youtube/pafy\r\n[pyzmq]:https://github.com/zeromq/pyzmq\r\n[zmq]:https://zeromq.org/\r\n[mss]:https://github.com/BoboTiG/python-mss\r\n[pip]:https://pip.pypa.io/en/stable/installing/\r\n[opencv-vc]:https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#a57c0e81e83e60f36c83027dc2a188e80\r\n[OV5647-picam]:https://github.com/techyian/MMALSharp/doc/OmniVision-OV5647-Camera-Module\r\n[IMX219-picam]:https://github.com/techyian/MMALSharp/doc/Sony-IMX219-Camera-Module\r\n[opencv-vw]:https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html\r\n[yt-dl]:https://github.com/ytdl-org/youtube-dl/\r\n[numpy]:https://github.com/numpy/numpy\r\n[zmq-pair]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pair.html\r\n[zmq-req-rep]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/client_server.html\r\n[zmq-pub-sub]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pubsub.html\r\n[zmq-pull-push]: https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pushpull.html#push-pull\r\n[picamera-setting]:https://picamera.readthedocs.io/en/release-1.13/quickstart.html"
 },
 {
  "repo": "skvark/opencv-python",
  "language": "Shell",
  "readme_contents": "[![Downloads](http://pepy.tech/badge/opencv-python)](http://pepy.tech/project/opencv-python)\n\n## OpenCV on Wheels\n\n**Unofficial** pre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA. \n\n### Installation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n    There are four different packages (see options 1, 2, 3 and 4 below) and you should **SELECT ONLY ONE OF THEM**. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (`cv2`). If you installed multiple different packages in the same environment, uninstall them all with ``pip uninstall`` and reinstall only one package.\n\n    **a.** Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)\n\n    - Option 1 - Main modules package: ``pip install opencv-python``\n    - Option 2 - Full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n    **b.** Packages for server (headless) environments (such as Docker, cloud environments etc.), no GUI library dependencies\n\n    These packages are smaller than the two other packages above because they do not contain any GUI functionality (not compiled with Qt / other GUI components). This means that the packages avoid a heavy dependency chain to X11 libraries and you will have for example smaller Docker images as a result. You should always use these packages if you do not use `cv2.imshow` et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI.\n\n    - Option 3 - Headless main modules package: ``pip install opencv-python-headless``\n    - Option 4 - Headless full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python-headless`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n4. Import the package:\n\n    ``import cv2``\n\n    All packages contain haarcascade files. ``cv2.data.haarcascades`` can be used as a shortcut to the data folder. For example:\n\n    ``cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")``\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Pip install fails with ``Could not find a version that satisfies the requirement ...``?**\n\nA: Most likely the issue is related to too old pip and can be fixed by running ``pip install --upgrade pip``. Note that the wheel (especially manylinux) format does not currently support properly ARM architecture so there are no packages for ARM based platforms in PyPI. However, ``opencv-python`` packages for Raspberry Pi can be found from https://www.piwheels.org/.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions).\n\nIf you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/skvark/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\n## Documentation for opencv-python\n\n[![AppVeyor CI test status (Windows)](https://img.shields.io/appveyor/ci/skvark/opencv-python.svg?maxAge=3600&label=Windows)](https://ci.appveyor.com/project/skvark/opencv-python)\n[![Travis CI test status (Linux and macOS)](https://img.shields.io/travis/com/skvark/opencv-python/master?label=Linux%20%26%20macOS)](https://travis-ci.com/github/skvark/opencv-python/)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\n### CI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example ``appveyor.yml`` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n   -  OpenCV is included as submodule and the version is updated\n      manually by maintainers when a new OpenCV release has been made\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV\n\n   -  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix \n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build. \n\nSee the next section for more info about manual builds outside the CI environment.\n\n### Manual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/skvark/opencv-python.git`\n2. ``cd opencv-python``\n    - you can use `git` to checkout some other version of OpenCV in the `opencv` and `opencv_contrib` submodules if needed\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n    - this might take anything from 5 minutes to over 2 hours depending on your hardware\n6. You'll have the wheel file in the `dist` folder and you can do with that whatever you wish\n    - Optional: on Linux use some of the `manylinux` images as a build hosts if maximum portability is needed and run `auditwheel` for the wheel after build\n    - Optional: on macOS use ``delocate`` (same as ``auditwheel`` but for macOS) for better portability\n\n#### Source distributions\n\nSince OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples: \n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies. \n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\n### Licensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [3-clause BSD License](https://github.com/opencv/opencv/blob/master/LICENSE).\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux and MacOS wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\n### Versioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\n### Releases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\n### Development builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\n### Manylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\n### Supported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.6\n- 3.7\n- 3.8\n- 3.9\n\n### Backward compatibility\n\nStarting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\n"
 },
 {
  "repo": "ivanseidel/Is-Now-Illegal",
  "language": "JavaScript",
  "readme_contents": "# Is Now Illegal!\nA NERD protest against Trump's Immigration ban \ud83d\udeab\nGo to [IsNowIllegal.com](http://isnowillegal.com) and type what you want to make illegal!\n\n## What's this?\nA webapp that gives you the Donald J. Trump power. Pretend you are Trump for a few seconds, make something illegal, share with friends and have fun!\n\n## Donate\n\nThe server costs are too high and we will shutdown very soon if we don't get enough donations. For real. \ud83d\ude14\nPlease click to [Donate via Patreon](https://www.patreon.com/isnowillegal) or contact us below.\n\n## Who made this?\n![](https://github.com/ivanseidel.png?size=100)\nIvan Seidel ([github](https://github.com/ivanseidel))\n\n![](https://github.com/brunolemos.png?size=100)\nBruno Lemos ([github](https://github.com/brunolemos), [twitter](https://twitter.com/brunolemos))\n\n![](https://github.com/joaopedrovbs.png?size=100)\nJo\u00e3o Pedro ([github](https://github.com/joaopedrovbs))\n\nSee full list of [contributors](https://github.com/ivanseidel/Is-Now-Illegal/graphs/contributors)."
 },
 {
  "repo": "datitran/object_detector_app",
  "language": "Python",
  "readme_contents": "# Object-Detector-App\n\nA real-time object recognition application using [Google's TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) and [OpenCV](http://opencv.org/).\n\n## Getting Started\n1. `conda env create -f environment.yml`\n2. `python object_detection_app.py` / `python object_detection_multithreading.py`\n    Optional arguments (default value):\n    * Device index of the camera `--source=0`\n    * Width of the frames in the video stream `--width=480`\n    * Height of the frames in the video stream `--height=360`\n    * Number of workers `--num-workers=2`\n    * Size of the queue `--queue-size=5`\n    * Get video from HLS stream rather than webcam '--stream-input=http://somertmpserver.com/hls/live.m3u8'\n    * Send stream to livestreaming server '--stream-output=--stream=http://somertmpserver.com/hls/live.m3u8'\n\n## Tests\n```\npytest -vs utils/\n```\n\n## Requirements\n- [Anaconda / Python 3.5](https://www.continuum.io/downloads)\n- [TensorFlow 1.2](https://www.tensorflow.org/)\n- [OpenCV 3.0](http://opencv.org/)\n\n## Notes\n- OpenCV 3.1 might crash on OSX after a while, so that's why I had to switch to version 3.0. See open issue and solution [here](https://github.com/opencv/opencv/issues/5874).\n- Moving the `.read()` part of the video stream in a multiple child processes did not work. However, it was possible to move it to a separate thread.\n\n## Copyright\n\nSee [LICENSE](LICENSE) for details.\nCopyright (c) 2017 [Dat Tran](http://www.dat-tran.com/).\n"
 },
 {
  "repo": "yangkun19921001/Blog",
  "language": "HTML",
  "readme_contents": "<p align=\"center\">\n<a href=\"https://github.com/yangkun19921001/Blog\" target=\"_blank\">\n\t<img src=\"https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200308171526.png\" width=\"300px\"/>\n</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"XXX\"><img src=\"https://img.shields.io/badge/\u9605\u8bfb-read-brightgreen.svg\" alt=\"\u5728\u7ebf\u9605\u8bfb\"></a>\n</p>\n\n\n\n   * [Blog](#blog)\n      * [\u9762\u8bd5](#\u9762\u8bd5)\n      * [Flutter \u7cfb\u5217](#flutter-\u7cfb\u5217)\n      * [\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217](#\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217)\n      * [Java \u6e90\u7801\u5206\u6790](#java-\u6e90\u7801\u5206\u6790)\n      * [Android \u6e90\u7801\u5206\u6790](#android-\u6e90\u7801\u5206\u6790)\n      * [\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790](#\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790)\n      * [\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217](#\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217)\n      * [\u8bbe\u8ba1\u6a21\u5f0f](#\u8bbe\u8ba1\u6a21\u5f0f)\n      * [\u9ad8\u7ea7 UI \u7cfb\u5217](#\u9ad8\u7ea7-ui-\u7cfb\u5217)\n      * [\u97f3\u89c6\u9891](#\u97f3\u89c6\u9891)\n      * [\u5f00\u6e90\u9879\u76ee](#\u5f00\u6e90\u9879\u76ee)\n      * [\u82f1\u8bed](#\u82f1\u8bed)\n      * [\u5173\u4e8e\u6211](#\u5173\u4e8e\u6211)\n\n# Blog\n\n\u7528\u4e8e\u8bb0\u5f55\u751f\u6d3b\u3001\u5b66\u4e60\u3001\u5de5\u4f5c\u7b49\u5185\u5bb9\u3002\n\n## \u9762\u8bd5\n\n- [Android \u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5b9d\u5178](https://github.com/yangkun19921001/Blog/blob/master/\u7b14\u8bd5\u9762\u8bd5/Android\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5fc5\u5907/README.md)\n- [1307 \u9875\u5b57\u8282\u8df3\u52a8 Android \u9762\u8bd5\u5168\u5957\u771f\u9898\u89e3\u6790\u5728\u4e92\u8054\u7f51\u706b\u4e86 \uff0c\u5b8c\u6574\u7248\u5f00\u653e\u4e0b\u8f7d](https://mp.weixin.qq.com/s/Crty_REXRVMEhI20XAeLGw)\n\n## Flutter \u7cfb\u5217\n\n- [Google \u4e3a\u4ec0\u4e48\u4ee5 Flutter \u4f5c\u4e3a\u539f\u751f\u7a81\u7834\u53e3](https://juejin.im/post/5c91f0f25188256b7463868e)\n- [Flutter (\u4e00) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3(\u53d8\u91cf\u3001\u5185\u7f6e\u7c7b\u578b\u3001\u51fd\u6570\u3001\u64cd\u4f5c\u7b26\u3001\u6d41\u7a0b\u63a7\u5236\u8bed\u53e5)](https://juejin.im/post/5c91ed15518825573578c31f)\n- [Flutter (\u4e8c) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 \uff08\u5f02\u5e38,\u7c7b,Mixin, \u6cdb\u578b,\u5e93\uff09](https://juejin.im/post/5c939b275188252d863cc797)\n- [Flutter (\u4e09) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 (\u5f02\u6b65,\u751f\u6210\u5668,\u9694\u79bb,\u5143\u6570\u636e,\u6ce8\u91ca)](https://juejin.im/post/5c962b356fb9a0710e47e361)\n- [Flutter (\u56db) \u57fa\u7840 Widgets\u3001Material Components Widget \u5168\u9762\u4ecb\u7ecd](https://juejin.im/post/5cbedc816fb9a03202221a37)\n\n## \u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217\n\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e00)\u5192\u6ce1\u4e0e\u9009\u62e9\u6392\u5e8f](https://juejin.im/post/5c9442cb5188252da9013153)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e8c)\u7ebf\u6027\u8868\u7684\u94fe\u5f0f\u5b58\u50a8\u7ed3\u6784](https://juejin.im/post/5c9449dd5188252da22508e3)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e09)\u6808\u4e0e\u6808\u7684\u5e94\u7528](https://juejin.im/post/5c9453965188252db02e4be6)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u56db)\u54c8\u5e0c\u8868\u7684\u601d\u60f3\u548c\u4e8c\u53c9\u6811\u5165\u95e8](https://juejin.im/post/5c9456f25188252d971438a9)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e94) \u5206\u6cbb\u6cd5 (\u4e8c\u5206\u67e5\u627e\u3001\u5feb\u901f\u6392\u5e8f\u3001\u5f52\u5e76\u6392\u5e8f)](https://juejin.im/post/5c945c245188252d863cc969)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u516d)\u4e8c\u53c9\u6392\u5e8f\u6811](https://juejin.im/post/5c9460e25188252d971438c4)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e03) huffman \u6811\u4e0e AVL \u6811](https://juejin.im/post/5c9464515188252d7e34df85)\n\n## Java \u6e90\u7801\u5206\u6790\n\n- [\u6e90\u7801\u5206\u6790 (\u4e00) ArrayList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c94695c5188252daa18f487)\n- [\u6e90\u7801\u5206\u6790 (\u4e8c) LinkedList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946b555188252d7941fef2)\n- [\u6e90\u7801\u5206\u6790 (\u4e09) Stack \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946d525188252d5f0fd9ee)\n- [\u9762\u8bd5\u5b98: \u6211\u5fc5\u95ee\u7684\u5bb9\u5668\u77e5\u8bc6\u70b9!](https://juejin.im/post/5e88afce518825085d6ced2e)\n\n## Android \u6e90\u7801\u5206\u6790\n\n- [\u4ece setContentView \u5165\u53e3\uff0c\u5168\u65b9\u4f4d\u5206\u6790 LayoutInflater](https://juejin.im/post/5d6a7f2be51d4561e43a6ce8)\n- [\u5206\u6790\u5e7f\u64ad \u7684 registerReceiver\u3001sendBroadcast\u3001 onReceive \u7cfb\u7edf\u5230\u5e95\u505a\u4e86\u4ec0\u4e48?](https://juejin.im/post/5d752aad518825346e5f2b31)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e00) SystemServer \u8fdb\u7a0b\u542f\u52a8](https://juejin.im/post/5db3f95ee51d4529e83947f9)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e8c) Launcher \u542f\u52a8](https://juejin.im/post/5db5565cf265da4d0f14053c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e09) \u5e94\u7528\u7a0b\u5e8f\u8fdb\u7a0b\u521b\u5efa\u5230\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u7684\u8fc7\u7a0b](https://juejin.im/post/5db599bc6fb9a0203b234b08)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u56db) Activity \u542f\u52a8](https://juejin.im/post/5db85da4e51d4529f73e27fb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e94) Service \u542f\u52a8](https://juejin.im/post/5dbb0507f265da4cf406f735)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516d) BroadcastReceiver \u542f\u52a8](https://juejin.im/post/5dbd5144e51d456eec1830af)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e03) ContentProvider \u542f\u52a8](https://juejin.im/post/5dbe8e6ce51d456f0006634a)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516b) ActivityManagerService](https://juejin.im/post/5dc4339c5188254e7a15585c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e5d) WindowManager](https://juejin.im/post/5dc7d729f265da4cf85d7feb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u5341) WindowManagerService \u7684\u7a97\u53e3\u7ba1\u7406](https://juejin.im/post/5dcab476f265da4d0a68e3ab)\n\n## \u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790\n\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e00) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d89e9c051882509662c5620)\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e8c) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5d8c83836fb9a04dec52f19d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Rxjava2 \u7684\u57fa\u672c\u6267\u884c\u6d41\u7a0b\u3001\u7ebf\u7a0b\u5207\u6362\u539f\u7406](https://juejin.im/post/5d9b489251882560e87e620e)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e00) \u540c\u6b65\u3001\u5f02\u6b65\u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d9ef57c51882514316fe33a)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e8c) \u62e6\u622a\u5668\u7684\u9b45\u529b](https://juejin.im/post/5da306965188252ba420a15d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e09) \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5da5dcd551882544432558f8)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Retrofit \u7f51\u7edc\u8bf7\u6c42\uff0c\u5305\u542b RxJava + Retrofit + OKhttp \u8bf7\u6c42\u8bb2\u89e3](https://juejin.im/post/5da802d051882508866e9463)\n\n## \u6027\u80fd\u4f18\u5316\u7cfb\u5217\n\n- [\u6027\u80fd\u4f18\u5316(\u4e00)APP \u542f\u52a8\u4f18\u5316\uff08\u4e0d\u6562\u8bf4\u79d2\u5f00\uff0c\u4f46\u662f\u6700\u7ec8\u4f18\u5316\u5b8c\u771f\u4e0d\u5230 1s\uff09](https://juejin.im/post/5cc19374e51d456e781f2036)\n- [\u6027\u80fd\u4f18\u5316(\u4e8c) UI \u7ed8\u5236\u4f18\u5316](https://juejin.im/post/5cc2dfc7e51d456e845b4260)\n- [\u6027\u80fd\u4f18\u5316(\u4e09)\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0,\u81f3\u5c11\u89e3\u51b3 APP \u4e2d 90 % \u7684\u5185\u5b58\u5f02\u5e38\u95ee\u9898](https://juejin.im/post/5cd82a3ee51d456e781f20ce)\n- [\u6027\u80fd\u4f18\u5316(\u56db) ubuntu \u5b8c\u7f8e\u7f16\u8bd1 libjpeg \u56fe\u50cf\u538b\u7f29\u5e93\uff0c\u5ab2\u7f8e\u5fae\u4fe1\u56fe\u7247\u538b\u7f29\u7b97\u6cd5](https://juejin.im/post/5ce15d0ce51d45106e5e6dac)\n- [\u6027\u80fd\u4f18\u5316 (\u4e94) \u957f\u56fe\u4f18\u5316\uff0c\u4eff\u5fae\u535a\u52a0\u8f7d\u957f\u56fe\u65b9\u5f0f](https://juejin.im/post/5ce96da06fb9a07ee4633f50)\n- [\u6027\u80fd\u4f18\u5316 (\u516d) \u8001\u677f\u95ee\u4f60\u54b1\u4eec APP \u8017\u7535\u91cf\uff0c\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u80fd\u77e5\u9053\u8fd8\u80fd\u505a\u51fa\u5bf9\u5e94\u4f18\u5316\u3002](https://juejin.im/post/5ce9088f6fb9a07ee4633ef3)\n- [\u6027\u80fd\u4f18\u5316 (\u4e03) APK \u52a0\u56fa\u4e4b Dex \u52a0\u89e3\u5bc6\uff0c\u53cd\u7f16\u8bd1\u90fd\u770b\u4e0d\u5230\u9879\u76ee\u4e3b\u8981\u4ee3\u7801\u3002](https://juejin.im/post/5cf3ee295188256aa76bb1e1)\n- [\u6027\u80fd\u4f18\u5316 (\u516b) APK \u52a0\u56fa\u4e4b\u52a8\u6001\u66ff\u6362 Application](https://juejin.im/post/5cf69d30f265da1b897abd53)\n- [\u6027\u80fd\u4f18\u5316 (\u4e5d) APP \u7a33\u5b9a\u6027\u4e4b\u70ed\u4fee\u590d\u539f\u7406\u63a2\u7d22](https://juejin.im/post/5cfce989f265da1b6c5f6991)\n- [\u6027\u80fd\u4f18\u5316 (\u5341) APP \u6301\u7eed\u8fd0\u884c\u4e4b\u8fdb\u7a0b\u4fdd\u6d3b\u5b9e\u73b0](https://juejin.im/post/5cffe4d4f265da1b695d55d4)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e00) ProGuard \u5bf9\u4ee3\u7801\u548c\u8d44\u6e90\u538b\u7f29](https://juejin.im/post/5d05dab06fb9a07ea9446e21)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e8c) APK \u6781\u9650\u538b\u7f29(\u8d44\u6e90\u8d8a\u591a,\u6548\u679c\u8d8a\u663e\u8457)](https://juejin.im/post/5d0627f7f265da1bd4247e76)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e09) \u6709\u4e86 breakpad , native \u5d29\u6e83\u518d\u4e5f\u4e0d\u6015\u4e86](https://juejin.im/post/5d811f82518825446d0d15e1)\n- [\u9762\u8bd5\u5b98: \u8bf4\u4e00\u4e0b\u4f60\u505a\u8fc7\u54ea\u4e9b\u6027\u80fd\u4f18\u5316?](https://juejin.im/post/5e7f12ba518825736d2780a0)\n\n## \u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217\n\n- [\u79fb\u52a8\u67b6\u6784 (\u4e00) \u67b6\u6784\u7b2c\u4e00\u6b65\uff0c\u5b66\u4f1a\u753b\u5404\u79cd UML \u56fe\u3002](https://juejin.im/post/5d2e048cf265da1b9163c7c8)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e8c) Android \u4e2d Handler \u67b6\u6784\u5206\u6790\uff0c\u5e76\u5b9e\u73b0\u81ea\u5df1\u7b80\u6613\u7248\u672c Handler \u6846\u67b6](https://juejin.im/post/5d30b4a8f265da1b855c8f45)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e09) AMS \u6e90\u7801\u5206\u6790](https://juejin.im/post/5d3463b4e51d45109725ff47)\n- [\u79fb\u52a8\u67b6\u6784 (\u56db) EventBus 3.1.1 \u6e90\u7801\u5206\u6790\u53ca\u5b9e\u73b0\u81ea\u5df1\u7684\u8f7b\u91cf\u7ea7 EventBus \u6846\u67b6\uff0c\u6839\u636e TAG \u53d1\u9001\u63a5\u6536\u4e8b\u4ef6\u3002](https://juejin.im/post/5d3c5b965188252c9c52beba)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e94) \u4ec5\u4ec5\u5bf9 Java Bean \u7684\u64cd\u4f5c\uff0c\u5c31\u80fd\u5b8c\u6210\u5bf9\u6570\u636e\u6301\u4e45\u5316\u3002](https://juejin.im/post/5d49a6c9518825056564a074)\n- [\u79fb\u52a8\u67b6\u6784 (\u516d) \u8f7b\u91cf\u7ea7\u8fdb\u7a0b\u95f4\u901a\u4fe1\u6846\u67b6\u8bbe\u8ba1](https://juejin.im/post/5d4fe70d518825168d37a740)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e03) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u7ec4\u4ef6\u5316\u6846\u67b6\u6a21\u578b](https://juejin.im/post/5d5bcb85f265da03e369839d)\n- [\u79fb\u52a8\u67b6\u6784 (\u516b) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u52a8\u6001\u5316\u52a0\u8f7d\u63d2\u4ef6\u6280\u672f\u6a21\u578b\u5b9e\u73b0](https://juejin.im/post/5d6246d36fb9a06b0f23ed6e)\n\n## \u8bbe\u8ba1\u6a21\u5f0f\n\n- [\u901a\u8fc7\u4ee3\u7801\u793a\u4f8b\u6765\u5b66\u4e60\u9762\u5411\u5bf9\u8c61\u516d\u5927\u539f\u5219](https://juejin.im/post/5d669bfc6fb9a06b1b19d25e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e00) \u901a\u8fc7\u7406\u8bba + \u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d\u5355\u4f8b\u6a21\u5f0f\u6765\u5b66\u4e60\u5355\u4f8b](https://juejin.im/post/5d6a8121e51d4561e6237193)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e8c ) \u7ed3\u5408\u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d Builder \u6765\u5b66\u4e60\u5efa\u9020\u8005\u6a21\u5f0f](https://juejin.im/post/5d6bcd0ee51d4561d41d2e36)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e09 ) \u539f\u578b\u6a21\u5f0f](https://juejin.im/post/5d6e7eaa5188250d9432b463)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u56db ) \u5de5\u5382\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7125d5f265da03d7283ce9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e94) \u62bd\u8c61\u5de5\u5382\u6a21\u5f0f](https://juejin.im/post/5d71278ef265da03d063c265)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516d) \u7b56\u7565\u6a21\u5f0f](https://juejin.im/post/5d7273abf265da03b31bf1ec)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e03) \u72b6\u6001\u6a21\u5f0f](https://juejin.im/post/5d738f40e51d4561c41fb8a6)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516b) \u8d23\u4efb\u94fe\u6a21\u5f0f](https://juejin.im/post/5d749589f265da03d871e36e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e5d) \u89c2\u5bdf\u8005\u6a21\u5f0f](https://juejin.im/post/5d7501f36fb9a06ac93cf457)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u5341 ) \u5907\u5fd8\u5f55\u6a21\u5f0f](https://juejin.im/post/5d77ab1de51d4561c83e7cd9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e00) \u8fed\u4ee3\u5668\u6a21\u5f0f](https://juejin.im/post/5d791e176fb9a06ae61ae3cc)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e8c) \u6a21\u677f\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7a759fe51d4561c02a25db)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e09) \u8bbf\u95ee\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b24b1e51d4561d41d2e96)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u56db) \u4e2d\u4ecb\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b63b3e51d4561ea1a94ed)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e94) \u4ee3\u7406\u6a21\u5f0f](https://juejin.im/post/5d7c6bc7f265da03f3338254)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516d) \u7ec4\u5408\u6a21\u5f0f](https://juejin.im/post/5d7cbda7f265da03d2116f64)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e03) \u9002\u914d\u5668\u3001\u88c5\u9970\u3001\u4eab\u5143\u6a21\u5f0f](https://juejin.im/post/5d7dfff751882539aa5ad79c)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516b) \u5916\u89c2\u3001\u6865\u63a5\u6a21\u5f0f](https://juejin.im/post/5d7e01f4f265da03b5747aac)\n\n## \u9ad8\u7ea7 UI \u7cfb\u5217\n\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e00) View \u7684\u57fa\u7840\u77e5\u8bc6\u4f60\u5fc5\u987b\u77e5\u9053](https://juejin.im/post/5dcff9d3f265da0bd20af0da)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e8c) \u6df1\u5165\u7406\u89e3 Android 8.0 View \u89e6\u6478\u4e8b\u4ef6\u5206\u53d1\u673a\u5236](https://juejin.im/post/5dd7a4796fb9a07a8f412d17)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e09) \u7406\u89e3 View \u5de5\u4f5c\u539f\u7406\u5e76\u5e26\u4f60\u5165\u81ea\u5b9a\u4e49 View \u95e8](https://juejin.im/post/5ddff234518825793218d2e4)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u56db) Paint \u6e32\u67d3/\u6ee4\u955c/xfermode \u4f7f\u7528](https://juejin.im/post/5de36c43f265da05de5881e8)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e94) \u770b\u5b8c\u8be5\u7bc7\u6587\u7ae0 Canvas \u4f60\u5e94\u8be5\u4f1a\u4e86](https://juejin.im/post/5de514fcf265da060115e02d)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u516d) PathMeasure \u5236\u4f5c\u8def\u5f84\u52a8\u753b](https://juejin.im/post/5de789dce51d4557e76a4a39)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e03) SVG \u57fa\u7840\u4f7f\u7528 + \u7ed8\u5236\u4e2d\u56fd\u5730\u56fe](https://juejin.im/post/5deb6d41e51d4558052f16ac)\n\n## \u97f3\u89c6\u9891\n\n- [\u97f3\u89c6\u9891\u4e4b\u8fdb\u7a0b\u95f4\u4f20\u9012 YUV \u683c\u5f0f\u89c6\u9891\u6d41\uff0c\u89e3\u51b3\u4e0d\u80fd\u540c\u65f6\u8c03\u7528 Camera \u95ee\u9898](https://juejin.im/post/5cf345ddf265da1b8c19731a)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e00) C \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5df8c917f265da339772a5d1)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e8c) C++ \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5e1347775188253a6c3966fd)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e09) JNI \u4ece\u5165\u95e8\u5230\u638c\u63e1](https://juejin.im/post/5e1606e0f265da5d2d0ffbdb)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u56db) \u4ea4\u53c9\u7f16\u8bd1\u52a8\u6001\u5e93\u3001\u9759\u6001\u5e93\u7684\u5165\u95e8\u5b66\u4e60](https://juejin.im/post/5e1ad6806fb9a02ff076e103)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e94) Shell \u811a\u672c\u5165\u95e8](https://juejin.im/post/5e1c0a4ce51d451c8771c487)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516d) FFmpeg 4.2.2 \u4ea4\u53c9\u7f16\u8bd1](https://juejin.im/post/5e1eace16fb9a02fec66474e)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e03) \u638c\u63e1\u97f3\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 AudioTrack\u3001OpenSL ES \u6e32\u67d3 PCM \u6570\u636e](https://juejin.im/post/5e3fcc5bf265da57685db2a9)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516b) \u638c\u63e1\u89c6\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 OpenGL ES 2.0 \u6e32\u67d3 YUV \u6570\u636e](https://juejin.im/post/5e4581476fb9a07cd80f15e0)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e5d) \u4ece 0 ~ 1 \u5f00\u53d1\u4e00\u6b3e Android \u7aef\u64ad\u653e\u5668(\u652f\u6301\u591a\u534f\u8bae\u7f51\u7edc\u62c9\u6d41/\u672c\u5730\u6587\u4ef6)](https://juejin.im/post/5e495ec1e51d452713551017)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341) \u57fa\u4e8e Nginx \u642d\u5efa(rtmp\u3001http)\u76f4\u64ad\u670d\u52a1\u5668](https://juejin.im/post/5e4ec66c5188254967067502)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e00) Android \u7aef\u5b9e\u73b0 rtmp \u63a8\u6d41](https://juejin.im/post/5e5d17276fb9a07cc01a29d3)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e8c) \u57fa\u4e8e FFmpeg + OpenSLES \u5b9e\u73b0\u97f3\u9891\u4e07\u80fd\u64ad\u653e\u5668](https://juejin.im/post/5eb1880be51d454de7772152)\n- [WebRTC \u5b66\u4e60\u8bb0\u5f55 (\u4e00) \u4e91\u670d\u52a1\u5668\u642d\u5efa AppRTC \u73af\u5883](https://juejin.im/post/5e8f4a606fb9a03c7a331bd3)\n\n## \u5f00\u6e90\u9879\u76ee\n\n- [\u70ed\u4fee\u590d DexEncryptionDecryption](https://github.com/yangkun19921001/DexEncryptionDecryption)\n- [\u56fe\u7247\u538b\u7f29 LIBJPEG_SAMPLE](https://github.com/yangkun19921001/LIBJPEG_SAMPLE)\n- [\u8fdb\u7a0b\u4fdd\u6d3b KeepAlive](https://github.com/yangkun19921001/KeepAlive)\n- [Java/Native \u5f02\u5e38\u6355\u83b7 YKCrash](https://github.com/yangkun19921001/YKCrash)\n- [\u63d2\u4ef6\u5316 YKPluginAPK](https://github.com/yangkun19921001/YKPluginAPK)\n- [YUV \u64ad\u653e YUVPlay](https://github.com/yangkun19921001/YUVPlay)\n- [\u9632\u5fae\u535a\u957f\u56fe\u52a0\u8f7d long_picture_view](https://github.com/yangkun19921001/long_picture_view)\n- [\u8fdb\u7a0b\u95f4\u901a\u4fe1 YKProBus](https://github.com/yangkun19921001/YKProBus)\n- [EventBus YEventBus](https://github.com/yangkun19921001/YEventBus)\n- [\u8fdb\u7a0b\u95f4\u5927\u6570\u636e\u4f20\u8f93 MemoryFileWriteBytesYUV](https://github.com/yangkun19921001/MemoryFileWriteBytesYUV)\n- [ Kotlin GitHub App](https://juejin.im/post/5dc294d5f265da4d4434afc9)\n- [Android rtmp\u63a8\u6d41\u3001\u62c9\u6d41](https://github.com/yangkun19921001/NDK_AV_SAMPLE/tree/master/ykav_common/src/main/cpp)\n\n## \u82f1\u8bed\n\n\u5f85\u66f4\u65b0...\n\n## \u5173\u4e8e\u6211\n\n- Email: yang1001yk@gmail.com\n- [\u4e2a\u4eba\u535a\u5ba2](https://www.devyk.top/)\n- [\u6398\u91d1](https://juejin.im/user/578259398ac2470061f3a3fb)\n- [GitHub](https://github.com/yangkun19921001)\n\n\u626b\u7801\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\uff0c\u8ba9\u6211\u4eec\u79bb\u5f97\u66f4\u8fdb\u4e00\u4e9b!\n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200328235020.jpg)\n\n\n\n## \u8d5e\u8d4f\n\n\u5982\u679c\u8fd9\u4e2a\u9762\u8bd5\u9898\u5e93\u5bf9\u4f60\u5f88\u6709\u5e2e\u52a9\uff0c\u53ef\u4ee5\u626b\u63cf\u4e0b\u65b9\u4e8c\u7ef4\u7801\u7ed9\u4f5c\u8005\u4e00\u70b9\u9f13\u52b1\u3002\u91d1\u989d\u968f\u610f, \u8c22\u8c22! \n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200330103229.png)\n\n\n\n\n\n\n\n"
 },
 {
  "repo": "abidrahmank/OpenCV2-Python-Tutorials",
  "language": "Python",
  "readme_contents": "OpenCV2-Python-Guide\n====================\n\nThis repo contains tutorials on OpenCV-Python library using new cv2 interface\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n=======================================================================\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\nPlease try the examples with OpenCV 3x before sending any bug reports\n\nData files\n-----------\n\nThe input data used in these tutorials are given in **Data** folder\n\nOnline\n---------\n\n* **For official tutorials, please visit : http://docs.opencv.org/trunk/doc/py_tutorials/py_tutorials.html**\n* https://opencv-python-tutroals.readthedocs.org/en/latest/index.html - This is only for checking. May contain lots of errors, please stick to the official tutorials.\n\nOffline\n---------\nTo build docs from source,\n* Install sphinx\n* Download/Clone this repo and navigate to the base folder\n* run command : `make html` , html docs will be available in **build/html/** folder\n"
 },
 {
  "repo": "emgucv/emgucv",
  "language": "C#",
  "readme_contents": "==================================================================\n\nA cross platform .Net wrapper for the OpenCV image-processing library. Allows OpenCV functions to be called from .NET compatible languages. The wrapper can be compiled by Visual Studio, Xamarin Studio and Unity, it can run on Windows, Linux, Mac OS, iOS and Android.\n\nPlease visit our project webpage for more information:\nhttp://www.emgu.com/wiki/index.php/Main_Page\n\nBuild instructions can be found here:\nhttp://www.emgu.com/wiki/index.php/Download_And_Installation#Building_from_Git\n"
 },
 {
  "repo": "atduskgreg/opencv-processing",
  "language": "Java",
  "readme_contents": "## OpenCV for Processing\n\n**A Processing library for the [OpenCV](http://opencv.org/) computer vision library.**\n\nOpenCV for Processing is based on OpenCV's official Java bindings. It attempts to provide convenient wrappers for common OpenCV functions that are friendly to beginners and feel familiar to the Processing environment.\n\nSee the included examples below for an overview of what's possible and links to the relevant example code. Complete documentation is available here:\n\n**[OpenCV for Processing reference](http://atduskgreg.github.io/opencv-processing/reference/)**\n\nOpenCV for Processing is based on the officially supported [OpenCV Java API](http://docs.opencv.org/java/), currently at version 2.4.5. In addition to using the wrapped functionality, you can import OpenCV modules and use any of its documented functions: [OpenCV javadocs](http://docs.opencv.org/java/). See the advanced examples (HistogramSkinDetection, DepthFromStereo, and Marker Detection) below for details. (This style of API was inspired by Kyle McDonald's [ofxCv addon](https://github.com/kylemcdonald/ofxCv) for OpenFrameworks.) \n\nContributions welcome.\n\n### Installing\n\nOpenCV for Processing currently supports Mac OSX, 32-bit and 64-bit Windows, 32- and 64-bit Linux. Android support is hopefully coming soon (pull requests welcome).\n\n_NB: When running on the Mac, make sure you have Processing set to 64-bit mode in the Preferences_\n\nSee [here](https://github.com/atduskgreg/opencv-processing/releases) for the latest release.\n\n### Examples\n\n#### LiveCamTest\n\nAccess a live camera and do image processing on the result, specifically face detection.\n\nCode: [LiveCamTest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/LiveCamTest/LiveCamTest.pde)\n\n_Note: There's a bug that prevents live camera access in current versions of Processing 2.0 on machines with a Retina display._\n\n#### FaceDetection\n\nDetect faces in images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8634017624/\" title=\"Screen Shot 2013-04-08 at 1.22.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8543/8634017624_35f7ef05ce.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-08 at 1.22.18 PM\"></a>\n\nCode: [FaceDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FaceDetection/FaceDetection.pde)\n\n#### BrightnessContrast\n\nAdjust the brightness and contrast of color and gray images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9155239258/\" title=\"brightness and contrast by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2841/9155239258_41a7df36c6.jpg\" width=\"500\" height=\"358\" alt=\"brightness and contrast\"></a>\n\nCode: [BrightnessContrast.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightnessContrast/BrightnessContrast.pde)\n\n#### FilterImages\n\nBasic filtering operations on images: threshold, blur, and adaptive thresholds.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8643666252/\" title=\"Screen Shot 2013-04-12 at 1.42.30 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8240/8643666252_be0da1c751.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-12 at 1.42.30 PM\"></a>\n\nCode: [FilterImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FilterImages/FilterImages.pde)\n\n#### FindContours\n\nFind contours in images and calculate polygon approximations of the contours (i.e., the closest straight line that fits the contour).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9024663015/\" title=\"contours with polygon approximations by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3719/9024663015_f419b117b1.jpg\" width=\"500\" height=\"208\" alt=\"contours with polygon approximations\"></a>\n\nCode: [FindContours.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindContours/FindContours.pde)\n\n#### FindEdges\n\nThree different edge-detection techniques: Canny, Scharr, and Sobel.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8635989723/\" title=\"Screen Shot 2013-04-10 at 2.03.59 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8109/8635989723_170b69dca0.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-10 at 2.03.59 AM\"></a>\n\nCode: [FindEdges.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindEdges/FindEdges.pde)\n\n#### FindLines\n\nFind straight lines in the image using Hough line detection.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9263329608/\" title=\"Hough line detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3781/9263329608_735ce228bb.jpg\" width=\"486\" height=\"500\" alt=\"Hough line detection\"></a>\n\nCode: [HoughLineDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HoughLineDetection/HoughLineDetection.pde)\n\n#### BrightestPoint\n\nFind the brightest point in an image.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9199572469/\" title=\"finding the brightest point by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7407/9199572469_4a25c83062.jpg\" width=\"500\" height=\"366\" alt=\"finding the brightest point\"></a>\n\nCode: [BrightestPoint.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightestPoint/BrightestPoint.pde)\n\n#### RegionOfInterest\n\nAssign a sub-section (or Region of Interest) of the image to be processed. Video of this example in action here: [Region of Interest demo on Vimeo](https://vimeo.com/69009345).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9077805277/\" title=\"region of interest by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3795/9077805277_084d87a3a5.jpg\" width=\"500\" height=\"358\" alt=\"region of interest\"></a>\n\nCode: [RegionOfInterest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/RegionOfInterest/RegionOfInterest.pde)\n\n#### ImageDiff\n\nFind the difference between two images in order to subtract the background or detect a new object in a scene.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8640005799/\" title=\"Screen Shot 2013-04-11 at 2.10.35 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8114/8640005799_44b48e01ae.jpg\" width=\"500\" height=\"409\" alt=\"Screen Shot 2013-04-11 at 2.10.35 PM\"></a>\n\nCode: [ImageDiff.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ImageDiff/ImageDiff.pde)\n\n#### DilationAndErosion\n\nThin (erode) and expand (dilate) an image in order to close holes. These are known as \"morphological\" operations.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9075875005/\" title=\"dilation and erosion by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2818/9075875005_8f7cde3ed7.jpg\" width=\"496\" height=\"500\" alt=\"dilation and erosion\"></a>\n\nCode: [DilationAndErosion.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DilationAndErosion/DilationAndErosion.pde)\n\n#### BackgroundSubtraction\n\nDetect moving objects in a scene. Use background subtraction to distinguish background from foreground and contour tracking to track the foreground objects.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9220336868/\" title=\"Background Subtraction by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7292/9220336868_bed3498528.jpg\" width=\"500\" height=\"369\" alt=\"Background Subtraction\"></a>\n\nCode: [BackgroundSubtraction.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BackgroundSubtraction/BackgroundSubtraction.pde)\n\n\n#### WorkingWithColorImages\n\nDemonstration of what you can do color images in OpenCV (threshold, blur, etc) and what you can't (lots of other operations).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9136033334/\" title=\"color operations: threshold and blur by atduskgreg, on Flickr\"><img src=\"http://farm6.staticflickr.com/5451/9136033334_3345dfa057.jpg\" width=\"500\" height=\"358\" alt=\"color operations: threshold and blur\"></a>\n\nCode: [WorkingWithColorImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WorkingWithColorImages/WorkingWithColorImages.pde)\n\n#### ColorChannels ####\n\nSeparate a color image into red, green, blue or hue, saturation, and value channels in order to work with the channels individually.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9246157901/\" title=\"ColorChannels by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2847/9246157901_08ccf19e7d.jpg\" width=\"488\" height=\"500\" alt=\"ColorChannels\"></a>\n\nCode: [ColorChannels](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ColorChannels/ColorChannels.pde)\n\n#### FindHistogram\n\nDemonstrates use of the findHistogram() function and the Histogram class to get and draw histograms for grayscale and individual color channels.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9174190443/\" title=\"gray, red, green, blue histograms by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7287/9174190443_224a740ce8.jpg\" width=\"500\" height=\"355\" alt=\"gray, red, green, blue histograms\"></a>\n\nCode: [FindHistogram.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindHistogram/FindHistogram.pde)\n\n#### HueRangeSelection\n\nDetect objects based on their color. Demonstrates the use of HSV color space as well as range-based image filtering.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9193745547/\" title=\"Hue-based color detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3799/9193745547_8f09e55a39.jpg\" width=\"500\" height=\"397\" alt=\"Hue-based color detection\"></a>\n\nCode: [HueRangeSelection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HueRangeSelection/HueRangeSelection.pde)\n\n#### CalibrationDemo (in progress)\n\nAn example of the process involved in calibrating a camera. Currently only detects the corners in a chessboard pattern.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8706849024/\" title=\"Screen Shot 2013-05-04 at 2.03.23 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8267/8706849024_f2d938ec51.jpg\" width=\"500\" height=\"382\" alt=\"Screen Shot 2013-05-04 at 2.03.23 AM\"></a>\n\nCode: [CalibrationDemo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/CalibrationDemo/CalibrationDemo.pde)\n\n#### HistogramSkinDetection\n\nA more advanced example. Detecting skin in an image based on colors in a region of color space. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8707167599/\" title=\"Screen Shot 2013-05-04 at 2.25.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8135/8707167599_d38fbdfe30.jpg\" width=\"500\" height=\"171\" alt=\"Screen Shot 2013-05-04 at 2.25.18 PM\"></a>\n\nCode: [HistogramSkinDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HistogramSkinDetection/HistogramSkinDetection.pde)\n\n#### DepthFromStereo\n\nAn advanced example. Calculates depth information from a pair of stereo images. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642493130/\" title=\"Screen Shot 2013-04-12 at 2.27.30 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8260/8642493130_f99dd76f3d.jpg\" width=\"500\" height=\"404\" alt=\"Screen Shot 2013-04-12 at 2.27.30 AM\"></a>\n\nCode: [DepthFromStereo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DepthFromStereo/DepthFromStereo.pde)\n\n#### WarpPerspective (in progress)\n\nUn-distort an object that's in perspective. Coming to the real API soon.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9279197332/\" title=\"Warp Perspective by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2861/9279197332_ca6beb3760.jpg\" width=\"500\" height=\"416\" alt=\"Warp Perspective\"></a>\n\nCode: [WarpPerspective.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WarpPerspective/WarpPerspective.pde)\n\n#### MarkerDetection\n\nAn in-depth advanced example. Detect a CV marker in an image, warp perspective, and detect the number stored in the marker. Many steps in the code. Uses many un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642309968/\" title=\"Screen Shot 2013-04-12 at 12.20.17 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8522/8642309968_257e397db2.jpg\" width=\"500\" height=\"225\" alt=\"Screen Shot 2013-04-12 at 12.20.17 AM\"></a>\n\nCode: [MarkerDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/MarkerDetection/MarkerDetection.pde)\n\n#### MorphologyOperations\n\nOpen and close an image, or do more complicated morphological transformations.\n\n<a href=\"https://flic.kr/p/tazj7r\" title=\"Morphology operations\"><img src=\"https://farm6.staticflickr.com/5340/17829980821_1734e8bab8_z_d.jpg\" width=\"640\" height=\"393\" alt=\"Morphology operations\"></a>\n\nCode: [MorphologyOperations.pde](examples/MorphologyOperations/MorphologyOperations.pde)\n"
 },
 {
  "repo": "RubensZimbres/Repo-2017",
  "language": "Python",
  "readme_contents": "# Python Codes in Data Science\n\nCodes in NLP, Deep Learning, Reinforcement Learning and Artificial Intelligence\n\n<b> Welcome to my GitHub repo. </b>\n\nI am a Data Scientist and I code in R, Python and Wolfram Mathematica. Here you will find some Machine Learning, Deep Learning, Natural Language Processing and Artificial Intelligence models I developed.\n\n<b> Outputs of the models can be seen at my portfolio: </b> https://drive.google.com/file/d/0B0RLknmL54khdjRQWVBKeTVxSHM/view?usp=sharing\n\n----------------\nKeras version used in models: keras==1.1.0\n\n<b> Autoencoder for Audio  </b> is a model where I compressed an audio file and used Autoencoder to reconstruct the audio file, for use in phoneme classification.\n\n<b> Collaborative Filtering  </b> is a Recommender System where the algorithm predicts a movie review based on genre of movie and similarity among people who watched the same movie.\n\n<b> Convolutional NN Lasagne  </b> is a Convolutional Neural Network model in Lasagne to solve the MNIST task.\n\n<b> Ensembled Machine Learning </b> is a .py file where 7 Machine Learning algorithms are used in a classification task with 3 classes and all possible hyperparameters of each algorithm are adjusted. Iris dataset of scikit-learn.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Ensembled.MachineLearning.png?raw=true>\n</p>\n\n<b> GAN Generative Adversarial  </b> are models of Generative Adversarial Neural Networks.\n\n<b> Hyperparameter Tuning RL  </b> is a model where hyperparameters of Neural Networks are adjusted via Reinforcement Learning. According to a reward, hyperparameter tuning (environment) is changed through a policy (mechanization of knowledge) using the Boston Dataset. Hyperparameters tuned are: learning rate, epochs, decay, momentum, number of hidden layers and nodes and initial weights.\n\n<b> Keras Regularization L2  </b> is a Neural Network model for regression made with Keras where a L2 regularization was applied to prevent overfitting.\n\n<b> Lasagne Neural Nets Regression  </b> is a Neural Network model based in Theano and Lasagne, that makes a linear regression with a continuous target variable and reaches 99.4% accuracy. It uses the DadosTeseLogit.csv sample file.\n\n<b> Lasagne Neural Nets + Weights  </b> is a Neural Network model based in Theano and Lasagne, where is possible to visualize weights between X1 and X2 to hidden layer. Can also be adapted to visualize weights between hidden layer and output. It uses the DadosTeseLogit.csv sample file.\n\n<b> Multinomial Regression  </b> is a regression model where target variable has 3 classes.\n\n<b> Neural Networks for Regression  </b> shows multiple solutions for a regression problem, solved with sklearn, Keras, Theano and Lasagne. It uses the Boston dataset sample file from sklearn and reaches more than 98% accuracy.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/HiddenLayers.jpg?raw=true>\n</p>\n\n<b> NLP + Naive Bayes Classifier  </b> is a model where movie reviews were labeled as positive and negative and the algorithm then classifies a totally new set of reviews using Logistic Regression, Decision Trees and Naive Bayes, reaching an accuracy of 92%.\n\n<b> NLP Anger Analysis  </b> is a Doc2Vec model associated with Word2Vec model to analyze level of anger using synonyms in consumer complaints of a U.S. retailer in Facebook posts.\n\n<b> NLP Consumer Complaint  </b> is a model where Facebook posts of a U.S. computer retailer were scraped, tokenized, lemmatized and applied Word2Vec. After that, t-SNE and Latent Dirichlet Allocation were developed in order to classify the arguments and weights of each keyword used by a consumer in his complaint. The code also analyzes frequency of words in 100 posts.\n\n<b> NLP Convolutional Neural Network </b> is a Convolutional Neural Network for Text in order to classify movie reviews.\n\n<b> NLP Doc2Vec  </b> is a Natural Language Procesing file where cosine similarity among phrases is measured through Doc2Vec.\n\n<b> NLP Document Classification  </b> is a code for Document Classification according to Latent Dirichlet Allocation.\n\n<b> NLP Facebook Analysis  </b> analyzes Facebook posts regarding Word Frequency and Topic Modelling using LDA.\n\n<b> NLP Facebook Scrap  </b> is a Python code for scraping data from Facebook.\n\n<b> NLP - Latent Dirichlet Allocation  </b> is a Natural Language Processing model where a Wikipedia page on Statistical Inference is classified regarding topics, using Latent Dirichlet Allocation with Gensim, NLTK, t-SNE and K-Means.\n\n<b> NLP Probabilistic ANN  </b> is a Natural Language Processing model where sentences are vectorized by Gensim and a probabilistic Neural Network model is deveoped using Gensim, for sentiment analysis.\n\n<b> NLP Semantic Doc2Vec + Neural Network  </b> is a model where positive and negative movie reviews were extracted and semantically classified with NLTK and BeautifulSoup, then labeled as positive or negative. Text was then used as an input for the Neural Network model training. After training, new sentences are entered in the Keras Neural Network model and then classified. It uses the zip file.\n\n<b> NLP Sentiment Positive  </b> is a model that identifies website content as positive, neutral or negative using BeautifulSoup and NLTK libraries, plotting the results.\n\n<b> NLP Twitter Analysis ID #  </b> is a model that extracts posts from Twitter based in ID of user or Hashtag.\n\n<b> NLP Twitter Scrap  </b> is a model that scraps Twitter data and shows the cleaned text as output.\n\n<b> NLP Twitter Streaming  </b> is a model of analysis of real-time data from Twitter (under development).\n\n<b> NLP Twitter Streaming Mood  </b> is a model where the evolution of mood Twitter posts is measured during a period of time.\n\n<b> NLP Wikipedia Summarization  </b> is a Python code that summarizes any given page in a few sentences.\n\n<b> NLP Word Frequency  </b> is a model that calculates the frequency of nouns, verbs, words in Facebook posts.\n\n<b> Probabilistic Neural Network  </b> is a Probabilistic Neural Network for Time Series Prediction.\n\n<b> REAL-TIME Twitter Analysis  </b> is a model where Twitter streaming is extracted, words and sentences tokenized, word embeddings were created, topic modeling was made and classified using K-Means. Then, NLTK SentimentAnalyzer was used to classify each sentence of the streaming into positive, neutral or negative. Accumulated sum was used to generate the plot and the code loops each 1 second, collecting new tweets.\n\n<b> RESNET-2  </b> is a Deep Residual Neural Network.\n\n<b> ROC Curve Multiclass  </b> is a .py file where Naive Bayes was used to solve the IRIS Dataset task and ROC curve of different classes are plotted.\n\n<b> SQUEEZENET  </b> is a simplified version of the AlexNet.\n\n<b> Stacked Machine Learning  </b> is a .py notebook where t-SNE, Principal Components Analysis and Factor Analysis were applied to reduce dimensionality of data. Classification performances were measured after applying K-Means.\n\n<b> Support Vector Regression  </b> is a SVM model for non linear regression in an artificial dataset.\n\n<b> Text-to-Speech  </b> is a .py file where Python speaks any given text and saves it as an audio .wav file.\n\n<b> Time Series ARIMA </b>  is a ARIMA model to forecast time series, with an error margin of 0.2%.\n\n<b> Time Series Prediction with Neural Networks - Keras </b>  is a Neural Network model to forecast time series, using Keras with an adaptive learning rate depending upon derivative of loss.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/ARIMA.10Period.png?raw=true> \n</p>\n\n<b> Variational Autoencoder  </b> is a VAE made with Keras.\n\n<b> Web Crawler  </b> is a code that scraps data from different URLs of a hotel website.\n\n<b> t-SNE Dimensionality Reduction  </b> is a t-SNE model for dimensionality reduction which is compared to Principal Components Analysis regarding its discriminatory power.\n\n<b> t-SNE PCA + Neural Networks  </b> is a model that compares performance or Neural Networks made after t-SNE, PCA and K-Means.\n\n<b> t-SNE PCA LDA embeddings </b> is a model where t-SNE, Principal Components Analysis, Linear Discriminant Analysis and Random Forest embeddings are compared in a task to classify clusters of similar digits.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Doc2Vec.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/t_SNE_Lk.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/RESNET_Me.jpg?raw=true>\n</p>\n\n"
 },
 {
  "repo": "QianMo/OpenCV3-Intro-Book-Src",
  "language": "C++",
  "readme_contents": "\u300aOpenCV3\u7f16\u7a0b\u5165\u95e8\u300b\u4e66\u672c\u914d\u5957\u6e90\u4ee3\u7801\n==============================\n#### \u300aIntroduction to OpenCV3 Programming\u300bBook Source Code<br>\n\n![](http://img.blog.csdn.net/20150325155409850)  \n<br>\u672c\u4e66\u6709OpenCV2\u3001OpenCV3\u4e24\u5957\u72ec\u7acb\u7684\u4e66\u672c\u914d\u5957\u793a\u4f8b\u7a0b\u5e8f\u4f9b\u9009\u62e9\u4f7f\u7528\u3002\n<br>  \u67094\u4e2a\u90e8\u520611\u7ae0\uff0c\u5171\u670995\u4e2a\u4e3b\u7ebf\u793a\u4f8b\u7a0b\u5e8f\uff0c\u4e3a\u65b9\u4fbf\u5927\u5bb6\u67e5\u9605\u548c\u5b66\u4e60\uff0c\u603b\u7ed3\u6210\u5982\u4e0b\u3002\n# \u6b63\u6587\u90e8\u5206\u6e90\u4ee3\u7801\n## \u7b2c\u4e00\u90e8\u5206 \u5feb\u901f\u4e0a\u624bOpenCV\n\t\t1\tOpenCV\u73af\u5883\u914d\u7f6e\u7684\u6d4b\u8bd5\u7528\u4f8b\t1.3.8\n\t\t2\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e00\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u663e\u793a\t1.4.1\n\t\t3\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e8c\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u8150\u8680\t1.4.2\n\t\t4\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e09\u4e2a\u7a0b\u5e8f\uff1ablur\u56fe\u50cf\u6a21\u7cca\t1.4.3\n\t\t5\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u56db\u4e2a\u7a0b\u5e8f\uff1acanny\u8fb9\u7f18\u68c0\u6d4b\t1.4.4\n\t\t6\t\u8bfb\u53d6\u5e76\u64ad\u653e\u89c6\u9891\t1.5.1\n\t\t7\t\u8c03\u7528\u6444\u50cf\u5934\u91c7\u96c6\u56fe\u50cf\t1.5.2\n\t\t8\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5f69\u8272\u76ee\u6807\u8ddf\u8e2a\uff1aCamshift\t2.1.1\n\t\t9\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5149\u6d41\uff1aoptical flow\t2.1.2\n\t\t10\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u70b9\u8ffd\u8e2a\uff1alkdemo\t2.1.3\n\t\t11\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u4eba\u8138\u8bc6\u522b\uff1aobjectDetection\t2.1.4\n\t\t12\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u652f\u6301\u5411\u91cf\u673a\u5f15\u5bfc\t2.1.5\n\t\t13\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u5904\u7406\u7ebf\u6027\u4e0d\u53ef\u5206\u6570\u636e\t2.1.5\n\t\t14\tprintf\u51fd\u6570\u7684\u7528\u6cd5\u793a\u4f8b\t2.6.2\n\t\t15\t\u7528imwrite\u51fd\u6570\u751f\u6210png\u900f\u660e\u56fe\t3.1.8\n\t\t16\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u56fe\u50cf\u7684\u8f7d\u5165\u3001\u663e\u793a\u4e0e\u8f93\u51fa\t3.1.9\n\t\t17\t\u4e3a\u7a0b\u5e8f\u754c\u9762\u6dfb\u52a0\u6ed1\u52a8\u6761\t3.2.1\n\t\t18\t\u9f20\u6807\u64cd\u4f5c\u793a\u4f8b\t3.3\n## \u7b2c\u4e8c\u90e8\u5206 \u521d\u63a2core\u7ec4\u4ef6\t\n\t\t19\t\u57fa\u7840\u56fe\u50cf\u5bb9\u5668Mat\u7c7b\u7684\u4f7f\u7528\t4.1.7\n\t\t20\t\u7528OpenCV\u8fdb\u884c\u57fa\u672c\u7ed8\u56fe\t4.3\n\t\t21\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e00\uff1a\u7528\u6307\u9488\u8bbf\u95ee\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t22\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e8c\uff1a\u7528\u8fed\u4ee3\u5668\u64cd\u4f5c\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t23\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e09\uff1a\u52a8\u6001\u5730\u5740\u8ba1\u7b97\t5.1.5\u30015.1.6\n\t\t24\t\u904d\u5386\u56fe\u50cf\u4e2d\u50cf\u7d20\u768414\u79cd\u65b9\u6cd5\t5.1.6\n\t\t25\t\u521d\u7ea7\u56fe\u50cf\u6df7\u5408\t5.2.4\n\t\t26\t\u591a\u901a\u9053\u56fe\u50cf\u6df7\u5408\t5.3.3\n\t\t27\t\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u3001\u4eae\u5ea6\u503c\u8c03\u6574\t5.4.3\n\t\t28\t\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\t5.5.8\n\t\t29\tXML\u548cYAML\u6587\u4ef6\u7684\u5199\u5165\t5.6.3\n\t\t30\tXML\u548cYAML\u6587\u4ef6\u7684\u8bfb\u53d6\t5.6.4\u3001\n## \u7b2c\u4e09\u90e8\u5206 \u638c\u63e1imgproc\u7ec4\u4ef6\t\n\t\t31\t\u65b9\u6846\u6ee4\u6ce2\uff1aboxFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t32\t\u5747\u503c\u6ee4\u6ce2\uff1ablur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t33\t\u9ad8\u65af\u6ee4\u6ce2\uff1aGaussianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t34\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u7ebf\u6027\u6ee4\u6ce2\t6.1.12\n\t\t35\t\u4e2d\u503c\u6ee4\u6ce2\uff1amedianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t36\t\u53cc\u8fb9\u6ee4\u6ce2\uff1abilateralFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t37\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u6ee4\u6ce2\t6.2.5\n\t\t38\t\u81a8\u80c0\uff1adilate\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t39\t\u8150\u8680\uff1aerode\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t40\t\u7efc\u5408\u793a\u4f8b\uff1a\u8150\u8680\u4e0e\u81a8\u80c0\t6.3.6\n\t\t41\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u81a8\u80c0\t6.4.8\n\t\t42\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u8150\u8680\t6.4.8\n\t\t43\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u5f00\u8fd0\u7b97\t6.4.8\n\t\t44\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u95ed\u8fd0\u7b97\t6.4.8\n\t\t45\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u68af\u5ea6\t6.4.8\n\t\t46\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9876\u5e3d\u201d\t6.4.8\n\t\t47\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9ed1\u5e3d\u201d\t6.4.8\n\t\t48\t\u7efc\u5408\u793a\u4f8b\uff1a\u5f62\u6001\u5b66\u6ee4\u6ce2\t6.4.9\n\t\t49\t\u6f2b\u6c34\u586b\u5145\u7b97\u6cd5\uff1afloodFill\u51fd\u6570\t6.5.3\n\t\t50\t\u7efc\u5408\u793a\u4f8b\uff1a\u6f2b\u6c34\u586b\u5145\t6.5.4\n\t\t51\t\u5c3a\u5bf8\u8c03\u6574\uff1aresize()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.5\n\t\t52\t\u5411\u4e0a\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrUp()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t53\t\u5411\u4e0b\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrDown()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t54\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u91d1\u5b57\u5854\u4e0e\u56fe\u7247\u5c3a\u5bf8\u7f29\u653e\t6.6.7\n\t\t55\t\u793a\u4f8b\u7a0b\u5e8f\uff1a\u57fa\u672c\u9608\u503c\u64cd\u4f5c\t6.7.3\n\t\t56\tCanny\u8fb9\u7f18\u68c0\u6d4b\t7.1.2\n\t\t57\tSobel \u7b97\u5b50\u7684\u4f7f\u7528\t7.1.3\n\t\t58\tLaplacian\u7b97\u5b50\u7684\u4f7f\u7528\t7.1.4\n\t\t59\tScharr\u6ee4\u6ce2\u5668\t7.1.5\n\t\t60\t\u7efc\u5408\u793a\u4f8b\uff1a\u8fb9\u7f18\u68c0\u6d4b\t7.1.6\n\t\t61\t\u6807\u51c6\u970d\u592b\u53d8\u6362\uff1aHoughLines()\u51fd\u6570\u7684\u4f7f\u7528\t7.2.4\n\t\t62\t\u7d2f\u8ba1\u6982\u7387\u970d\u592b\u53d8\u6362\uff1aHoughLinesP()\u51fd\u6570\t7.2.5\n\t\t63\t\u970d\u592b\u5706\u53d8\u6362\uff1aHoughCircles()\u51fd\u6570\t7.2.8\n\t\t64\t\u7efc\u5408\u793a\u4f8b\uff1a\u970d\u592b\u53d8\u6362\t7.2.9\n\t\t65\t\u5b9e\u73b0\u91cd\u6620\u5c04\uff1aremap()\u51fd\u6570\t7.3.3\n\t\t66\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u5b9e\u73b0\u591a\u79cd\u91cd\u6620\u5c04\t7.3.4\n\t\t67\t\u4eff\u5c04\u53d8\u6362\t7.4.5\n\t\t68\t\u76f4\u65b9\u56fe\u5747\u8861\u5316\t7.5.3\n\t\t69\t\u8f6e\u5ed3\u67e5\u627e\t8.1.3\n\t\t70\t\u67e5\u627e\u5e76\u7ed8\u5236\u8f6e\u5ed3\t8.1.4\n\t\t71\t\u51f8\u5305\u68c0\u6d4b\u57fa\u7840\t8.2.3\n\t\t72\t\u5bfb\u627e\u548c\u7ed8\u5236\u7269\u4f53\u7684\u51f8\u5305\t8.2.4\n\t\t73\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u77e9\u5f62\u8fb9\u754c\t8.3.6\n\t\t74\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u5706\u5f62\u8fb9\u754c\t8.3.7\n\t\t75\t\u4f7f\u7528\u591a\u8fb9\u5f62\u5305\u56f4\u8f6e\u5ed3\t8.3.8\n\t\t76\t\u56fe\u50cf\u8f6e\u5ed3\u77e9\t8.4.4\n\t\t77\t\u5206\u6c34\u5cad\u7b97\u6cd5\u7684\u4f7f\u7528\t8.5.2\n\t\t78\t\u5b9e\u73b0\u56fe\u50cf\u4fee\u8865\t8.6.2\n\t\t79\tH-S\u4e8c\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.3\n\t\t80\t\u4e00\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.4\n\t\t81\tRGB\u4e09\u8272\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.5\n\t\t82\t\u76f4\u65b9\u56fe\u5bf9\u6bd4\t9.3.2\n\t\t83\t\u53cd\u5411\u6295\u5f71\t9.4.7\n\t\t84\t\u6a21\u677f\u5339\u914d\t9.5.3\n\t\t\n## \u7b2c\u56db\u90e8\u5206 \u6df1\u5165featrue2d\u7ec4\u4ef6\t\n\t\t85\t\u5b9e\u73b0Harris\u89d2\u70b9\u68c0\u6d4b\uff1acornerHarris()\u51fd\u6570\u7684\u4f7f\u7528\t10.1.4\n\t\t86\tharris\u89d2\u70b9\u68c0\u6d4b\u4e0e\u7ed8\u5236\t10.1.5\n\t\t87\tShi-Tomasi\u89d2\u70b9\u68c0\u6d4b\t10.2.3\n\t\t88\t\u4e9a\u50cf\u7d20\u7ea7\u89d2\u70b9\u68c0\u6d4b\t10.3.3\n\t\t89\tSURF\u7279\u5f81\u70b9\u68c0\u6d4b\t11.1.6\n\t\t90\tSURF\u7279\u5f81\u63d0\u53d6\t11.2.3\n\t\t91\t\u4f7f\u7528FLANN\u8fdb\u884c\u7279\u5f81\u70b9\u5339\u914d\t11.3.3\n\t\t92\tFLANN\u7ed3\u5408SURF\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u548c\u5339\u914d\t11.3.4\n\t\t93\tSIFT\u914d\u5408\u66b4\u529b\u5339\u914d\u8fdb\u884c\u5173\u952e\u70b9\u63cf\u8ff0\u548c\u63d0\u53d6\t11.3.5\n\t\t94\t\u5bfb\u627e\u5df2\u77e5\u7269\u4f53\t11.4.3\n\t\t95\t\u5229\u7528ORB\u7b97\u6cd5\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u4e0e\u5339\u914d\t11.5.4\n\n\n\n# \u989d\u5916\u7684\u9644\u8d60\u7a0b\u5e8f\u4e00\u89c8\n\n\u9664\u4e66\u672c\u672c\u8eab\u7684\u793a\u4f8b\u7a0b\u5e8f\u4e4b\u5916\uff0c\u989d\u5916\u9644\u52a0\u4e86OpenCV2\u7248\u768421\u4e2a\u76f8\u8f83\u4e8e\u6b63\u6587\u4e3b\u7ebf\u7684\u793a\u4f8b\u4ee3\u7801\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u7684\u7a0b\u5e8f\u6e90\u4ee3\u7801\u3002\n\u73b0\u5c06\u9644\u52a0\u768421\u4e2a\u793a\u4f8b\u7a0b\u5e8f\u5217\u4e3e\u5982\u4e0b\uff1a\n\n\t\t1\t\u968f\u673a\u56fe\u5f62\u548c\u6587\u5b57\u751f\u6210\u793a\u4f8b\uff08randomtext\uff09\n\t\t2\t\u751f\u6210\u5f69\u8272\u8272\u6761\uff08gencolors\uff09\t\n\t\t3\t\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08kalman\uff09\t\n\t\t4\t\u6e10\u53d8\u8fc7\u6e21\u5404\u79cd\u56fe\u5f62\u6ee4\u6ce2\uff08median_blur\uff09\n\t\t5\t\u8ddd\u79bb\u53d8\u6362\uff08distanceTransform\uff09\n\t\t6\t\u628a\u56fe\u50cf\u6620\u5c04\u5230\u6781\u6307\u6570\u7a7a\u95f4\uff08Log Polar\uff09\n\t\t7\tfilter2D\u6ee4\u6ce2\u5668\u7684\u7528\u6cd5\t\n\t\t8\tgrabCut\u56fe\u50cf\u5206\u5272\u793a\u4f8b\t\n\t\t9\tMeanShift\u56fe\u50cf\u5206\u5272\u793a\u4f8b\n\t\t10\t\u7528\u6ed1\u52a8\u63a7\u5236\u56fe\u50cf\u76f4\u65b9\u56fe\n\t\t11\t\u627e\u5230\u56fe\u50cf\u6700\u5c0f\u7684\u5c01\u95ed\u8f6e\u5ed3\n\t\t12\tRetina\u7279\u5f81\u70b9\u68c0\u6d4b\n\t\t13\t\u6444\u50cf\u5934\u5e27\u6570\u68c0\u6d4b\n\t\t14\t\u89c6\u9891\u622a\u56fe\n\t\t15\t\u5bf9\u89c6\u9891\u7684\u5feb\u901f\u89d2\u70b9\u68c0\u6d4b\n\t\t16\t\u89c6\u9891\u7b80\u5355\u8272\u5f69\u68c0\u6d4b\n\t\t17\t\u8ddf\u8e2a\u5206\u5272\u89c6\u9891\u4e2d\u8fd0\u52a8\u7684\u7269\u4f53\n\t\t18\t\u89c6\u9891\u7684\u76f4\u65b9\u56fe\u53cd\u5411\u6295\u5f71\u3002\n\t\t19\t\u8ba1\u7b97\u89c6\u9891\u4e2d\u4e24\u4e2a\u56fe\u50cf\u533a\u57df\u7684\u76f8\u4f3c\u5ea6\n\t\t20\t\u89c6\u9891\u524d\u540e\u80cc\u666f\u5206\u79bb\n\t\t21\t\u7528\u9ad8\u65af\u80cc\u666f\u5efa\u6a21\u5206\u79bb\u80cc\u666f\n\n\n<br>\n\n\n# \u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\n\n[\u3010\u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\u3011](http://blog.csdn.net/poem_qianmo/article/details/44416709)\n\n![](http://img.blog.csdn.net/20150325202951885)  \n<br>Please Enjoy~\n\n"
 },
 {
  "repo": "ANYbotics/grid_map",
  "language": "C++",
  "readme_contents": "# Grid Map\n\n## Overview\n\nThis is a C++ library with [ROS] interface to manage two-dimensional grid maps with multiple data layers. It is designed for mobile robotic mapping to store data such as elevation, variance, color, friction coefficient, foothold quality, surface normal, traversability etc. It is used in the [Robot-Centric Elevation Mapping](https://github.com/anybotics/elevation_mapping) package designed for rough terrain navigation.\n\nFeatures:\n\n* **Multi-layered:** Developed for universal 2.5-dimensional grid mapping with support for any number of layers.\n* **Efficient map re-positioning:** Data storage is implemented as two-dimensional circular buffer. This allows for non-destructive shifting of the map's position (e.g. to follow the robot) without copying data in memory.\n* **Based on Eigen:** Grid map data is stored as [Eigen] data types. Users can apply available Eigen algorithms directly to the map data for versatile and efficient data manipulation.\n* **Convenience functions:** Several helper methods allow for convenient and memory safe cell data access. For example, iterator functions for rectangular, circular, polygonal regions and lines are implemented.\n* **ROS interface:** Grid maps can be directly converted to and from ROS message types such as PointCloud2, OccupancyGrid, GridCells, and our custom GridMap message. Conversion packages provide compatibility with [costmap_2d], [PCL], and [OctoMap] data types.\n* **OpenCV interface:** Grid maps can be seamlessly converted from and to [OpenCV] image types to make use of the tools provided by [OpenCV].\n* **Visualizations:** The *grid_map_rviz_plugin* renders grid maps as 3d surface plots (height maps) in [RViz]. Additionally, the *grid_map_visualization* package helps to visualize grid maps as point clouds, occupancy grids, grid cells etc.\n* **Filters:** The *grid_map_filters* provides are range of filters to process grid maps as a sequence of filters. Parsing of mathematical expressions allows to flexibly setup powerful computations such as thresholding, normal vectors, smoothening, variance, inpainting, and matrix kernel convolutions.\n\nThis is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.\n\nThe source code is released under a [BSD 3-Clause license](LICENSE).\n\n**Author: P\u00e9ter Fankhauser<br />\nAffiliation: [ANYbotics](https://www.anybotics.com/)<br />\nMaintainer: P\u00e9ter Fankhauser, pfankhauser@anybotics.com<br />**\nWith contributions by: Tanja Baumann, Jeff Delmerico, Remo Diethelm, Perry Franklin, Dominic Jud, Ralph Kaestner, Philipp Kr\u00fcsi, Alex Millane, Daniel Stonier, Elena Stumm, Martin Wermelinger, Christos Zalidis, Edo Jelavic, Ruben Grandia, Simone Arreghini, Magnus G\u00e4rtner\n\nThis projected was initially developed at ETH Zurich (Autonomous Systems Lab & Robotic Systems Lab).\n\n[This work is conducted as part of ANYmal Research, a community to advance legged robotics.](https://www.anymal-research.org/)\n\n![Grid map example in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin_example.png)\n\n## Publications\n\nIf you use this work in an academic context, please cite the following publication:\n\n> P. Fankhauser and M. Hutter,\n> **\"A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation\"**,\n> in Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1), A. Koubaa (Ed.), Springer, 2016. ([PDF](http://www.researchgate.net/publication/284415855))\n\n\n    @incollection{Fankhauser2016GridMapLibrary,\n      author = {Fankhauser, P{\\'{e}}ter and Hutter, Marco},\n      booktitle = {Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1)},\n      title = {{A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation}},\n      chapter = {5},\n      editor = {Koubaa, Anis},\n      publisher = {Springer},\n      year = {2016},\n      isbn = {978-3-319-26052-5},\n      doi = {10.1007/978-3-319-26054-9{\\_}5},\n      url = {http://www.springer.com/de/book/9783319260525}\n    }\n\n## Documentation\n\nAn introduction to the grid map library including a tutorial is given in [this book chapter](http://www.researchgate.net/publication/284415855).\n\nThe C++ API is documented here:\n* [grid_map_core](http://docs.ros.org/kinetic/api/grid_map_core/html/index.html)\n* [grid_map_ros](http://docs.ros.org/kinetic/api/grid_map_ros/html/index.html)\n* [grid_map_costmap_2d](http://docs.ros.org/kinetic/api/grid_map_costmap_2d/html/index.html)\n* [grid_map_cv](http://docs.ros.org/kinetic/api/grid_map_cv/html/index.html)\n* [grid_map_filters](http://docs.ros.org/kinetic/api/grid_map_filters/html/index.html)\n* [grid_map_octomap](http://docs.ros.org/kinetic/api/grid_map_octomap/html/index.html)\n* [grid_map_pcl](http://docs.ros.org/kinetic/api/grid_map_pcl/html/index.html)\n\n## Installation\n\n### Installation from Packages\n\nTo install all packages from the grid map library as Debian packages use\n\n    sudo apt-get install ros-$ROS_DISTRO-grid-map\n\n### Building from Source\n\n#### Dependencies\n\nThe *grid_map_core* package depends only on the linear algebra library [Eigen].\n\n    sudo apt-get install libeigen3-dev\n\nThe other packages depend additionally on the [ROS] standard installation (*roscpp*, *tf*, *filters*, *sensor_msgs*, *nav_msgs*, and *cv_bridge*). Other format specific conversion packages (e.g. *grid_map_cv*, *grid_map_pcl* etc.) depend on packages described below in *Packages Overview*.\n\n#### Building\n\nTo build from source, clone the latest version from this repository into your catkin workspace and compile the package using\n\n    cd catkin_ws/src\n    git clone https://github.com/anybotics/grid_map.git\n    cd ../\n    catkin_make\n\nTo maximize performance, make sure to build in *Release* mode. You can specify the build type by setting\n\n    catkin_make -DCMAKE_BUILD_TYPE=Release\n\n\n### Packages Overview\n\nThis repository consists of following packages:\n\n* ***grid_map*** is the meta-package for the grid map library.\n* ***grid_map_core*** implements the algorithms of the grid map library. It provides the `GridMap` class and several helper classes such as the iterators. This package is implemented without [ROS] dependencies.\n* ***grid_map_ros*** is the main package for [ROS] dependent projects using the grid map library. It provides the interfaces to convert grid maps from and to several [ROS] message types.\n* ***grid_map_demos*** contains several nodes for demonstration purposes.\n* ***grid_map_filters*** builds on the [ROS Filters] package to process grid maps as a sequence of filters.\n* ***grid_map_msgs*** holds the [ROS] message and service definitions around the [grid_map_msg/GridMap] message type.\n* ***grid_map_rviz_plugin*** is an [RViz] plugin to visualize grid maps as 3d surface plots (height maps).\n* ***grid_map_visualization*** contains a node written to convert GridMap messages to other [ROS] message types for example for  visualization in [RViz].\n\nAdditional conversion packages:\n\n* ***grid_map_costmap_2d*** provides conversions of grid maps from [costmap_2d] map types.\n* ***grid_map_cv*** provides conversions of grid maps from and to [OpenCV] image types.\n* ***grid_map_octomap*** provides conversions of grid maps from OctoMap ([OctoMap]) maps.\n* ***grid_map_pcl*** provides conversions of grid maps from Point Cloud Library ([PCL](http://pointclouds.org/)) polygon meshes and point clouds. For details, see the grid map pcl package [README](grid_map_pcl/README.md).\n\n### Unit Tests\n\nRun the unit tests with\n\n    catkin_make run_tests_grid_map_core run_tests_grid_map_ros\n\nor\n\n    catkin build grid_map --no-deps --verbose --catkin-make-args run_tests\n\nif you are using [catkin tools](http://catkin-tools.readthedocs.org/).\n\n## Usage\n\n### Demonstrations\n\nThe *grid_map_demos* package contains several demonstration nodes. Use this code to verify your installation of the grid map packages and to get you started with your own usage of the library.\n\n* *[simple_demo](grid_map_demos/src/simple_demo_node.cpp)* demonstrates a simple example for using the grid map library. This ROS node creates a grid map, adds data to it, and publishes it. To see the result in RViz, execute the command\n\n        roslaunch grid_map_demos simple_demo.launch\n\n* *[tutorial_demo](grid_map_demos/src/tutorial_demo_node.cpp)* is an extended demonstration of the library's functionalities. Launch the *tutorial_demo* with\n\n        roslaunch grid_map_demos tutorial_demo.launch\n\n* *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* showcases the usage of the grid map iterators. Launch it with\n\n        roslaunch grid_map_demos iterators_demo.launch\n\n* *[image_to_gridmap_demo](grid_map_demos/src/ImageToGridmapDemo.cpp)* demonstrates how to convert data from an [image](grid_map_demos/data/eth_logo.png) to a grid map. Start the demonstration with\n\n        roslaunch grid_map_demos image_to_gridmap_demo.launch\n\n    ![Image to grid map demo result](grid_map_demos/doc/image_to_grid_map_demo_result.png)\n    \n* *[grid_map_to_image_demo](grid_map_demos/src/GridmapToImageDemo.cpp)* demonstrates how to save a grid map layer to an image. Start the demonstration with\n\n        rosrun grid_map_demos grid_map_to_image_demo _grid_map_topic:=/grid_map _file:=/home/$USER/Desktop/grid_map_image.png\n\n* *[opencv_demo](grid_map_demos/src/opencv_demo_node.cpp)* demonstrates map manipulations with help of [OpenCV] functions. Start the demonstration with\n\n        roslaunch grid_map_demos opencv_demo.launch\n\n    ![OpenCV demo result](grid_map_demos/doc/opencv_demo_result.gif)\n\n* *[resolution_change_demo](grid_map_demos/src/resolution_change_demo_node.cpp)* shows how the resolution of a grid map can be changed with help of the [OpenCV] image scaling methods. The see the results, use\n\n        roslaunch grid_map_demos resolution_change_demo.launch\n\n* *[filters_demo](grid_map_demos/src/FiltersDemo.cpp)* uses a chain of [ROS Filters] to process a grid map. Starting from the elevation of a terrain map, the demo uses several filters to show how to compute surface normals, use inpainting to fill holes, smoothen/blur the map, and use math expressions to detect edges, compute roughness and traversability. The filter chain setup is configured in the [`filters_demo_filter_chain.yaml`](grid_map_demos/config/filters_demo_filter_chain.yaml) file. Launch the demo with\n\n        roslaunch grid_map_demos filters_demo.launch\n\n    [![Filters demo results](grid_map_demos/doc/filters_demo_preview.gif)](grid_map_demos/doc/filters_demo.gif)\n\n For more information about grid map filters, see [grid_map_filters](#grid_map_filters).\n\n* *[interpolation_demo](grid_map_demos/src/InterpolationDemo.cpp)* shows the result of different interpolation methods on the resulting surface. The start the demo, use\n\n        roslaunch grid_map_demos interpolation_demo.launch\n\n<img src=\"grid_map_core/doc/interpolationSineWorld.gif\" width=\"256\" height=\"252\">\n<img src=\"grid_map_core/doc/interpolationGaussWorld.gif\" width=\"256\" height=\"252\">\n\nThe user can play with different worlds (surfaces) and different interpolation settings in the [`interpolation_demo.yaml`](grid_map_demos/config/interpolation_demo.yaml) file. The visualization displays the ground truth in green and yellow color. The interpolation result is shown in red and purple colors. Also, the demo computes maximal and average interpolation errors, as well as the average time required for a single interpolation query.\n\nGrid map features four different interpolation methods (in order of increasing accuracy and increasing complexity):\n* **NN** - Nearest Neighbour (fastest, but least accurate).\n* **Linear** - Linear interpolation.\n* **Cubic convolution** - Piecewise cubic interpolation. Implemented using the cubic convolution algorithm.\n* **Cubic** - Cubic interpolation (slowest, but most accurate).\n\nFor more details check the literature listed in  [`CubicInterpolation.hpp`](grid_map_core/include/grid_map_core/CubicInterpolation.hpp) file.\n\n### Conventions & Definitions\n\n[![Grid map layers](grid_map_core/doc/grid_map_layers.png)](grid_map_core/doc/grid_map_layers.pdf)\n\n[![Grid map conventions](grid_map_core/doc/grid_map_conventions.png)](grid_map_core/doc/grid_map_conventions.pdf)\n\n\n### Iterators\n\nThe grid map library contains various iterators for convenience.\n\nGrid map | Submap | Circle | Line | Polygon\n:---: | :---: | :---: | :---: | :---:\n[![Grid map iterator](grid_map_core/doc/iterators/grid_map_iterator_preview.gif)](grid_map_core/doc/iterators/grid_map_iterator.gif) | [![Submap iterator](grid_map_core/doc/iterators/submap_iterator_preview.gif)](grid_map_core/doc/iterators/submap_iterator.gif) | [![Circle iterator](grid_map_core/doc/iterators/circle_iterator_preview.gif)](grid_map_core/doc/iterators/circle_iterator.gif) | [![Line iterator](grid_map_core/doc/iterators/line_iterator_preview.gif)](grid_map_core/doc/iterators/line_iterator.gif) | [![Polygon iterator](grid_map_core/doc/iterators/polygon_iterator_preview.gif)](grid_map_core/doc/iterators/polygon_iterator.gif)\n__Ellipse__ | __Spiral__\n[![Ellipse iterator](grid_map_core/doc/iterators/ellipse_iterator_preview.gif)](grid_map_core/doc/iterators/ellipse_iterator.gif) | [![Spiral iterator](grid_map_core/doc/iterators/spiral_iterator_preview.gif)](grid_map_core/doc/iterators/spiral_iterator.gif)\n\nUsing the iterator in a `for` loop is common. For example, iterate over the entire grid map with the `GridMapIterator` with\n\n    for (grid_map::GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        cout << \"The value at index \" << (*iterator).transpose() << \" is \" << map.at(\"layer\", *iterator) << endl;\n    }\n\nThe other grid map iterators follow the same form. You can find more examples on how to use the different iterators in the *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* node.\n\nNote: For maximum efficiency when using iterators, it is recommended to locally store direct access to the data layers of the grid map with `grid_map::Matrix& data = map[\"layer\"]` outside the `for` loop:\n\n    grid_map::Matrix& data = map[\"layer\"];\n    for (GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        const Index index(*iterator);\n        cout << \"The value at index \" << index.transpose() << \" is \" << data(index(0), index(1)) << endl;\n    }\n\nYou can find a benchmarking of the performance of the iterators in the `iterator_benchmark` node of the `grid_map_demos` package which can be run with\n\n    rosrun grid_map_demos iterator_benchmark\n\nBeware that while iterators are convenient, it is often the cleanest and most efficient to make use of the built-in [Eigen] methods. Here are some examples:\n\n* Setting a constant value to all cells of a layer:\n\n        map[\"layer\"].setConstant(3.0);\n\n* Adding two layers:\n\n        map[\"sum\"] = map[\"layer_1\"] + map[\"layer_2\"];\n\n* Scaling a layer:\n\n        map[\"layer\"] = 2.0 * map[\"layer\"];\n\n* Max. values between two layers:\n\n        map[\"max\"] = map[\"layer_1\"].cwiseMax(map[\"layer_2\"]);\n\n* Compute the root mean squared error:\n\n        map.add(\"error\", (map.get(\"layer_1\") - map.get(\"layer_2\")).cwiseAbs());\n        unsigned int nCells = map.getSize().prod();\n        double rootMeanSquaredError = sqrt((map[\"error\"].array().pow(2).sum()) / nCells);\n\n\n### Changing the Position of the Map\n\nThere are two different methods to change the position of the map:\n* `setPosition(...)`: Changes the position of the map without changing data stored in the map. This changes the corresponce between the data and the map frame.\n* `move(...)`: Relocates the grid map such that the corresponce between data and the map frame does not change. Data in the overlapping region before and after the position change remains stored. Data that falls outside of the map at its new position is discarded. Cells that cover previously unknown regions are emptied (set to nan). The data storage is implemented as two-dimensional circular buffer to minimize computational effort.\n\n`setPosition(...)` | `move(...)`\n:---: | :---:\n![Grid map iterator](grid_map_core/doc/setposition_method.gif) | ![Submap iterator](grid_map_core/doc/move_method.gif)|\n\n\n## Packages\n\n### grid_map_rviz_plugin\n\nThis [RViz] plugin visualizes a grid map layer as 3d surface plot (height map). A separate layer can be chosen as layer for the color information.\n\n![Grid map visualization in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin.png)\n\n\n### grid_map_visualization\n\nThis node subscribes to a topic of type [grid_map_msgs/GridMap] and publishes messages that can be visualized in [RViz]. The published topics of the visualizer can be fully configure with a YAML parameter file. Any number of visualizations with different parameters can be added. An example is [here](grid_map_demos/config/tutorial_demo.yaml) for the configuration file of the *tutorial_demo*.\n\nPoint cloud | Vectors | Occupancy grid | Grid cells\n--- | --- | --- | ---\n[![Point cloud](grid_map_visualization/doc/point_cloud_preview.jpg)](grid_map_visualization/doc/point_cloud.jpg) | [![Vectors](grid_map_visualization/doc/vectors_preview.jpg)](grid_map_visualization/doc/vectors.jpg) | [![Occupancy grid](grid_map_visualization/doc/occupancy_grid_preview.jpg)](grid_map_visualization/doc/occupancy_grid.jpg) | [![Grid cells](grid_map_visualization/doc/grid_cells_preview.jpg)](grid_map_visualization/doc/grid_cells.jpg)\n\n#### Parameters\n\n* **`grid_map_topic`** (string, default: \"/grid_map\")\n\n    The name of the grid map topic to be visualized. See below for the description of the visualizers.\n\n\n#### Subscribed Topics\n\n* **`/grid_map`** ([grid_map_msgs/GridMap])\n\n    The grid map to visualize.\n\n\n#### Published Topics\n\nThe published topics are configured with the [YAML parameter file](grid_map_demos/config/tutorial_demo.yaml). Possible topics are:\n\n* **`point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a point cloud. Select which layer to transform as points with the `layer` parameter.\n\n        name: elevation\n        type: point_cloud\n        params:\n         layer: elevation\n         flat: false # optional\n\n* **`flat_point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a \"flat\" point cloud, i.e. with all points at the same height *z*. This is convenient to visualize 2d maps or images (or even video streams) in [RViz] with help of its `Color Transformer`. The parameter `height` determines the desired *z*-position of the flat point cloud.\n\n        name: flat_grid\n        type: flat_point_cloud\n        params:\n         height: 0.0\n\n    Note: In order to omit points in the flat point cloud from empty/invalid cells, specify the layers which should be checked for validity with `setBasicLayers(...)`.\n\n* **`vectors`** ([visualization_msgs/Marker])\n\n    Visualizes vector data of the grid map as visual markers. Specify the layers which hold the *x*-, *y*-, and *z*-components of the vectors with the `layer_prefix` parameter. The parameter `position_layer` defines the layer to be used as start point of the vectors.\n\n        name: surface_normals\n        type: vectors\n        params:\n         layer_prefix: normal_\n         position_layer: elevation\n         scale: 0.06\n         line_width: 0.005\n         color: 15600153 # red\n\n* **`occupancy_grid`** ([nav_msgs/OccupancyGrid])\n\n    Visualizes a layer of the grid map as occupancy grid. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bound with `data_min` and `data_max`.\n\n        name: traversability_grid\n        type: occupancy_grid\n        params:\n         layer: traversability\n         data_min: -0.15\n         data_max: 0.15\n\n* **`grid_cells`** ([nav_msgs/GridCells])\n\n    Visualizes a layer of the grid map as grid cells. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bounds with `lower_threshold` and `upper_threshold`.\n\n        name: elevation_cells\n        type: grid_cells\n        params:\n         layer: elevation\n         lower_threshold: -0.08 # optional, default: -inf\n         upper_threshold: 0.08 # optional, default: inf\n\n* **`region`** ([visualization_msgs/Marker])\n\n    Shows the boundary of the grid map.\n\n        name: map_region\n        type: map_region\n        params:\n         color: 3289650\n         line_width: 0.003\n\n*Note: Color values are in RGB form as concatenated integers (for each channel value 0-255). The values can be generated like [this](http://www.wolframalpha.com/input/?i=BitOr%5BBitShiftLeft%5Br%2C16%5D%2C+BitShiftLeft%5Bg%2C8%5D%2C+b%5D+where+%7Br%3D0%2C+g%3D255%2C+b%3D0%7D) as an example for the color green (red: 0, green: 255, blue: 0).*\n\n### grid_map_filters\n\nThe *grid_map_filters* package containts several filters which can be applied a grid map to perform computations on the data in the layers. The grid map filters are based on [ROS Filters], which means that a chain of filters can be configured as a YAML file. Furthermore, additional filters can be written and made available through the ROS plugin mechanism, such as the [`InpaintFilter`](grid_map_cv/include/grid_map_cv/InpaintFilter.hpp) from the `grid_map_cv` package.\n\nSeveral basic filters are provided in the *grid_map_filters* package:\n\n* **`gridMapFilters/ThresholdFilter`**\n\n    Set values below/above a threshold to a specified value.\n\n        name: lower_threshold\n        type: gridMapFilters/ThresholdFilter\n        params:\n          layer: layer_name\n          lower_threshold: 0.0 # alternative: upper_threshold\n          set_to: 0.0 # # Other uses: .nan, .inf\n\n* **`gridMapFilters/MeanInRadiusFilter`**\n\n    Compute for each cell of a layer the mean value inside a radius.\n\n        name: mean_in_radius\n        type: gridMapFilters/MeanInRadiusFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.06 # in m.\n* **`gridMapFilters/MedianFillFilter`**\n\n    Compute for each _NaN_ cell of a layer the median (of finites) inside a patch with radius. \n    Optionally, apply median calculations for values that are already finite, the patch radius for these points is given by existing_value_radius. \n\n        name: median\n        type: gridMapFilters/MedianFillFilter\n        params:\n          input_layer: input\n          output_layer: output\n          fill_hole_radius: 0.11 # in m. \n          filter_existing_values: false # Default is false. If enabled it also does a median computation for existing values. \n          existing_value_radius: 0.2 # in m. Note that this option only has an effect if filter_existing_values is set true. \n    \n* **`gridMapFilters/NormalVectorsFilter`**\n\n    Compute the normal vectors of a layer in a map.\n\n        name: surface_normals\n        type: gridMapFilters/NormalVectorsFilter\n        params:\n          input_layer: input\n          output_layers_prefix: normal_vectors_\n          radius: 0.05\n          normal_vector_positive_axis: z\n\n* **`gridMapFilters/NormalColorMapFilter`**\n\n    Compute a new color layer based on normal vectors layers.\n\n        name: surface_normals\n        type: gridMapFilters/NormalColorMapFilter\n        params:\n          input_layers_prefix: normal_vectors_\n          output_layer: normal_color\n\n* **`gridMapFilters/MathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression with layers of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/MathExpressionFilter\n        params:\n          output_layer: output\n          expression: acos(normal_vectors_z) # Slope.\n          # expression: abs(elevation - elevation_smooth) # Surface roughness.\n          # expression: 0.5 * (1.0 - (slope / 0.6)) + 0.5 * (1.0 - (roughness / 0.1)) # Weighted and normalized sum.\n\n* **`gridMapFilters/SlidingWindowMathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression within a sliding window on a layer of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/SlidingWindowMathExpressionFilter\n        params:\n          input_layer: input\n          output_layer: output\n          expression: meanOfFinites(input) # Box blur\n          # expression: sqrt(sumOfFinites(square(input - meanOfFinites(input))) ./ numberOfFinites(input)) # Standard deviation\n          # expression: 'sumOfFinites([0,-1,0;-1,5,-1;0,-1,0].*elevation_inpainted)' # Sharpen with kernel matrix\n          compute_empty_cells: true\n          edge_handling: crop # options: inside, crop, empty, mean\n          window_size: 5 # in number of cells (optional, default: 3), make sure to make this compatible with the kernel matrix\n          # window_length: 0.05 # instead of window_size, in m\n\n* **`gridMapFilters/DuplicationFilter`**\n\n    Duplicate a layer of a grid map.\n\n        name: duplicate\n        type: gridMapFilters/DuplicationFilter\n        params:\n          input_layer: input\n          output_layer: output\n\n* **`gridMapFilters/DeletionFilter`**\n\n    Delete layers from a grid map.\n\n        name: delete\n        type: gridMapFilters/DeletionFilter\n        params:\n          layers: [color, score] # List of layers.\n\nAdditionally, the *grid_map_cv* package provides the following filters:\n\n* **`gridMapCv/InpaintFilter`**\n\n    Use OpenCV to inpaint/fill holes in a layer.\n\n        name: inpaint\n        type: gridMapCv/InpaintFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.05 # in m\n\n\n## Build Status\n\n### Devel Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idev__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idev__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdev__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdev__grid_map__ubuntu_bionic_amd64/) |\n| doc | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idoc__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idoc__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdoc__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdoc__grid_map__ubuntu_bionic_amd64/) |\n\n### Release Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary/) |\n| grid_map_core | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary/) |\n| grid_map_costmap_2d | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary/) |\n| grid_map_cv | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary/) |\n| grid_map_demos | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary/) |\n| grid_map_filters | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary/) |\n| grid_map_loader | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary/) |\n| grid_map_msgs | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary/) |\n| grid_map_octomap | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary/) |\n| grid_map_pcl | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary/) |\n| grid_map_ros | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary/) |\n| grid_map_rviz_plugin | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary/) |\n| grid_map_sdf | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary/) |\n| grid_map_visualization | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary/) |\n\n\n## Bugs & Feature Requests\n\nPlease report bugs and request features using the [Issue Tracker](https://github.com/anybotics/grid_map/issues).\n\n[ROS]: http://www.ros.org\n[RViz]: http://wiki.ros.org/rviz\n[Eigen]: http://eigen.tuxfamily.org\n[OpenCV]: http://opencv.org/\n[OctoMap]: https://octomap.github.io/\n[PCL]: http://pointclouds.org/\n[costmap_2d]: http://wiki.ros.org/costmap_2d\n[grid_map_msgs/GridMapInfo]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMapInfo.html\n[grid_map_msgs/GridMap]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMap.html\n[grid_map_msgs/GetGridMap]: http://docs.ros.org/api/grid_map_msgs/html/srv/GetGridMap.html\n[sensor_msgs/PointCloud2]: http://docs.ros.org/api/sensor_msgs/html/msg/PointCloud2.html\n[visualization_msgs/Marker]: http://docs.ros.org/api/visualization_msgs/html/msg/Marker.html\n[geometry_msgs/PolygonStamped]: http://docs.ros.org/api/geometry_msgs/html/msg/PolygonStamped.html\n[nav_msgs/OccupancyGrid]: http://docs.ros.org/api/nav_msgs/html/msg/OccupancyGrid.html\n[nav_msgs/GridCells]: http://docs.ros.org/api/nav_msgs/html/msg/GridCells.html\n[ROS Filters]: http://wiki.ros.org/filters\n[EigenLab]: https://github.com/leggedrobotics/EigenLab\n"
 },
 {
  "repo": "go-opencv/go-opencv",
  "language": "Go",
  "readme_contents": "Go OpenCV binding\n==================\n\n[![Join the chat at https://gitter.im/lazywei/go-opencv](https://badges.gitter.im/lazywei/go-opencv.svg)](https://gitter.im/lazywei/go-opencv?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nA Golang binding for [OpenCV](http://opencv.org/).\n\nOpenCV 1.x C API bindings through CGO, and OpenCV 2+ C++ API ([`GoCV`](gocv/)) through SWIG.\n\n-------------------\n\n## Disclaimer\n\nThis is a fork of [chai's go-opencv](https://github.com/chai2010/opencv), which has only OpenCV1 support through CGO, and all credits for OpenCV1 wrapper (except files in `gocv/` folder) should mainly go to Chai. At the time of the fork (Dec 9, 2013) the original project was inactive and was hosted on Google Code, which was a little inconvenient for community contribution. Hence, I decided to host a fork on Github so people can contribute to this project easily. Since then, some patches were added by community, and some experimental OpenCV 2 wrappers were added as well. That means this fork went on a little bit divergent way comparing to the origin project. However, now the origin project seems to be active again and be moved to GitHub starting from Aug 25, 2014. Efforts to merge the two projects are very welcome.\n\n-------------------\n\n## Install\n\n### Linux & Mac OS X\n\nInstall Go and OpenCV, you might want to install both of them via `apt-get` or `homebrew`.\n\nYou can reference the [link](https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html) to install required packages.\n\n```\ngo get github.com/go-opencv/go-opencv\ncd $GOPATH/src/github.com/go-opencv/go-opencv/samples\ngo run hellocv.go\n```\n\n### Windows\n\n- Install Go and MinGw\n- install OpenCV-2.4.x to MinGW dir\n\n```\n# libopencv*.dll --> ${MinGWRoot}\\bin\n# libopencv*.lib --> ${MinGWRoot}\\lib\n# include\\opencv --> ${MinGWRoot}\\include\\opencv\n# include\\opencv2 --> ${MinGWRoot}\\include\\opencv2\n\ngo get github.com/go-opencv/go-opencv\ncd ${GoOpenCVRoot}/trunk/samples && go run hellocv.go\n```\n\n## [WIP] OpenCV2 (GoCV)\n\nAfter OpenCV 2.x+, the core team no longer develop and maintain C API. Therefore, CGO will not be used in CV2 binding. Instead, we are using SWIG for wrapping. The support for OpenCV2 is currently under development, and whole code will be placed under `gocv` package.\n\nIf you want to use CV2's API, please refer to the code under `gocv/` directory. There is no too many documents for CV2 wrapper yet, but you can still find the example usages in `*_test.go`.\n\nPlease also note that the basic data structures in OpenCV (e.g., `cv::Mat`, `cv::Point3f`) are wrapped partially for now. For more detail on how to use these types, please refer to [GoCV's README](gocv/README.md).\n\n*Requirement*: we will build the wrappers based on [mat64](https://godoc.org/github.com/gonum/matrix/mat64), given it is much easier to manipulate the underlaying data. In most case, it is not necessary to access the original CV data, e.g., `cv::Mat` can be converted from/to `*mat64.Dense`.\n\n## Example\n\n### OpenCV2's initCameraMatrix2D\n\n```go\npackage main\n\nimport . \"github.com/go-opencv/go-opencv/gocv\"\nimport \"github.com/gonum/matrix/mat64\"\n\nfunc main() {\n\n\tobjPts := mat64.NewDense(4, 3, []float64{\n\t\t0, 25, 0,\n\t\t0, -25, 0,\n\t\t-47, 25, 0,\n\t\t-47, -25, 0})\n\n\timgPts := mat64.NewDense(4, 2, []float64{\n\t\t1136.4140625, 1041.89208984,\n\t\t1845.33190918, 671.39581299,\n\t\t302.73373413, 634.79998779,\n\t\t1051.46154785, 352.76107788})\n\n\tcamMat := GcvInitCameraMatrix2D(objPts, imgPts)\n\tfmt.Println(camMat)\n}\n```\n\n\n### Resizing\n\n```go\npackage main\n\nimport opencv \"github.com/go-opencv/go-opencv/opencv\"\n\nfunc main() {\n\tfilename := \"bert.jpg\"\n\tsrcImg := opencv.LoadImage(filename)\n\tif srcImg == nil {\n\t\tpanic(\"Loading Image failed\")\n\t}\n\tdefer srcImg.Release()\n\tresized1 := opencv.Resize(srcImg, 400, 0, 0)\n\tresized2 := opencv.Resize(srcImg, 300, 500, 0)\n\tresized3 := opencv.Resize(srcImg, 300, 500, 2)\n\topencv.SaveImage(\"resized1.jpg\", resized1, nil)\n\topencv.SaveImage(\"resized2.jpg\", resized2, nil)\n\topencv.SaveImage(\"resized3.jpg\", resized3, nil)\n}\n```\n\n### Webcam\n\nYet another cool example is created by @saratovsource which demos how to use webcam:\n\n```\ncd samples\ngo run webcam.go\n```\n\n### More\n\nYou can find more samples at: https://github.com/go-opencv/go-opencv/tree/master/samples\n\n## How to contribute\n\n- Fork this repo\n- Clone the main repo, and add your fork as a remote\n\n  ```\n  git clone https://github.com/go-opencv/go-opencv.git\n  cd go-opencv\n  git remote rename origin upstream\n  git remote add origin https://github.com/your_github_account/go-opencv.git\n  ```\n\n- Create new feature branch\n\n  ```\n  git checkout -b your-feature-branch\n  ```\n\n- Commit your change and push it to your repo \n\n  ```\n  git commit -m 'new feature'\n  git push origin your-feature-branch\n  ```\n\n- Open a pull request!\n\n"
 },
 {
  "repo": "Breakthrough/PySceneDetect",
  "language": "Python",
  "readme_contents": "\n![PySceneDetect](https://raw.githubusercontent.com/Breakthrough/PySceneDetect/master/docs/img/pyscenedetect_logo_small.png)\n==========================================================\nVideo Scene Cut Detection and Analysis Tool\n----------------------------------------------------------\n\n[![Build Status](https://img.shields.io/travis/com/Breakthrough/PySceneDetect)](https://travis-ci.com/github/Breakthrough/PySceneDetect) [![PyPI Status](https://img.shields.io/pypi/status/scenedetect.svg)](https://pypi.python.org/pypi/scenedetect/) [![PyPI Version](https://img.shields.io/pypi/v/scenedetect?color=blue)](https://pypi.python.org/pypi/scenedetect/)  [![PyPI License](https://img.shields.io/pypi/l/scenedetect.svg)](http://pyscenedetect.readthedocs.org/en/latest/copyright/)\n\n\n### Latest Release: v0.5.4 (September 14, 2020)\n\n**Main Webpage**:  [py.scenedetect.com](http://py.scenedetect.com)\n\n**Documentation**:  [manual.scenedetect.com](http://manual.scenedetect.com)\n\n**Installation and Dependencies**: https://pyscenedetect.readthedocs.io/en/latest/download/\n\n----------------------------------------------------------\n\n**Quick Install**: To install PySceneDetect via `pip` with all dependencies:\n\n    pip install scenedetect[opencv]\n\nTo enable video splitting support, you will also need to have `mkvmerge` or `ffmpeg` installed on your system. See the documentation on [Video Splitting Support](https://pyscenedetect.readthedocs.io/en/latest/examples/video-splitting/) after installation for details.\n\nRequires Python modules `numpy`, OpenCV `cv2`, and (optional) `tqdm` for displaying progress.\n\n----------------------------------------------------------\n\n**Quick Start (Command Line)**:\n\nSplit the input video wherever a new scene is detected:\n\n    scenedetect -i video.mp4 detect-content split-video\n\nSkip the first 10 seconds of the input video, and output a list of scenes to the terminal:\n\n    scenedetect -i video.mp4 time -s 10s detect-content list-scenes\n\nTo show a summary of all other options and commands:\n\n    scenedetect help\n\nYou can find more examples [on the website](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/) or [in the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/cli/global_options.html).\n\n**Quick Start (Python API)**:\n\nIn the code example below, we create a function `find_scenes()` which will\nload a video, detect the scenes, and return a list of tuples containing the\n(start, end) timecodes of each detected scene.  Note that you can modify\nthe `threshold` argument to modify the sensitivity of the scene detection.\n\n```python\n# Standard PySceneDetect imports:\nfrom scenedetect import VideoManager\nfrom scenedetect import SceneManager\n\n# For content-aware scene detection:\nfrom scenedetect.detectors import ContentDetector\n\ndef find_scenes(video_path, threshold=30.0):\n    # Create our video & scene managers, then add the detector.\n    video_manager = VideoManager([video_path])\n    scene_manager = SceneManager()\n    scene_manager.add_detector(\n        ContentDetector(threshold=threshold))\n\n    # Base timestamp at frame 0 (required to obtain the scene list).\n    base_timecode = video_manager.get_base_timecode()\n\n    # Improve processing speed by downscaling before processing.\n    video_manager.set_downscale_factor()\n\n    # Start the video manager and perform the scene detection.\n    video_manager.start()\n    scene_manager.detect_scenes(frame_source=video_manager)\n\n    # Each returned scene is a tuple of the (start, end) timecode.\n    return scene_manager.get_scene_list(base_timecode)\n```\n\nTo get started, try printing the result from calling `find_scenes` on a small video clip:\n\n```python\n    scenes = find_scenes('video.mp4')\n    print(scenes)\n```\n\nSee [the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/api.html) for the full PySceneDetect API documentation.\n\n----------------------------------------------------------\n\nPySceneDetect is a command-line tool and Python library, which uses OpenCV to analyze a video to find scene changes or cuts.  If `ffmpeg` or `mkvmerge` is installed, the video can also be split into scenes automatically.  A frame-by-frame analysis can also be generated for a video, to help with determining optimal threshold values or detecting patterns/other analysis methods for a particular video.  See [the Usage documentation](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/) for details.\n\nThere are two main detection methods PySceneDetect uses: `detect-threshold` (comparing each frame to a set black level, useful for detecting cuts and fades to/from black), and `detect-content` (compares each frame sequentially looking for changes in content, useful for detecting fast cuts between video scenes, although slower to process).  Each mode has slightly different parameters, and is described in detail below.\n\nIn general, use `detect-threshold` mode if you want to detect scene boundaries using fades/cuts in/out to black.  If the video uses a lot of fast cuts between content, and has no well-defined scene boundaries, you should use the `detect-content` mode.  Once you know what detection mode to use, you can try the parameters recommended below, or generate a statistics file (using the `-s` / `--statsfile` flag) in order to determine the correct paramters - specifically, the proper threshold value.\n\nNote that PySceneDetect is currently in beta; see Current Features & Roadmap below for details.  For help or other issues, you can contact me on [my website](http://www.bcastell.com/about/), or we can chat in #pyscenedetect on Freenode.  Feel free to submit any bugs or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues) here on Github.\n\n\nUsage\n----------------------------------------------------------\n\n - [Basic Usage](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/)\n - [PySceneDetect Manual](https://pyscenedetect-manual.readthedocs.io/en/latest/), covers `scenedetect` command and Python API\n - [Example: Detecting and Splitting Scenes in Movie Clip](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/)\n\n\nCurrent Features & Roadmap\n----------------------------------------------------------\n\nYou can [view the latest features and version roadmap on Readthedocs](http://pyscenedetect.readthedocs.org/en/latest/features/).\nSee [`docs/changelog.md`](https://github.com/Breakthrough/PySceneDetect/blob/master/docs/changelog.md) for a list of changes in each version, or visit [the Releases page](https://github.com/Breakthrough/PySceneDetect/releases) to download a specific version.  Feel free to submit any bugs/issues or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues).\n\nAdditional features being planned or in development can be found [here (tagged as `feature`) in the issue tracker](https://github.com/Breakthrough/PySceneDetect/issues?q=is%3Aissue+is%3Aopen+label%3Afeature).  You can also find additional information about PySceneDetect at [http://www.bcastell.com/projects/PySceneDetect/](http://www.bcastell.com/projects/PySceneDetect/).\n\n\n----------------------------------------------------------\n\nLicensed under BSD 3-Clause (see the `LICENSE` file for details).\n\nCopyright (C) 2014-2020 Brandon Castellano.\nAll rights reserved.\n\n"
 },
 {
  "repo": "ahmetozlu/tensorflow_object_counting_api",
  "language": "Python",
  "readme_contents": "# TensorFlow Object Counting API\nThe TensorFlow Object Counting API is an open source framework built on top of TensorFlow and Keras that makes it easy to develop object counting systems. ***Please contact if you need professional object detection & tracking & counting project with the super high accuracy.***\n\n## QUICK DEMO\n\n---\n### Cumulative Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=411>\n</p>\n\n---\n### Real-Time Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42237325-1f964e82-7f06-11e8-966b-dfde98701c66.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/42238435-77ac0d34-7f09-11e8-9609-e7c3c2c5af74.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42241094-14163cc8-7f12-11e8-83ed-68021b5e3b33.gif\" | width=430><img src=\"https://user-images.githubusercontent.com/22610163/42237904-d6a3ac22-7f07-11e8-88f8-5f21430d9503.gif\" | width=430>\n</p>\n\n---\n\n---\n### Object Tracking Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389634-4682ea00-19d3-11ea-84a2-3996a43e98fe.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/70389738-6bc42800-19d4-11ea-971f-f19cb5b90140.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389764-e9883380-19d4-11ea-8c54-80935811c3fa.gif\" | width=680>\n</p>\n\n- Tracking module was built on top of [this approach](https://github.com/kcg2015/Vehicle-Detection-and-Tracking).\n\n---\n\n### Object Counting On Single Image (TensorFlow implementation):\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/47524870-7c830e80-d8a4-11e8-8fd1-741193615a04.png\" | width=750></p>\n\n---\n\n### Object Counting based R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969852-0569b080-7983-11e9-8051-07d6766ca0e4.png\" | width=750></p>\n\n### Object Segmentation & Counting based Mask R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969871-1c100780-7983-11e9-9660-7b8571b01ff7.png\" | width=750></p>\n\n---\n\n### BONUS: Custom Object Counting Mode (TensorFlow implementation):\n\nYou can train TensorFlow models with your own training data to built your own custom object counter system! If you want to learn how to do it, please check one of the sample projects, which cover some of the theory of transfer learning and show how to apply it in useful projects, are given at below.\n\n**Sample Project#1: Smurf Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/smurf_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62861574-9d6e0080-bd0c-11e9-9e38-b63226df8aa1.gif\" | width=750>\n</p>\n\n**Sample Project#2: Barilla-Spaghetti Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counting_api_keras_tensorflow/barilla_spaghetti_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62903429-46e3df00-bd6b-11e9-9f97-4de477fa8769.png\" | width=750>  \n</p>\n\n---\n\n***The development is on progress! The API will be updated soon, the more talented and light-weight API will be available in this repo!***\n\n- ***Detailed API documentation and sample jupyter notebooks that explain basic usages of API will be added!***\n\n**You can find a sample project - case study that uses TensorFlow Object Counting API in [*this repo*](https://github.com/ahmetozlu/vehicle_counting_tensorflow).**\n\n---\n\n## USAGE\n\n### 1.) Usage of \"Cumulative Counting Mode\"\n\n#### 1.1) For detecting, tracking and counting *the pedestrians* with disabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"pedestrian counting\" case:*\n\n    fps = 30 # change it with your input video fps\n    width = 626 # change it with your input video width\n    height = 360 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 385 # roi line position\n    deviation = 1 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_x_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"pedestrian counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"pedestrian counting case-study\": [pedestrian_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/pedestrian_counting.py)**\n\n---\n\n**1.2)** For detecting, tracking and counting *the vehicles* with enabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"vehicle counting\" case:*\n\n    fps = 24 # change it with your input video fps\n    width = 640 # change it with your input video width\n    height = 352 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 200 # roi line position\n    deviation = 3 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_y_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"vehicle counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"vehicle counting case-study\": [vehicle_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/vehicle_counting.py)**\n\n---\n\n### 2.) Usage of \"Real-Time Counting Mode\"\n\n#### 2.1) For detecting, tracking and counting the *targeted object/s* with disabled color prediction\n \n *Usage of \"the targeted object is bicycle\":*\n \n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"bicycle\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n    \n *Result of \"the targeted object is bicycle\":*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411751-1ae1d3f0-820a-11e8-8465-9ec9b44d4fe7.gif\" | width=700>\n</p>\n\n*Usage of \"the targeted object is person\":*\n\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"person\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n \n *Result of \"the targeted object is person\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411749-1a80362c-820a-11e8-864e-acdeed85b1f2.gif\" | width=700>\n</p>\n\n*Usage of \"detecting, counting and tracking all the objects\":*\n\n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n \n *Result of \"detecting, counting and tracking all the objects\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411750-1aae0d72-820a-11e8-8726-4b57480f4cb8.gif\" | width=700>\n</p>\n\n---\n*Usage of \"detecting, counting and tracking **the multiple targeted objects**\":*\n\n    targeted_objects = \"person, bicycle\" # (for counting targeted objects) change it with your targeted objects\n    fps = 25 # change it with your input video fps\n    width = 1280 # change it with your input video width\n    height = 720 # change it with your input video height\n    is_color_recognition_enabled = 0\n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n---\n \n#### 2.2) For detecting, tracking and counting \"all the objects with disabled color prediction\"\n\n*Usage of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n    \n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411748-1a5ab49c-820a-11e8-8648-d78ffa08c28c.gif\" | width=700>\n</p>\n\n\n*Usage of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n    is_color_prediction_enabled = 1 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411747-1a215e4a-820a-11e8-8aef-faa500df6836.gif\" | width=700>\n</p>\n\n### 3.) Usage of \"Object Tracking Mode\"\n\nJust run [object_tracking.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/object_tracking.py)\n\n---\n\n**For sample usages of \"Real-Time Counting Mode\": [real_time_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/real_time_counting.py)**\n\n---\n\n*The minimum object detection threshold can be set [in this line](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/utils/visualization_utils.py#L443) in terms of percentage. The default minimum object detecion threshold is 0.5!*\n\n## General Capabilities of The TensorFlow Object Counting API\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421361-6662c280-e76d-11e8-9680-ec86e245fdac.jpg\" | width = 720>\n</p>\n\nHere are some cool capabilities of TensorFlow Object Counting API:\n\n- Detect just the targeted objects\n- Detect all the objects\n- Count just the targeted objects\n- Count all the objects\n- Predict color of the targeted objects\n- Predict color of all the objects\n- Predict speed of the targeted objects\n- Predict speed of all the objects\n- Print out the detection-counting result in a .csv file as an analysis report\n- Save and store detected objects as new images under [detected_object folder](www)\n- Select, download and use state of the art [models that are trained by Google Brain Team](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n- Use [your own trained models](https://www.tensorflow.org/guide/keras) or [a fine-tuned model](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb) to detect spesific object/s\n- Save detection and counting results as a new video or show detection and counting results in real time\n- Process images or videos depending on your requirements\n\nHere are some cool architectural design features of TensorFlow Object Counting API:\n\n- Lightweigth, runs in real-time\n- Scalable and well-designed framework, easy usage\n- Gets \"Pythonic Approach\" advantages\n- It supports REST Architecture and RESTful Web Services\n\nTODOs:\n\n- Kalman Filter based object tracker util will be developed.\n- Autonomus Training Image Annotation Tool will be developed.\n\n## Theory\n\n### System Architecture\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421362-6662c280-e76d-11e8-9b63-da9698626f75.jpg\" | width=720>\n</p>\n\n- Object detection and classification have been developed on top of TensorFlow Object Detection API, [see](https://github.com/tensorflow/models/tree/master/research/object_detection) for more info.\n\n- Object color prediction has been developed using OpenCV via K-Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features, [see](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/utils/color_recognition_module) for more info.\n\n[TensorFlow\u2122](https://www.tensorflow.org/) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n[OpenCV (Open Source Computer Vision Library)](https://opencv.org/about.html) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n\n### Tracker\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/41812993-a4b5a172-7735-11e8-89f6-083ec0625f21.png\" | width=700>\n</p>\n\nSource video is read frame by frame with OpenCV. Each frames is processed by [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) is developed on TensorFlow. This is a loop that continue working till reaching end of the video. The main pipeline of the tracker is given at the above Figure.\n\n### Models\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48481757-b1d5a900-e81f-11e8-824b-4317115fe5b4.png\">\n</p>\n\nBy default I use an [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) in this project. You can find more information about SSD in [here](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab). \n\nPlease, See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies. You can easily select, download and use state-of-the-art models that are suitable for your requeirements using TensorFlow Object Detection API.\n\nYou can perform transfer learning on trained TensorFlow models to build your custom object counting systems!\n\n## Project Demo\n\nDemo video of the project is available on [My YouTube Channel](https://www.youtube.com/watch?v=bas6c8d1JyU).\n\n## Installation\n\n### Dependencies\n\nTensorflow Object Counting API depends on the following libraries:\n\n- TensorFlow Object Detection API\n- Protobuf 3.0.0\n- Python-tk\n- Pillow 1.0\n- lxml\n- tf Slim (which is included in the \"tensorflow/models/research/\" checkout)\n- Jupyter notebook\n- Matplotlib\n- Tensorflow\n- Cython\n- contextlib2\n- cocoapi\n\nFor detailed steps to install Tensorflow, follow the [Tensorflow installation instructions](https://www.tensorflow.org/install/). \n\nTensorFlow Object Detection API have to be installed to run TensorFlow Object Counting API, for more information, please see [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).\n\n## Citation\nIf you use this code for your publications, please cite it as:\n\n    @ONLINE{tfocapi,\n        author = \"Ahmet \u00d6zl\u00fc\",\n        title  = \"TensorFlow Object Counting API\",\n        year   = \"2018\",\n        url    = \"https://github.com/ahmetozlu/tensorflow_object_counting_api\"\n    }\n\n## Author\nAhmet \u00d6zl\u00fc\n\n## License\nThis system is available under the MIT license. See the LICENSE file for more info.\n\n\n\n\n\n\n"
 },
 {
  "repo": "RaiMan/SikuliX1",
  "language": "Java",
  "readme_contents": "[![RaiMan's Stuff](https://raw.github.com/RaiMan/SikuliX-2014-Docs/master/src/main/resources/docs/source/RaiManStuff64.png)](http://sikulix.com) SikuliX\n============\n\n**What is SikuliX**<br>SikuliX automates anything you see on the screen of your desktop computer \nrunning Windows, Mac or some Linux/Unix. It uses image recognition powered by OpenCV to identify \nGUI components and can act on them with mouse and keyboard actions.\nThis is handy in cases when there is no easy access to a GUI's internals or \nthe source code of the application or web page you want to act on. [More details](http://sikulix.com)\n\n<hr>\n\n**You need at least Java 8, but it works on Java 9 up to latest (currently 14)**\n\n**Windows:** Works out of the box ([for exceptions look here](https://github.com/RaiMan/SikuliX1/wiki/Windows:-Problems-with-libraries-OpenCV-or-Tesseract))\n\n**Mac:** you have to make Tesseract OCR available ([for HowTo look here](https://github.com/RaiMan/SikuliX1/wiki/macOS-Linux:-Support-libraries-for-Tess4J-Tesseract-4-OCR)). **Java 14: open problems with Tesseract**\n\n**Linux:** you have to make OpenCV and Tesseract OCR available ([for HowTo look here](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html#version-1-1-4-special-for-linux-people)).\n\n<hr>\n\n**Latest stable version is 2.0.4** (branch `release_2.0.x` - [see what is fixed](https://github.com/RaiMan/SikuliX1/wiki/ZZZ-Bug-Fixes))\n\n**Development version 2.1.0 currently not useable until further notice**<br>\nNew features will only be available in new major versions (currently 2.1.0, branches master and/or dev_...). \n<br>Until release of a stable 2.1.0, there will be nightly builds and snapshots available (see below).\n\n[Here you can read about the changes/enhancements](https://sikulix-2014.readthedocs.io/en/latest/news.html)\n\n**Get SikuliX ready to use**\n- [SikuliX IDE for editing and running scripts](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixide-2.0.4.jar)\n  - [Jython support for the IDE](https://repo1.maven.org/maven2/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar)\n  - [JRuby support for the IDE](https://repo1.maven.org/maven2/org/jruby/jruby-complete/9.2.0.0/jruby-complete-9.2.0.0.jar)\n  - download all needed to one folder and run sikulix-2.0.x.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixapi-2.0.4.jar)\n  - for use in non-Maven projects\n \nFor use in **Java Maven projects** the dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.0.4</version>\n</dependency>\n```\n<hr>\n\n**Current development version is 2.1.0** (branch `master` nightly builds / snapshots):<br>\n[![Build Status](https://travis-ci.org/RaiMan/SikuliX1.svg?branch=master)](https://travis-ci.org/RaiMan/SikuliX1)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1?ref=badge_shield)\n\n[Read about fixes, enhancements and new features](https://github.com/RaiMan/SikuliX1/wiki/About-fixes-and-enhancements-in-2.1.0)\n\n**Get the nightly builds ready to use** \n- [SikuliX IDE for editing and running scripts]()\n  - [Jython support for the IDE]()\n  - [JRuby support for the IDE]()\n  - download all needed to one folder and run sikulix-2.1.0.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages]()\n  - for use in non-Maven projects\n\nFor use in **Java Maven projects** use the SNAPSHOT dependency information:<br><br>\nThe repository URL:\n```\n<repositories>\n  <repository>\n    <id>sonatype-ossrh</id>\n    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n  </repository>\n</repositories>\n```\nThe dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.1.0-SNAPSHOT</version>\n</dependency>\n```\n<hr>\n\n**Development environment**\n\n - Java 11 (current JDK LTS release)\n - Java 8 (Oracle) for comatibility test\n - Source and target level for Java is version 8 as long as supported by Oracle\n - Maven project\n - Windows 10 latest (Pro 64-Bit)\n - Mac 10.15 latest\n - Ubuntu 18.04 in WSL on Windows 10 (basic tests only, headless)\n - Ubuntu 18.04 running in Oracle VM VirtualBox 6.1 on Windows 10\n - Using IntelliJ IDEA CE in all environments\n\n<hr>\n\n#### Contributions are welcome and appreciated\n - for `bugreports and requests for features or enhancements` use the issue tracker here\n - for `bugfixes` related to the latest release version you should create a pull request against the release branch (currently `release_2.0.x`), so your fix will be in the next bug-fix release (see milestones).\n- for `smaller bugfixes and/or feature enhancements` related to the running development (currently branch master as version 2.1.0-SNAPSHOT and dev_... branches) you should create a pull request against the target branch\n- a pull request should target only one branch. It is the resposibility and job of the maintainer to apply the changes to other branches in case \n- for `more complex revisions and/or enhancements` you should ask for a development branch together with a short description of your ideas\n \n **Please respect the following rules and guidelines when contributing**\n  - Start with smaller fixes. E.g. choose an issue from the issue tracker and try to fix it. Or fix issues you encounter while using SikuliX.\n  - Only fix cosmetic stuff if it's related to an issue you want to fix.\n  - Before you change stuff like dependencies / overall code style and so on, talk with the maintainer beforehand.<br>Sometimes there is a a reason that things are as they are (... and sometimes not :-)).\n  - Try to accept the individual coding styles of the acting contributors, even if some of the stuff might be sub-optimal in your eyes.<br>But feel free to talk about your ideas and the reasons behind.\n\n \n"
 },
 {
  "repo": "kylemcdonald/FaceTracker",
  "language": "C++",
  "readme_contents": "# FaceTracker\n\n**This repository is no longer maintained, due to challenges of upgrading to OpenCV 4.**\n\nFaceTracker is a library for deformable face tracking written in C++ using OpenCV 2, authored by [Jason Saragih](http://jsaragih.org/) and maintained by [Kyle McDonald](http://kylemcdonald.net/).\n\nAny publications arising from the use of this software, including but not limited to academic journal and conference publications, technical reports and manuals, should cite the following work: `J. M. Saragih, S. Lucey, and J. F. Cohn. Face Alignment through Subspace Constrained Mean-Shifts. International Conference of Computer Vision (ICCV), September, 2009.`\n\n## FAQ\n\n1. **\"I successfully compiled the code, so why is my app is crashing?\"** Make sure that your model files are in the right location. If you see the error `Assertion failed: s.is_open()` when running your app, that means you forgot to put the model files in the right directory.\n2. **\"Is there an example of using FaceTracker on a mobile device?\"** There is no official example. But there is an example of using ofxFaceTracker on iOS [here](https://github.com/kylemcdonald/ofxFaceTracker-iOS) and a native Android example [here](https://github.com/ajdroid/facetrackerapp).\n3. **\"Why is the tracking is slow, and why is there high CPU usage?\"** The face detection step (finding the general location of the face) can be slow. If this is causing an issue, you might want to put the tracking in a separate thread. If the detection is very slow you might try using a face detector that is native to your platform, and initializing FaceTracker with that rectangle.\n4. **Can I use this for my commercial project/product?** Yes. FaceTracker was re-licensed under the MIT license on April 8, 2020. Previously it was available under a custom non-commercial use license, with a separate license for commercial use available for purchase.\n\nWrappers are available for:\n\n* Android: [facetrackerapp](https://github.com/ajdroid/facetrackerapp)\n* [openFrameworks](http://www.openframeworks.cc/): [ofxFaceTracker](https://github.com/kylemcdonald/ofxFaceTracker)\n* [Cinder](http://libcinder.org/): [ciFaceTracker](https://github.com/Hebali/ciFaceTracker)\n* Python: [pyfacetracker](https://bitbucket.org/amitibo/pyfacetracker)\n\n## Installation\n\nThese instructions are for compiling the code on OS X and Ubuntu, but it should be possible to compile on other platforms.\n\nFirst, install OpenCV3 (if you're using OpenCV2, use the [opencv2](https://github.com/kylemcdonald/FaceTracker/tree/opencv2) branch of this repo). On OSX you can use [homebrew](http://brew.sh/):\n\n```\n$ brew tap homebrew/science\n$ brew install opencv3\n```\n\nAnd on Ubuntu use:\n\n```\n$ sudo apt-get install libcv-dev libopencv-dev\n```\n\nAlternatively, you can download [OpenCV from the GitHub](https://github.com/opencv/opencv) and compile it manually.\n\nAfter installing OpenCV, clone this repository with `git clone git://github.com/kylemcdonald/FaceTracker.git`. This repository contains a few subdirectories within the root directory:\n   - src (contains all source code)\n   - model (contains a pre-trained tracking model)\n   - bin (will contain the executable after building)\n\nNext, make sure that your copy of OpenCV is located in `/usr/local` (this should be the case if you used `apt-get`). If it isn't located there, modify the `OPENCV_PATH` in the `Makefile`. If you installed with Homebrew, it should be set to `/usr/local/opt/opencv3`.\n\nOptionally, you can also add `-fopenmp` to the `CFLAGS` and `-lgomp` to the `LIBRARIES` variable to compile with [OpenMP](http://openmp.org/) support.\n\nFrom the root `FaceTracker` directory, build the library and example by running `make`.\n\nTo test the demo, `cd bin` and `./face_tracker`. Because many webcams are 1280x720, try running `./face_tracker -s .25` to rescale the image before processing for a smoother framerate.\n\n## `face_tracker` Usage\n\n````\nUsage: face_tracker [options]\nOptions:\n-m <string> : Tracker model (default: ../model/face2.tracker)\n-c <string> : Connectivity (default: ../model/face.con)\n-t <string> : Triangulation (default: ../model/face.tri)\n-s <double> : Image scaling (default: 1)\n-d <int>    : Frames/detections (default: -1)\n--check     : Check for failure \n--help      : Print help\n-?          : Print help\n````\n"
 },
 {
  "repo": "shanren7/real_time_face_recognition",
  "language": "Python",
  "readme_contents": "# real_time_face_detection and recognition\nThis is a real time face detection and recognition project base  on opencv/tensorflow/mtcnn/facenet. Chinese version of description is [here](https://zhuanlan.zhihu.com/p/25025596) .Face detection is based on [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html).Face embedding is based on [Facenet](https://arxiv.org/abs/1503.03832).\n##Workflow\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/real%20time%20face%20detection%20and%20recognition.jpg)\n\n##Inspiration\nThe code was inspired by several projects as follows:\n\n1.[OpenFace](https://github.com/cmusatyalab/openface). The main idea was inspired by openface. However, I prefer python and tensorflow,so there comes this project.\n\n2.[davidsandberg/facenet](https://github.com/davidsandberg/facenet).\n\n   facenet.py was taken from https://github.com/davidsandberg/facenet/blob/master/facenet/src/facenet.py\n    \n   nn4.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/models/nn4.py\n    \n   detect_face.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/align/detect_face.py\n    \n3.[yobibyte/yobiface](https://github.com/yobibyte/yobiface).\n\n##Dependencies\n1.tensorflow\n2.opencv with python bindings (cv2)\n3.jupyter notebook for running .ipynb examples\n\n##Running\n1.Downloading pre-trained facenet from https://github.com/yobibyte/yobiface/blob/master/model/model-20160506.ckpt-500000 and putting in model_check_point folder.\n\n2.Running [real time face detection and recognition.ipynb](https://github.com/shanren7/real_time_face_recognition/blob/master/real%20time%20face%20detection%20and%20%20recognition.ipynb) with jupyter notebook\n\n##Results\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_20.jpg)\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_2192.jpg)\n"
 },
 {
  "repo": "christopher5106/FastAnnotationTool",
  "language": "C++",
  "readme_contents": "# Fast Image Data Annotation Tool (FIAT)\n\nFIAT enables image data annotation, data augmentation, data extraction, and result visualisation/validation.\n\n- annotate images for image classification, optical character reading (digit classification, letter classification), ...\n\n- extract data into different format (Caffe LMDB, OpenCV Cascade Classifiers, Tesseract ... ) with data augmentation (resizing, noise in translation / rotation / scaling, pepper noise , gaussian noise, rectangle merging, line extraction ...)\n\nThe philosophy of this tool is\n\n- to enable fast annotation : annotate data just by selecting the diagonal of the object, for a fixed ratio. Press Enter if the class is always the same. Type the letter of the class otherwise,\n\n- to be re-usable for different scenarios, and leave you free to build any other tool as input of the annotation process, using any pre-existing weaky classifier (depending on your case) or bounding box proposal algorithm such as selective search, to facilitate annotation with a list of rectangles to select or discard, by just typing the letter of the class or ESCAPE KEY,\n\n- to have the extraction tool act like a monad, so that you can apply transformation in any order, at any stage of your process, the format remaining the same : given a directory and a CSV file, extraction tool will produce a new directory and CSV file, in the same format, by default,\n\n- to feed any classification / training process,\n\n- to be usable for visualisation and export : a visual check that the data is correctly annotated, after manual annotation, extraction, or even after your own bounding box prediction algorithm if it uses the Output class to its produce results.\n\nRequires OPENCV 3 and Google Protobuf.\n\n### Build on Ubuntu 18.04\n    sudo apt-get install caffe-cpu caffe-doc caffe-tools-cpu libcaffe-cpu-dev libcaffe-cpu1 python3-caffe-cpu\n    sudo apt-get install libgoogle-glog-dev\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install liblmdb-dev\n    make all\n\n### File format\n\nRectangle extraction tools create annotations CSV files in the RotatedRect file format. [This blog post will give you the reasons motivating this choice](http://christopher5106.github.io/computer/vision/2015/12/26/file-format-for-computer-vision-annotations.html).\n\n\n### Annotation tool\n\n```bash\n./bin/annotateRect [FLAGS] input_dir output_file.csv\n```\n\nA tool used to annotate rectangles or to show the results (rectangles with `--init` option).\n\n![data annotation](http://christopher5106.github.io/img/annotator_erase.png)\n\n\n**Meaning of the colors** :\n\n- blue rectangle : currently active rectangle to annotate\n\n- green rectangles : remaining rectangles to annotate. Use `--init init_file.csv` option to feed rectangles to annotate to the annotation tool.\n\n- yellow rectangles : annotated rectangles\n\n**Add a rectangle or modify the rectangle** with the following keys :\n\n- Click with the mouse to set the center of the active rectangle (blue) or create a new active rectangle at this position.\n\n- Use arrow keys to move the active rectangle (blue)\n\n    - move left (left arrow key)\n    - move right (right arrow key)\n    - move up (up arrow key)\n    - move down (down arrow key)\n\n- Press **FN** while using arrow keys to change orientation/scale of the active rectangle (blue)\n\n    - rotate left (left arrow key)\n    - rotate right (right arrow key)\n    - augment size (up arrow key)\n    - downsize (down arrow key)\n\nFor platforms for which pressing FN with arrow key does not change key value, press **Space bar** to change into \"Rotation/Scale\" mode and use arrow keys.\n\n- **BACKSPACE**: erase the currently active rectangle.\n\n- **ESC**: next init rectangle or next image (without save)\n\n**Annotate the class** with :\n\n- **Any letter**: save the currently active rectangle (blue) with this letter as category / class . For example \"0\", if there is only one category. The blue rectangle will become yellow.\n\n- **ENTER**: save the letter with the same class as previously.\n\n\nFLAGS :\n\n- `--ratio 1.0` is the ratio height/width of the annotation rectangles.\n\n- `--cross` display a cross instead of a rectangle for non-current rectangles, ie previous (yellow) or future (green) rectangles. This is a useful display option when rectangles are too close together.\n\n![](http://christopher5106.github.io/img/annotation_cross.png)\n\n- `--init init_file.csv` to initialize the rectangles instead of selecting them manually (appear in green). The first one of them will be use as the currently active rectangle (blue). You can still add new rectangles when all init rectangles (green or blue) have been annotated.\n\n- `--export=output_dir` will not display annotation interface. Saves the annotated images to directory.\n\nNOTES :\n\n- the annotation tool can be stopped and launched again : it will resume the work from *output_file.csv*, previously annotated rectangles appear in yellow.\n\n- in case the init rectangles are bigger than the image, a white border is added to the image to show the rectangles outside the image.\n\n- although annotation tool can read images in an output_file.csv or init_file.csv outside current exe directory (by adding the CSV dir to the image path), it will save images with the input_dir as base path. So, when annotating, execute the command in the same directory as the output_file.csv.\n\nEXAMPLE :\n\nAnnotate images in the current directory :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nA first image will appear :\n\n![](pic.jpg)\n\nTo annotate fast, just select the diagonal, by clicking first the top left corner of the object, then the bottom right corner, as shown here with the red arrows :\n\n![](tutorial/pic_selection.jpg)\n\nImage is now selected and appear in blue:\n\n![](tutorial/selected.png)\n\nPress the key corresponding to its class, for example 'a'. Now it appears in yellow.\n\n![](tutorial/labeled.png)\n\nSelect the second book.\n\n![](tutorial/second_selected.png)\n\nAnd press the key corresponding to its class, for example 'e'.\n\n![](tutorial/result.png)\n\nThe output annotation file in CSV format *out.csv* will look like :\n\n```\n./pic.jpg,a,709,816,826,1116,-14.6958\n./pic.jpg,e,1510,607,741,1001,6.32224\n```\n\nAt any time, you can view how the annotations are, and potentially add new annotations with the same command :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nThe `--export` option is also very useful : it allows to export the images with the rectangles, without having the burden of the annotation interface and to facilitate results sharing for instance.\n\n### Extraction tool\n\nUse annotation information to extract a version of the images :\n\n```bash\n./bin/extractRect [FLAGS] annotations.csv output_dir\n```\n\nMoreove, the tool will create an output CSV file listing the new rectangle coordinates in the format `path,label,center_x,center_y,width,height,rotation,noise_x,noise_y,noise_rotation,noise_scale`.\n\nExtraction extracts at best quality possible.\n\nImage will be rotated so that annotation window will be parallel to the image borders.\n\nFor example in the previous example,\n\n```bash\n./bin/extractRect out.csv out\n```\n\nwill create an output directory *out* with two subdirectories corresponding to each label, *out/a* and *out/b* and its corresponding extracted objects, and a CSV file *out/results.csv* with image path, labels and new rectangle coordinates after the extraction.\n\n![](tutorial/a.jpg) ![](tutorial/e.jpg)\n\nExtract with a noise in rotation with `./bin/extractRect out3.csv out8 --noise_rotation=30`\n\n![](tutorial/a_with_rotation_noise.jpg) ![](tutorial/e_with_rotation_noise.jpg)\n\nSeveral options such as noise in translation, scale, or pepper/gaussian noise are available.\n\nThe `--full_image` option makes transformation available without extracting the rectangle.\n\nSee below for more options.\n\nINPUT SELECTION FLAGS :\n\n- `--input_class_filter` select entries of the specified class only.\n\n- `--limit` limit the number of annotation rectangles to consider, good for debuging purposes.\n\nEDIT RECTANGLES\n\n- `--skip_rotation` skips rotation information in annotation (all set to 0.0).\n\n- `--factor=1.2` extends the extraction box by a factor of the width and height. `--factor_width` and `--factor_height` do the same on one axis only and are cumulative with `--factor` (multiplication). In the example, `./bin/extractRect out.csv out7 --factor 1.2`\n\n![](tutorial/a_with_factor.jpg) ![](tutorial/e_with_factor.jpg)\n\n\n- `--offset_x` and `--offset_y` add an offset on each axis of the rectangle, in percentage of the width of the rectangle. In the example, select the titles of the book with the command `./bin/extractRect out3.csv out12 --offset_y=0.5 --factor_height=0.3`\n\n![](tutorial/a_subselection.jpg) ![](tutorial/e_subselection.jpg)\n\n- `--merge` : if multiple bounding box per images, will extract the global bounding box containing all rectangles in each image. In the example : `./bin/extractRect out.csv out --merge` will produce :\n\n![](tutorial/merge.jpg)\n\n\n- `--merge_line` : If multiple rectangle per images, merge rectangles that are roughly on the same line.\n\n- `--correct_ratio` : corrects the ratio of the annotated rectangles to the specified `--ratio` by augmenting one of the two dimensions (height or width). Default is false.\n\n- `--add_borders=true` : adds borders to the extracted image to fit the ratio. Default is false. Available only when `--resize_width` not zero.\n\n- `--ratio=1.0` is used in combination to `--correct_ratio` or `--resize-width` options. It defines the ratio (height/width) of the window to extract. The use of `--resize-width` option without `--correct_ratio` will stretch the image to final dimensions.\n\nNOISE FLAGS\n\n- `--noise_translation=0.2` adds a noise in translation of 20% of the width/height. Not taken into consideration for negatives generation. `--noise_translation_offset` can be used to specify a mininum noise (for negative generation for example).\n\n- `--noise_rotation=30` adds a noise in rotation in `[-noise_rotation\u00b0,noise_rotation\u00b0]`.\n\n- `--noise_zoomin=2 --noise_zoomout=3` adds a noise in scale factor uniformly distributed in `[1/3,200%]`. Not taken into consideration for negatives generation and in `--full_image` mode.\n\n- `--pepper_noise=0.1 --gaussian_noise=30` adds a pixel noise\n\n- `--samples` is the number of sample to extract per image. Default is 1. Useful in combination with noise option.\n\n\nOUTPUT FLAGS\n\n- `--full_image` will not extract the rectangle along the given annotation. Always true in `--backend=opencv` output mode.\n\n- `--resize_width=400` resizes the output to a width of `resize_width` and a height of `resize_width*ratio`. `--resize_width=0` will not resize the output. Default value is no resize. Not available in `--full_image` and `--backend=opencv` mode.\n\n- `--gray=true` extracts as a gray image. Default is false. Always true in `--backend=opencv` output mode.\n\n- `--backend=directory` defines the output format for storing the results. Possible values are : directory, lmdb, tesseract, opencv. Default value is directory.\n\n- `--output_class` override the class by the specified class\n\n- `--output_by_label=false` avoids creation of different output directories per label. Available for `--backend=directory` only.\n\n- `--append` append new extracts to an existing directory. Available for `--backend=directory` only.\n\n\nNEGATIVE GENERATION\n\n- `--neg_per_pos` defines the number of negative samples per positives to extract. By default, no negative (0).\n\n- `--neg_width=0.2` defines the width of negative samples to extract, in pourcentage to the largest image dimension (width or height).\n\n\n# License conditions\n\nCopyright (c) 2016 Christopher5106\n\nThis tool has been developped for a work at Axa, and is a contribution to OpenSource by Axa.\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n"
 },
 {
  "repo": "jayrambhia/Install-OpenCV",
  "language": "Shell",
  "readme_contents": "Install-OpenCV\n==============\n\nshell scripts to install different version of OpenCV in different distributions of Linux\n\n### Ubuntu\nif your system is Ubuntu, run the commands below.\n```\n$ cd Ubuntu\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### RedHat\nif your system is RedHat, run the commands below.\n```\n$ cd RedHat\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### ArchLinux\nif your system is ArchLinux, run the commands below.\n```\n$ cd ArchLinux\n$ chmod +x * \n$ ./opencv2_4_0.sh\n```\n"
 },
 {
  "repo": "joelibaceta/video-to-ascii",
  "language": "Python",
  "readme_contents": "<div align=center>\n\n  ![Logo](./images/logo.svg)\n\n<p>\n\n  It's a simple python package to play videos in a terminal using [ASCII](https://en.wikipedia.org/wiki/ASCII) characters.\n\n  [![Financial Contributors on Open Collective](https://opencollective.com/video-to-ascii/all/badge.svg?label=financial+contributors)](https://opencollective.com/video-to-ascii) [![PyPI version](https://badge.fury.io/py/video-to-ascii.svg)](https://badge.fury.io/py/video-to-ascii)\n  [![Maintainability](https://api.codeclimate.com/v1/badges/a5fcdf2b0cab41654ca3/maintainability)](https://codeclimate.com/github/joelibaceta/video-to-terminal/maintainability)\n  [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/joelibaceta/video-to-ascii)\n  [![HitCount](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii.svg)](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii)\n\n</p>\n\n![Screenshot](./images/Simpsons.apng)\n\n</div>\n\n<details><summary><b>Translations</b></summary>\n<p>\n\n- [\ud83c\uddfa\ud83c\uddf8 English](./README.md)\n- [\ud83c\uddea\ud83c\uddf8 Espa\u00f1ol](./translations/README_es.md)\n- [\ud83c\uddf9\ud83c\uddfc \u7e41\u9ad4\u4e2d\u6587](./translations/README_zh-TW.md)\n\n<p>\n</details>\n\n## Requirements\n\n- Python3\n- PortAudio (_Only required for installation with audio support_)\n- FFmpeg (_Only required for installation with audio support_)\n\n## Installation\n\nStandard installation\n\n```bash\n$ pip3 install video-to-ascii\n```\n\nWith audio support installation\n\n```bash\n$ pip3 install video-to-ascii --install-option=\"--with-audio\"\n```\n\n## How to use\n\nJust run `video-to-ascii` in your terminal\n\n```bash\n$ video-to-ascii -f myvideo.mp4\n```\n\n### Options\n\n**`--strategy`**\nAllow to choose an strategy to render the output.\n\n![Render Strategies](./images/Strategies.png)\n\n**`-o --output`**\nExport the rendering output to a bash file to share with someone.\n\n![Exporting](./images/export.png)\n\n**`-a --with-audio`**\nIf an installation with audio support was made, you can use this option to play the audio track while rendering the video ascii characters.\n\n## How it works\n\nEvery video is composed by a set of frames that are played at a certain frame rate.\n\n![Video Frames](./images/imgVideoFrames.png)\n\nSince a terminal has a specific number of rows and columns, we have to resize our video to adjust to the terminal size limitations.\n\n![Terminal](./images/imgTerminal.png)\n\nTo reach a correct visualization of an entire frame we need to adjust the _frame height_ to match the _terminal rows_, avoiding using more _characters_ than the number of _terminal columns_.\n\n![Resizing](./images/imgResizing.png)\n\nWhen picking a character to represent a pixel we need to measure the relevance of that pixel's color in the frame, based on that we can then select the most appropriate character based on the [relative luminance](https://en.wikipedia.org/wiki/Relative_luminance) in colorimetric spaces, using a simplify version of the luminosity function.\n\n<p align=\"center\">\n  <img src=\"./images/Luminosity.svg\">\n</p>\n\n> Green light contributes the most to the intensity perceived by humans, and blue light the least.\n\nThis function returns an integer in the range from 0 to 255, we assign a character according to density to show more colored surface for areas with more intense color (highest values).\n\n```python\nCHARS_LIGHT \t= [' ', ' ', '.', ':', '!', '+', '*', 'e', '$', '@', '8']\nCHARS_COLOR \t= ['.', '*', 'e', 's', '@']\nCHARS_FILLED    = ['\u2591', '\u2592', '\u2593', '\u2588']\n```\n\nThe reduced range of colors supported by the terminal is a problem we need to account for. Modern terminals support up to 256 colors, so we need to find the closest 8 bit color that matches the original pixel in 16 or 24 bit color, we call this set of 256 colors [ANSI colors](https://stackoverflow.com/questions/4842424/list-of-ansi-color-escape-sequences).\n\n![The Mapping of RGB and ANSI Colors](./images/imgPixelSection.png)\n\n![8 Bits Color Table](./images/8-bit_color_table.png)\n\nFinally, when putting it all together, we will have an appropriate character for each pixel and a new color.\n\n![Frame Image by Characters](../images/imgPixelImage.png)\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](./CONTRIBUTING.md)].\n\n<a href=\"https://github.com/joelibaceta/video-to-ascii/graphs/contributors\"><img src=\"https://opencollective.com/video-to-ascii/contributors.svg?width=890&button=false\" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/video-to-ascii/contribute/)].\n\nOr maybe just [buy me a coffee](https://ko-fi.com/joelibaceta).\n\n#### Individuals\n\n<a href=\"https://opencollective.com/video-to-ascii#backers\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/individuals.svg?width=890\"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/video-to-ascii/contribute)]\n\n<a href=\"https://opencollective.com/video-to-ascii/organization/0/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/1/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/2/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/3/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/4/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/5/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/6/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/7/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/8/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/9/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/9/avatar.svg\"></a>\n"
 },
 {
  "repo": "ryfeus/lambda-packs",
  "language": "Python",
  "readme_contents": "# lambda-packs \n\nPrecompiled packages for AWS lambda\n\n## How to start\n\n1. https://aws.amazon.com/lambda/ and create/sign in into account\n2. Lambda > Functions - Create lambda function\n3. Blank function\n4. Configure triggers - Next\n5. Configure function\n  - Runtime - Python 2.7\n6. Lambda function handler and role\n  - Handler - service.handler\n  - Role - Create new role from template(s)\n  - Role name - test\n  - Policy templates - Simple Microservice Permissions\n7. Advanced settings\n  - Memory (MB) 128\n  - Timeout 1 min 0 sec\n6. Code entry type - Upload a .ZIP file - choose Pack.zip from rep\n7. Test -> Save and test\n\n## How to modify\n\n1. Modify service.py file from sources folder\n2. Choose all files in sources folder when compressing, don't put it in one folder\n3. Upload zip file on function page\n\n## Current packs\n\n### Selenium PhantomJS\n\n#### Intro\n\nSelenium on PhantomJS. In fact - a ready-made tool for web scraping. For example, the demo now opens a random page in Wikipedia and sends its header. (PhantomJS at the same time disguises itself as a normal browser, knows how to log in, click and fill out forms) Also added requests, so you can do API requests for different resources to discard / take away the information.\n\nUseful for web testing and scraping.\n\n#### Demo\n\nCurrent demo opens random page from wiki (https://en.wikipedia.org/wiki/Special:Random) and prints title.\n\n#### Serverless start\n\n```\ngit clone https://github.com/ryfeus/lambda-packs.git\ncd lambda-packs/Selenium_PhantomJS/source/\nserverless deploy\nserverless invoke --function main --log\n```\n\nYou can also see the results from the API Gateway endpoint in a web browser.\n\n#### Documentation\n\nhttps://selenium-python.readthedocs.io/\n\n\n\n---\n### Pyresttest + WRK\n\n#### Intro\n\nWhat does the lambda have to do with it? In a nutshell on AWS in one region you can simultaneously run 200 lambdas (more if you write to support). Lambda works in 11 regions. So you can run in parallel more than 2000 lambdas, each of which will conduct load testing of your service. Five minutes of such testing will cost just one dollar.\n\n#### Demo\n\nDemo in this package tries to send requests to github.com for 5 seconds with 1 connection and also conduct pyresttest dummy test.\n\n#### Tools\n\n1. WRK (https://github.com/wg/wrk) - the main tool for load testing. It works with multiple threads, you can specify the number of connections and length of the load. For more fine-tuning, you can use LuaJIT scripts (https://www.lua.org/).\n2. Pyrestest (https://github.com/svanoort/pyresttest) is a handy tool for testing the full pipeline of the API. For example, the user registers, then uses the api key to create tasks / make notes / downloads files, then reads them, then deletes them.\n\n#### Documentation\n\nhttps://github.com/wg/wrk\n\nhttps://github.com/svanoort/pyresttest\n\n---\n### Lxml + requests\n\n#### Intro\n\nPackage for parsing static HTML pages. Difference here is that it works faster and consumes less memory than PhantomJS but is limited in terms websites it can parse and other features.\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/Lxml_requests/source -n lxml-requests\ncd lxml-requests\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\n```\nwget https://github.com/ryfeus/lambda-packs/blob/master/Lxml_requests/buildPack.sh\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\n#### Tools\n\nLxml 3.7.1\n\n#### Documentation\n\nhttp://lxml.de/\n\n---\n### Tensorflow\n\n#### Intro \n\nOpen source library for Machine Intelligence. Basically revolutionized AI and made it more accessible. Using tensorflow on lambda is not as bad as it may sound - for some simple models it is the simplest and the cheapest way to deploy.\n\n#### Demo\n\nAs hello world code I used recognition of images trained on imagenet (https://www.tensorflow.org/tutorials/image_recognition). Given the price tag lambda one run (recognition of one picture) will cost $0.00005. Therefore for a dollar you can recognize 20,000 images. It is much cheaper than almost any alternatives, though completely scalable (200 functions can be run in parallel), and can be easily integrated into cloud infrastructure. Current demo downloads image from link 'imagelink' from event source ( if empty - then downloads https://s3.amazonaws.com/ryfeuslambda/tensorflow/imagenet/cropped_panda.jpg)\n\n#### Tools\n\nTensorflow 1.4.0\n\n#### Documentation\n\nhttps://www.tensorflow.org/tutorials/image_recognition\n\n#### Nightly version\n\nNightly version archive is more than 50 MB in size but it is still eligible for using with AWS Lambda (though you need to upload pack through S3). For more read here:\n\nhttps://hackernoon.com/exploring-the-aws-lambda-deployment-limits-9a8384b0bec3\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/tensorflow/source -n tensorflow\ncd tensorflow\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\nfor Python2:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\nfor Python3:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack_py3.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index_py3.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack_py3.sh\n```\n\n> Note: Remember You should set `python3.6` for AWS Lambda function environment.\n\n#### Layer ARN\n\narn:aws:lambda:us-east-1:339543757547:layer:tensorflow-pack\n\n---\n### Sklearn\n\n#### Intro\n\nPackage for fans of machine learning, building models and the like. I doubt that there is a more convenient way to deploy model to the real world.\n\n#### Tools\n\n1. Scikit-learn 0.17.1\n2. Scipy 0.17.0\n\n#### Documentation\n\nhttp://scikit-learn.org/\n\n---\n### Skimage\n\n#### Intro\n\nPackage of image processing tools, and not only to style image, but also a large set of computer vision algorithms.\n\nThere are currently two zipped packs available, Pack.zip and Pack_nomatplotlib.zip, you probably want to use Pack_nomatplotlib.zip. See https://github.com/ryfeus/lambda-packs/issues/5 for more information.\n\n#### Tools\n\nScikit-image 0.12.3\n\n#### Documentation\n\nhttp://scikit-image.org/\n\n---\n### OpenCV + PIL\n\n#### Intro\n\nAnother package of image processing tools, and not only to style image, but also a large set of Computer vision algorithms.\n\n#### Tools\n\n1. OpenCV 3.1.0\n2. PIL 4.0.0\n\n#### Documentation\n\nhttps://pillow.readthedocs.io/\n\nhttp://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n\n---\n### Pandas\n\n#### Intro\n\nPackage for fans of statistics, data scientists and data engineers. RAM at lambda is 1.5 gigabytes, and the maximum operating time - 5 minutes. I am sure that will be enough for most tasks.\n\n#### Tools\n\nPandas 0.19.0\n\n#### Documentation\n\nhttp://pandas.pydata.org/\n\n---\n### Spacy\n\n#### Intro\n\nOpensource library for Natural Language Processing in python.\n\n#### Tools\n\n1. Spacy 2.0.11\n\n#### Documentation\n\nhttps://spacy.io/\n\n#### Example\n\nExample code loads language model from S3 and uses it to analyze sentence.\n\n---\n### Tesseract\n\n#### Intro\n\nOCR (optical character recognition) library for text recognition from the image.\n\n#### Documentation\n\nhttps://github.com/tesseract-ocr/tesseract\n\n---\n\n### PDF generator + Microsoft office file generator (docx, xlsx, pptx) + image generator (jpg, png) + book generator (epub)\n\n#### Intro\n\n\"Hello world\" code in package creates example of every document. Basically these libs are low memory (less than 128MB) and high speed (less than 0.5 seconds) so it's something like ~1m documents generated per 1$ in terms of AWS Lambda pricing.\n\n#### Tools\n\n- docx (python-docx - https://pypi.python.org/pypi/python-docx)\n- xlsx (XlsxWriter - https://pypi.python.org/pypi/XlsxWriter)\n- pptx (python-pptx - https://pypi.python.org/pypi/python-pptx)\n- pdf (Reportlab - https://pypi.python.org/pypi/reportlab)\n- epub (EbookLib - https://pypi.python.org/pypi/EbookLib)\n- png/jpg/... (Pillow - https://pypi.python.org/pypi/Pillow)\n\n\n---\n\n### Satellite imagery processing (rasterio + OSGEO + pyproj + shapely + PIL)\n\n#### Intro\n\nAWS Lambda pack in Python for processing satellite imagery. Basically it enables to deploy python code in an easy and cheap way for processing satellite imagery or polygons. In \u201chello world\u201d code of the pack I download red, green, blue Landsat 8 bands from AWS, make True Color image out of it and upload it to S3. It takes 35 seconds and 824MB of RAM for it so ~2500 scenes can be processed for 1$.\n\n#### Tools\n\n- Rasterio (https://github.com/mapbox/rasterio 0.36)\n- OSGEO (https://trac.osgeo.org/gdal/wiki/GdalOgrInPython)\n- Pyproj (https://github.com/jswhit/pyproj)\n- Shapely (https://github.com/Toblerity/Shapely)\n- PIL (https://pillow.readthedocs.io/)\n\n---\n\n### PyTorch\n\nPython 3.6 based PyTorch\n\n#### Tools\n\n- PyTorch 1.0.1 (CPU)\n- torchvision 0.2.1\n\n#### Installed Packages (deps)\n\n- numpy-1.16.1 \n- pillow-5.4.1 \n- six-1.12.0 \n- torchvision-0.2.1\n\n#### Build Pack\n\n```bash\n# You need `docker` before run\n./build-with-docker.sh\n```\n"
 },
 {
  "repo": "Image-Py/imagepy",
  "language": "Python",
  "readme_contents": "# Introduction\n\nImagePy is an open source image processing framework written in Python. Its UI interface, image data structure and table data structure are wxpython-based, Numpy-based and pandas-based respectively. Furthermore, it supports any plug-in based on Numpy and pandas, which can talk easily between scipy.ndimage, scikit-image, simpleitk, opencv and other image processing libraries.\n\n![newdoc01](http://idoc.imagepy.org/imgs/newdoc01.png)\n<div align=center>Overview, mouse measurement, geometric transformation, filtering, segmentation, counting, etc.</div><br>\n\n![newdoc02](http://idoc.imagepy.org/imgs/newdoc02.png)\n<div align=center>If you are more a IJ-style user, try `Windows -> Windows Style` to switch</div><br>\n\nImagePy:\n- has a user-friendly  interface\n- can read/save a variety of image data formats\n- supports ROI settings, drawing, measurement and other mouse operations\n- can perform image filtering, morphological operations and other routine operations\n- can do image segmentation, area counting, geometric measurement and density analysis.\n- is able to perform data analysis, filtering, statistical analysis and others related to the parameters extracted from the image.\n\nOur long-term goal of this project is to be used as ImageJ + SPSS (although not achieved yet)\n\n## Installation\n\n__OS support\uff1awindows, linux, mac, with python3.x__\n\n1.  ImagePy is a ui framework based on wxpython, which can not be installed\n    with pip on Linux. You need download\u00a0[the whl according to your\n    Linux system](https://wxpython.org/pages/downloads/).\n2.  On Linux and Mac, there may be permission denied promblem, for\n    ImagePy will write some config information, so please\u00a0start with\n    sudo. If you install with pip, please add \\--user parameter like\n    this:\u00a0pip install --user imagepy\n3.  If you install ImagePy in an Anaconda virtual environment, you may\n    get a error when starting like this:\u00a0This program needs access to the\n    screen. Please run with a Framework build of python, and only when\n    you are logged in on the main display, if so, please start with\n    pythonw -m imagepy.\n\n### - Pre-compiled package\nThis is the simplest option to run ImagePy.  \nA precompiled archive can be downloaded from the [release tab](https://github.com/Image-Py/imagepy/releases) of the repository.  \nSimply unzip the archive and run the ImagePy.bat file.  \nThis will open a command line window and open the GUI of ImagePy.\n\n### - Using pip\nIn a command-prompt type `pip install imagepy`.\n~~On Windows you currently need to first install shapely using conda.~~ This should also work for windows, now that shapely is available via pip.\nOnce installed, ImagePy can be run by typing `python -m imagepy` in a command prompt.\n\n\n\n## Citation\uff1a\n[ImagePy: an open-source, Python-based and platform-independent software package for bioimage analysis](https://academic.oup.com/bioinformatics/article/34/18/3238/4989871)\n\n## Forum\n\nImagePy is a community partner of forum.image.sc, Anything about the usage and development of ImagePy could be discussed in https://forum.image.sc.\n\n\n\n## Contribute\n\n**Contribute Manual:** All markdown file under [doc folder](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc) be parsed as manual. Plugins and manual are paired by plugins's title and manual's file name. We can browse document from the parameter dialog's Help button. We need more manual contributors, just pull request markdown file [here](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc).\n\n**Contribute Plugins:** Here is a [demo plugin](https://github.com/Image-Py/demoplugin) repositories with document to show how to write plugins and publish on ImagePy. You are wellcom and feel free to contact with us if you need help.\n\n**Improve Main Framework:** Just fork ImagePy, then give Pull Request. But if you want to add some new feature, Please have a issue with us firstly.\n\n## Basic operations\uff1a\n\nImagePy has a very rich set of features, and here, we use a specific example to show you a glimpse of the capacity of ImagePy. We choose the official coin split of scikit-image, since this example is simple and comprehensive.\n\n### Open image\n\n`menu: File -> Local Samples -> Coins` to open the sample image within ImagePy.\n_PS: ImagePy supports bmp, jpg, png, gif, tif and other commonly used file formats. By installing ITK plug-in\uff0cdicom\uff0cnii and other medical image formats can also be read/saved. It is also possible to read/write wmv, avi and other video formats by installing OpenCV._\n\n![newdoc03](http://idoc.imagepy.org/imgs/newdoc03.png)\n<div align=center>Coins</div><br>\n\n\n### Filtering & Segmentation\n\n`menu\uff1aProcess -> Hydrology -> Up And Down Watershed` Here, a composite filter is selected to perform sobel gradient extraction on the image, and then the upper and lower thresholds are used as the mark, and finally we watershed on the gradient map.\nFiltering and segmentation are the crucial skills in the image processing toolkit, and are the key to the success or failure of the final measurement.\nSegmentation methods such as adaptive thresholds, watersheds and others are also supported.\n\n![newdoc04](http://idoc.imagepy.org/imgs/newdoc04.png)\n<div align=center>Up And Down Watershed</div><br>\n\n![newdoc05](http://idoc.imagepy.org/imgs/newdoc05.png)\n<div align=center>Mask</div><br>\n\n### Binarization\n\n`menu\uff1aProcess -> Binary -> Binary Fill Holes` After the segmentation, we obtained a relatively clean mask image, but there is still some hollowing out, as well as some impurities, which will interfere with counting and measurement.\n_ImagePy supports binary operations such as erode, dilate, opening and closing, as well as skeletonization, central axis extraction, and distance transformation._\n\n![newdoc06](http://idoc.imagepy.org/imgs/newdoc06.png)\n<div align=center>Fill Holes</div><br>\n\n### Geometry filtering\n\n`menu\uff1aAnalysis -> Region Analysis -> Geometry Filter` ImagePy can perform geometric filtering based on :__the area, the perimeter, the topology, the solidity, the eccentricity__ and other parameters. You can also use multiple conditions for filtering. Each number can be positive|negative, which indicates the kept object will have the corresponding parameter greater|smaller than the value respectively. The kept objects will be set to the front color, the rejected ones will be set to the back color. In this demo, the back color is set to 100 in order to see which ones are filtered out. Once satisfied with the result, set the back color to 0 to reject them. In addition, ImagePy also supports gray density filtering, color filtering, color clustering and other functions.\n\n![newdoc07](http://idoc.imagepy.org/imgs/newdoc07.png)\n<div align=center>Geometry filtering (the area is over-chosen to emphasize the distinction)</div><br>\n\n\n### Geometry Analysis\n\n`menu\uff1aProcess -> Region Analysis -> Geometry Analysis` Count the area and analyze the parameters. By choosing the `cov` option, ImagePy will fit each area with an ellipse calculated via the covariance.\nThe parameters such as area, perimeter, eccentricity, and solidity shown in the previous step are calculated here. In fact, the filtering of the previous step is a downstream analysis of this one.\n\n![newdoc08](http://idoc.imagepy.org/imgs/newdoc08.png)\n<div align=center>Geometry Analysis</div><br>\n\n![newdoc09](http://idoc.imagepy.org/imgs/newdoc09.png)\n<div align=center>Generate the result table (dark to emphasize the ellipse)</div><br>\n\n\n### Sort Table by area\n\n`menu\uff1aTable -> Statistic -> Table Sort By Key` Select the major key as area, and select descend. The table will be sorted in descending order of area. A table is another important piece of data other than an image. In a sense, many times we need to get the required information on the image and then post-process the data in the form of a table. ImagePy supports table I/O (xls, xlsx, csv), filtering, slicing, statistical analysis, sorting and more.  (Right click on the column header to set the text color, decimal precision, line style, etc.)\n\n![newdoc10](http://idoc.imagepy.org/imgs/newdoc10.png)\n<div align=center>Table</div><br>\n\n\n### Charts\n\n`menu\uff1aTable -> Chart -> Hist Chart` From tabular data, we often need to draw a graph. Here, we plot the histograms of the area and the perimeter columns. ImagePy's tables can be used to draw common charts such as line charts, pie charts, histograms, and scatter plots (matplotlib-based). The chart comes with zooming, moving and other functions. The table can also be saved as an image.\n\n![newdoc11](http://idoc.imagepy.org/imgs/newdoc11.png)\n<div align=center>Histograms</div><br>\n\n\n### 3D chart\n\n`menu\uff1aKit3D -> Viewer 3D -> 2D Surface` Surface reconstruction of the image. This image shows the three reconstructed results including, sobel gradient map, high threshold and low threshold. It shows how the Up And Down Watershed works:\n- calculate the gradient.\n- mark the coin and background through the high and low thresholds,\n- simulate the rising water on the dem diagram to form the segmentation.\n\nImagePy can perform 3D filtering of images, 3D skeletons, 3D topological analysis, 2D surface reconstruction, and 3D surface visualization. The 3D view can be freely dragged, rotated, and the image results can be saved as a .stl file.\n\n![newdoc12](http://idoc.imagepy.org/imgs/newdoc12.png)\n<div align=center>3D visualisation</div><br>\n\n\n\n### Macro recording and execution\n\n`menu\uff1aWindow -> Develop Tool Suite` Macro recorder is shown in the develop tool panel. We have manually completed an image segmentation. However, batch processing more than 10 images can be tedious. So, assuming that these steps are highly repeatable and robust for dealing with such problems, we can record a macro to combine several processes into a one-click program. The macro recorder is similar to a radio recorder. When it is turned on, each step of the operation will be recorded. We can click the pause button to stop recording, then click the play button to execute. When the macro is running, the recorded commands will be executed sequentially, therefore achieving simplicity and reproducibility.\n\nMacros are saved into .mc files. drag and drop the file to the status bar at the bottom of ImagePy, the macro will be executed automatically. we can also copy the .mc file to the submenu of the menus under the ImagePy file directory. When ImagePy is started, the macro file will be parsed into a menu item at the corresponding location. By clicking the menu, the macro will also be executed.\n\n![newdoc13](http://idoc.imagepy.org/imgs/newdoc13.png)\n<div align=center>Macro Recording</div><br>\n\n\n### Workflow\n\nA macro is a sequence of predefined commands. By recording a series of fixed operations into macros, you can improve your work efficiency. However, the disadvantage is the lack of flexibility. For example, sometimes the main steps are fixed, but the parameter tuning needs human interaction. In this case, the workflow is what you want. A workflow in ImagePy is a flow chart that can be visualized, divided into two levels: __chapters and sections__.\nThe chapter corresponds to a rectangular area in the flow chart, and the section is a button in the rectangular area, which is also a command and is accompanied by a graphic explanation. The message window on the right will display the corresponding function description, while mousing hovering above. Click on the `Detail Document` in the top right corner to see the documentation of the entire process.\n\n\nThe workflow is actually written in MarkDown (a markup language), but it needs to be written respecting several specifications, as follows:\n\n```markdown\nTitle\n=====\n## Chapter1\n1. Section1\nsome coment for section1 ...\n2. ...\n## Chapter 2\n\t...\n```\n![newdoc14](http://idoc.imagepy.org/imgs/newdoc14.png)\n<div align=center>Workflow</div><br>\n\n### Report Plugin\n\nSometimes we need to make a report to print or generate a PDF document. ImagePy can generate report from a xlsx template. We just need put specific mark in some cells, ImagePy will parse the template and generate a parameter dialog, then we can input some information, or give image/table in, the report will be generated! more about how to make template please see [here](https://github.com/Image-Py/demoplugin/blob/master/doc/report.md).\n\n![newdoc14](http://idoc.imagepy.org/demoplugin/38.png)\n\n<div align=center>generate report</div><br>\n\n### Filter Plugin\n\nWe introduced macros and workflows in the last sections, using macros and workflows to connect existing functions is convenient. But sometimes we need to create new features. In this section, we are trying to add a new feature to ImagePy. ImagePy can easily access any Numpy-based function. Let's take the Canny operator of scikit-image as an example.\n\n```python\nfrom skimage import feature\nfrom imagepy.core.engine import Filter\n\nclass Plugin(Filter):\n    title = 'Canny'\n    note = ['all', 'auto_msk', 'auto_snap', 'preview']\n    para = {'sigma':1.0, 'low_threshold':10, 'high_threshold':20}\n\n    view = [(float, 'sigma', (0,10), 1, 'sigma', 'pix'),\n            ('slide', 'low_threshold', (0,50), 4, 'low_threshold'),\n            ('slide', 'high_threshold', (0,50), 4, 'high_threshold')]\n\ndef run(self, ips, snap, img, para = None):\n    return feature.canny(snap, para['sigma'], para['low_threshold'],\n        para['high_threshold'], mask=ips.get_msk())*255\n```\n![newdoc15](http://idoc.imagepy.org/imgs/newdoc15.png)\n<div align=center>Canny Filter Demo</div><br>\n\n#### Steps to create a your own filter:\n\n1. Import the package(s), often third party.\n2. Inherit the __`Filter`__ class\u3002\n3. The __`title`__ will be used as the name of the menu and the title of the parameter dialog, also as a command for macro recording.\n4. Tell the framework what needs to do for you in __`Note`__, whether to do type checking, to support the selection, to support _UNDO_, etc.\n5. __`Para`__ is the a dictionary of parameters, including needed parameters for the\n   functions.\n6. Define the interaction method for each of the parameters in __`View`__, the framework will automatically generate the dialog for parameter tuning by reading these information.\n7. Write the core function __`run`__. `img` is the current image, `para` is the result entre by user. if `auto_snap` is set in `note`, `snap` will be a duplicate of `img`. We can process the `snap`, store the result in `img`. <span style=\"color:red\">If the function does not support the specified output</span>, we can also return the result, and the framework will help us copy the result to img and display it.\n8. Save the file as `xxx_plg.py` and copy to the `menu` folder, restart ImagePy.\n   It will be loaded as a menu item.\n\n#### What did the framework do for us?\n\nThe framework unifies the complex tasks in a formal manner and helps us to perform:\n- type checking. If the current image type does not meet the requirements in the note, the analysis is terminated.\n- according to the `para`, generate automatically a dialog box to detect the input legality from the `view`.\n- Real-time preview\n- automatic ROI support\n- undo support\n- parallelization support\n- image stack support\n- etc.\n\n### Table\n\nAs mentioned earlier, the table is another very important data type other than the image. Similarly, ImagePy also supports the extension of table. Here we give an example of sorting-by-key used in the previous description.\n\n```python\nfrom imagepy.core.engine import Table\nimport pandas as pd\n\nclass Plugin(Table):\n    title = 'Table Sort By Key'\n    para = {'major':None, 'minor':None, 'descend':False}\n\n    view = [('field', 'major', 'major', 'key'),\n    \t    ('field', 'minor', 'minor', 'key'),\n    \t    (bool, 'descend', 'descend')]\n\ndef run(self, tps, data, snap, para=None):\n    by = [para['major'], para['minor']]\n    data.sort_values(by=[i for i in by if i != 'None'],\n        axis=0, ascending = not para['descend'], inplace=True)\n```\n![newdoc16](http://idoc.imagepy.org/imgs/newdoc16.png)\n<div align=center>Table Sort Demo</div><br>\n\n#### How Table works\n\nSame as `Filter`\uff0c`Table` also has parameters such as `title`\uff0c`note`\uff0c`para`\uff0c`view`.\nWhen the plugin is running, the framework will generate a dialog box according to `para`\nand `view`. After the parameters are chosen, they are passed to the `run` together with the current table and be processed. The table data is a pandas.DataFrame object in the current table, stored in `tps`. Other information, such as `tps.rowmsk`, `tps.colmsk` can also be retrieved from `tps` to get the row and column mask of the current selected table.\n\n### Other type of plugins\n\nThe `Filter` and `Table` described above are the two most important plugins, but ImagePy also supports some other types of plugin extensions. There are currently ten, they are:\n\n1. `Filter`: mainly for image processing\n2. `Simple`: similar to `Filter`, but focus on the overall characteristics of the image, such as the operation of the ROI, the operation of the false color, the area measurement, or the three-dimensional analysis of the entire image stack, visualization, and so on.\n3. `Free`: operate that are independant of image. Used to open image, close software etc.\n4. `Tool`: use the mouse to interact on the diagram and show small icons on the toolbar, such as a brush.\n5. `Table`: operate on the table, such as statistics analysis, sorting, plotting.\n6. `Widget`: widgets that are displayed in panels, such as the navigation bar on the right, the macro recorder, and others.\n7. `Markdown`: markup language, when clicked, a separate window will pop up to display the document.\n8. `Macros`\uff1acommand sequence file for serially fixed operational procedures.\n9. `Workflow`: combination of macro and MarkDown to create an interactive guidance process.\n10. `Report`: a xlsx template with specific mark, rename as `.rpt`, used to auto generate report.\n\n## Motivation & Goal\n\nPython is a simple, elegant, powerful language, and has very rich third-party libraries for scientific computing. Based on the universal matrix structure and the corresponding rules, numpy-based libraries such as scipy, scikit-image, scikit-learn and other scientific computing libraries have brought great convenience to scientific research. On the other hand, more and more problems in biology, material science and other scientific research can be efficiently and accurately solved via scientific computing, image processing.\n\nHowever there are still many researchers that lack programming skills. Thus it is a crucial to make the Numpy-based scientific computing libraries available to more researchers. ImagePy brings the computing capacities closer to the non-programmer researchers, so that they won't need to be concerned about the UI and interaction design, and focus exclusively on the algorithm itself, and finally, accelerate open-source tool building or even commercial products incubation. These tools, meanwhile, can let more researchers, who are not good at programming, gain, promote and popularize scientific knowledge such as image processing and statistics.\n"
 },
 {
  "repo": "zhengyima/DeepNude_NoWatermark_withModel",
  "language": "Python",
  "readme_contents": "\n# DeepNude\n\nDeepNude Source Code \n\n[The README.md in English](https://github.com/zhengyima/DeepNude_NoWatermark_withModel/blob/master/README_EN.md)\n\nDeepNude\u6e90\u4ee3\u7801\n\n\u53bb\u6c34\u5370 \n\n\u5e26\u4e09\u4e2a\u6a21\u578b.lib\u6587\u4ef6\u4e0b\u8f7d\u5730\u5740\n\n\u4f9b\u5e7f\u5927\u7a0b\u5e8f\u5458\u6280\u672f\u4ea4\u6d41\u4f7f\u7528\n\n~~[demo\u5730\u5740](http://39.105.149.229/dn): demo\u5f88\u539f\u59cb\u8106\u5f31\u4e0d\u9c81\u68d2\uff0c\u6240\u4ee5\u611f\u5174\u8da3\u7684\u8bdd\u5c3d\u91cf\u8fd8\u662f\u81ea\u5df1\u53bb\u8dd1\u4ee3\u7801\u5427\u3002\u4e0d\u8981\u5bf9demo\u505a\u574f\u4e8b\u54e6\uff0c\u4e0d\u7136\u5c31\u5173\u6389= =~~\n\n# Preinstallation\n\nBefore launch the script install these packages in your **Python3** environment:\n- numpy\n- Pillow\n- setuptools\n- six\n- pytorch \n- torchvision\n- wheel\n```\npip3 install numpy pillow setuptools six pytorch torchvision wheel\n```\n\n\u5efa\u8bae\u4f7f\u7528Conda\u5b89\u88c5 :) \n\n\n```\n conda create -n deepnude python=3.6 numpy Pillow setuptools six pytorch torchvision wheel\n conda activate deepnude\n```\n\n**\u6ce8\uff1a\u5982\u679c\u61d2\u5f97\u6298\u817ePython\u73af\u5883\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\uff0c\u89c1\u4e0b**\n\n\u611f\u8c22\u7f51\u53cb[\u98de\u54e5](https://github.com/fizzday)\u63d0\u4f9bdocker\u4e00\u952e\u8fd0\u884c\u90e8\u5206\u6280\u672f\u652f\u6301\n\n## \u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\n```bash\ncd ~\n\ngit clone https://github.com/zhengyima/DeepNude_NoWatermark_withModel.git --depth 1 deepnude\n\ncd deepnude\n\ndocker run --rm -it -v $PWD:/app:rw ababy/python-deepnude /bin/bash\n\npython main.py\n```\n> \u6ce8\u610f: docker\u8fd0\u884c\u53ea\u80fd\u4f7f\u7528cpu,\u6240\u4ee5,\u9700\u8981\u4fee\u6539gpu\u8fd0\u884c\u4e3acpu, \u4fee\u6539\u65b9\u6cd5\u8bf7\u53c2\u8003 [#GPU](#gpu). \u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u4e5f\u6162\u4e0d\u4e86\u591a\u5c11.  \n\n> \u5bf9\u5e94\u7684\u4e09\u4e2a .lib \u6587\u4ef6\u9700\u8981\u81ea\u5df1\u624b\u52a8\u4e0b\u8f7d\u540e, \u6dfb\u52a0\u5230\u9879\u76ee\u6839\u76ee\u5f55 `checkpoints` \u76ee\u5f55\u4e0b, \u624d\u80fd\u6b63\u5e38\u8fd0\u884c, \u7531\u4e8e\u6587\u4ef6\u592a\u5927, \u5c31\u6ca1\u6709\u653e\u5165docker\u955c\u50cf\n\n# Models\n\n\u5728\u8fd0\u884c\u4e4b\u524d\u9700\u4e0b\u8f7d\u4e09\u4e2a.lib\u6587\u4ef6\uff0c\u4e4b\u540e\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u65b0\u5efacheckpoints\u76ee\u5f55\uff0c\u5c06\u4e0b\u8f7d\u7684\u4e09\u4e2a\u6587\u4ef6\u653e\u81f3checkpoints\u76ee\u5f55\u4e0b\u3002\n\n\u53cb\u60c5\u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u4e0b\u8f7d\u6e20\u9053\uff1a\n\n\n* [Link](http://39.105.149.229/dn.zip)\n\n* [Google Drive](https://drive.google.com/drive/folders/1OKuIp0nxMUucgEScTc2vESvlpKzIWav4?usp=sharing)\n\n\n# Launch the script\n\n\u73af\u5883\u914d\u597d\uff0c\u6a21\u578b\u4e0b\u597d\u4e4b\u540e\u4fbf\u53ef\u4ee5\u8fd0\u884c\u4ee3\u7801\u4e86\uff01\n\n```\n python main.py\n```\n\nThe script will transform *input.png* to *output.png*.\n\n\n\n\n# GPU\n\n\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528id\u4e3a0\u7684GPU\u8fd0\u884c\u3002\n\n\u82e5\u8fd0\u884c\u73af\u5883\u4e0d\u5e26GPU\uff0c\u5219\u62a5\u9519\u3002\u5982\u679c\u6ca1\u6709GPU\u6216\u60f3\u4f7f\u7528CPU\u8fd0\u884c\u7a0b\u5e8f\uff0c\u8bf7\u5c06gan.py\u4e2d\n\n```\nself.gpu_ids = [0] #FIX CPU\n```\n\n\u6539\u4e3a (\n\n```\nself.gpu_ids = [] #FIX CPU\n```\n\n## Links\n- https://pytorch.org/\n\n"
 },
 {
  "repo": "foundry/OpenCVSwiftStitch",
  "language": "Objective-C++",
  "readme_contents": "__OpenCV computer vision with iOS: stitching panoramas__  \n\n<img src = \"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/example.big.jpg\">\n\nVersion 4.0 of [OpenCVStitch](http://github.com/foundry/OpenCVStitch) - updated for Swift 4.2  \nSee appropriate branches and tags for Swift 2.x - 3.x\n\nThis project was created to a answer a couple of Stack Overflow questions:  \n[libraries to CAPTURE panorama in iOS](http://stackoverflow.com/q/14062932/1375695)  \n[Can I mix Swift with C++? Like the Objective - C .mm files](http://stackoverflow.com/q/24042774/1375695)    \n\nv2 demonstrates how to mix Swift, Objective-C and C++ in one project whilst keeping the code clearly separate. \n\nThe project AppDelegate and View Controller are written in Swift. Swift cannot talk directly to C++ (which we need for OpenCV), so we provide an Objective-C++ wrapper class to mediate between Swift and C++. We also provide an Objective-C++ category on UIImage to mediate between UIImage and CV::Mat image formats. The CVWrapper header file is pure Objective-C. For [v1](https://github.com/foundry/OpenCVStitch)(which doesn't use Swift) this separation was a matter of clean style. For v2, it is a requirement: if any C++ headers are included in the wrapper, the app will not compile (Swift won't like it).\n\n__Installation__  \nTo run the project you need to install the OpenCV framework using Cocoapods    \n\nAssuming you have first [installed CocoaPods](https://guides.cocoapods.org/using/getting-started.html), run 'pod install' in this directory to install OpenCV for the project. From then on, always open the project in XCode from the `SwiftStitch.xcworkspace` file that the pod install creates. \n\nv3.1.0: The default podfile will install openCV v3.1.0, with a hotfix for arm64 compataibility\n\nv2.4.9: Recomment the podfile:  \n    pod 'OpenCV', '2.4.9-zp'  \n    #pod 'OpenCV', '~> 3.1.0.1'  \nChange the `#include` line for 2.4.9 compatibility as indicated in `stitching.cpp`  \n\n__Use__  \nOpenCVStitch is a very simple iOS/openCV example showing basic use of the Stitcher class. The c++ code is adapted from a sample included with the openCV distribution.  \n\nThe app has almost no user interface. On launch, the stitching code operates on four sample images, displaying the result in a UIScrollView.\n\n__OpenCVStitch Versions__   \n[Version 4.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/4.0)   \nSwift / Objective-C / C++   \nTested with XCode 10.0 / Swift 4.2 for iOS 8.0+  \n[Version 3.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/3.0)   \nSwift / Objective-C / C++   \nTested with XCode 8.0 / Swift 3.0 for iOS 8.0+  \n[Version 2.1](https://github.com/foundry/OpenCVSwiftStitch)   \nSwift / Objective-C / C++   \nTested with XCode 7.0 / Swift 2.0 for iOS 7.0+  \n[Version 2.0](https://github.com/foundry/OpenCVSwiftStitch/tree/v2.0)   \nSwift / Objective-C / C++   \nTested with XCode 6.4 / Swift 1.2 for iOS 7.0+  \n[Version 1.0](https://github.com/foundry/OpenCVStitch)  \nObjective-C / C++   \nTested with XCode 4.5.2 -> 6.3 for iOS 5.1 upwards   \n\nProvides a partial answer to: [Libraries to capture panoramas in iOS 6](http://stackoverflow.com/questions/14062932/libraries-to-capture-panorama-in-ios-6/14064788#14064788) (Stack Overflow)\n\n__OpenCV Versions__  \n_OpenCV 3.x_   \nThe podfile installs a hotfixed version of 3.1 as the prebuilt binary provided by openCV  [breaks on arm64 devices](https://github.com/kylefleming/opencv/releases/tag/3.1.0-ios-fix).\n\nThe stitching seems to be much more efficient (85% faster on my iphone 5s). However the quality is noticeably inferior with the test images supplied, so v2.4x remains the default for now [_some improvement with openCV 3.1_].\n\n_OpenCV 2.4.x_  \nThe openCV distribution is not as clean as we would like.  \n2.4.10 - the pod spec and the distribution build for iOS [is broken](http://stackoverflow.com/questions/28331397/cocoapods-opencv-2-4-10-linker-error/28820510)  \n2.4.9 - the pod spec [is broken](http://stackoverflow.com/questions/31005022/cant-install-opencv-with-cocoapods-could-not-resolve-host-hivelocity-dl-sourc). This is likely a result of relying on Sourceforge for hosting.  \n\nTherefore we are using a [self-hosted podspec](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV) (_thanks Matteo!_) until official channels are fixed. Specs are available for [2.4.9](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.9-zp) and [2.4.10](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.10-zp), but as the latter won't run we use 2.4.9.\n\n_this version of OpenCVStitch opted to use cocoapods to overcome the [notorious](http://stackoverflow.com/q/13905471/1375695) [installation](http://stackoverflow.com/q/15855894/1375695) [issues](http://stackoverflow.com/a/14186883/1375695) with previous versions of the openCV 2.4.x framework.. it seems our optimism was slightly premature!_\n\n__XCode 10__  \n\nProject is now updated for Swift 4.1 and XCode 10. For backwards compatibility checkout the 2.0 / 2.1 branches, 3.0 release or refer to the Objective-C version v1.0.\n\n__Comparisons__\n\n<table><tr>\n<td>OpenCV 2.4.9</td><td>OpenCV 3.0.0</td><td>OpenCV 3.1.0</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n</tr></table>\n"
 },
 {
  "repo": "jhansireddy/AndroidScannerDemo",
  "language": "C++",
  "readme_contents": "# ScanLibrary\nScanLibrary is an android document scanning library built on top of OpenCV, using the app you will be able to select the exact edges and crop the document accordingly from the selected 4 edges and change the perspective transformation of the cropped image.\n\n# Screenshots\n\n<div align=\"center\">\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" alt=\"Scan Input\" title=\"Scan Input\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" alt=\"Scan Points\" title=\"Scan Points\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" alt=\"After Scan\" title=\"After Scan\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" alt=\"Scanned Result\" title=\"Scanned Result\"></img>\n\n</div>\n\n# Videos\n\n<div align=\"center\" >\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Kl7rRZ79m6k\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" \nalt=\"Scan Video\" width=\"40%\" border=\"10\" /></a>\n</div>\n\n# Using it in your project\n\n* `git clone https://github.com/jhansireddy/AndroidScannerDemo.git` into a standalone dir;\n* Create or using your project: `File -> New -> Import module...`;\n* As source directory point to: `~/_dirWhereYouClonedAndroidScannerDemo_/AndroidScannerDemo/ScanDemoExample/scanlibrary` and confirm;\n\n- Add the dependency to your main app build.gradle this way: \n```\t    \ncompile project(':scanlibrary')\n```\n- In your activity or fragment when you want to give an option of document scanning to user then:\nStart the scanlibrary ScanActivity, with this the app will go to library, below is the sample code snippet:\nNote: preference can be one of OPEN_CAMERA or OPEN_MEDIA or left empty, based on the passed preference the scan library decides to open camera or media or open the scan home page.\n```java\n       int REQUEST_CODE = 99;\n       int preference = ScanConstants.OPEN_CAMERA;\n       Intent intent = new Intent(this, ScanActivity.class);\n       intent.putExtra(ScanConstants.OPEN_INTENT_PREFERENCE, preference);\n       startActivityForResult(intent, REQUEST_CODE);\n```\n\n- Once the scanning is done, the application is returned from scan library to main app, to retrieve the scanned image, add onActivityResult in your activity or fragment from where you have started startActivityForResult, below is the sample code snippet:\n```java\n    @Override\n    protected void onActivityResult(int requestCode, int resultCode, Intent data) {\n        super.onActivityResult(requestCode, resultCode, data);\n        if (requestCode == REQUEST_CODE && resultCode == Activity.RESULT_OK) {\n            Uri uri = data.getExtras().getParcelable(ScanConstants.SCANNED_RESULT);\n            Bitmap bitmap = null;\n            try {\n                bitmap = MediaStore.Images.Media.getBitmap(getContentResolver(), uri);\n                getContentResolver().delete(uri, null, null);\n                scannedImageView.setImageBitmap(bitmap);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n```\n# License\n\n\tCopyright (c) 2016 Jhansi Karee\n\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\n\n- **IMPORTANT:** This project uses the OPENCV Framework. Download the newest version here 'http://opencv.org/.\n"
 },
 {
  "repo": "trishume/eyeLike",
  "language": "C++",
  "readme_contents": "## eyeLike\nAn OpenCV based webcam gaze tracker based on a simple image gradient-based eye center algorithm by Fabian Timm.\n\n## DISCLAIMER\n**This does not track gaze yet.** It is basically just a developer reference implementation of Fabian Timm's algorithm that shows some debugging windows with points on your pupils.\n\nIf you want cheap gaze tracking and don't mind hardware check out [The Eye Tribe](https://theeyetribe.com/).\nIf you want webcam-based eye tracking contact [Xlabs](http://xlabsgaze.com/) or use their chrome plugin and SDK.\nIf you're looking for open source your only real bet is [Pupil](http://pupil-labs.com/) but that requires an expensive hardware headset.\n\n## Status\nThe eye center tracking works well but I don't have a reference point like eye corner yet so it can't actually track\nwhere the user is looking.\n\nIf anyone with more experience than me has ideas on how to effectively track a reference point or head pose\nso that the gaze point on the screen can be calculated contact me.\n\n## Building\n\nCMake is required to build eyeLike.\n\n### OSX or Linux with Make\n```bash\n# do things in the build directory so that we don't clog up the main directory\nmkdir build\ncd build\ncmake ../\nmake\n./bin/eyeLike # the executable file\n```\n\n### On OSX with XCode\n```bash\nmkdir build\n./cmakeBuild.sh\n```\nthen open the XCode project in the build folder and run from there.\n\n### On Windows\nThere is some way to use CMake on Windows but I am not familiar with it.\n\n## Blog Article:\n- [Using Fabian Timm's Algorithm](http://thume.ca/projects/2012/11/04/simple-accurate-eye-center-tracking-in-opencv/)\n\n## Paper:\nTimm and Barth. Accurate eye centre localisation by means of gradients.\nIn Proceedings of the Int. Conference on Computer Theory and\nApplications (VISAPP), volume 1, pages 125-130, Algarve, Portugal,\n2011. INSTICC.\n\n(also see youtube video at http://www.youtube.com/watch?feature=player_embedded&v=aGmGyFLQAFM)\n"
 },
 {
  "repo": "airob0t/idcardgenerator",
  "language": "Python",
  "readme_contents": "# idcardgenerator\n\u3010\u4ec5\u505a\u7814\u7a76\u4f7f\u7528\uff0c\u8bf7\u9075\u5b88\u5f53\u5730\u6cd5\u5f8b\u6cd5\u89c4\uff0c\u6cd5\u5f8b\u540e\u679c\u81ea\u8d1f\u3011\n\n\u8eab\u4efd\u8bc1\u56fe\u7247\u751f\u6210\u5de5\u5177,\u586b\u5165\u4fe1\u606f\uff0c\u9009\u62e9\u4e00\u5f20\u5934\u50cf\u56fe\u7247,\u5373\u53ef\u751f\u6210\u9ed1\u767d\u548c\u5f69\u8272\u8eab\u4efd\u8bc1\u56fe\u7247\u3002\n\n\u53ef\u4ee5\u9009\u62e9\u662f\u5426\u81ea\u52a8\u62a0\u56fe\uff0c\u81ea\u52a8\u62a0\u56fe\u76ee\u524d\u4ec5\u652f\u6301\u7eaf\u8272\u80cc\u666f\uff0c\u5bf9\u81ea\u52a8\u62a0\u56fe\u6548\u679c\u4e0d\u6ee1\u610f\u53ef\u4ee5\u624b\u52a8\u62a0\u56fe\u3002\n\n\u5728\u7ebf\u62a0\u56fe\u5730\u5740:(https://burner.bonanza.com/)\n\n(https://www.gaoding.com/koutu)\n\n## \u66f4\u65b0:\n- \u81ea\u52a8\u6539\u53d8\u5934\u50cf\u5927\u5c0f\n- \u81ea\u52a8\u4ece\u7eaf\u8272\u80cc\u666f\u4e2d\u62a0\u56fe\n- \u652f\u6301pip\u5b89\u88c5\n\n## ToDo\n- \u81ea\u52a8\u4ece\u590d\u6742\u80cc\u666f\u4e0b\u62a0\u56fe\n\n## \u73af\u5883\n- numpy\n- pillow\n- opencv\n\n## \u4e0b\u8f7d\n## pip\u5b89\u88c5\n`pip install idcardgenerator`\n\n```\nfrom idcardgenerator import gui\ngui.run()\n```\n\u6587\u4ef6\u4f1a\u751f\u6210\u5728\u8fd0\u884c\u76ee\u5f55\n\n### Windows\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/win_v1.3/idcardgenerator.exe)\n### Mac\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/v1.1/idcardgenerator)\n\n## \u6253\u5305\u7a0b\u5e8f\n\n\u5b89\u88c5pyinstaller\n\n`pip install pyinstaller`\n\nMac\u6253\u5305(\u6253\u5305\u6210Mac app\u5c1a\u6709\u95ee\u9898\u672a\u89e3\u51b3)\n\n    pyinstaller -i usedres/ico.icns --windowed --clean --noconfirm --onefile --add-data ./usedres:./usedres idcardgenerator.py\n\nWindows\u6253\u5305\n\n    pyinstaller -i usedres/ico.ico --windowed --clean --noconfirm --onefile --add-data \"usedres;usedres\" idcardgenerator.py\n\n## \u53c2\u7167\u6807\u51c6\uff1a\n\n### \u6b63\u9762\n\u3000\u3000\u5de6\u4e0a\u89d2\u4e3a\u56fd\u5fbd\uff0c\u7528\u7ea2\u8272\u6cb9\u58a8\u5370\u5237;\u5176\u53f3\u4fa7\u4e3a\u8bc1\u4ef6\u540d\u79f0\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\uff0c\u5206\u4e0a\u4e0b\u4e24\u6392\u6392\u5217\uff0c\u5176\u4e2d\u4e0a\u6392\u7684\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u201d\u4e3a4\u53f7\u5b8b\u4f53\u5b57\uff0c\u4e0b\u6392\u7684\u201c\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\u4e3a2\u53f7\u5b8b\u4f53\u5b57;\u201c\u7b7e\u53d1\u673a\u5173\u201d\u3001\u201c\u6709\u6548\u671f\u9650\u201d\u4e3a6\u53f7\u52a0\u7c97\u9ed1\u4f53\u5b57;\u7b7e\u53d1\u673a\u5173\u767b\u8bb0\u9879\u91c7\u7528\uff0c\u201cxx\u5e02\u516c\u5b89\u5c40\u201d;\u6709\u6548\u671f\u9650\u91c7\u7528\u201cxxxx.xx-xxxx.xx.xx\u201d\u683c\u5f0f\uff0c\u4f7f\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff0c\u5168\u90e8\u7528\u9ed1\u8272\u6cb9\u58a8\u5370\u5237\u3002\n\n### \u80cc\u9762\n\u3000\u3000\u201c\u59d3\u540d\u201d\u3001\u201c\u6027\u522b\u201d\u3001\u201c\u6c11\u65cf\u201d\u3001\u201c\u51fa\u751f\u5e74\u6708\u65e5\u201d\u3001\u201c\u4f4f\u5740\u201d\u3001\u201c\u516c\u6c11\u8eab\u4efd\u53f7\u7801\u201d\u4e3a6\u53f7\u9ed1\u4f53\u5b57\uff0c\u7528\u84dd\u8272\u6cb9\u58a8\u5370\u5237\uff1b\u767b\u8bb0\u9879\u76ee\u4e2d\u7684\u59d3\u540d\u9879\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u5176\u4ed6\u9879\u76ee\u5219\u7528\u5c0f5\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u51fa\u751f\u5e74\u6708\u65e5 \u65b9\u6b63\u9ed1\u4f53\u7b80\u4f53\u5b57\u7b26\u5927\u5c0f\uff1a\u59d3\u540d\uff0b\u53f7\u7801\uff0811\u70b9\uff09\u5176\u4ed6\uff089\u70b9\uff09\u5b57\u7b26\u95f4\u8ddd\uff08AV\uff09\uff1a\u53f7\u7801\uff0850\uff09\u5b57\u7b26\u884c\u8ddd\uff1a\u4f4f\u5740\uff0812\u70b9\uff09\uff1b\u8eab\u4efd\u8bc1\u53f7\u7801\u5b57\u4f53   OCR-B 10 BT   \u6587\u5b57 \u534e\u6587\u7ec6\u9ed1\u3002\n"
 },
 {
  "repo": "asingh33/CNNGestureRecognizer",
  "language": "Python",
  "readme_contents": "[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/V7V01IK53)\n\nIf you find my work useful, then please do consider supporting me. This will help me keep motivated and do more of such projects. \nThanks !\n\n\n\n\n[![DOI](https://zenodo.org/badge/89872749.svg)](https://zenodo.org/badge/latestdoi/89872749)\n# CNNGestureRecognizer\nGesture recognition via CNN neural network implemented in Keras + Theano + OpenCV\n\n\nKey Requirements:\nPython 3.6.1\nOpenCV 3.4.1\nKeras 2.0.2\nTensorflow 1.2.1\nTheano 0.9.0   (obsolete and not supported any further)\n\nSuggestion: Better to download Anaconda as it will take care of most of the other packages and easier to setup a virtual workspace to work with multiple versions of key packages like python, opencv etc.\n\n# New changes\nI have uploaded few more changes to this repo -\n- Project is Python3 compatible now.\n- Added TensorFlow support, as Theano's development has been stopped.\n- Added a new background subtraction filter, which is by far the best performing filter for this project\n- Added lots of performance improving changes. There is now literally no FPS drop when prediction mode is enabled\n- An in-app graph plotting has been added to observe the probability of the gesture predictions \n \n# Repo contents\n- **trackgesture.py** : The main script launcher. This file contains all the code for UI options and OpenCV code to capture camera contents. This script internally calls interfaces to gestureCNN.py.\n- **gestureCNN.py** : This script file holds all the CNN specific code to create CNN model, load the weight file (if model is pretrained), train the model using image samples present in **./imgfolder_b**, visualize the feature maps at different layers of NN (of pretrained model) for a given input image present in **./imgs** folder.\n- **imgfolder_b** : This folder contains all the 4015 gesture images I took in order to train the model.\n```diff\n- Note: I have replaced ori_4015imgs_weights.hdf5 weight file with these two OS specific weight files. \n```\n- **_pretrained_weights_MacOS.hdf5_** : This is pretrained weight file on MacOS. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1j7K96Dkatz6q6zr5RsQv-t68B3ZOSfh0/view\n- **_pretrained_weights_WinOS.hdf5_** : This is pretrained weight file on Windows. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1PA7rJxHYQsW5IvcZAGeoZ-ExYSttFuGs/view\n- **_imgs_** - This is an optional folder of few sample images that one can use to visualize the feature maps at different layers. These are few sample images from imgfolder_b only.\n- **_ori_4015imgs_acc.png_** : This is just a pic of a plot depicting model accuracy Vs validation data accuracy after I trained it.\n- **_ori_4015imgs_loss.png_** : This is just a pic of a plot depicting model loss Vs validation loss after I training.\n\n# Usage\n**On Mac**\n```bash\neg: With Theano as backend\n$ KERAS_BACKEND=tensorflow python trackgesture.py \n```\n**On Windows**\n```bash\neg: With Tensorflow as backend\n> set \"KERAS_BACKEND=tensorflow\"\n> python trackgesture.py \n```\n\nWe are setting KERAS_BACKEND to change backend to Theano, so in case you have already done it via Keras.json then no need to do that. But if you have Tensorflow set as default then this will be required.\n\n# Features\nThis application comes with CNN model to recognize upto 5 pretrained gestures:\n- OK\n- PEACE\n- STOP\n- PUNCH\n- NOTHING (ie when none of the above gestures are input)\n\nThis application provides following functionalities:\n- Prediction : Which allows the app to guess the user's gesture against pretrained gestures. App can dump the prediction data to the console terminal or to a json file directly which can be used to plot real time prediction bar chart (you can use my other script - https://github.com/asingh33/LivePlot)\n- New Training : Which allows the user to retrain the NN model. User can change the model architecture or add/remove new gestures. This app has inbuilt options to allow the user to create new image samples of user defined gestures if required.\n- Visualization : Which allows the user to see feature maps of different NN layers for a given input gesture image. Interesting to see how NN works and learns things.\n\n\n# Demo \nYoutube link - https://www.youtube.com/watch?v=CMs5cn65YK8\n\n![](https://j.gifs.com/X6zwYm.gif)\n\n# Gesture Input\nI am using OpenCV for capturing the user's hand gestures. In order to simply things I am doing post processing on the captured images to highlight the contours & edges. Like applying binary threshold, blurring, gray scaling.\n\nI have provided two modes of capturing:\n- Binary Mode : In here I first convert the image to grayscale, then apply a gaussian blur effect with adaptive threshold filter. This mode is useful when you have an empty background like a wall, whiteboard etc.\n- SkinMask Mode : In this mode, I first convert the input image to HSV and put range on the H,S,V values based on skin color range. Then apply errosion followed by dilation. Then gaussian blur to smoothen out the noises. Using this output as a mask on original input to mask out everything other than skin colored things. Finally I have grayscaled it. This mode is useful when there is good amount of light and you dont have empty background.\n\n**Binary Mode processing**\n```python\ngray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray,(5,5),2)   \nth3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\nret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n```\n\n![OK gesture in Binary mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiiok160.png)\n\n\n**SkindMask Mode processing**\n```python\nhsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n    \n#Apply skin color range\nmask = cv2.inRange(hsv, low_range, upper_range)\n\nmask = cv2.erode(mask, skinkernel, iterations = 1)\nmask = cv2.dilate(mask, skinkernel, iterations = 1)\n\n#blur\nmask = cv2.GaussianBlur(mask, (15,15), 1)\n#cv2.imshow(\"Blur\", mask)\n\n#bitwise and mask original frame\nres = cv2.bitwise_and(roi, roi, mask = mask)\n# color to grayscale\nres = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n```\n![OK gesture in SkinMask mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiok44.png)\n\n\n# CNN Model used\nThe CNN I have used for this project is pretty common CNN model which can be found across various tutorials on CNN. Mostly I have seen it being used for Digit/Number classfication based on MNIST database.\n\n```python\nmodel = Sequential()\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv),\n                    padding='valid',\n                    input_shape=(img_channels, img_rows, img_cols)))\nconvout1 = Activation('relu')\nmodel.add(convout1)\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\nconvout2 = Activation('relu')\nmodel.add(convout2)\nmodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n```\n\nThis model has following 12 layers -\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 32, 198, 198)      320       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 32, 198, 198)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 32, 196, 196)      9248      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 32, 196, 196)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 32, 98, 98)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32, 98, 98)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 307328)            0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               39338112  \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 5)                 0         \n=================================================================\n```\nTotal params: 39,348,325.0\nTrainable params: 39,348,325.0\n\n# Training\nIn version 1.0 of this project I had used 1204 images only for training. Predictions probability was ok but not satisfying. So in version 2.0 I increased the training image set to 4015 images i.e. 803 image samples per class. Also added an additional class 'Nothing' along with the previous 4 gesture classes.\n\nI have trained the model for 15 epochs.\n\n![Training Accuracy Vs Validation Accuracy](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_acc.png)\n\n![Training Loss Vs Validation Loss](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_loss.png)\n\n\n# Visualization\nCNN is good in detecting edges and thats why its useful for image classificaion kind of problems. In order to understand how the neural net is understanding the different gesture input its possible to visualize the layer feature map contents.\n\nAfter launching the main script choose option 3 for visualizing different or all layer for a given image (currently it takes images from ./imgs, so change it accordingly)\n```\nWhat would you like to do ?\n    1- Use pretrained model for gesture recognition & layer visualization\n    2- Train the model (you will require image samples for training under .\\imgfolder)\n    3- Visualize feature maps of different layers of trained model\n    3\nWill load default weight file\nImage number 7\nEnter which layer to visualize -1\n(4015, 40000)\nPress any key\nsamples_per_class -  803\nTotal layers - 12\nDumping filter data of layer1 - Activation\nDumping filter data of layer2 - Conv2D\nDumping filter data of layer3 - Activation\nDumping filter data of layer4 - MaxPooling2D\nDumping filter data of layer5 - Dropout\nCan't dump data of this layer6- Flatten\nCan't dump data of this layer7- Dense\nCan't dump data of this layer8- Activation\nCan't dump data of this layer9- Dropout\nCan't dump data of this layer10- Dense\nCan't dump data of this layer11- Activation\nPress any key to continue\n```\n\nTo understand how its done in Keras, check visualizeLayer() in gestureCNN.py\n```python\nlayer = model.layers[layerIndex]\n\nget_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\nactivations = get_activations([input_image, 0])[0]\noutput_image = activations\n```\nLayer 4 visualization for PUNCH gesture\n![Layer 4 visualization for PUNCH gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_4_layer4_MaxPooling2D.png)\n\nLayer 2 visualization for STOP gesture\n![Layer 2 visualization for STOP gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_7_layer2_Conv2D.png)\n\n\n\n# Conclusion\nSo where to go from here? Well I thought of testing out the responsiveness of NN predictions and games are good benchmark. On MAC I dont have any games installed but then this Chrome Browser Dino Jump game came handy. So I bound the 'Punch' gesture with jump action of the Dino character. Basically can work with any other gesture but felt Punch gesture is easy. Stop gesture was another candidate.\n\nWell here is how it turned out :)\n\nWatch full video - https://www.youtube.com/watch?v=lnFPvtCSsLA&t=49s\n\n![](https://j.gifs.com/58pxVx.gif)\n\n\n\n# In case you want to cite my work\nAbhishek Singh,\u201dasingh33/CNNGestureRecognizer: CNNGestureRecognizer (Version 1.3.0)\u201d, Zenodo. http://doi.org/10.5281/zenodo.1064825, Nov. 2017.  \nDo tell me how you used this work in your project. Would love to see your work. Good Luck!\n\n\n\nDont forget to check out my other github project where I used this framework and applied SuperVised machine learning technique to train the Chrome Browser's TRex character :)\nhttps://github.com/asingh33/SupervisedChromeTrex\nYoutube link - https://youtu.be/ZZgvklkQrss\n\n![](https://j.gifs.com/DRg4mn.gif)\n\n"
 },
 {
  "repo": "Paperspace/DataAugmentationForObjectDetection",
  "language": "Jupyter Notebook",
  "readme_contents": "# Data Augmentation For Object Detection\n\nAccompanying code for the [Paperspace tutorial series on adapting data augmentation methods for object detection tasks](https://blog.paperspace.com/data-augmentation-for-bounding-boxes/)\n\n## Dependencies\n1. OpenCV 3.x\n2. Numpy\n3. Matplotlib\n\nWe support a variety of data augmentations, like.\n\n### Horizontal Flipping\n![Horizontal Flip](Images/hflip.png)\n\n### Scaling\n![Scaling](Images/scale_aug.png)\n\n### Translation\n![Translation](Images/transl_aug.png)\n\n### Rotation\n![Rotation](Images/rotate.png)\n\n### Shearing\n![Shearing](Images/shear_box.png)\n\n### Resizing\n![Resizing](Images/resize.png)\n\n\n## Quick Start\nA quick start tutorial can be found in the file `quick-start.ipynb` in this repo.\n\n## Documentation\nA list of all possible transforms and extensive documentation can be found in by opening `docs/build/html/index.html` in your browser or at this [link.](https://augmentationlib.paperspace.com/)\n"
 },
 {
  "repo": "xiangjiana/Android-MS",
  "language": null,
  "readme_contents": "[**\u7248\u6743\u58f0\u660e**](#\u7248\u6743\u58f0\u660e)\n\n# Android \u9ad8\u7ea7\u9762\u8bd5\n\n### ![\u9762\u8bd5](img/2020Android\u6700\u65b0\u6280\u672f\u8be6\u89e3.png)\n#### \u6700\u65b0\u66f4\u6587\uff1a\n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n\n#### \u5199\u7ed9Android\u7684\u4e00\u5c01\u4fe1\n### \u5bf9\u4e8e\u8eab\u8fb9\u6b63\u5728\u9762\u8bd5\u548c\u9762\u8bd5\u4e2d\u7684\u4eba\uff0c\u52a0\u4e0a\u6211\u4ee5\u5f80\u7684\u9762\u8bd5\u7ecf\u5386\u6574\u7406\u4e86\u5982\u4e0b\u9762\u8bd5\u5907\u8003\u8def\u7ebf\uff0c\u548cPDF\u7248\uff08\u6709\u76f8\u5e94\u7684\u89c6\u9891\u6559\u7a0b\u5728\u540e\u9762\uff09\n### ![MS](img/MS.png)\n### ![PDF](img/PDF.png)\n\n\u6700\u8fd1\u534a\u5e74\uff0c\u5e38\u5e38\u6709\u4eba\u95ee\u6211 \u201cAndroid\u5c31\u4e1a\u5e02\u573a\u7a76\u7adf\u600e\u4e48\u6837\uff0c\u6211\u8fd8\u80fd\u4e0d\u80fd\u575a\u6301\u4e0b\u53bb ?\u201d\n\n\u73b0\u5728\u60f3\u60f3\uff0c\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u53d1\u5c55\u4e0d\u77e5\u4e0d\u89c9\u5df2\u7ecf\u5341\u591a\u5e74\u4e86\uff0cMobile First \u4e5f\u5df2\u7ecf\u53d8\u6210\u4e86 AI First\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u518d\u662f\u201c\u98ce\u53e3\u4e0a\u7684\u732a\u201d\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u5149\u73af\u548c\u6ea2\u4ef7\u5f00\u59cb\u6162\u6162\u6d88\u5931\uff0c\u5e76\u4e14\u6b63\u5728\u5411 AI\u3001\u533a\u5757\u94fe\u7b49\u65b0\u7684\u9886\u57df\u8f6c\u79fb\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u65b0\u9c9c\u8840\u6db2\u4e5f\u5df2\u7ecf\u53d8\u5c11\uff0c\u6700\u660e\u663e\u7684\u662f\u56fd\u5185\u5e94\u5c4a\u751f\u90fd\u7eb7\u7eb7\u6d8c\u5411\u4e86 AI \u65b9\u5411\u3002\n\n\u200b       \u53ef\u4ee5\u8bf4\uff0c\u56fd\u5185\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u7ea2\u5229\u671f\u5df2\u7ecf\u8fc7\u53bb\u4e86\uff0c\u73b0\u5728\u662f\u589e\u91cf\u4e0b\u964d\u3001\u5b58\u91cf\u53ae\u6740\uff0c\u4ece\u4e89\u593a\u7528\u6237\u5230\u4e89\u593a\u65f6\u957f\u3002\u6bd4\u8f83\u660e\u663e\u7684\u662f\u624b\u673a\u5382\u5546\u7eb7\u7eb7\u4e92\u8054\u7f51\u5316\uff0c\u4e0e\u4f20\u7edf\u4e92\u8054\u7f51\u4f01\u4e1a\u76f4\u63a5\u7ade\u4e89\u3002\u53e6\u5916\u4e00\u65b9\u9762\uff0c\u8fc7\u53bb\u6e20\u9053\u7684\u6253\u6cd5\u5931\u7075\uff0c\u5c0f\u7a0b\u5e8f\u3001\u5feb\u5e94\u7528\u7b49\u65b0\u5174\u6e20\u9053\u5d1b\u8d77\uff0c\u65e0\u8bba\u662f\u624b\u673a\u5382\u5546\uff0c\u8fd8\u662f\u5404\u5927 App \u90fd\u628a\u51fa\u6d77\u6446\u5230\u4e86\u6218\u7565\u7684\u4f4d\u7f6e\u3002\n\n\u5404\u5927\u57f9\u8bad\u5e02\u573a\u4e5f\u4e0d\u518d\u57f9\u8badAndroid\uff0c**\u4f5c\u4e3a\u5f00\u53d1Android\u7684\u6211\u4eec\u8be5\u4f55\u53bb\u4f55\u4ece\uff1f**\n\n\u200b        \u5176\u5b9e\u5982\u679c\u4f60\u6280\u672f\u6df1\u5ea6\u8db3\u591f\uff0c\u5927\u5fc5\u4e0d\u7528\u4e3a\u5c31\u4e1a\u800c\u5fe7\u6101\u3002\u6bcf\u4e2a\u884c\u4e1a\u4f55\u5c1d\u4e0d\u662f\u8fd9\u6837\uff0c\u6700\u5f00\u59cb\u7684\u98ce\u53e3\uff0c\u5230\u6162\u6162\u7684\u6210\u719f\u3002Android\u521d\u7ea7\u57282019\u5e74\u7684\u65e5\u5b50\u91cc\u98ce\u5149\u4e0d\u518d\uff0c \u9760\u4f1a\u56db\u5927\u7ec4\u4ef6\u5c31\u80fd\u591f\u83b7\u53d6\u5230\u6ee1\u610f\u85aa\u8d44\u7684\u65f6\u4ee3\u4e00\u53bb\u4e0d\u590d\u8fd4\u3002**\u7ecf\u8fc7\u4e00\u6ce2\u4e00\u6ce2\u7684\u6dd8\u6c70\u4e0e\u6d17\u724c\uff0c\u5269\u4e0b\u7684\u90fd\u662f\u6280\u672f\u7684\u91d1\u5b50\u3002\u5c31\u50cf\u5927\u6d6a\u892a\u53bb\uff0c\u88f8\u6cf3\u7684\u4f1a\u6162\u6162\u4e0a\u5cb8\u3002**\u800c\u771f\u6b63\u575a\u6301\u4e0b\u6765\u7684\u4e00\u5b9a\u4f1a\u53d6\u5f97\u4e0d\u9519\u6210\u7ee9\u3002\u6bd5\u7adfAndroid\u5e02\u573a\u662f\u5982\u6b64\u4e4b\u5927\u3002\u4eceAndroid\u9ad8\u7ea7\u7684\u84ec\u52c3\u7684\u5c31\u4e1a\u5c97\u4f4d\u9700\u6c42\u6765\u770b\uff0c\u80fd\u575a\u4fe1\u6211\u4eec\u6bcf\u4e00\u4f4dAndroid\u5f00\u53d1\u8005\u7684\u68a6\u60f3 \u3002\n\n### ![2020\u9762\u8bd5\u4e13\u9898](img/2020\u9762\u8bd5\u4e13\u9898.png)\n### ![2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55](img/2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55.png)\n### ![23\u79cd\u8bbe\u8ba1\u6a21\u5f0f](img/23\u79cd\u8bbe\u8ba1\u6a21\u5f0f.png)\n\n \u63a5\u4e0b\u6765\u6211\u4eec\u9488\u5bf9Android\u9ad8\u7ea7\u5c55\u5f00\u7684\u5b8c\u6574\u9762\u8bd5\u9898 \n ### \u4e00\u4e36kotlin(\u89c6\u9891\uff09\n - [kotlin\u5927\u51681-10\u89c6\u9891\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/Kotlin_%E5%89%AF%E6%9C%AC.png)\n ### \u4e8c\u4e36flutter\uff08\u89c6\u9891\uff09\n - [flutter\u89c6\u9891\u5305](https://github.com/xiangjiana/Android-MS/blob/master/img/flutter.png)\n ### \u4e09\u4e36\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u4ece0\u5f00\u59cb\u8ba4\u8bc6\uff08\u89c6\u9891\uff09\n - [\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u7cfb\u5217\u89c6\u98911-10](https://github.com/xiangjiana/Android-MS/blob/master/img/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%911-10%E8%A7%86%E9%A2%91.png)\n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08\u89c6\u9891\u89e3\u7801\u57fa\u7840-\u5c01\u88dd\u683c\u5f0f\uff09](https://mp.weixin.qq.com/s/vNKkl7xXsZhgALu7VigcDg) \n \n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08H264\u7f16\u7801-\u5e27\u5185\u9884\u6d4b\uff09](https://mp.weixin.qq.com/s/OA6DV_hnCoWnn_0lcheVbg)\n \n \n \n ### \u56db\u4e36\u6700\u65b0\u89c6\u9891\u66f4\u65b0\uff1a\n - [1.90\u5206\u949f\u641e\u5b9a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide\uff0c\u9762\u8bd5\u5b9e\u6218\u4e00\u6761\u9f99](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n - [2.\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [3.\u963f\u91cc\u9762\u8bd5\u9898\uff1a\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [4.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u5f00\u53d1\u6280\u80fd-\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [5.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u8fdb\u9636\uff0c\u5b9e\u73b0\u8d44\u6e90\u66f4\u65b0\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [6.Binder\u673a\u5236\u8be6\u89e3\uff0c\u7528Binder\u4e2dmmap\u601d\u60f3\u89e3\u51b3\u4f60\u7684APP\u5361\u987f\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [7.\u5317\u4e0a\u5e7f\u6df110\u5e74\u9762\u8bd5\u7ecf\u9a8c\u8001\u53f8\u673a\u503e\u56ca\u76f8\u6388\uff0c\u8ba9\u4f60\u5c11\u8d705-10\u5e74\u5f2f\u8def\u7684\u9762\u8bd5\u79c1\u623f\u8bfe\uff08\u804c\u4e1a\u8def\u5f84.\u7b80\u5386\u89c4\u5212.\u9762\u8bd5\u5b98\u5fc3\u7406\u5206\u6790.\u6280\u672f\u9762\u8bd5\u5b9e\u6218\uff0cGlide\uff0cOkhttp\uff0c\u4f18\u5316\u9762\u8bd5\u9898\u52a9\u4f60\u6253\u901a\u4efb\u7763\u4e8c\u8109\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [8.\u4e0d\u4f1aNDK\u600e\u4e48\u73a9\u70ed\u4fee\u590d\uff1f\u4eca\u665a\u6559\u4f60\u4eceJava\u5c42\u5b9e\u73b0\u817e\u8bafTinker\u70ed\u4fee\u590d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [9.\u8fc8\u5411\u67b6\u6784\u5e08\u7684\u7b2c\u4e00\u6b65-\u4ece\u6253\u9020\u81ea\u5df1\u7684\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u5f00\u59cb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [10.Android\u9879\u76ee\u7684\u6700\u7ec8\u8fdb\u5316,\u63d2\u4ef6\u5316\u5f00\u53d1\u8ba9\u4f60\u7684\u5e94\u7528\u52a0\u8f7d\u6d77\u91cf\u63d2\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [11.\u544a\u522b\u201c\u642c\u8fd0\u5de5\u201d\u624b\u5199\u5fae\u4fe1\uff0cQQ\u90fd\u5728\u7528\u7684\u6570\u636e\u6846\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [12.JVM\u865a\u62df\u673a\u5c42\u770bKlass \u5bf9\u8c61\u751f\u6210\u673a\u5236\uff0c\u63ed\u79d8\u4ece\u672a\u770b\u8fc7\u7684\u7ec6\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [13.Android\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u6218\uff0c\u6253\u9020\u5168\u81ea\u52a8\u6ce8\u5165\u6846\u67b6ButterKnife](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [14.\u6027\u80fd\u4f18\u4ece\u53ea\u4f1a\u5f00\u53d1\u5230\u5168\u76d8\u638c\u63a7App\u6027\u80fd\uff0c\u53ea\u9700\u8981\u4ece\u8fd9\u8282\u8bfe\u5f00\u59cb\u5316](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [15.\u8001\u53f8\u673a\u6253\u7834Bitmap\u5e38\u89c4\u601d\u7ef4\uff0c\u4eceSkia\u5f15\u64ce\u770bBitmap\u52a0\u8f7d\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [16.\u4f60\u7684\u5e94\u7528\u505a\u4e86\u57cb\u70b9\u4e0a\u4f20\u5417\uff1f\u624b\u5199\u7f16\u8bd1\u65f6\u4eca\u65e5\u5934\u6761\u7684\u57cb\u70b9\u67b6\u6784](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [17.NDK\u5927\u725b\u5e26\u4f60\u4e00\u5802\u8bfe\u641e\u5b9a\u4e00\u7ebf\u5927\u5382\u97f3\u89c6\u9891\u9762\u8bd5\u96c6\u5408](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [18.\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\u5417\uff1f\uff0c\u5982\u679c\u8fde\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u90fd\u62e7\u4e0d\u6e05\u600e\u4e48\u884c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [19.\u9762\u8bd5\u4e13\u9898-Okhttp\u76f8\u5173\u9762\u8bd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [20.\u9762\u8bd5\u4e13\u9898-Okhttp\u9762\u8bd5\u4e13\u9898\u5b8c\u7ed3\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [21.\u9762\u8bd5\u4e13\u9898\u4e4b\u6253\u901aGlide\u6e90\u7801\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [22.UI\u4f18\u5316\u662f\u4e0d\u662f\u53ea\u4f1a\u8bf4\u5e03\u5c40\u5c42\u7ea7\u4e0d\u80fd\u592a\u6df1\uff1f\u6765\uff0c\u8fd9\u91cc\u6709\u5168\u76d8\u6df1\u5ea6\u5206\u6790\uff01](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [23.\u6ce8\u89e3\u53cd\u5c04\u7684\u9ad8\u7ea7\u6280\u5de7\uff0c\u8ba9\u4f60\u5f7b\u5e95\u4e86\u89e3EventBus\u662f\u5982\u4f55\u8fdb\u884c\u7ec4\u4ef6\u901a\u4fe1\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [24.\u542c\u8bf4\u4f60\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\uff1f\u90a3\u4f60\u6709\u67b6\u6784style\u6ca1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [25.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u9ad8\u7ea7\u8fdb\u9636\uff0c\u624b\u5199\u5b9e\u73b0\u7f51\u6613\u4e91\u4e3b\u9898\u6362\u80a4\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [26.Binder\u4e13\u9898\uff08\u4e00\uff09\u76f4\u6363Binder\u56db\u5c42\u6846\u67b6\uff0c\u8da3\u8bb2Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [27.Binder\u4e13\u9898\uff08\u4e8c\uff09\u8fdb\u7a0b\u901a\u4fe1\u7684\u6838\u5fc3\u5185\u5b58\u7ba1\u7406\u4e0e\u8c03\u5ea6\uff0c\u6df1\u5165\u7406\u89e3Binder\u4e2d\u5185\u5b58\u64cd\u4f5c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [28.Binder\u4e13\u9898\uff08\u4e09\uff09\u57fa\u4e8eBinder\u7684\u5e95\u5c42\u5b9e\u73b0\uff0c\u624b\u5199Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [29.Jetpack\u4e4b\u540e\u4f60\u8fd8\u6ca1\u6709\u89e3\u9501LiveData\uff1f\u6765\uff0c\u4e00\u8282\u8bfe\u5e26\u4f60\u89e3\u9501\u5e76\u4e14\u8fd0\u7528\u5230\u9879\u76ee\u4e2d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [30.\u4e3a\u4ec0\u4e48\u9009\u62e9Glide\u4f5c\u4e3a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u4e0eFresco\uff0cPascco\u4f18\u52bf\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [31.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u6280\u80fd\uff0c\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0(\u54c8\u592b\u66fc\u7b97\u6cd5\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [32.\u9ad8\u7ea7UI\u8981\u4e0d\u8981\u4e86\u89e3\u4e0b\uff1f\u8001\u53f8\u673a\u5e26\u4f60\u6765\u4e00\u5802\u81ea\u5b9a\u4e49ViewGroup\u5b9e\u6218\u8bfe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [33.\u9762\u8bd5\u65f6\u603b\u88ab\u5185\u5b58\u95ee\u9898\u8650\u5343\u904d\uff1f\u7406\u8bba\u77e5\u8bc6\u53c8\u770b\u4e0d\u61c2\uff1f\u6765\uff0c\u8fd9\u8282\u8bfe\u5f7b\u5e95\u641e\u5b9a\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [34.Android\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u5230\u5e95\u6709\u591a\u7b80\u5355\uff0c\u56db\u4e2a\u6838\u5fc3\u7c7b\u9610\u8ff0\u5176\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Android\u5e38\u7528\u7ec4\u4ef6\u901a\u4fe1\u65b9\u5f0f\u9610\u8ff0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ce8\u89e3\u4e0e\u53cd\u5c04\u5728\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u4e2d\u7684\u4f7f\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u4ece\u5b9e\u6218\u4e2d\u4e86\u89e3\u89e3\u8026\u7684\u6838\u5fc3\u601d\u60f3\u4e0e\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [35.\u865a\u62df\u673a\u8be6\u89e3\u5185\u5b58\u7ed3\u6784\u539f\u7406\uff0c\u7528\u4ee3\u7801\u7684\u89d2\u5ea6\u5206\u6790\u5185\u5b58\u5206\u5e03](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u65b9\u6cd5\u533a\uff0c\u5806\u533a\uff0c\u6808\u533a\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5b57\u8282\u7801\u6307\u4ee4\u4e0eDex\u6307\u4ee4\u96c6\u6267\u884c\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5bc4\u5b58\u5668\u4e0e\u865a\u62df\u6808\u6307\u4ee4\u6d41\u7a0b\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [36.\u5927\u5382\u67b6\u6784\u5e08\u5e26\u4f60\u624b\u5199Glide\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u8ba9\u4f60\u79bb\u67b6\u6784\u5e08\u7684\u8ddd\u79bb\u66f4\u8fd1\u4e00\u6b65](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Glide\u6846\u67b6\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u8981\u600e\u6837\u5c01\u88c5?](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u600e\u4e48\u5904\u7406\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u7684\u9ad8\u5e76\u53d1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.Glide\u7684\u4e09\u7ea7\u7f13\u5b58\u673a\u5236\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [37.\u5e26\u4f60\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u89e3\u8bfbHandler\u6838\u5fc3\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [1.Handler\u6e90\u7801\u5206\u6790\u7684\u4e09\u6761\u4e3b\u7ebf](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6e90\u7801\u4e2d\u9690\u85cf\u7684\u90a3\u4e9b\u4e0d\u80fd\u5ffd\u89c6\u7684\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.Handler\u76f8\u5173\u9762\u8bd5\u9898\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [38.FFmpeg\u548cMediaCodec\u7684API\u770b\u4e0d\u61c2\uff1f\u5e94\u8be5\u4ece\u97f3\u89c6\u9891H264\u7f16\u7801\u539f\u7406\u5165\u624b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.H264\u539f\u7406\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4fe1\u6e90\u7f16\u7801\u5668\u662f\u5982\u4f55\u5bf9\u89c6\u9891\u5e27\u8fdb\u884c\u7f16\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3. slice(\u5207\u7247) Nal(\u5b8f\u5757) \uff08\u50cf\u7d20\u9884\u6d4b\uff09\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [39.\u5982\u679c\u4f60\u662f\u67b6\u6784\u5e08\uff1f\u600e\u6837\u80fd\u8ba9\u4f60\u7684\u5e94\u7528\u53ea\u670910M\u7684\u4f53\u79ef\u786e\u62e5\u67091000M\u7684\u529f\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5982\u4f55\u8ba9\u4f60\u7684\u9879\u76ee\u4f53\u79ef\u5c0f\u529f\u80fd\u591a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ca1\u6709\u5b89\u88c5\u7684APK\u5305\u6211\u4eec\u600e\u4e48\u8ba9\u5b83\u201c\u52a8\u8d77\u6765\u201d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u67b6\u6784\u5e08\u8be5\u5e72\u7684\u6d3b\u5c31\u662f\u628a\u4e0d\u53ef\u80fd\u53d8\u6210\u53ef\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u624b\u5199\u5b9e\u73b0\u5927\u5382\u90fd\u5728\u73a9\u7684\u63d2\u4ef6\u5316\u5f00\u53d1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [40.\u6296\u97f3\u89c6\u9891\u526a\u8f91\u539f\u7406\uff0c\u624b\u5199\u89c6\u9891\u526a\u8f91\u4e0e\u80cc\u666f\u97f3\u4e50\u5408\u6210](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  - [1.MeidiaCodec\u7f16\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4e0d\u66ff\u6362\u89c6\u9891\u97f3\u4e50\u524d\u63d0\u4e0b\uff0c\u5c06\u6b4c\u66f2\u5408\u6210\u5230\u89c6\u9891\u58f0\u97f3  ](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u767b\u9876IT\u754c\u7684\u7687\u51a0-\u624b\u5199\u89c6\u9891\u526a\u8f91\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [41.Android\u9ad8\u7ea7\u6280\u80fd-\u5927\u56fe\u52a0\u8f7d\uff0c\u800c\u4f60\u5728\u672c\u8282\u8bfe\u5b66\u5230\u7684\u53ef\u4e0d\u6b62\u8fd9\u4e00\u4e2a\u70b9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5927\u56fe\u52a0\u8f7d\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5185\u5b58\u590d\u7528\uff0c\u56fe\u7247\u5360\u7528\u5185\u5b58\u5206\u6790\u4ee5\u53ca\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5982\u4f55\u521b\u5efa\u5927\u56fe\u800c\u4e0dOOM\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u989d\u5916\u5206\u4eab\uff1a\u9762\u8bd5\u9047\u5230\u5b8c\u5168\u4e0d\u4f1a\u7684\u95ee\u9898\u600e\u4e48\u529e\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [42. jepg\u56fe\u50cf\u5f15\u64ce\u5b9e\u73b0\u8d85\u8fc7\u539f\u751f\u7684\u56fe\u7247\u538b\u7f29\u6027\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u54c8\u592b\u66fc\u538b\u7f29\u7b97\u6cd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.Bitmap\u6e90\u7801\u89e3\u8bfb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.native\u5c42\u8bfb\u53d6\u56fe\u7247](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [43.\u7834\u89e3\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7aa5\u63a2\u963f\u91ccARouter\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u8def\u7531\u6846\u67b6\u7a76\u7adf\u662f\u4ec0\u4e48\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u963f\u91ccARouter\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.APT\u6280\u672f\u5b9e\u73b0\u624b\u5199Arouter\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [44.\u963f\u91ccP7\u5c97\u9762\u8bd5\u5173\u4e8eRecyclerView\u7684\u8fde\u73af\u70ae\uff0c\u4e00\u5c0f\u65f6\u89e3\u51b3RecyclerView\u6240\u6709\u5e95\u5c42\u7591\u60d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.RecyclerView\u7684\u590d\u7528\u673a\u5236\uff0c\u7b80\u5355\u8bf4\u8bf4View\u56de\u6536\u4e0e\u590d\u7528\u7684\u8fc7\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.RecyclerView\u652f\u6301\u591a\u4e2a\u4e0d\u540c\u7c7b\u578b\u5e03\u5c40\uff0c\u4ed6\u4eec\u600e\u4e48\u7f13\u5b58\uff0c\u5e76\u4e14\u67e5\u627e\u7684\u5462](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u8bf4\u4e00\u8bf4RecyclerView\u9002\u914d\u5668\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u7406\u6e05RecyclerView\u67b6\u6784\u601d\u60f3\uff0c\u624b\u5199RecyclerView\u81ea\u5b9a\u4e49\u63a7\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [45.\u5343\u4e07\u7ea7\u5e94\u7528\u7f8e\u56e2Robust\u4fee\u590d\u539f\u7406\uff0c\u624b\u5199\u5b57\u8282\u7801\u63d2\u4ef6\u6280\u672f\u6280\u672f\u70b9\uff1a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u7f8e\u56e2robust\u4fee\u590d\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.robust\u4f9d\u8d56\u7684\u63d2\u4ef6\u5b9e\u73b0\u65b9\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.apk\u7f16\u8bd1\u539f\u7406\uff0cgroovy\u5b9e\u73b0\u52a8\u6001\u63d2\u5165\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u81ea\u5df1\u5b9e\u73b0robust\u63d2\u4ef6\uff0c\u52a8\u6001\u6539\u52a8\u4e3b\u5de5\u7a0b\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [46.\u67b6\u6784\u5e08\u4fee\u70bc\u4e4b\u8def-\u7ad9\u5728\u67b6\u6784\u5e08\u7684\u89d2\u5ea6\u5982\u4f55\u5999\u7528\u81ea\u5b9a\u4e49\u6ce8\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [1.\u4e3a\u4ec0\u4e48\u4e0d\u7528EventBus\u4e86](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [2. \u4e8b\u4ef6\u81a8\u80c0\u5982\u4f55\u89e3\u51b3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [3. \u7ec4\u4ef6\u901a\u4fe1\u8fd8\u80fd\u600e\u4e48\u73a9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  \n  \n \n ### \u4e94\u4e36\u4e13\u9898\u7bc7\n #### 2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898\n  - [2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n - [1.\u4ec0\u4e48\u662f\u6ca1\u6709\u65b9\u5411\u5c31\u505c\u4e0b\u6765\uff0c\u4e86\u89e3Android\u53d1\u5c55\u624d\u80fd\u7a33\u6b65\u524d\u884c\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u7528\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1Android\u5f00\u53d1\u8005\u7684\u524d\u666f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u8ba9offer\u8ffd\u7740\u4f60\u7684\u79d8\u8bc0\u662f\u4ec0\u4e48\uff1f\u7b80\u5386\u5168\u76d8\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5236\u5b9a\u804c\u4e1a\u89c4\u5212\uff0c\u4e3a\u672a\u6765\u94fa\u5e73\u9053\u8def](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [2.\u5de5\u6b32\u5584\u5176\u4e8b\u5fc5\u5148\u5229\u5176\u5668-OkHttp\u6e90\u7801\u89e3\u8bfb](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4e3a\u4ec0\u4e48OkHttp\u4f7f\u7528Socket\u800c\u4e0d\u662fHttpUrlConnection](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Okhttp\u6d41\u7a0b\uff0c\u6838\u5fc3\u7c7b\u5168\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4ece\u6e90\u7801\u89d2\u5ea6\u4e86\u89e3Okhttp\u4e2d\u6784\u5efa\u8005\u4e0e\u8d23\u4efb\u94fe\u6a21\u5f0f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5982\u4f55\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [3.\u6280\u80fd\u6301\u7eed\u8fdb\u9636-Glide\u8be6\u89e3\uff0c\u8ba9\u4f60\u5bf9\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u77e5\u6839\u77e5\u5e95](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u6e90\u7801\u4e09\u6761\u4e3b\u7ebf\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u751f\u547d\u5468\u671f\u7ba1\u7406\u7b56\u7565\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n\n\n #### \u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab\n  - [\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n - [1.\u544a\u522b\u4f20\u7edf\u5355\u4e00\u6a21\u5757\u5f00\u53d1\uff0c\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\u6a21\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff0c\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u600e\u6837\u5bf9\u9879\u76ee\u4e2d\u6240\u6709\u7684\u4e1a\u52a1\u6a21\u5757\u8fdb\u884c\u79d1\u5b66\u7ba1\u7406\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u600e\u6837\u8fdb\u884c\u7a97\u4f53\u8df3\u8f6c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7ec4\u4ef6\u5316\u7406\u7531\u6846\u67b6\u624b\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b8c\u7f8e\u5229\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7c7b\u751f\u6210\u795e\u5668Java poet\u795e\u52a9\u653b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u8def\u7531\u8868\u7684\u7a7a\u865a\u5bc2\u5bde\u51b7\u8ba9\u6211\u4eec\u5982\u4f55\u6ee1\u8db3\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u624b\u5199\u5b9e\u73b0\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u8ba9\u7ec4\u4ef6\u5316\u9879\u76ee\u5982\u864e\u6dfb\u7ffc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u8981\u5982\u4f55\u901a\u4fe1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u9762\u5bf9Android\u5e02\u573a\u7684N\u4e2d\u901a\u4fe1\u65b9\u5f0f\u6211\u4eec\u8be5\u5982\u4f55\u6289\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Jetpack\u4e2d\u7684liveData\u6253\u9020\u7ec4\u4ef6\u5316\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n #### Android\u7cfb\u7edf\u6e90\u7801FrameWork\u5b9e\u6218\u4e13\u9898\n - [1.Android10.0 9.0 8.0\u6df1\u5165\u865a\u62df\u673a\u5e95\u5c42\u4e2d\u8bb2\u89e3\u5185\u5b58\u5206\u5e03\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [30\u5206\u949f\u7406\u6e05\u7a0b\u5e8f\u5458\u5bf9\u5185\u5b58\u7684\u6050\u60e7\uff0c\u5e73\u65f6\u5199\u7684\u53d8\u91cf\uff0c\u5bf9\u8c61\u5728\u5c4b\u91cc\u5185\u5b58\u7684\u5206\u90e8](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u65b9\u6cd5\u533a\uff0c\u6808\u533a\uff0c\u5806\u533a\u8fd0\u884c\u673a\u5236\u548c\u6d41\u7a0b\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u57fa\u4e8e\u5bc4\u5b58\u5668\u4e0eJVM\u57fa\u4e8eJava\u6808\u7684\u533a\u522b\u548c\u539f\u7406-\u5bc4\u5b58\u5668\u7684\u4f5c\u7528\u4e8e\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u53cd\u7f16\u8bd1dex\u6587\u4ef6\u4e4barm\u6307\u4ee4\u96c6\u5206\u6790\uff0c\u770b\u770bnew\u4e00\u4e2a\u5bf9\u8c61\u6700\u7ec8\u53d8\u6210\u4e86\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u67e5\u770bAndroid\u7cfb\u7edf\u6e90\u7801\uff0cdex\u4e2dARM\u516c\u53f8\u7684\u6307\u4ee4\u96c6\u52a0\u8f7d\u4e0e\u6267\u884c\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6df1\u5165\u7406\u89e3Android\u6838\u5fc3\u5173\u952e\u5b57 new synchronized volatile final\u5e95\u5c42\u5b9e\u73b0\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Java\u5bf9\u8c61\u5728\u5806\u533a\u5927\u5c0f\u4e3a8\u4e2a\u5b57\u8282\u53ca\u6bcf\u4e00\u4e2a\u5206\u90e8\u8be6\u89e3-hashcode\u51fd\u6570\u6267\u884c\u673a\u5236\uff0chashcode\u5728\u5185\u5b58\u4e2d\u5b58\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [class\u7c7b\u5728\u65b9\u6cd5\u533a\u4e3a\u4f55\u662f426\u4e2a\u5b57\u8282\uff0c\u65b9\u6cd5\u4e3a\u4f55\u4f1a\u589e\u52a04\u4e2a\u5b57\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4eceJNIEnv\u5165\u624b\u627e\u5230\u7c7b\u52a0\u8f7d\u4e0eclass\u7c7b\u6784\u9020\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3. sophix\u70ed\u4fee\u590d\u524d\u8eab--Arm\u6307\u4ee4\u96c6\u66ff\u6362\u5b9e\u73b0java\u65b9\u6cd5\u52a8\u6001\u4fee\u590d\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [artMethod\u7ed3\u6784\u4f53 \u8be6\u7ec6\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6267\u884c\u5f15\u64ce\u4e2dclass\u5982\u4f55\u88ab\u52a0\u8f7d\uff0c\u4e09\u90e8\u66f2\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Findclass\u51fd\u6570\uff0c\u7c7b\u5982\u4f55\u5728\u865a\u62df\u673a\u5c42\u53ea\u52a0\u8f7d\u4e00\u6b21](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Defineclass\u51fd\u6570\uff0c\u5b9a\u4e49\u4e00\u4e2a\u7a7a\u7684class](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Loadclass\u51fd\u6570\uff0cdex\u6587\u4ef6\u4e2d\u5c06\u7c7b\u6709\u78c1\u76d8\u7f13\u5b58\u5230\u5185\u5b58](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [\u865a\u62df\u673a\u5c42\u66ff\u6362\u6267\u884c\u5f15\u64ce\u4e2darm\u6307\u4ee4](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u865a\u62df\u673a\u5c42Java\u65b9\u6cd5\u66ff\u6362\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7cfb\u7edf\u6e90\u7801\u79fb\u690d\u5230AS\u7f16\u8bd1\u4e0e\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n### \u516d\u4e362020\u6700\u65b0Android\u6587\u7ae0\u7cfb\u5217\uff1a\n - [\u9762\u8bd5\u5b98\uff1a\u4f60\u6709\u7528\u8fc7Flutter\u5417? Flutte\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c\u67b6\u6784\u662f\u600e\u4e48\u6837\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u6bd4\u5176\u4ed6\u5982ReactNative\u597d](https://www.jianshu.com/p/3e2d9b23cfd6)\n \n - [\u5982\u4f55\u52a0\u8f7d100M\u7684\u56fe\u7247\u5374\u4e0d\u6491\u7206\u5185\u5b58,\u4e00\u5f20 100M \u7684\u5927\u56fe\uff0c\u5982\u4f55\u9884\u9632 OOM\uff1f](https://www.jianshu.com/p/878e4ddaa51b)\n  \n - [\u5b9d\u5b9d\u5df4\u58eb\uff1aKotlin\u4e3a\u4ec0\u4e48\u4f1a\u706b\u8d77\u6765\uff0c\u6709\u4ec0\u4e48\u7279\u70b9\uff0c\u8ddfJava\u533a\u522b](https://www.jianshu.com/p/dd9c0b9af2a1)\n  \n - [IGG\uff1aAndroid\u5185\u5b58\u56de\u6536\u673a\u5236\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/2b2642ce379f)\n  \n - [\u6012\u5237Android\u9762\u8bd5100\u9898\uff0c\u518d\u4e5f\u4e0d\u62c5\u5fc3\u4e0d\u80fd\u540a\u6253\u9762\u8bd5\u5b98\u4e86](https://www.jianshu.com/p/c01c3d0b1ee9)\n \n - [\u5b57\u8282\u8df3\u52a8:Android R\u5982\u4f55\u8bbf\u95ee\u6587\u4ef6\uff0c\u4fee\u6539\u6587\u4ef6\uff0c\u4f60\u4eec\u5bf9R\u9002\u914d\u4e86\u5417](https://www.jianshu.com/p/994b72f06af9)\n  \n  - [\u5b57\u8282\u8df3\u52a8\uff1aIO\u4f18\u5316\u662f\u600e\u4e48\u505a\u7684\uff0c\u4f7f\u7528 SharedPreferences\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u5361\uff0cmmkv\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/12428890ae1e)\n   \n  - [2020\u5e74Android\u6700\u5148\u9762\u8bd5\u4e13\u9898\u52a9\u4f60\u65a9\u83b7offer\uff0c\u4ee5\u53ca\u6559\u4f60\u5982\u4f55\u4e00\u6b65\u6b65\u7b80\u5386](https://www.jianshu.com/p/af7938c116bb)\n   \n - [\u9762\u8bd5\u5b98\u8bf4\uff1a\u5927\u5bb6\u90fd\u8bf4 Java \u53cd\u5c04\u6548\u7387\u4f4e\uff0c\u4f60\u77e5\u9053\u539f\u56e0\u5728\u54ea\u91cc\u4e48](https://www.jianshu.com/p/4a32b9b71115)\n \n - [Android \u5f00\u53d1\u4e2d\u7684\u67b6\u6784\u6a21\u5f0f -- MVC / MVP / MVVM](https://www.jianshu.com/p/218f9432ee52)\n \n - [\u5173\u4e8e\u963f\u91cc\u63a8\u51fa\u7684\u8def\u7531\u6846\u67b6ARouter\u6e90\u7801\u89e3\u6790](https://www.jianshu.com/p/7b4d085e23a9)\n  \n - [\u8131\u4e86\u9a6c\u7532\u6211\u4e5f\u8ba4\u8bc6\u4f60: \u804a\u804a Android \u4e2d\u7c7b\u7684\u771f\u5b9e\u5f62\u6001](https://www.jianshu.com/p/1094f6e4444f)\n\n- [Android\u9762\u8bd5\u5206\u6790\u4e00\uff1a\u5173\u4e8eOKhttp\u8be6\u89e3\uff08\u9644\u5e26\u89c6\u9891\u6559\u7a0b\uff09](https://www.jianshu.com/p/f4e353336b86)\n \n - [\u9762\u8bd5\u5b98\uff1a\u5185\u5b58\u6cc4\u6f0f\u8fde\u73af\u95ee\u3002\u88ab\u95ee\u61f5\u4e86\uff1f\u6765\u770b\u770b\u8fd9\u4e2a](https://www.bilibili.com/video/BV1ck4y1r7PK)\n \n  - [\u963f\u91cc\u9762\u8bd5\u5b98\uff1a\u5173\u4e8eRecyclerView\u505a\u4e0b\u5206\u4eab](https://www.jianshu.com/p/e5b2963706c7)\n  \n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n \n#### \u4e03\u4e36Android\u6027\u80fd\u4f18\u5316\u7bc7\n\n- [Android\u6027\u80fd\u4f18\u5316(1)- \u542f\u52a8\u4f18\u5316](https://mp.weixin.qq.com/s/gZdL4rNuw0cHTXL7RkkJ1A)\n  \n - [Android\u6027\u80fd\u4f18\u5316(2)-UI\u6e32\u67d3\u4f18\u5316](https://mp.weixin.qq.com/s/YeSkBcMB2tf0pVmddxA2Mg)\n   \n - [Android\u6027\u80fd\u4f18\u5316(3)-\u5954\u6e83\u4f18\u5316 ](https://mp.weixin.qq.com/s/J8qPFv9Fbt9_UGKqGxS5YQ)\n    \n - [Android\u6027\u80fd\u4f18\u5316(4)-\u5185\u5b58\u4f18\u5316 ](https://mp.weixin.qq.com/s/Rte_e7R7grfrYUE6RXJYKg)\n     \n - [Android\u6027\u80fd\u4f18\u5316(5)-\u5361\u987f\u4f18\u5316](https://pan.baidu.com/s/15rJ_qEWSJdlODA6ifjHTAw)\n      \n - [Android\u6027\u80fd\u4f18\u5316(6)-\u5b58\u50a8\u4f18\u5316](https://mp.weixin.qq.com/s/6A4jU8spcLJa2BKdMS10yA)\n     \n - [Android\u6027\u80fd\u4f18\u5316(7)-\u7f51\u7edc\u4f18\u5316](https://mp.weixin.qq.com/s/rJChehNdyPFbB_LcKSV_jA)\n\n - [Android\u6027\u80fd\u4f18\u5316(8)-\u8017\u7535\u4f18\u5316](https://mp.weixin.qq.com/s/wJdzAM5a9a6rFHDdJMBfAA)\n         \n - [Android\u6027\u80fd\u4f18\u5316(9)-\u591a\u7ebf\u7a0b\u5e76\u53d1\u4f18\u5316](https://mp.weixin.qq.com/s/sZ1MgTlDlusgGDWGZUZZzw)\n\n - [Android\u6027\u80fd\u4f18\u5316(10)-\u5b89\u88c5\u5305\u4f18\u5316](https://mp.weixin.qq.com/s/Qo0y6xbZ8LFdYvcWdqmKjQ)\n \n #### \u516b\u4e36Android Framework\u5c42\u9762\u8bd5\n \n- [Android Framework\u5c42\u9762\u8bd5\uff081\uff09-\u4e4bActivity\u542f\u52a8\u9762\u8bd5\u8fde\u73af\u70ae](https://mp.weixin.qq.com/s/LYQXe93evbHrleUPs62Jvw)\n  \n - [Android Framework\u5c42\u9762\u8bd5\uff082\uff09-\u4e4bBinder\u901a\u4fe1\u673a\u5236](https://mp.weixin.qq.com/s/Qnf79D54UF3o9k_VmuAIWQ)\n   \n - [Android Framework\u5c42\u9762\u8bd5\uff083\uff09-\u4e4bHandler\u9762\u8bd5\u96c6\u5408](https://mp.weixin.qq.com/s/MAAQLTgMYD3FxVS6ZFPDww)\n    \n - [Android Framework\u5c42\u9762\u8bd5\uff084\uff09-\u4e4b\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u5168\u9762\u6574\u7406 ](https://mp.weixin.qq.com/s/UEL_jxU8nugGGBerYFuN_g)\n     \n - [Android Framework\u6e90\u7801\u9762\u8bd5\uff085\uff09-\u4e4bonMeasure\u6d4b\u91cf\u539f\u7406](https://mp.weixin.qq.com/s/jydw_NT3AMIyLCoiynFFnA)\n      \n - [AndroidFramework\u5c42\u9762\u8bd5\uff086\uff09-\u4e4bAndroid \u5c4f\u5e55\u5237\u65b0\u673a\u5236(\u8bf4\u8bf4\u5361\u987f\u539f\u56e0)](https://mp.weixin.qq.com/s/xqoJRfXHUawQjfbioCcBfw)\n\n  \n\n\n### \u4e5d\u4e36\u4e92\u8054\u7f51\u7684\u5bd2\u51ac\u4e0b\uff0c\u5982\u4f55\u624b\u63e1\u5b89\u535370\u4e07\u5e74\u85aa\u3002\u4e00\u5802\u8bfe\u5e26\u4f60\u8d70\u8fdb\u8c61\u7259\u5854\n\n[\u4e3a\u4ec0\u4e48\u4f1a\u53d1\u751f\u4e92\u8054\u7f51\u7684\u5bd2\u51ac](android/videowhy.md)\n\n[\u97f3\u9891\u964d\u566a\u539f\u7406\uff0c\u97f3\u9891\u539f\u7406](android/voice.md)\n\n[\u97f3\u89c6\u9891\u662f\u4ec0\u4e48\uff0c\u89c6\u9891\u4e3a\u4ec0\u4e48\u9700\u8981\u538b\u7f29](android/videowhy.md)\n\n[\u89c6\u9891\u538b\u7f29\u538b\u7f29\u7684\u662f\u4ec0\u4e48\u4fe1\u606f? \u5e27\u5185\u538b\u7f29\u4e0e\u5e27\u95f4\u538b\u7f29\u539f\u7406](android/videoencode.md)\n\n[\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u4f4e\u5ef6\u65f6\u600e\u4e48\u5b9e\u73b0](android/live-optimitor.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n[\u5fae\u4fe1---\u97f3\u89c6\u9891\u901a\u8bdd\u5982\u4f55\u5b9e\u73b0\uff0c\u89c6\u9891\u4f1a\u8bae\u539f\u7406](android/mediacodec.md)\n\n[5G\u65f6\u4ee3\u5982\u4f55\u91cd\u751f\u79fb\u52a8\u4e92\u8054\u7f51,\u5e26\u4f60\u770b\u770b\u4ec0\u4e48\u662f5G\u5e94\u7528]()\n\n[\u5c0f\u7c73--\u4e07\u7269\u4e92\u8054\u5b9e\u73b0\u539f\u7406](android/net.md)\n\n### \u5341\u4e362019Android\u5e74\u9ad8\u7ea7\u9762\u8bd5\n\n * [2019\u5e74Bat\u9762\u8bd5\u96c6\u5408](#2019\u5e74Bat\u9762\u8bd5\u96c6\u5408)\n * [\u67b6\u6784\u76f8\u5173\u9762\u8bd5](#\u67b6\u6784\u76f8\u5173\u9762\u8bd5)\n * [NDK\u76f8\u5173\u9762\u8bd5](#NDK\u76f8\u5173\u9762\u8bd5)\n * [\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5](#\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5)\n * [\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5](#\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5)\n * [\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5](#\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5)\n * [\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5](#\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5)\n * [\u5176\u4ed6](#\u5176\u4ed6)\n\n### \u5341\u4e00\u4e362019\u5e74Bat\u9762\u8bd5\u96c6\u5408\n\n\n\n> \u963f\u91cc\u5df4\u5df4\u9762\u8bd5\u96c6\u5408\n\n- [Android P\u7981\u6b62\u4e86http\u5bf9\u4f60\u4eec\u6709\u5f71\u54cd\u5417\uff1fhttps\u539f\u7406\u4f60\u77e5\u9053\u5417\uff1f](android/https.md)\n\n- [\u4ec0\u4e48\u662f\u5bf9\u79f0\u52a0\u5bc6\uff0c\u4ec0\u4e48\u662f\u975e\u5bf9\u79f0\u52a0\u5bc6\uff0c\u516c\u94a5\u4e0e\u79c1\u94a5\u5c5e\u4e8e\u5bf9\u79f0\u52a0\u5bc6\u5417](android/cert.md)\n\n- [https\u8bf7\u6c42\u4f1a\u4e0d\u4f1a\u5b58\u5728\u88ab\u62e6\u622a\u7684\u53ef\u80fd\uff1f\u4f60\u5bf9\u8fd9\u65b9\u9762\u6709\u8fc7\u7814\u7a76\u5417](android/cert.md)\n\n- [\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u5982\u4f55\u4f7f\u753b\u9762\u4fdd\u8bc1\u6d41\u7545\u7684\u6548\u679c](android/live-optimitor.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n- [Flutter\u4e3a\u4ec0\u4e48\u4f1a\u505a\u5230\u4e00\u5904\u5199 \u5904\u5904\u8fd0\u884c \u4e0eRN\u7684\u533a\u522b](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [Flutter\u7684\u56fe\u5f62\u5f15\u64ce\u4e0eAndroid\u7684\u6e32\u67d3\u5f15\u64ce\u539f\u7406](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u5bf9\u4e8eTersorflow\u4f60\u600e\u4e48\u7406\u89e3\u7684\uff0c\u6709\u505a\u8fc7\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u5417](android/tersorflow.md)\n\n- [\u4e3a\u4ec0\u4e48Android\u4f1a\u51fa\u73b0\u5361\u987f](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u7ed9\u4f60\u4e00\u4e2aDemo \u4f60\u5982\u4f55\u5feb\u901f\u5b9a\u4f4dANR](android/anr.md)\n\n- [Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n- [Glide\u5bf9Bitmap\u7684\u7f13\u5b58\u4e0e\u89e3\u7801\u590d\u7528\u5982\u4f55\u505a\u5230\u7684](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u8bf4\u8bf4\u4f60\u5bf9Dalvik\u865a\u62df\u673a\u7684\u8ba4\u8bc6 ](android/dalvik.md)\n\n- [\u63a5\u4e0b\u6765\u8bf4\u8bf4 Android \u865a\u62df\u673aDalvik\u4e0eART\u533a\u522b\u5728\u54ea\u91cc\uff1f](android/artordalvik.md)\n\n- [Handler\u7684\u539f\u7406\u662f\u4ec0\u4e48?\u80fd\u6df1\u5165\u5206\u6790\u4e0b Handler\u7684\u5b9e\u73b0\u673a\u5236\u5417\uff1f](./study/framework/Handler\u673a\u5236\u6e90\u7801.md)\n\n- [ Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n  \n\n> \u817e\u8baf\u9762\u8bd5\u96c6\u5408\n\n- [\u8be6\u7ec6\u8bf4\u8bf4Binder\u901a\u4fe1\u539f\u7406\u4e0e\u673a\u5236](android/binder.md)\n\n- [Linux\u81ea\u5e26\u591a\u79cd\u8fdb\u7a0b\u901a\u4fe1\u65b9\u5f0f\uff0c\u4e3a\u4ec0\u4e48Android\u90fd\u6ca1\u91c7\u7528\u4e8c\u504f\u504f\u4f7f\u7528Binder\u901a\u4fe1](android/binder1.md)\n\n- [\u8c08\u4e00\u8c08Binder\u7684\u539f\u7406\u548c\u5b9e\u73b0\u4e00\u6b21\u62f7\u8d1d\u7684\u6d41\u7a0b](android/binder2.md)\n\n- [\u8fdb\u7a0b\u4fdd\u6d3b\u5982\u4f55\u505a\u5230\uff0c\u4f60\u4eec\u4fdd\u6d3b\u7387\u6709\u591a\u9ad8\uff1f](android/process.md)\n\n- [ButterKnife\u4e3a\u4ec0\u4e48\u6267\u884c\u6548\u7387\u4e3a\u4ec0\u4e48\u6bd4\u5176\u4ed6\u6ce8\u5165\u6846\u67b6\u9ad8\uff1f\u5b83\u7684\u539f\u7406\u662f\u4ec0\u4e48](android/butterknife.md)\n\n- [\u7ec4\u4ef6\u5316\u5982\u4f55\u5b9e\u73b0\uff0c\u7ec4\u4ef6\u5316\u4e0e\u63d2\u4ef6\u5316\u7684\u5dee\u522b\u5728\u54ea\u91cc\uff0c\u8be5\u600e\u4e48\u9009\u578b](android/commpont.md)\n\n- [\u8bf4\u4e0b\u7ec4\u4ef6\u4e4b\u95f4\u7684\u8df3\u8f6c\u548c\u7ec4\u4ef6\u901a\u4fe1\u539f\u7406\u673a\u5236](android/commpontrounter.md)\n\n- [\u6709\u6ca1\u6709\u4f7f\u7528\u8fc7\u7ec4\u4ef6\u5316\uff0c\u7ec4\u4ef6\u5316\u901a\u4fe1\u5982\u4f55\u505a\u5230\u7684\uff0cARouter\u6709\u7528\u8fc7\u5417](android/router.md)\n\n- [\u6709\u7528\u8fc7\u63d2\u4ef6\u5316\u5417\uff1f\u8c08\u8c08\u63d2\u4ef6\u5316\u539f\u7406\uff1f](android/plugin.md)\n\n- [\u70ed\u4fee\u590d\u8fde\u73af\u70ae(\u70ed\u4fee\u590d\u662f\u4ec0\u4e48  \u6709\u63a5\u89e6\u8fc7tinker\u5417\uff0ctinker\u539f\u7406\u662f\u4ec0\u4e48)](tencent/tinker.md)\n\n- [\u589e\u91cf\u5347\u7ea7\u4e3a\u4ec0\u4e48\u51cf\u5c11\u5347\u7ea7\u4ee3\u4ef7\uff0c\u589e\u91cf\u5347\u7ea7\u539f\u7406](tencent/update.md)\n\n- [ PMS\u4e4b\u524d\u4e86\u89e3\u8fc7\u5417?\u4f60\u5bf9PMS\u600e\u4e48\u770b\u7684\uff0c\u80fd\u804a\u804aPMS\u7684\u8be6\u7ec6\u5b9e\u73b0\u6d41\u7a0b\u5417](android/pms.md)\n\n- [ AMS\u5728Android\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff0cActivtiy\u542f\u52a8\u8ddfAMS\u6709\u4ec0\u4e48\u5173\u7cfb](android/ams.md)\n\n- [\u4f60\u77e5\u9053\u4ec0\u4e48\u662fAOP\u5417\uff1fAOP\u4e0eOOP\u6709\u4ec0\u4e48\u533a\u522b\uff0c\u8c08\u8c08AOP\u7684\u539f\u7406](android/aop.md)\n\n- [\u7f51\u6613\u4e91--\u624b\u673aQQ\u7684\u6362\u80a4\u662f\u600e\u4e48\u505a\u5230\u7684\uff0c\u4f60\u5bf9\u6362\u80a4\u6709\u4e86\u89e3\u5417\uff1f\u770b\u8fc7\u6362\u80a4\u7684\u539f\u7406\u6ca1\uff1f](android/load.md)\n\n- [\u5783\u573e\u56de\u6536\u673a\u5236\u662f\u5982\u4f55\u5b9e\u73b0\u7684](android/traked.md)\n\n- [\u6570\u636e\u5e93\u7248\u672c\u5982\u4f55\u5355\u72ec\u5347\u7ea7\uff0c\u5e76\u4e14\u5c06\u539f\u6709\u6570\u636e\u8fc1\u79fb\u8fc7\u53bb](tencent/sqlite.md)\n\n- [\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u591a\u7528\u6237\uff0c\u591a\u89d2\u8272\u7684App\u67b6\u6784](android/thread.md)\n\n- [\u8c08\u8c08volatile\u5173\u952e\u5b57\u4e0esynchronized\u5173\u952e\u5b57\u5728\u5185\u5b58\u7684\u533a\u522b](android/volatile.md)\n\n- [synchronize\u5173\u952e\u5b57\u5728\u865a\u62df\u673a\u6267\u884c\u539f\u7406\u662f\u4ec0\u4e48\uff0c\u80fd\u8c08\u4e00\u8c08\u4ec0\u4e48\u662f\u5185\u5b58\u53ef\u89c1\u6027\uff0c\u9501\u5347\u7ea7\u5417](android/synchronize.md)\n\n- [\u7c7b\u6bd4\u4e8e\u5fae\u4fe1\uff0c\u5982\u4f55\u5bf9Apk\u8fdb\u884c\u6781\u9650\u538b\u7f29,\u8c08\u4e0bAndroid\u538b\u7f298\u5927\u6b65 ](android/AndResGuard.md)\n\n- [\u5982\u4f55\u5f7b\u5e95\u9632\u6b62\u53cd\u7f16\u8bd1\uff0cdex\u52a0\u5bc6\u600e\u4e48\u505a ](android/dex.md)\n\n- [\u5e8f\u5217\u5316\u4e0e\u53cd\u5e8f\u5217\u5316\u7684\u539f\u7406\uff0cAndroid\u7684Parcelable\u4e0eSerializable\u533a\u522b\u662f\u4ec0\u4e48](android/herms.md)\n\n- [\u4f60\u66fe\u7ecf\u6709\u6ca1\u6709\u5bf9SqliteDatabase\u505a\u8fc7\u5c01\u88c5\uff0c\u4f60\u81ea\u5df1\u6709\u8bbe\u8ba1\u8fc7\u6570\u636e\u5e93\u6846\u67b6\u5417?\u6216\u8005\u53ea\u662f\u505c\u7559\u5728\u4f7f\u7528ormlite  greenDao\u8fd9\u7c7b\u6846\u67b6](android/sqlite.md)\n\n  \n### \u5341\u4e8c\u4e36\u89c6\u9891\u533a\u57df\n- [\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u963f\u91cc\u9762\u8bd5\u9898\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u624b\u5199\u7ec4\u4ef6Lifecycle](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6Navigation\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6DataBinding-Ex\uff1a\u53cc\u5411\u7ed1\u5b9a\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u6700\u65b0Jetpack\u67b6\u6784\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Foundation\uff08\u6846\u67b6\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Jetpack\u6e90\u7801\u5206\u6790\u3002\u5de8\u4eba\u662f\u5982\u4f55\u70bc\u6210\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [MVVM+Jetpack\u5b9e\u73b0\u7684GitHub\u5ba2\u6237\u7aef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u8df5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [ButterKnife\u8be6\u89e3\u4e0e\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Dagger2\u6838\u5fc3\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u997f\u4e86\u4e48\u8fdb\u7a0b\u901a\u4fe1\u6838\u5fc3\u6280\u672fherms\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199handler\uff0c\u5f15\u53d1\u5185\u5b58\u6cc4\u6f0f\u7684\u6839\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7ec4\u4ef6\u5316\u67b6\u6784\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u63d2\u4ef6\u5316\u6846\u67b6\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199\u70ed\u4fee\u590d\u6846\u67b6Tinker](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u6613\u4e91\u6362\u80a4\u6280\u672f\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u7edc\u52a0\u8f7d\u6846\u67b6OKHTTP\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Rxjava2\u67b6\u6784\u5206\u6790\u4e0e\u6e90\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [C/C++\u5165\u95e8\u8bed\u6cd5\u4ee5\u53ca\u57fa\u7840\u77e5\u8bc6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-JNI\u7f16\u7a0b\u5b9e\u6218](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-\u6784\u5efa\u811a\u672c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n### \u5341\u4e09\u4e36OPCV\u5b66\u4e60\u8d44\u6599\n#### \u7b2c\u4e00\u7ae0 \u9884\u5907\u77e5\u8bc6\n- [1.1.\u7f16\u7a0b\u7684\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.2.\u4ec0\u4e48\u53eb\u7f16\u8f91](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.3.\u4ec0\u4e48\u53eb\u7f16\u8bd1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.4.\u4ec0\u4e48\u53eb\u8fde\u63a5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.5.\u4ec0\u4e48\u53eb\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.6.Visual C++\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.7.\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.8.\u5e93\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.9.OpenCV\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.10.\u4ec0\u4e48\u662f\u547d\u4ee4\u884c\u53c2\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.11.\u5e38\u89c1\u7f16\u8bd1\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.1\u627e\u4e0d\u5230\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.2\u62fc\u5199\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [1.12.\u5e38\u89c1\u94fe\u63a5\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.13.\u8fd0\u884c\u65f6\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e8c\u7ae0 OpenCV\u4ecb\u7ecd\n\n- [2.1.OpenCV\u7684\u6765\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [2.2.OpenCV\u7684\u534f\u8bae](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e09\u7ae0 \u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c\n\n- [3.1.\u56fe\u50cf\u7684\u8868\u793a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.2.Mat\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.3.\u521b\u5efaMat\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.1\u6784\u9020\u51fd\u6570\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.2create\uff08\uff09\u51fd\u6570\u521b\u5efa\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.3Matlab\u98ce\u683c\u7684\u521b\u5efa\u5bf9\u8c61\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.4.\u77e9\u9635\u7684\u57fa\u672c\u5143\u7d20\u8868\u8fbe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.5.\u50cf\u7d20\u503c\u7684\u8bfb\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.1 at()\u51fd\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.2 \u4f7f\u7528\u8fed\u4ee3\u5668](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.3 \u901a\u8fc7\u6570\u636e\u6307\u9488](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.6.\u9009\u53d6\u56fe\u50cf\u5c40\u90e8\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.1 \u5355\u884c\u6216\u5355\u5217\u9009\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.2 \u7528Range\u9009\u62e9\u591a\u884c\u6216\u591a\u5217](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.3 \u611f\u5174\u8da3\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n  - [3.6.4 \u53d6\u5bf9\u89d2\u7ebf\u5143\u7d20](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.7.Mat\u8868\u8fbe\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.8.Mat_\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.9.Mat\u7c7b\u7684\u5185\u5b58\u7ba1\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.10.\u8f93\u51fa](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.11.Mat\u4e0elpllmage\u548cCvMat\u7684\u8f6c\u6362](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n   - [3.11.1 Mat\u8f6c\u4e3alpllmage\u548cCvMat\u7684\u683c\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)   \n   - [3.11.2 lpllmage\u548cCvMat\u683c\u5f0f\u8f6c\u4e3aMat](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n   \n#### \u7b2c\u56db\u7ae0 \u6570\u636e\u83b7\u53d6\u4e0e\u5b58\u50a8\n\n - [4.1.\u8bfb\u53d6\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.1\u8bfb\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   \n - [4.2\u8bfb\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.2.1\u8bfb\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n\n   \n  ## \u66f4\u591a\u76f8\u5173\u9762\u8bd5\u5185\u5bb9\uff0c\u89c6\u9891\u6587\u6863\uff0c2020\u9762\u6700\u65b0\u9762\u8bd5\u4e13\u9898PPT\uff0c\u9ad8\u7ea7\u8fdb\u9636\n  ## (\u5907\u6ce8GitHub\uff09 VX\uff1amm14525201314\n  \n  # \u7248\u6743\u58f0\u660e\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"\u77e5\u8bc6\u5171\u4eab\u8bb8\u53ef\u534f\u8bae\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />\u672c\u4f5c\u54c1\u91c7\u7528<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">\u77e5\u8bc6\u5171\u4eab\u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u7981\u6b62\u6f14\u7ece 4.0 \u56fd\u9645\u8bb8\u53ef\u534f\u8bae</a>\u8fdb\u884c\u8bb8\u53ef\u3002\n"
 },
 {
  "repo": "abreheret/PixelAnnotationTool",
  "language": "C++",
  "readme_contents": "PixelAnnotationTool\n============================\n\n-----------------\n| **` Linux/MAC `** | **` Windows `** | **` Donate  `** | \n|-----------------|---------------------|---------------------|\n| [![Build Status](https://api.travis-ci.org/abreheret/PixelAnnotationTool.svg?branch=master)](https://travis-ci.org/abreheret/PixelAnnotationTool) | [![Appveyor Build Status](https://img.shields.io/appveyor/ci/abreheret/pixelannotationtool.svg)](https://ci.appveyor.com/project/abreheret/pixelannotationtool) |  [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) |\n\n\n\nSoftware that allows you to manually and quickly annotate images in directories.\nThe method is pseudo manual because it uses the algorithm [watershed marked](http://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1) of OpenCV. The general idea is to manually provide the marker with brushes and then to launch the algorithm. If at first pass the segmentation needs to be corrected, the user can refine the markers by drawing new ones on the erroneous areas (as shown on video below).\n\n[![gif_file](giphy.gif)](https://youtu.be/wxi2dInWDnI)\n\nExample :\n\n<img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road.jpg\" width=\"300\"/> <img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road_color_mask.png\" width=\"300\"/>\n\nLittle example from an user ([tenjumh](https://github.com/tenjumh/Pixel-Annotation-Tool)) of PixelAnnotationTools : https://www.youtube.com/watch?v=tX-xcg5wY4U\n\n----------\n\n### Building Dependencies :\n* [Qt](https://www.qt.io/download-open-source/)  >= 5.x\n* [CMake](https://cmake.org/download/) >= 2.8.x \n* [OpenCV](http://opencv.org/releases.html) >= 2.4.x \n* For Windows Compiler : Works under Visual Studio >= 2015\n\nHow to build go to [here](scripts_to_build)\n\n### Download binaries :\nGo to release [page](https://github.com/abreheret/PixelAnnotationTool/releases)\n\n### Donate :\nIf you like, donate !\n\n\nDonating is very simple - and secure. Please click [here](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) to make a donation. \n\n**Thank you!**\n\nYour donation will help me to maintain and update PixelAnnotationTool.\n\n### License :\n\nGNU Lesser General Public License v3.0 \n\nPermissions of this copyleft license are conditioned on making available complete source code of licensed works and modifications under the same license or the GNU GPLv3. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. However, a larger work using the licensed work through interfaces provided by the licensed work may be distributed under different terms and without source code for the larger work.\n\n[more](https://github.com/abreheret/PixelAnnotationTool/blob/master/LICENSE)\n\n### Citation :\n\n```bib\n  @MISC{Breheret:2017\n    author = {Amaury Br{\\'e}h{\\'e}ret},\n    title = {{Pixel Annotation Tool}},\n    howpublished = \"\\url{https://github.com/abreheret/PixelAnnotationTool}\",\n    year = {2017},\n  }\n```\n\n\n"
 },
 {
  "repo": "movidius/ncappzoo",
  "language": "Python",
  "readme_contents": "# Neural Compute Application Zoo (ncappzoo) \n[![Stable release](https://img.shields.io/badge/For_OpenVINO\u2122_Version-2020.1-green.svg)](https://github.com/opencv/dldt/releases/tag/2020.1)\n[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\nWelcome to the Neural Compute Application Zoo (_ncappzoo_). This repository is a place for any interested developers to share their projects (code and Neural Network content) that make use of the [Intel&reg; Neural Compute Stick 2 (Intel&reg; NCS2)](https://software.intel.com/en-us/neural-compute-stick)  or the original [Intel&reg; Movidius&trade; Neural Compute Stick](https://software.intel.com/en-us/movidius-ncs) and the Deep Learning Deployment Toolkit (DLDT) portion of the [OpenVINO&trade; Toolkit](https://software.intel.com/en-us/openvino-toolkit).\n \nThe _ncappzoo_ is a community repository with many content owners and maintainers. All _ncappzoo_ content is open source and being made available in this central location for others to download, experiment with, modify, build upon, and learn from.\n\n## _ncappzoo_ Quick Start\nIf you have an  Intel&reg; NCS2 (or the first generation Intel&reg; Movidus&trade; NCS) device and want to jump into the _ncappzoo_, follow these steps: \n\nClone the repo with the following command:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\nRun this command inside of any app/network folder to check your system software dependencies for that particular sample:\n```bash\nmake install_reqs\n```\nIf the script returns successfully, you're ready to run the app or network sample!\n\n## _ncappzoo_ Apps and Networks\nExplore apps by opening a terminal window navigating to any directory under **_ncappzoo_/apps** and execute this command:\n```bash\nmake run\n```\nExplore the neural networks by navigating to any network directory under **_ncappzoo_/networks** and execute the same command:\n```bash\nmake run\n```\nThats it! All of the network and app directories have simple consistent makefiles. To see other make targets supported from these directories just execute this command:\n```bash\nmake help\n```\n\n\n## _ncappzoo_ Repository Branches\nThere are main three branches in the repository; their descriptions are below.  **The master branch is the one most developers will want.**  The others are provided for legacy compatibility.\n\n- **master** branch: This is the most current branch, and the content relies on the DLDT from the OpenVINO&trade; Toolkit.  This is the only branch that is compatible with the Intel&reg; NCS2 however, it is also compatible with the original Intel&reg; Movidius&trade; NCS device.\n- **ncsdk2** branch: This branch is a legacy branch and the content relies on the NCSDK 2.x tools and APIs rather than the OpenVINO&trade; toolkit. This branch is only compatible with the original Intel&reg; Movidius&trade; NCS device and is **NOT** compatible with the Intel&reg; NCS2 device.\n- **ncsdk1** branch: This branch is a legacy branch and the content relies on the NCSDK 1.x tools and APIs rather than OpenVINO&trade; toolkit.  This branch is only compatible with the original Intel&reg; Movidius&trade; Neural Compute Stick and is **NOT** compatible with the Intel&reg; NCS2 device.\n\nYou can use the following git command to use the master branch of the repo:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\n## _ncappzoo_ Compatibility Requirements\n\n### Hardware compatibility\nThe projects in the _ncappzoo_ are periodically tested on Intel&reg; x86-64 Systems unless otherwise stated in the project's README.md file.  Although not tested on other harware platforms most projects should also work on any hardware which can run the OpenVINO&trade; toolkit including the Raspberry Pi 3/3B/3B+/4B hardware<br><br>\nThe projects in the _ncappzoo_ work on both the Intel&reg; NCS2 and the original Intel&reg; Movidius NCS devices.\n\n\n### Operating System Compatibility\nThe projects in the _ncappzoo_ are tested and known to work on the **Ubuntu 18.04** OS.  These projects will likely work on other Linux based operating systems as well but they aren't tested on those unless explicitly stated in the project's README.md file and there may be some tweaks required as well.  If any specific issues are found for other OSes please submit a pull request as broad compatibility is desirable.\n\n### OpenVINO and DLDT Compatibility\nThe projects in the **master branch** depend on the Deep Learning Deployment Toolkit (DLDT) portion of the OpenVINO&trade; toolkit.  There are two flavors of the the OpenVINO&trade; toolkit's DLDT:  \n- The [Intel&reg; Distribution of the OpenVINO&trade; toolkit](https://software.intel.com/en-us/openvino-toolkit) is a binary installation available for  supported platforms.  Here are some links regarding the Intel Distribution of the OpenVINO&trade; Toolkit and the Intel&reg; NCS2\n  - Getting started web page: https://software.intel.com/en-us/articles/get-started-with-neural-compute-stick\n  - Getting Started Video for Linux: https://youtu.be/AeEjQKKkPzg?list=PL61cFkSnEEmOF3AJvLtlDTSbwjlCP4iCs\n  - OpenVINO Toolkit documentation: https://docs.openvinotoolkit.org/latest/index.html\n- The [open source distribution of the OpenVINO&trade; toolkit DLDT](https://github.com/opencv/dldt).  This is the means by which the Intel&reg; NCS2 device can be used with most single board computers on the market and is also helpful for other non-Ubuntu development systems.  Here are some links regarding the open source distribution of the OpenVINO&trade; with the Intel&reg;  NCS2: \n  - Applies to all target system: https://software.intel.com/en-us/articles/intel-neural-compute-stick-2-and-open-source-openvino-toolkit\n  - ARMv7: https://software.intel.com/en-us/articles/ARM-sbc-and-NCS2\n  - ARM64: https://software.intel.com/en-us/articles/ARM64-sbc-and-NCS2\n  - Python on all: https://software.intel.com/en-us/articles/python3-sbc-and-ncs2\n\n**Note:** When using the open source distribution of the OpenVINO&trade; toolkit, you may need to modify your shell's path and environment variables to point to the toolkit's directories.\n\nThe projects in the _ncappzoo_ work with both flavors of the OpenVINO&trade; Toolkit and unless otherwise specified in a project's README.md file all projects are targeted for the **OpenVINO&trade; Toolkit 2020.1 release**.\n\n### OpenCV Compatibility\nSome projects also rely on OpenCV. For these projects, OpenCV distributed with the OpenVINO&trade; release is the recommended version.  Other versions may also work but are not tested an may require tweaks to get working.  \n\n### Python Compatibility\nThe Python projects in the _ncappzoo_ rely on Python 3.5, unless otherwise stated in the individual project's README.\n\n### Raspberry Pi Compatibility\nThe _ncappzoo_ is compatible with the Raspberry Pi 3 B+ and the Raspberry Pi 4. Some additional configuration steps are required:\n\n#### Intel&reg; Distribution of OpenVINO&trade; for Raspbian* OS\nThe **Intel&reg; Distribution of OpenVINO&trade; toolkit for Raspbian OS** does not include the Model Optimizer. To use the _ncappzoo_, you must clone the open source version of the OpenVINO&trade; Deep Learning Development Toolkit (DLDT) and use that version of the Model Optimizer. Clone the repository, install dependencies for TensorFlow* and Caffe*, and set up your **PATH** and **PYTHONPATH** variables to point to the Model Optimizer:\n```\ncd ~\ngit clone https://github.com/opencv/dldt.git\ncd dldt/model-optimizer\npip3 install -r requirements_tf.txt\npip3 install -r requirements_caffe.txt\nexport PATH=~/dldt/model-optimizer:$PATH\nexport PYTHONPATH=~/dldt/model-optmizer:$PYTHONPATH\n```\n#### Open Source OpenVINO&trade; Deep Learning Development Toolkit (DLDT)\nTo setup the open source version of OpenVINO&trade; with your Raspberry Pi, add to the PATH, PYTHONPATH, and LD_LIBRARY_PATH environment variables the location of the build Inference Engine libraries and Python API.\n\n## _ncappzoo_ Repository Layout\nThe _ncappzoo_ contains the following top-level directories.  See the README file in each of these directories or just click on the links below to explore the contents of the _ncappzoo_.\n- **[apps](apps/README.md)** : Applications built to use the Intel&reg; Movidius Intel&reg; NCS and Intel&reg; Neural Compute Stick 2.  **This is a great place to start in the _ncappzoo_!**\n- **[networks](networks/README.md)** : Scripts to download models and optimize neural networks based on any framework for use with the Intel&reg; NCS and Intel&reg; NCS2.\n- **[caffe](caffe/README.md)** : Scripts to download caffe models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[tensorflow](tensorflow/README.md)** : Scripts to download TensorFlow&trade; models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[data](data/README.md)** : Data and scripts to download data for use with models and applications that use the Intel&reg; NCS and Intel&reg; NCS2\n\nThe top-level directories above have subdirectories that hold project content. Each of these project subdirectories has one or more owners that assumes responsibility for it. The [OWNERS](OWNERS) file contains the mapping of subdirectory to owner. \n\n## Contributing to the ncappzoo\nThe _ncappzoo_ is meant to explore and teach features available for the Intel&reg; Movidius&trade; Neural Compute Stick and Intel&reg; Neural Compute Stick 2 with the Intel&reg; OpenVINO&trade; toolkit. The more contributions to the ncappzoo, the more successful this community will be! We always encourage everyone with Neural Compute Stick related content to share by contributing their applications and model-related work to the ncappzoo. It's easy to do, and even when contributing new content you will be the owner and maintainer of the content.\n\nIf your inclusion is an opportunity to explore a new idea in computer vision, add as much documentation about the functionality and your process in creating your app or network, including smartly commenting your code. This will give others - and you! - a chance to learn from your addition. Your addition will help grow our community and improve all of our AI and computer vision skills. Most importantly, the insights you get from releasing your app into the wild here will only help you down the line if you ever want to commercialize your idea. As always, your work in the _ncappzoo_ should be properly attributed so that its ownership will always be managed by you and those you grant additional rights to.\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file for instructions and guidelines for contributing.\n\n## Licensing\nAll content in the _ncappzoo_ is licensed via the [MIT license](https://opensource.org/licenses/MIT) unless specifically stated otherwise in lower-level projects. Individual model and code owners maintain the copyrights for their content, but provide it to the community in accordance with the MIT License.\n\nSee the [LICENSE](LICENSE) file in the top-level directory for all licensing details, including reuse and redistribution of content in the _ncappzoo_ repository.\n\nIntel and the Intel logo are trademarks of Intel Corporation or its subsidiaries.\n\nOpenVINO is a trademark of Intel Corporation or its subsidiaries.\n\nRaspberry Pi is a trademark of the Raspberry Pi Foundation.\n\nTensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc.\n\n"
 },
 {
  "repo": "ITCoders/Human-detection-and-Tracking",
  "language": "Python",
  "readme_contents": "# Human detection and Tracking\n\n[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/arpit1997)\n\n## Introduction\n_In this project we have worked on the problem of human detection,face detection, face recognition and tracking an individual. Our project is capable of detecting a human and its face in a given video and storing Local Binary Pattern Histogram (LBPH) features of the detected faces. LBPH features are the key points extracted from an image which is used to recognize and categorize images. Once a human is detected in video, we have tracked that person assigning him a label. We have used the stored LBPH features of individuals to recognize them in any other videos. After scanning through various videos our program gives output like- person labeled as subject1 is seen in video taken by camera1, subject1 is seen in video by camera2. In this way we have tracked an individual by recognizing him/her in the video taken by multiple cameras. Our whole work is based on the application of machine learning and image processing with the help of [openCV](http://opencv.org)._**This code is built on opencv 3.1.1, python 3.4 and C++, other versions of opencv are NOT SUPPORTED.**\n## Requirements\n* **opencv [v3.1.1]**\n\t* **Installation in linux:**\n\t\t\tFor complete installation of opencv in ubuntu you can refer [here](http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/).\n\t* **Installation in windows**\n\t\t\tFor complete installation of opencv in windows you can refer [here](https://putuyuwono.wordpress.com/2015/04/23/building-and-installing-opencv-3-0-on-windows-7-64-bit/)\n* **python3**\n\t* In Ubuntu python 3.4 can be installed via terminal with the command given below:\n\t\t`sudo apt-get install python3`\n* **python libraries:**\n\tHere is a list of all the python dependencies \n\t* Python Image Library (PILLOW)\n\t* Imutils\n\t* numpy\n\n* **C++**\n\n## Approach\n* The code follows the steps given below:\n\t1. First it reads a video and process each frame one by one.\n\t2. For each frame it tries to detect a human. If a human is detected it draws a rectangle around it.\n\t3. after completing step 2 it tries to detect human face.\n\t4. if a human face is detected it tries to recognize it with a pre-trained model file.\n\t5. If human face is recognized it puts the label on that human face else it moves to step 2 again for next frame \n* The repository is structured as follows:\n\t* `main.py` : This is the main python file that detects and recognizes humans.\n\t* `main.cpp` : This is the main C++ file that detects and recognizes humans.\n\t* `create_face_model.py` : This python script is used to create model file using the given data in `data/` folder \n\t* `model.yaml` : This file contains trained model for given data. This trained model contains LBPH features of each and every face for given data.\n\t* `face_cascades/` : This directory contains sample data for testing our codes. This data is prepared by extracting face images of a praticular person from some videos.\n\t* `scripts/` : This directory contains some useful scripts that we used to work on different problems.\n\t* `video/` : This directory contains some of the videos that we used to while testing.\n\n## Installation \n\n## Python\nDon't forget to install the necessary libraries described in the install paragraph above.\n\nFirst you need to run the create_face_model.py file, which uses the images in /data to create a .yaml file\n* In the project folder run \n```sh \npython create_face_model.py\n```\n* To run the python version of the code you have to put all the input videos in one folder and then provide the path of that folder as command line argument:\n```sh\npython3 main.py -v /path/to/input/videos/  \n```\nExample- for our directory structure it is: \n```sh\n python3 main.py -v /video \n```\n\n## C++\n* To compile the C++ version of the code with openCV the command is:\n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename name_of_file.cpp .cpp` name_of_file.cpp `pkg-config --libs opencv` \n```\nExample- for our directory structure it is: \n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename main.cpp .cpp` main.cpp `pkg-config --libs opencv` \n```  \n* To run the C++ version of the code you have to put all the input videos in one folder and then provide the path of that video as command line argument:\n```sh\n./name_of_file /path/to/input/video_file \n```  \nExample- for our directory structure it is: \n```sh\n ./main /video/2.mp4\n```\n* creating your own model file; just follow the steps given below to create your own model file:\n\t* for each individual rename the images as `subjectx.y.jpg` for example for person 1 images should be named as `subject01.0.jpg` , `subject01.1.jpg` and so on.\n\t* put all the images of all the persons in a single folder for example you can see `data\\` folder then run this command given below:\n\t\t`python3 create_face_model.py -i /path/to/persons_images/` \n\n## Performance of code\n* Since this is a computer vision project it requires a lot of computation power and performance of the code is kind of an issue here.\n* The code was tested on two different machines to analyse performace. The input was 30fps 720p video.\n\t* On a machine with AMD A4 dual-core processor we got an output of 4fps which is quite bad.\n\t* on a machine with Intel i5 quad-core processor we got an output of 12fps.\n\n## Results\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/g.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/o.jpg \"Logo Title Text 1\")\n\nYou can find project report [here](https://github.com/ITCoders/Human-detection-and-Tracking/raw/master/results/HUMAN%20DETECTION%20ANDaRECOGNITION.pdf)\n## To do\n* improve the performance of the code\n* improve the accuracy of the code and reducing the false positive rate.\n* improve the face recognition accuracy to over 90 percent\n\n## Special Thanks to:\n* [Jignesh S. Bhatt](http://www.iiitvadodara.ac.in/faculty/jsb001.html) - Thank you for mentoring this project\n* [Kamal Awasthi](http://github.com/KamalAwasthi) - Helped in testing the code\n"
 },
 {
  "repo": "JimmyHHua/opencv_tutorials",
  "language": "Python",
  "readme_contents": "# OpenCV 4.0 Tutorial\n[![](https://img.shields.io/badge/opencv-v4.0.0-orange.svg)](https://opencv.org/)       [![](https://img.shields.io/badge/opencv-tutorial-brightgreen.svg)](https://docs.opencv.org/4.0.0/d9/df8/tutorial_root.html)\n\n\u2712\ufe0f [\u4e2d\u6587\u7248\u672c](./README_CN.md)\n## Introduction\n\nThis repository contains source code of OpenCV Tutorial application, the environment is python3.0 and opencv4.0.\n\n## Sample\n- **Image load**\n```python\nimport cv2\n\nsrc = cv2.imread(\"test.png\")\ncv2.namedWindow(\"input\", cv2.WINDOW_AUTOSIZE)\ncv2.imshow(\"input\", src)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n<div align=center><img src=\"https://i.loli.net/2019/05/22/5ce4b40258c9155103.jpg\" width=200></div>\n\n- **Gray Image**\n```python\ngray = cv2.cvtColor(src, cv.COLOR_BGR2GRAY)\n```\n\n<div align=center><img src=https://i.loli.net/2019/05/22/5ce4b2ae1e7ce86434.png width=120>       <img src=https://i.loli.net/2019/05/22/5ce4b2ae220a248459.png width=120></div>\n\n\n\n***More opencv4.0 tutorials plese follow the learning road as below*** \ud83d\udc47\ud83d\udc47\ud83d\udc47\n\n## Learning Road \u26f3\ufe0f\n\n***Annotation:***\n- \u2714\ufe0f  **: Basic**\n- \u270f\ufe0f  **: Attention**\n- \u2763\ufe0f  **: Important**\n\nNo    | Description   | Annotation\n:--------: | :--------: | :--------:\ncode_001 | [Load Image](python/code_001/opencv_001.py)   | \u2714\ufe0f\ncode_002 | [Gray Image](python/code_002/opencv_002.py)   | \u2714\ufe0f\ncode_003 | [Image Create](python/code_003/opencv_003.py)   | \u2714\ufe0f\ncode_004 | [Pixel Read and Write](python/code_004/opencv_004.py)   | \u2714\ufe0f\ncode_005 | [Image Pixel Arithmetic Operations](python/code_005/opencv_005.py)   | \u2714\ufe0f\ncode_006 | [Image Pseudo-Color Enhancement](python/code_006/opencv_006.py)   | \u2714\ufe0f\ncode_007 | [Image Pixel Operation (Logical Operation)](python/code_007/opencv_007.py)   | \u2714\ufe0f\ncode_008 | [Image Channel Separation and Merging](python/code_008/opencv_008.py)   | \u2714\ufe0f\ncode_009 | [Color Space Conversion](python/code_009/opencv_009.py)   | \u270f\ufe0f\ncode_010 | [Image Pixel Value Statistics](python/code_010/opencv_010.py)   | \u2714\ufe0f\ncode_011 | [Image Pixel Normalization](python/code_011/opencv_011.py)   | \u2714\ufe0f\ncode_012 | [Video Read and Write](python/code_012/opencv_012.py)   | \u2714\ufe0f\ncode_013 | [Image Flip](python/code_013/opencv_013.py)   | \u2714\ufe0f\ncode_014 | [Image Interpolation](python/code_014/opencv_014.py)   | \u2714\ufe0f\ncode_015 | [Draw Geometry](python/code_015/opencv_015.py)   | \u2714\ufe0f\ncode_016 | [ROI of Image](python/code_016/opencv_016.py)   | \u2714\ufe0f\ncode_017 | [Image Histogram](python/code_017/opencv_017.py)   | \u2714\ufe0f\ncode_018 | [Histogram Dqualization](python/code_018/opencv_018.py)   | \u270f\ufe0f\ncode_019 | [Histogram Comparison](python/code_019/opencv_019.py)   | \u2714\ufe0f\ncode_020 | [Histogram Backprojection](python/code_020/opencv_020.py)   | \u2714\ufe0f\ncode_021 | [Image Convolution](python/code_021/opencv_021.py)   | \u2714\ufe0f\ncode_022 | [Averaging and Gaussian Blur](python/code_022/opencv_022.py)   | \u2763\ufe0f\ncode_023 | [Median Blur](python/code_023/opencv_023.py)   | \u2714\ufe0f\ncode_024 | [Image Noise](python/code_024/opencv_024.py)   | \u2714\ufe0f\ncode_025 | [Smoothing Images](python/code_025/opencv_025.py)   | \u2714\ufe0f\ncode_026 | [Gaussian Bilateral Blur](python/code_026/opencv_026.py)   | \u2714\ufe0f\ncode_027 | [Mean-shift Blur)](python/code_027/opencv_027.py)   | \u2714\ufe0f\ncode_028 | [Image Integral Algorithm](python/code_028/opencv_028.py)   | \u2714\ufe0f\ncode_029 | [Fast Image Edge Filtering Algorithm](python/code_029/opencv_029.py)   | \u2714\ufe0f\ncode_030 | [Custom Filter](python/code_030/opencv_030.py)   | \u2714\ufe0f\ncode_031 | [Sobel Operator](python/code_031/opencv_031.py)   | \u2714\ufe0f\ncode_032 | [More Gradient Operators](python/code_032/opencv_032.py)   | \u2714\ufe0f\ncode_033 | [Laplace Operator](python/code_033/opencv_033.py)   | \u2714\ufe0f\ncode_034 | [Image Sharpening ](python/code_034/opencv_034.py)   | \u2714\ufe0f\ncode_035 | [USM Sharpen Algorithm](python/code_035/opencv_035.py)   | \u2714\ufe0f\ncode_036 | [Canny Edge Detection](python/code_036/opencv_036.py)   | \u2763\ufe0f\ncode_037 | [Image Pyramid](python/code_037/opencv_037.py)   | \u2714\ufe0f\ncode_038 | [Laplace Pyramid](python/code_038/opencv_038.py)   | \u2714\ufe0f\ncode_039 | [Image Template Matching](python/code_039/opencv_039.py)   | \u2714\ufe0f\ncode_040 | [Binary introduction](python/code_040/opencv_040.py)   | \u2714\ufe0f\ncode_041 | [Basic Thresholding](python/code_041/opencv_041.py)   | \u2714\ufe0f\ncode_042 | [OTSU Thresholding](python/code_042/opencv_042.py)   | \u270f\ufe0f\ncode_043 | [TRIANGLE Thresholding](python/code_043/opencv_043.py)   | \u2714\ufe0f\ncode_044 | [Adaptive Thresholding](python/code_044/opencv_044.py)   | \u270f\ufe0f\ncode_045 | [Binary and Smoothing](python/code_045/opencv_045.py)   | \u270f\ufe0f\ncode_046 | [Image Connectivity component](python/code_046/opencv_046.py)   | \u2714\ufe0f\ncode_047 | [Image Connected component state statistics](python/code_047/opencv_047.py)   | \u2714\ufe0f\ncode_048 | [Image Contours](python/code_048/opencv_048.py)   | \u2763\ufe0f\ncode_049 | [Bounding Rectangle](python/code_049/opencv_049.py)   | \u2763\ufe0f\ncode_050 | [Contour Area and Perimeter](python/code_050/opencv_050.py)   | \u270f\ufe0f\ncode_051 | [Contour Approximation](python/code_051/opencv_051.py)   | \u2714\ufe0f\ncode_052 | [Contour Centroid Calculate](python/code_052/opencv_052.py)   | \u2714\ufe0f\ncode_053 | [HuMoment for Contour Matching](python/code_053/opencv_053.py)   | \u2714\ufe0f\ncode_054 | [Contour Cricle and Ellipse fitting](python/code_054/opencv_054.py)   | \u2714\ufe0f\ncode_055 | [Convex Hull](python/code_055/opencv_055.py)   | \u270f\ufe0f\ncode_056 | [Fitting a Line](python/code_056/opencv_056.py)   | \u2714\ufe0f\ncode_057 | [Point Polygon Test](python/code_057/opencv_057.py)   | \u2714\ufe0f\ncode_058 | [The Largest Inner Circle](python/code_058/opencv_058.py)   | \u2714\ufe0f\ncode_059 | [Hoffman Line Detection](python/code_059/opencv_059.py)   | \u2714\ufe0f\ncode_060 | [Probability Hoffman Line Detection](python/code_060/opencv_060.py)   | \u2763\ufe0f\ncode_061 | [Hoffman Cricle Detection](python/code_061/opencv_061.py)   | \u2763\ufe0f\ncode_062 | [Dilation and Erosion](python/code_062/opencv_062.py)   | \u2763\ufe0f\ncode_063 | [Structuring Element](python/code_063/opencv_063.py)   | \u2714\ufe0f\ncode_064 | [Opening Transformation](python/code_064/opencv_064.py)   | \u270f\ufe0f\ncode_065 | [Closing Transformation](python/code_065/opencv_065.py)   | \u270f\ufe0f\ncode_066 | [Application of Opening and Closing Operations](python/code_066/opencv_066.py)   | \u270f\ufe0f\ncode_067 | [Top Hat](python/code_067/opencv_067.py)   | \u2714\ufe0f\ncode_068 | [Black Hat](python/code_068/opencv_068.py)   | \u2714\ufe0f\ncode_069 | [Morph Gradient](python/code_069/opencv_069.py)   | \u2714\ufe0f\ncode_070 | [Contour based on Morph Gradient](python/code_070/opencv_070.py)   | \u270f\ufe0f\ncode_071 | [Hit and Miss](python/code_071/opencv_071.py)   | \u2714\ufe0f\ncode_072 | [Defect Detecting-1](python/code_072)   | \u2714\ufe0f\ncode_073 | [Defect Detecting-2](python/code_073/opencv_073.py)   | \u2714\ufe0f\ncode_074 | [Extract the Maximum Contour and Coding Key Points](python/code_074)   | \u2714\ufe0f\ncode_075 | [Image Inpainting](python/code_075/opencv_075.py)   | \u2714\ufe0f\ncode_076 | [Perspective Transformation](python/code_076/opencv_076.py)   | \u270f\ufe0f\ncode_077 | [Video Read, Write and Process](python/code_077/opencv_077.py)   | \u270f\ufe0f\ncode_078 | [Identify and Track Specific Color Objects in Video](python/code_078)   | \u2714\ufe0f\ncode_079 | [Video Analysis-Background/Foreground Extraction](python/code_079/opencv_079.py)   | \u2714\ufe0f\ncode_080 | [Video Analysis\u2013Background Subtraction and ROI Extraction of the Foreground](python/code_080)   | \u2714\ufe0f\ncode_081 | [Corner Detection-Harris](python/code_081)   | \u2714\ufe0f\ncode_082 | [Corner Detection-Shi-Tomas](python/code_082)   | \u270f\ufe0f\ncode_083 | [Corner Detection-Sub-Pixel ](python/code_083)   | \u2714\ufe0f\ncode_084 | [Video Analysis-KLT Optical Flow-1](python/code_084)   | \u270f\ufe0f\ncode_085 | [Video Analysis-KLT Optical Flow-2](python/code_085)   | \u270f\ufe0f\ncode_086 | [Video Analysis-Dense Optical Flow](python/code_086)   | \u270f\ufe0f\ncode_087 | [Video Analysis-Frame Difference Moving Object Analysis](python/code_087/opencv_087.py)   | \u2714\ufe0f\ncode_088 | [Video Analysis-Meanshift](python/code_088)   | \u270f\ufe0f\ncode_089 | [Video Analysis-CamShift](python/code_089)   | \u270f\ufe0f\ncode_090 | [Video Analysis-Object Movement Trajectory Drawing](python/code_090)   | \u2714\ufe0f\ncode_091 | [Object Detection-HAAR Cascade Classification ](python/code_091)   | \u2714\ufe0f\ncode_092 | [Object Detection-HAAR Feature Analysis](python/code_092)   | \u2714\ufe0f\ncode_093 | [Object Detection-LBP Feature Analysis](python/code_093/opencv_093.py)   | \u2714\ufe0f\ncode_094 | [ORB Feature Critical Point Detection](python/code_094)   | \u270f\ufe0f\ncode_095 | [ORB Feature Descriptor Matching](python/code_095)   | \u2714\ufe0f\ncode_096 | [Multiple  Descriptor Matching Methods](python/code_096)   | \u270f\ufe0f\ncode_097 | [Location of Known Objects Based on Descriptor Matches](python/code_097)   | \u270f\ufe0f\ncode_098 | [SIFT Feature Critical Point Detection](python/code_097)   | \u2714\ufe0f\ncode_099 | [SIFT Feature Descriptor Matching](python/code_097)   | \u2714\ufe0f\ncode_100 | [HOG Pedestrian Detection](python/code_100/opencv_100.py)   | \u2714\ufe0f\ncode_101 | [HOG Multiscale Detection](python/code_101/opencv_101.py)   | \u270f\ufe0f\ncode_102 | [HOG Extract Descriptor](python/code_102/opencv_102.py)   | \u2714\ufe0f\ncode_103 | [HOG Use Descriptors to Generate Sample Data](python/code_103/opencv_103.py)   | \u2714\ufe0f\ncode_104 | [(Detection Case)-HOG+SVM Train](python/code_104/opencv_104.py)   | \u2714\ufe0f\ncode_105 | [(Detection Case)-HOG+SVM Predict](python/code_105/opencv_105.py)   | \u2714\ufe0f\ncode_106 | [AKAZE Features and Descriptors](python/code_106)   | \u2714\ufe0f\ncode_107 | [Brisk Features and Descriptors](python/code_107)   | \u2714\ufe0f\ncode_108 | [GFTT Detector](python/code_108)   | \u2714\ufe0f\ncode_109 | [BLOB Feature Analysis](python/code_109)   | \u2714\ufe0f\ncode_110 | [KMeans Data Classification](python/code_110)   | \u2714\ufe0f\ncode_111 | [KMeans Image Segmentation](python/code_111)   | \u2714\ufe0f\ncode_112 | [KMeans Background Change](python/code_112)   | \u2714\ufe0f\ncode_113 | [KMeans Extract Image Color Card](python/code_113)   | \u2714\ufe0f\ncode_114 | [KNN Classification](python/code_114)   | \u2714\ufe0f\ncode_115 | [KNN-Train Data Save and Load](python/code_115)   | \u2714\ufe0f\ncode_116 | [Decision Tree Algorithm](python/code_116)   | \u2714\ufe0f\ncode_117 | [Image Mean-shift Segmentation](python/code_117)   | \u2714\ufe0f\ncode_118 | [Grabcut-Image Segmentation](python/code_118)   | \u2714\ufe0f\ncode_119 | [Grabcut-Background Change](python/code_119)   | \u270f\ufe0f\ncode_120 | [Qrcode detect and decode](python/code_120)   | \u270f\ufe0f\ncode_121 | [DNN- Read the information of each layer of the model](python/code_121)   | \u2714\ufe0f\ncode_122 | [DNN- Realize image classification](python/code_122)   | \u2714\ufe0f\ncode_123 | [DNN- Model runs to set the target device and compute the background](python/code_123)   | \u2714\ufe0f\ncode_124 | [DNN- SSD Single Image Detection](python/code_124)   | \u2714\ufe0f\ncode_125 | [DNN- SSD Real-time Video Detection](python/code_125)   | \u2714\ufe0f\ncode_126 | [DNN- Face Detection based on Residual Network](python/code_126)   | \u2714\ufe0f\ncode_127 | [DNN- Video Face Detection based on Residual Network](python/code_127)   | \u2714\ufe0f\ncode_128 | [DNN- Call the Detection Model of Tensorflow](python/code_128)   | \u2714\ufe0f\ncode_129 | [DNN- Call the Openpose Implementation Attitude Assessment](python/code_129)   | \u2714\ufe0f\ncode_130 | [DNN- Call YOLO Object Detection Network](python/code_130)   | \u2714\ufe0f"
 },
 {
  "repo": "RaftLib/RaftLib",
  "language": "C++",
  "readme_contents": "[RaftLib](http://raftlib.io) is a C++ Library for enabling stream/data-flow parallel computation. Using simple right shift operators (just like the C++ streams that you would use for string manipulation), you can link parallel compute kernels together. With RaftLib, we do away with explicit use of pthreads, std::thread, OpenMP, or any other parallel \"threading\" library. These are often mis-used, creating non-deterministic behavior. RaftLib's model allows lock-free FIFO-like access to the communications channels connecting each compute kernel. The full system has many auto-parallelization, optimization, and convenience features that enable relatively simple authoring of performant applications. Feel free to give it a shot, if you have any issues, please create an issue request. Minor issues, \nthe Slack group is the best way to resolve. We take pull requests!! For benchmarking, feel free to send the \nauthors an email. We've started a benchmark collection, however, it's far from complete. We'd love to add your \ncode!! \n\nUser Group / Mailing List: [slack channel](https://join.slack.com/t/raftlib/shared_invite/zt-3sk6ms6f-eEBd23dz98JnoRiXLaRmNw)\n\n=============\n\n### Build status\n\n![CI](https://github.com/RaftLib/RaftLib/workflows/CI/badge.svg?event=push)\n\n### Pre-requisites\n\n#### OS X & Linux\nCompiler: c++14 capable -> Clang, GNU GCC 5.0+, or Intel icc\n\n#### Windows\n* Latest merge from pull request to main should enable compilation on VS on Win10.\n\n### Install\nMake a build directory (for the instructions below, we'll \nwrite [build]). If you want to build the OpenCV example, then\nyou'll need to add to your cmake invocation:\n```bash\n-DBUILD_WOPENCV=true \n```\n\nTo use the [QThreads User space HPC threading library](http://www.cs.sandia.gov/qthreads/) \nyou will need to use the version with the RaftLib org and follow the RaftLib specific readme. \nThis QThreads version has patches for hwloc2.x applied and fixes for test cases. To compile\nRaftLib with QThreads linked, add the following (assumes the QThreads library is in your path):\n```bash\n-DUSEQTHREAD=1\n```\n\nBuilding the examples, benchmarks and tests can be disabled using:\n```bash\n-DBUILD_EXAMPLES=false\n-DBUILD_BENCHMARKS=false\n-DBUILD_TESTS=false\n```\n\nTo build:\n\n```bash\nmkdir [build]\ncd [build]\ncmake ..\nmake && make test\nsudo make install\n```\nNOTE: The default prefix in the makefile is: \n```\nPREFIX ?= /usr/local\n```\n\n## Using\n* When building applications with RaftLib, on Linux it is best to \nuse the **pkg-config** file, as an example, using the _poc.cpp_ example,\n```bash\ng++ `pkg-config --cflags raftlib` poc.cpp -o poc `pkg-config --libs raftlib`\n```\n\nFeel free to substitute your favorite build tool. I use Ninja and make depending on which machine I'm on. To change out, use cmake to generate the appropriate build files with the -Gxxx flag.\n\n### Citation\nIf you use this framework for something that gets published, please cite it as:\n```bibtex\n@article{blc16,\n  author = {Beard, Jonathan C and Li, Peng and Chamberlain, Roger D},\n  title = {RaftLib: A C++ Template Library for High Performance Stream Parallel Processing},\n  year = {2016},\n  doi = {http://dx.doi.org/10.1177/1094342016672542},\n  eprint = {http://hpc.sagepub.com/content/early/2016/10/18/1094342016672542.full.pdf+html},\n  journal = {International Journal of High Performance Computing Applications}\n}\n```\n### Other Info Sources\n* [OpenCV wrappers for RaftLib](https://github.com/RaftLib/RaftOCV)\n* [Project web page](http://raftlib.io)\n* [Project wiki page](https://github.com/jonathan-beard/RaftLib/wiki)\n* [Blog post intro](https://goo.gl/4VDlbr)\n* [Jonathan Beard's thesis](http://goo.gl/obkWUh)\n* [Views on parallel computing, general philosphy](https://goo.gl/R5fQAl)\n* Feel free to e-mail one of the authors of the repo\n"
 },
 {
  "repo": "opencv/opencv_extra",
  "language": null,
  "readme_contents": "### OpenCV: Open Source Computer Vision Library\n\nThis repository contains extra data for the OpenCV library.\n\n#### Resources\n* Homepage: http://opencv.org\n* Docs: http://docs.opencv.org\n* Q&A forum: http://answers.opencv.org\n* Issue tracking: https://github.com/opencv/opencv/issues\n\n#### Contributing\n\nPlease read before starting work on a pull request: https://github.com/opencv/opencv/wiki/How_to_contribute\n\nSummary of guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the coding style guide.\n"
 },
 {
  "repo": "mrnugget/opencv-haar-classifier-training",
  "language": "Perl",
  "readme_contents": "# Train your own OpenCV Haar classifier\n\n**Important**: This guide assumes you work with OpenCV 2.4.x. Since I no longer work with OpenCV, and don't have the time to keep up with changes and fixes, this guide is **unmaintained**. Pull requests will be merged of course, and if someone else wants commit access, feel free to ask!\n\nThis repository aims to provide tools and information on training your own\nOpenCV Haar classifier.  Use it in conjunction with this blog post: [Train your own OpenCV Haar\nclassifier](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n\n\n## Instructions\n\n1. Install OpenCV & get OpenCV source\n\n        brew tap homebrew/science\n        brew install --with-tbb opencv\n        wget http://downloads.sourceforge.net/project/opencvlibrary/opencv-unix/2.4.9/opencv-2.4.9.zip\n        unzip opencv-2.4.9.zip\n\n2. Clone this repository\n\n        git clone https://github.com/mrnugget/opencv-haar-classifier-training\n\n3. Put your positive images in the `./positive_images` folder and create a list\nof them:\n\n        find ./positive_images -iname \"*.jpg\" > positives.txt\n\n4. Put the negative images in the `./negative_images` folder and create a list of them:\n\n        find ./negative_images -iname \"*.jpg\" > negatives.txt\n\n5. Create positive samples with the `bin/createsamples.pl` script and save them\nto the `./samples` folder:\n\n        perl bin/createsamples.pl positives.txt negatives.txt samples 1500\\\n          \"opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\\\n          -maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40\"\n\n6. Use `tools/mergevec.py` to merge the samples in `./samples` into one file:\n\n        python ./tools/mergevec.py -v samples/ -o samples.vec\n\n   Note: If you get the error `struct.error: unpack requires a string argument of length 12`\n   then go into your **samples** directory and delete all files of length 0.\n\n7. Start training the classifier with `opencv_traincascade`, which comes with\nOpenCV, and save the results to `./classifier`:\n\n        opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024\n          \n    If you want to train it faster, configure feature type option with LBP:\n\n         opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024 -featureType LBP\n\n    After starting the training program it will print back its parameters and then start training. Each stage will print out some analysis as it is trained:\n\n      ```\n      ===== TRAINING 0-stage =====\n      <BEGIN\n      POS count : consumed   1000 : 1000\n      NEG count : acceptanceRatio    600 : 1\n      Precalculation time: 11\n      +----+---------+---------+\n      |  N |    HR   |    FA   |\n      +----+---------+---------+\n      |   1|        1|        1|\n      +----+---------+---------+\n      |   2|        1|        1|\n      +----+---------+---------+\n      |   3|        1|        1|\n      +----+---------+---------+\n      |   4|        1|        1|\n      +----+---------+---------+\n      |   5|        1|        1|\n      +----+---------+---------+\n      |   6|        1|        1|\n      +----+---------+---------+\n      |   7|        1| 0.711667|\n      +----+---------+---------+\n      |   8|        1|     0.54|\n      +----+---------+---------+\n      |   9|        1|    0.305|\n      +----+---------+---------+\n      END>\n      Training until now has taken 0 days 3 hours 19 minutes 16 seconds.\n      ```\n\n    Each row represents a feature that is being trained and contains some output about its HitRatio and FalseAlarm ratio. If a training stage only selects a few features (e.g. N = 2) then its possible something is wrong with your training data.\n\n    At the end of each stage the classifier is saved to a file and the process can be stopped and restarted. This is useful if you are tweaking a machine/settings to optimize training speed.\n\n8. Wait until the process is finished (which takes a long time \u2014 a couple of days probably, depending on the computer you have and how big your images are).\n\n9. Use your finished classifier!\n\n        cd ~/opencv-2.4.9/samples/c\n        chmod +x build_all.sh\n        ./build_all.sh\n        ./facedetect --cascade=\"~/finished_classifier.xml\"\n\n\n## Acknowledgements\n\nA huge thanks goes to Naotoshi Seo, who wrote the `mergevec.cpp` and\n`createsamples.cpp` tools and released them under the MIT licencse. His notes\non OpenCV Haar training were a huge help. Thank you, Naotoshi!\n\n## References & Links:\n\n- [Naotoshi Seo - Tutorial: OpenCV haartraining (Rapid Object Detection With A Cascade of Boosted Classifiers Based on Haar-like Features)](http://note.sonots.com/SciSoftware/haartraining.html)\n- [Material for Naotoshi Seo's tutorial](https://code.google.com/p/tutorial-haartraining/)\n- [OpenCV Documentation - Cascade Classifier Training](http://docs.opencv.org/doc/user_guide/ug_traincascade.html)\n"
 },
 {
  "repo": "mukyasa/MMCamScanner",
  "language": "Objective-C",
  "readme_contents": "# MMCamScanner\n#### Simulation to CamScanner app With Custom Camera and Crop Rect Validation \n\n\n------------------\n\n![MMCamScanner](https://github.com/mukyasa/MMCamScanner/blob/master/camscan.gif)\n\n#### Video preview [Here](https://www.youtube.com/watch?v=vO1kA6fjKQ4)\n \n#### ChangeLog(31/7/2015)\n * Crop Feature Tweaked for more control in horizontal and vertical dragging.\n * NOW , Landscape and Potrait both images can be used in scanning the objects in images.\n * Addded Image Filters (Gray Scale, Magic Color ,Black and White)\n * Left Rotate and Right Rotate\n \n\n**Framework**\nAdd the Frameworks to see the Demo\n\nOpenCV:http://opencv.org/<br />\nTesseract OCR:https://github.com/gali8/Tesseract-OCR-iOS\n\n**Thanks to Stackoverflow for solving queries related to OpenCV**<br />\n\n**Credits**<br />\n\nExcellent Square Detection Code Ref:[Here](http://stackoverflow.com/questions/8667818/opencv-c-obj-c-detecting-a-sheet-of-paper-square-detection) and [Here](https://github.com/Itseez/opencv/blob/master/samples/cpp/squares.cpp)<br />\n\nOpenCV Tutorials:[Here](http://opencv.org/\">http://opencv.org)<br />\n\nCustom Camera Project Ref:[Here](https://github.com/LoganWright/SimpleCam)<br />  (LoganWright)\n\nCrop Rect Validation Vector Ref:[Here](https://github.com/mysterioustrousers/MTGeometry) (Mysterious Trousers)\n\n\n**Also Thanks to this Project from where it is Inspired From**\n\nMaximilian Mackh :[Here](https://github.com/mmackh/MAImagePickerController-of-InstaPDF)\n\n\n**This Project Contains**\n\n1.Custom Camera(Support Potrait Only Now) with Zoom Slider,Pinch Zoom,Tap to Focus features also.<br />\n2.Custom Ripple View Controller Animation.<br />\n3.OpenCV square Detection in Image for detecting objects.<br />\n4.OpenCV perpespective and warptransform for CROP feature.<br />\n5.Crop Validation for checking INVALID RECT.<br />\n6.Memory Efficient Camera(Thanks to SimpleCam Project).<br />\n7.Tesseract OCR.<br />\n\n\n**My Other Repositories**\n\n**MMPaper:**<br />\nhttps://github.com/mukyasa/MMPaper<br />\n\n**MMTextFieldEffects:**<br />\nhttps://github.com/mukyasa/MMTextFieldEffects<br />\n\n**MMGooglePlayNewsStand:**<br />\nhttps://github.com/mukyasa/MMGooglePlayNewsStand\n\n**MMPaperPanFlip:**<br /> \nhttps://github.com/mukyasa/MMPaperPanFlip<br />\n\n**MMTransitionEffect:**<br />\nhttps://github.com/mukyasa/MMTransitionEffect<br />\n\n\nContact Me\n==========\nMukesh Mandora\n\nContact: mandoramuku07@gmail.com\n\nTwitter: http://twitter.com/mandymuku\n\nLinkedIn: https://in.linkedin.com/in/mukeshmandora\n\nGithub:https://github.com/mukyasa\n\n\n## License\nMMCamScanner is available under the Apache license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "uvipen/QuickDraw",
  "language": "Python",
  "readme_contents": "# [PYTHON] QuickDraw\n\n## Introduction\n\nHere is my python source code for QuickDraw - an online game developed by google. with my code, you could: \n* **Run an app which you could draw in front of a camera (If you use laptop, your webcam will be used by default)**\n* **Run an app which you could draw on a canvas**\n\n## Camera app\nIn order to use this app, you need a pen (or any object) with blue, red or green color. When the pen (object) appears in front of camera, it will be catched and highlighted by an yellow circle. When you are ready for drawing, you need to press **space** button. When you want to stop drawing, press **space** again\nBelow is the demo by running the sript **camera_app.py**:\n<p align=\"center\">\n  <img src=\"demo/quickdraw.gif\" width=600><br/>\n  <i>Camera app demo</i>\n</p>\n\n## Drawing app\nThe script and demo will be released soon\n\n## Dataset\nThe dataset used for training my model could be found at [Quick Draw dataset] https://console.cloud.google.com/storage/browser/quickdraw_dataset/sketchrnn. Here I only picked up 20 files for 20 categories\n\n## Categories:\nThe table below shows 20 categories my model used:\n\n|           |           |           |           |\n|-----------|:-----------:|:-----------:|:-----------:|\n|   apple   |   book    |   bowtie  |   candle  |\n|   cloud   |    cup    |   door    | envelope  |\n|eyeglasses |  guitar   |   hammer  |    hat    |\n| ice cream |   leaf    | scissors  |   star    |\n|  t-shirt  |   pants   | lightning |    tree   |\n\n## Trained models\n\nYou could find my trained model at **trained_models/whole_model_quickdraw**\n\n## Training\n\nYou need to download npz files corresponding to 20 classes my model used and store them in folder **data**. If you want to train your model with different list of categories, you only need to change the constant **CLASSES** at **src/config.py** and download necessary npz files. Then you could simply run **python3 train.py**\n\n## Experiments:\n\nFor each class, I take the first 10000 images, and then split them to training and test sets with ratio 8:2. The training/test loss/accuracy curves for the experiment are shown below:\n\n<img src=\"demo/loss_accuracy_curves.png\" width=\"800\"> \n\n## Requirements\n\n* **python 3.6**\n* **cv2**\n* **pytorch** \n* **numpy**\n"
 },
 {
  "repo": "alyssaq/face_morpher",
  "language": "Python",
  "readme_contents": "Face Morpher\n============\n\n| Warp, average and morph human faces!\n| Scripts will automatically detect frontal faces and skip images if\n  none is detected.\n\nBuilt with Python, `dlib`_, Numpy, Scipy, dlib.\n\n| Supported on Python 2.7, Python 3.6+\n| Tested on macOS Mojave and 64bit Linux (dockerized).\n\nRequirements\n--------------\n-  ``pip install -r requirements.txt``\n- Download `http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2` and extract file.\n- Export environment variable ``DLIB_DATA_DIR`` to the folder where ``shape_predictor_68_face_landmarks.dat`` is located. Default ``data``. E.g ``export DLIB_DATA_DIR=/Downloads/data``\n\nEither:\n\n-  `Use as local command-line utility`_\n-  `Use as pip library`_\n-  `Try out in a docker container`_\n\n.. _`Use as local command-line utility`:\n\nUse as local command-line utility\n---------------------------------\n::\n\n    $ git clone https://github.com/alyssaq/face_morpher\n\nMorphing Faces\n--------------\n\nMorph from a source to destination image:\n\n::\n\n    python facemorpher/morpher.py --src=<src_imgpath> --dest=<dest_imgpath> --plot\n\nMorph through a series of images in a folder:\n\n::\n\n    python facemorpher/morpher.py --images=<folder> --out_video=out.avi\n\nAll options listed in ``morpher.py`` (pasted below):\n\n::\n\n    Morph from source to destination face or\n    Morph through all images in a folder\n\n    Usage:\n        morpher.py (--src=<src_path> --dest=<dest_path> | --images=<folder>)\n                [--width=<width>] [--height=<height>]\n                [--num=<num_frames>] [--fps=<frames_per_second>]\n                [--out_frames=<folder>] [--out_video=<filename>]\n                [--plot] [--background=(black|transparent|average)]\n\n    Options:\n        -h, --help              Show this screen.\n        --src=<src_imgpath>     Filepath to source image (.jpg, .jpeg, .png)\n        --dest=<dest_imgpath>   Filepath to destination image (.jpg, .jpeg, .png)\n        --images=<folder>       Folderpath to images\n        --width=<width>         Custom width of the images/video [default: 500]\n        --height=<height>       Custom height of the images/video [default: 600]\n        --num=<num_frames>      Number of morph frames [default: 20]\n        --fps=<fps>             Number frames per second for the video [default: 10]\n        --out_frames=<folder>   Folder path to save all image frames\n        --out_video=<filename>  Filename to save a video\n        --plot                  Flag to plot images to result.png [default: False]\n        --background=<bg>       Background of images to be one of (black|transparent|average) [default: black]\n        --version               Show version.\n\nAveraging Faces\n---------------\n\nAverage faces from all images in a folder:\n\n::\n\n    python facemorpher/averager.py --images=<images_folder> --out=average.png\n\nAll options listed in ``averager.py`` (pasted below):\n\n::\n\n    Face averager\n\n    Usage:\n        averager.py --images=<images_folder> [--blur] [--plot]\n                [--background=(black|transparent|average)]\n                [--width=<width>] [--height=<height>]\n                [--out=<filename>] [--destimg=<filename>]\n\n    Options:\n        -h, --help             Show this screen.\n        --images=<folder>      Folder to images (.jpg, .jpeg, .png)\n        --blur                 Flag to blur edges of image [default: False]\n        --width=<width>        Custom width of the images/video [default: 500]\n        --height=<height>      Custom height of the images/video [default: 600]\n        --out=<filename>       Filename to save the average face [default: result.png]\n        --destimg=<filename>   Destination face image to overlay average face\n        --plot                 Flag to display the average face [default: False]\n        --background=<bg>      Background of image to be one of (black|transparent|average) [default: black]\n        --version              Show version.\n\nSteps (facemorpher folder)\n--------------------------\n\n1. Locator\n^^^^^^^^^^\n\n-  Locates face points\n-  For a different locator, return an array of (x, y) control face\n   points\n\n2. Aligner\n^^^^^^^^^^\n\n-  Align faces by resizing, centering and cropping to given size\n\n3. Warper\n^^^^^^^^^\n\n-  Given 2 images and its face points, warp one image to the other\n-  Triangulates face points\n-  Affine transforms each triangle with bilinear interpolation\n\n4a. Morpher\n^^^^^^^^^^^\n\n-  Morph between 2 or more images\n\n4b. Averager\n^^^^^^^^^^^^\n\n-  Average faces from 2 or more images\n\nBlender\n^^^^^^^\n\nOptional blending of warped image:\n\n-  Weighted average\n-  Alpha feathering\n-  Poisson blend\n\nExamples - `Being John Malkovich`_\n----------------------------------\n\nCreate a morphing video between the 2 images:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_video=out.avi``\n\n(out.avi played and recorded as gif)\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/being_john_malvokich.gif\n   :alt: gif\n\nSave the frames to a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_frames=out_folder --num=30``\n\nPlot the frames:\n^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--num=12 --plot``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/plot.png\n   :alt: plot\n\nAverage all face images in a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n85 images used\n\n| ``> python facemorpher/averager.py --images=images --blur --background=transparent``\n| ``--width=220 --height=250``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/average_faces.png\n   :alt: average\\_faces\n\n.. _`Use as pip library`:\n\nUse as pip library\n---------------------------------\n::\n\n    $ pip install facemorpher\n\nExamples\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAdditional options are exactly the same as the command line\n\n::\n\n    import facemorpher\n\n    # Get a list of image paths in a folder\n    imgpaths = facemorpher.list_imgpaths('imagefolder')\n\n    # To morph, supply an array of face images:\n    facemorpher.morpher(imgpaths, plot=True)\n\n    # To average, supply an array of face images:\n    facemorpher.averager(['image1.png', 'image2.png'], plot=True)\n\n\nOnce pip installed, 2 binaries are also available as a command line utility:\n\n::\n\n    $ facemorpher --src=<src_imgpath> --dest=<dest_imgpath> --plot\n    $ faceaverager --images=<images_folder> --plot\n\nTry out in a docker container\n---------------------------------\nMount local folder to `/images` in docker container, run it and enter a bash session.\n--rm removes the container when you close it.\n::\n\n    $ docker run -v  /Users/alyssa/Desktop/images:/images --name py3 --rm -it jjanzic/docker-python3-opencv bash\n\nOnce you're in the container, install ``facemorpher`` and try the examples listed above\n::\n\n    root@0dad0912ebbe:/# pip install facemorpher\n    root@0dad0912ebbe:/# facemorpher --src=<img1> --dest=<img2> --plot\n\nDocumentation\n-------------\n\nhttp://alyssaq.github.io/face_morpher\n\nBuild & publish Docs\n^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    ./scripts/publish_ghpages.sh\n\nLicense\n-------\n`MIT`_\n\n.. _Being John Malkovich: http://www.rottentomatoes.com/m/being_john_malkovich\n.. _Mac installation steps: https://gist.github.com/alyssaq/f60393545173379e0f3f#file-4-opencv3-with-python3-md\n.. _MIT: http://alyssaq.github.io/mit-license\n.. _OpenCV: http://opencv.org\n.. _Homebrew: https://brew.sh\n.. _source: https://github.com/opencv/opencv\n.. _dlib: http://dlib.net\n"
 },
 {
  "repo": "youyuge34/Anime-InPainting",
  "language": "Python",
  "readme_contents": "Anime-InPainting: An application Tool based on [Edge-Connect](https://github.com/knazeri/edge-connect)\n------------------------------------------------------------------------------------------------------\n<p align=\"left\">\n\t\t<img src=\"https://img.shields.io/badge/version-0.2-brightgreen.svg?style=flat-square\"\n\t\t\t alt=\"Version\">\n\t\t<img src=\"https://img.shields.io/badge/status-Release-gold.svg?style=flat-square\"\n\t\t\t alt=\"Status\">\n\t\t<img src=\"https://img.shields.io/badge/platform-win | linux-lightgrey.svg?style=flat-square\"\n\t\t\t alt=\"Platform\">\n\t\t<img src=\"https://img.shields.io/badge/PyTorch version-1.0-blue.svg?style=flat-square\"\n\t\t\t alt=\"PyTorch\">\n\t\t<img src=\"https://img.shields.io/badge/License-CC BY\u00b7NC 4.0-green.svg?style=flat-square\"\n\t\t\t alt=\"License\">\n</p>\n\nEnglish | [\u4e2d\u6587\u7248\u4ecb\u7ecd](#jump_zh)\n\n<hr>\n\n### Important\n**2019.3.27 Update:**     \nOur **latest** drawing method [PI-REC](https://github.com/youyuge34/PI-REC) is more powerful.      \n Take a look on it, and I'm sure it won't disappoint you.\n<hr>\n\n<p align=\"center\">\n<img src=\"files/banner.png\" width=\"720\" height=\"240\">\n</p>\n\n## Tool show time \ud83c\udff3\ufe0f\u200d\ud83c\udf08\n#### Outputs\n<p align=\"center\">\n<img src=\"files/show1.jpg\" width=\"720\" height=\"400\">\n</p>\n\n#### Tool operation\n<p align=\"center\">\n<img src=\"files/cut2.gif\" width=\"425\" height=\"425\">\n<img src=\"files/cut3.gif\" width=\"406\" height=\"222\">\n</p>\n\nIntroduction:\n-----\nThis is an optimized application tool which has a frontend based on `Opencv`, whose backend used [Edge-Connect](https://github.com/knazeri/edge-connect).\nMake sure you have read their awesome work and license thoroughly.\nCompared with the original work, this project has such <span id=\"improve\">improvements</span> :\n- Add tool application modes\n- Optimize the training phase\n  - Auto-save and auto-load latest weights files\n  - Add a fast training phase combined with origin phase 2 and 3\n- bugs fixed (most of them are merged into the original work)\n- Add utility files\n- Add configs in `config.yml`\n  - PRINT_FREQUENCY\n  - DEVICE : cpu or gpu\n- ... ...\n\n**You can do the amazing Anime inpainting conveniently here.**\n\n**And detailed [training manual](training_manual.md) is released. You may train your own dataset smoothly now.**\n\n## <span id='pre'>Prerequisites</span>\n- Python 3\n- PyTorch `1.0` (`0.4` is not supported)\n- NVIDIA GPU + CUDA cuDNN\n\n## <span id='ins'>Installation</span>\n- Clone this repo\n- Install PyTorch and dependencies from http://pytorch.org\n- Install python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## Run the Tool\nI want to run the tool! Calm down and follow such step:\n\n\n**Info: The following weights files are trained on anime face dataset which performs not good on a large whole anime character.**\n1. Download the well trained model weights file --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. Unzip the `.7z` and put it under your root directory.\nSo make sure your path now is: `./model/getchu/<xxxxx.pth>`\n3. Complete the above [Prerequisites](#pre) and [Installation](#ins)\n4. (Optional) Check and edit the `./model/getchu/config.yml` config file as you wish\n5. Run the cooool tool:\n\n#### Default Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### Tool with edge window:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### Args help\n```bash\npython tool_patch.py -h\n```\n\n> PS. You can run any well trained model, not only above one. You can download more model weights files\nfrom the original work [Edge-Connect](https://github.com/knazeri/edge-connect). Then you can run the Tool as above.\nOnly one thing to be careful: The `config.yml` in this project has some additional options than the config from the [Edge-Connect](https://github.com/knazeri/edge-connect).\n\n\n## Tool operation\nFor detailed manual, refer to your `terminal` prints or the `__doc__` in `tool_patch.py`.\n\nBelow is the simplified tool operation manual:\n\n\n\nKey | description\n-----|-----\nMouse `Left` | To draw out the defective area in window `input` and to draw the edge in window `edge`\nMouse `Right` | To erase edge in window `edge`\nKey `[` | To make the brush thickness smaller\nKey `]` | To make the brush thickness larger\nKey `0` | Todo\nKey `1` | Todo\nKey `n` | To patch the black part of image, just use input image\nKey `e` | To patch the black part of image, use the input image and edit edge (only work under edge window opened)\nKey `r` | To reset the setup\nKey `s` | To save the output\nKey `q` | To quit\n\n## Training manual\nClick here --> [Training manual by yourself](training_manual.md)\n\n\n\n<span id=\"jump_zh\">\u4e2d\u6587\u7248\u4ecb\u7ecd\ud83c\udde8\ud83c\uddf3 </span>\n-----\n\n<hr>\n\n### \u91cd\u8981\n**2019.3.27 \u66f4\u65b0:**     \n\u6211\u4eec\u7684\u6700\u65b0\u6a21\u578b [PI-REC](https://github.com/youyuge34/PI-REC) \u66f4\u5f3a\u5927.            \n\u5982\u679c\u4f60\u60f3\u7528\u6700\u65b0\u7684AI\u7ed8\u753b\u9ed1\u79d1\u6280\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u4fee\u8865\u56fe\u50cf\uff0c\u8bf7\u70b9\u51fb\u4e0a\u9762\u7684\u94fe\u63a5\ud83d\udc46\n<hr>\n\n\n## \u7b80\u4ecb\nTool\u6548\u679c\u770b\u4e0a\u9762\ud83d\udc46 | Bilibili\u89c6\u9891\u6559\u7a0b\uff1aTO DO\n\n\u8fd9\u662f\u56fe\u50cf\u4fee\u8865\u65b9\u5411\u6700\u65b0\u7814\u7a76\u6210\u679c[Edge-Connect](https://github.com/knazeri/edge-connect)\u7684~~\u963f\u59c6\u65af\u7279\u6717\u6c2e\u6c14\u52a0\u901f\u9b54\u6539~~\uff08\u4f18\u5316\uff09\u7248\u3002\n\u7528`Opencv`\u5199\u4e86\u4e2a\u524d\u7aef\u90e8\u5206\uff0c\u540e\u7aef\u662f[Edge-Connect](https://github.com/knazeri/edge-connect)\uff0c\u65b9\u4fbf\u5f53\u4f5c\u5de5\u5177\u4f7f\u7528\u3002\n\u6b64\u5de5\u5177\u53ef\u4ee5\u7528\u6765\u81ea\u52a8\u56fe\u50cf\u4fee\u8865\uff0c\u53bb\u9a6c\u8d5b\u514b\u2026\u2026\u540c\u6837\u4f18\u5316\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u8fc7\u7a0b\u3002\u5177\u4f53\u4f18\u5316\u5185\u5bb9\u8bf7\u770b[\u82f1\u6587\u7248Improvements](#improve)\u3002\n\n\u66f4\u65b0\uff1a[\u8bad\u7ec3\u624b\u518c](training_manual.md#jump_zh)\u5df2\u7ecf\u586b\u5751\u5b8c\u53d1\u5e03\u4e86\uff01\u4f60\u53ef\u4ee5\u7167\u7740\u6307\u5357\u8bad\u7ec3\u81ea\u5df1\u6570\u636e\u96c6\u4e86~\n\n## \u57fa\u7840\u73af\u5883\n- Python 3\n- PyTorch `1.0` (`0.4` \u4f1a\u62a5\u9519)\n- NVIDIA GPU + CUDA cuDNN \uff08\u5f53\u524d\u7248\u672c\u5df2\u53ef\u9009cpu\uff0c\u8bf7\u4fee\u6539`config.yml`\u4e2d\u7684`DEVICE`\uff09\n\n## \u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\n- Clone this repo\n- \u5b89\u88c5PyTorch\u548ctorchvision --> http://pytorch.org\n- \u5b89\u88c5 python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## \u8fd0\u884cTool\n\u6559\u7ec3\uff01\u6211\u6709\u4e2a\u5927\u80c6\u7684\u60f3\u6cd5\ud83c\ude32\u2026\u2026\u522b\u6025\uff0c\u4e00\u6b65\u6b65\u6765\uff1a\n\n\n**\u6ce8\u610f\uff1a\u4ee5\u4e0b\u6a21\u578b\u662f\u5728\u52a8\u6f2b\u5934\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\uff0c\u6240\u4ee5\u5bf9\u52a8\u6f2b\u5168\u8eab\u5927\u56fe\u4fee\u8865\u6548\u679c\u4e00\u822c\uff0c\u60f3\u81ea\u5df1\u518d\u8bad\u7ec3\u7684\u53c2\u8003\u4e0b\u9762\u7684\u8bad\u7ec3\u6307\u5357**\n1. \u4e0b\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6587\u4ef6 --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. \u89e3\u538b `.7z` \u653e\u5230\u4f60\u7684\u6839\u76ee\u5f55\u4e0b.\n\u786e\u4fdd\u4f60\u7684\u76ee\u5f55\u73b0\u5728\u662f\u8fd9\u6837: `./model/getchu/<xxxxx.pth>`\n3. \u5b8c\u6210\u4e0a\u9762\u7684\u57fa\u7840\u73af\u5883\u548c\u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\u6b65\u9aa4\n4. (\u53ef\u9009) \u68c0\u67e5\u5e76\u7f16\u8f91 `./model/getchu/config.yml` \u914d\u7f6e\u6587\u4ef6\n5. \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\uff1a\n\n#### \u9ed8\u8ba4Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### \u5e26Edge\u7f16\u8f91\u7a97\u53e3\u7684Tool:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### \u547d\u4ee4\u884c\u53c2\u6570\u5e2e\u52a9\n```bash\npython tool_patch.py -h\n```\n\n> PS. \u4f60\u4e5f\u80fd\u7528tool\u8dd1\u522b\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5728\u8fd9\u91cc\u4e0b\u8f7d\u539f\u4f5c\u66f4\u591a\u6a21\u578b[Edge-Connect](https://github.com/knazeri/edge-connect).\n\u6587\u4ef6\u7ec4\u7ec7\u65b9\u5f0f\u53c2\u8003\u4e0a\u9762\uff0c\u5176\u4f59\u8fd0\u884c\u547d\u4ee4\u90fd\u4e00\u6837\u3002\u552f\u4e00\u6ce8\u610f\u7684\u662f\u8fd9\u4e2a\u9879\u76ee\u7684 `config.yml` \u6bd4\u539f\u4f5c\u7684\u591a\u4e86\u51e0\u4e2a\u9009\u9879\uff0c\u62a5\u9519\u4e86\u7684\u8bdd\u6ce8\u610f\u4fee\u6539\u3002\n\n## Tool\u64cd\u4f5c\u6307\u5357\n\u8be6\u7ec6\u5185\u5bb9\u8bf7\u7ffb\u770b\u63a7\u5236\u53f0\u7684\u6253\u5370\u5185\u5bb9\uff0c\u6216\u67e5\u770b`tool_patch.py`\u91cc\u7684`__doc__`      \n\u7b80\u7565\u7248tool\u4f7f\u7528\u6307\u5357\uff1a\n\n\u6309\u952e | \u8bf4\u660e\n-----|------\n\u9f20\u6807\u5de6\u952e | Input\u7a97\u53e3\uff1a\u753b\u51fa\u7455\u75b5\u533a\u57df\u7684\u906e\u76d6\uff0cEdge\u7a97\u53e3\uff1a\u624b\u52a8\u753b\u8fb9\u7f18\n\u9f20\u6807\u53f3\u952e | Edge\u7a97\u53e3\uff1a\u6a61\u76ae\u64e6\n\u6309\u952e `[` | \u7b14\u5237\u53d8\u7ec6 \uff08\u63a7\u5236\u53f0\u6253\u5370\u7c97\u7ec6\u5927\u5c0f\uff09\n\u6309\u952e `]` | \u7b14\u5237\u53d8\u7c97\n\u6309\u952e `0` | Todo\n\u6309\u952e `1` | Todo\n\u6309\u952e `n` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u53ea\u4f7f\u7528\u4e00\u5f20\u8f93\u5165\u56fe\u7247\n\u6309\u952e `e` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u4f7f\u7528\u8f93\u5165\u56fe\u7247\u548c\u8fb9\u7f18\u56fe\u7247\uff08\u4ec5\u5f53edge\u7a97\u53e3\u542f\u52a8\u65f6\u6709\u6548\uff09\n\u6309\u952e `r` | \u5168\u90e8\u91cd\u7f6e\n\u6309\u952e `s` | \u4fdd\u5b58\u8f93\u51fa\u56fe\u7247\n\u6309\u952e `q` | \u9000\u51fa\n\n\n## \u8bad\u7ec3\u6307\u5357\n\u8bad\u7ec3\u6307\u5357 --> [\u9605\u8bfb](training_manual.md#jump_zh)\n\n\n## License\nLicensed under a [Creative Commons Attribution-NonCommercial 4.0 International](https://creativecommons.org/licenses/by-nc/4.0/).\n\nExcept where otherwise noted, this content is published under a [CC BY-NC](https://creativecommons.org/licenses/by-nc/4.0/) license, which means that you can copy, remix, transform and build upon the content as long as you do not use the material for commercial purposes and give appropriate credit and provide a link to the license.\n\n\n## Citation\nIf you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/1901.00212\">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a>:\n\n```\n@inproceedings{nazeri2019edgeconnect,\n  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},\n  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},\n  journal={arXiv preprint},\n  year={2019},\n}\n```\n\n"
 },
 {
  "repo": "vipul-sharma20/document-scanner",
  "language": "Python",
  "readme_contents": "document-scanner\n================\n\nA document scanner built using OpenCV + Python.\nI highly recommend to see my blog post for better understanding: [http://vipulsharma20.blogspot.on](http://vipulsharma20.blogspot.in)\n\nMy sincere thanks to the article and the author here: [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/) which has some really good set of articles on OpenCV and way more informative.\n\n* Original Image\n![Alt](http://1.bp.blogspot.com/-gbFBHQKFU7w/VpGzzVfxmLI/AAAAAAAAEks/jtX2fikHs5o/s1600/Original.jpg \"original\")\n\n* Grayscaled Image\n![Alt](http://2.bp.blogspot.com/--LzOU44-dhM/VpG2B6DJbxI/AAAAAAAAEk4/4BGXnfhvsk4/s1600/Original%2BGray.jpg \"gray\")\n\n* Gaussian Blur\n![Alt](http://2.bp.blogspot.com/-KfEWWzIXxBg/VpG2RY0upjI/AAAAAAAAElA/psuYvv1rnm0/s1600/Original%2BBlurred.jpg \"blurred\")\n\n* Edge Detection (Canny Edge Detection)\n![Alt](http://3.bp.blogspot.com/-5TVP2UFeGXk/VpG5-bIYNqI/AAAAAAAAElM/zmyNrbvnh8Q/s1600/Original%2BEdged.jpg \"edge\")\n\n* Contour Detection\n![Alt](http://1.bp.blogspot.com/-Es0PkMvJJxU/VpHcQEXzXaI/AAAAAAAAElc/NuCZmuW1K6o/s1600/Outline_all.jpg \"contour\")\n\n* Approximated Contour\n![Alt](http://4.bp.blogspot.com/-DL7XWsLvWg8/VpHeN6bA3gI/AAAAAAAAElo/1TMug5_tCeQ/s1600/Outline.jpg \"approx\")\n\n* Perspective Transform\n![Alt](http://4.bp.blogspot.com/-1dhSo9PrD6o/VpHjhgH0viI/AAAAAAAAEl4/AzYqjzLiNbI/s1600/dst.jpg \"transform\")\n\n"
 },
 {
  "repo": "bethesirius/ChosunTruck",
  "language": "Python",
  "readme_contents": "# <img src=\"https://github.com/bethesirius/ChosunTruck/blob/master/README/Logo.png\" width=\"64\">ChosunTruck\n\n## Introduction\nChosunTruck is an autonomous driving solution for [Euro Truck Simulator 2](https://eurotrucksimulator2.com/).\nRecently, autonomous driving technology has become a big issue and as a result we have been studying technology that incorporates this.\nIt is being developed in a simulated environment called Euro Truck Simulator 2 to allow us to study it using vehicles.\nWe chose Euro Truck Simulator 2 because this simulator provides a good test environment that is similar to the real road.\n\n## Features\n* You can drive a vehicle without handling it yourself.\n* You can understand the principles of autonomous driving.\n* (Experimental/Linux only) You can detect where other vehicles are.\n\n## How To Run It\n### Windows\n\n#### Dependencies\n- OS: Windows 7, 10 (64bit)\n\n- IDE: Visual Studio 2013, 2015\n\n- OpenCV version: >= 3.1\n\n- [Cuda Toolkit 7.5](https://developer.nvidia.com/cuda-75-downloads-archive) (Note: Do an ADVANCED INSTALLATION. ONLY install the Toolkit + Integration to Visual Studio. Do NOT install the drivers + other stuff it would normally give you. Once installed, your project properties should look like this: https://i.imgur.com/e7IRtjy.png)\n\n- If you have a problem during installation, look at our [Windows Installation wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Windows-Installation)\n\n#### Required to allow input to work in Windows:\n- **Go to C:\\Users\\YOURUSERNAME\\Documents\\Euro Truck Simulator 2\\profiles and edit controls.sii from** \n```\nconfig_lines[0]: \"device keyboard `di8.keyboard`\"\nconfig_lines[1]: \"device mouse `fusion.mouse`\"\n```\nto \n```\nconfig_lines[0]: \"device keyboard `sys.keyboard`\"\nconfig_lines[1]: \"device mouse `sys.mouse`\"\n```\n(thanks Komat!)\n- **While you are in controls.sii, make sure your sensitivity is set to:**\n```\n config_lines[33]: \"constant c_rsteersens 0.775000\"\n config_lines[34]: \"constant c_asteersens 4.650000\"\n```\n#### Then:\n- Set controls.sii to read-only\n- Open the visual studio project and build it. \n- Run ETS2 in windowed mode and set resolution to 1024 * 768.(It will work properly with 1920 * 1080 screen resolution and 1024 * 768 window mode ETS2.)\n\n### Linux\n#### Dependencies\n- OS: Ubuntu 16.04 LTS\n\n- [OpenCV version: >= 3.1](http://embedonix.com/articles/image-processing/installing-opencv-3-1-0-on-ubuntu/)\n\n- (Optional) Tensorflow version: >= 0.12.1\n\n### Build the source code with the following command (inside the linux directory).\n```\nmake\n```\n### If you want the car detection function then:\n````\nmake Drive\n````\n#### Then:\n- Run ETS2 in windowed mode and set its resolution to 1024 * 768. (It will work properly with 1920 * 1080 screen resolution and 1024 * 768 windowed mode ETS2)\n- It cannot find the ETS2 window automatically. Move the ETS2 window to the right-down corner to fix this.\n- In ETS2 Options, set controls to 'Keyboard + Mouse Steering', 'left click' to acclerate, and 'right click' to brake.\n- Go to a highway and set the truck's speed to 40~60km/h. (I recommend you turn on cruise mode to set the speed easily)\n- Run this program!\n\n#### To enable car detection mode, add -D or --Car_Detection.\n```\n./ChosunTruck [-D|--Car_Detection]\n```\n## Troubleshooting\nSee [Our wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Troubleshooting).\n\nIf you have some problems running this project, reference the demo video below. Or, [open a issue to contact our team](https://github.com/bethesirius/ChosunTruck/issues).\n\n## Demo Video\nLane Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/vF7J_uC045Q/0.jpg)](http://www.youtube.com/watch?v=vF7J_uC045Q)\n[![youtube link](http://img.youtube.com/vi/qb99czlIklA/0.jpg)](http://www.youtube.com/watch?v=qb99czlIklA)\n\nLane Detection + Vehicle Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/w6H2eGEvzvw/0.jpg)](http://www.youtube.com/watch?v=w6H2eGEvzvw)\n\n## Todo\n* For better detection performance, Change the Tensorbox to YOLO2.\n* The information from in-game screen have Restrictions. Read ETS2 process memory to collect more driving environment data.\n\n## Founders\n- Chiwan Song, chi3236@gmail.com\n\n- JaeCheol Sim, simjaecheol@naver.com\n\n- Seongjoon Chu, hs4393@gmail.com\n\n## Contributors\n- [zappybiby](https://github.com/zappybiby)\n\n## How To Contribute\nAnyone who is interested in this project is welcome! Just fork it and pull requests!\n\n## License\nChosunTruck, Euro Truck Simulator 2 auto driving solution\nCopyright (C) 2017 chi3236, bethesirius, uoyssim\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n"
 },
 {
  "repo": "opencv/opencv_for_ios_book_samples",
  "language": "Objective-C",
  "readme_contents": "OpenCV for iOS (samples for the book)\n=====================================\n\n - Authors: Alexander Shishkov and Kirill Kornyakov\n - Book: <http://bit.ly/OpenCV_for_iOS_book>\n - Copyright: Packt Publishing 2013\n - License: see the `LICENSE.txt` file\n\nBuild & run sample projects\n---------------------------\n\nAlmost every chapter of the book describes a separate project. There are 15\nXcode projects for 17 chapters in the book. Chapter 16 extends the project of\nChapter 14, and for Chapter 17 you need only OpenCV source code.\n\n- First of all you will need a computer with Mac OSX and Xcode. An iOS device\n  is also helpful, since not all samples can be executed on Simulator.\n- Install OpenCV v2.4.6 or newer. You can go to <http://opencv.org>, click on\n  _Downloads_, and download the latest OpenCV framework for iOS. Copy it to the\n  folder with this code.\n- Now you can import sample projects to Xcode and run them on Simulator or a\n  real device.\n\nFor detailed instructions and explanations please refer to the book.\n\nChapters\n--------\n\n  1. _Getting Started with iOS_ helps you to setup your development environment\n     and run your first \"Hello World\" iOS application.\n  2. _Displaying Image from Resources_ introduces you to basic GUI concepts on\n     iOS, and covers loading of an image from resources and displaying it on the\n     display.\n  3. _Linking OpenCV to iOS Project_ explains how to link OpenCV library and\n     call any function from it.\n  4. _Detecting Faces with Cascade Classifier_ shows how to detect faces using\n     OpenCV.\n  5. _Printing Postcard_ demonstrates how a simple photo effect can be\n     implemented.\n  6. _Working with Images in Gallery_ explains how to load and save images\n     from/to Gallery.\n  7. _Applying Retro Effect_ demonstrates another interesting photo effect,\n     which makes photos look old.\n  8. _Taking Photos From Camera_ shows how to capture static images with camera.\n  9. _Creating Static Library_ explains how to create a Static Library project\n     in Xcode.\n  10. _Capturing Video from Camera_ shows how to capture video stream from\n      camera.\n  11. _Control Advanced Camera Settings_ explains how to control advanced camera\n      settings, like exposure, focus and white balance.\n  12. _Applying Effects to Live Video_ shows how to process captured video\n      frames on the fly.\n  13. _Saving Video from Camera_ explains how to save video stream to the device\n      with hardware encoding.\n  14. _Optimizing Performance with ARM NEON_ explains how to use SIMD\n      instructions to vectorize you code and improve performance.\n  15. _Detecting Facial Features_ presents a simple facial feature detection\n      demo.\n  16. _Using Accelerate Framework_ explains how to link the framework, and how\n      to use it for performance optimization.\n  17. _Building OpenCV for iOS from sources_ explains where to get and how to\n      build the latest OpenCV sources.\n\nScreenshots\n-----------\n\n### Chapter 5. \"Printing Postcard\"\n![](./Chapter05_PrintingPostcard/screenshot.png)\n\n### Chapter 6. \"Working with Images in Gallery\"\n![](./Chapter06_WorkingWithGallery/screenshot.png)\n\n### Chapter 7. \"Applying Retro Effect\"\n![](./Chapter07_ApplyingRetroEffect/screenshot.png)\n\n### Chapter 12. \"Applying Effects to Live Video\"\n![](./Chapter12_ProcessingVideo/screenshot.png)\n\n### Chapter 15. \"Detecting Facial Features\"\n![](./Chapter15_DetectingFacialFeatures/screenshot.png)"
 },
 {
  "repo": "Cartucho/OpenLabeling",
  "language": "Python",
  "readme_contents": "# OpenLabeling: open-source image and video labeler\n\n[![GitHub stars](https://img.shields.io/github/stars/Cartucho/OpenLabeling.svg?style=social&label=Stars)](https://github.com/Cartucho/OpenLabeling)\n\nImage labeling in multiple annotation formats:\n- PASCAL VOC (= [darkflow](https://github.com/thtrieu/darkflow))\n- [YOLO darknet](https://github.com/pjreddie/darknet)\n- ask for more (create a new issue)...\n\n<img src=\"https://media.giphy.com/media/l49JDgDSygJN369vW/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc1csRs9PoDgCeuk/giphy.gif\" width=\"40%\">\n<img src=\"https://media.giphy.com/media/3o752fXKwTJJkhXP32/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc11t9auzSo6fwLS/giphy.gif\" width=\"40%\">\n\n## Citation\n\nThis project was developed for the following paper, please consider citing it:\n\n```bibtex\n@INPROCEEDINGS{8594067,\n  author={J. {Cartucho} and R. {Ventura} and M. {Veloso}},\n  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, \n  title={Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots}, \n  year={2018},\n  pages={2336-2341},\n}\n```\n\n## Latest Features\n\n- Jun 2019: Deep Learning Object Detection Model\n- May 2019: [ECCV2018] Distractor-aware Siamese Networks for Visual Object Tracking\n- Jan 2019: easy and quick bounding-boxe's resizing!\n- Jan 2019: video object tracking with OpenCV trackers!\n- TODO: Label photos via Google drive to allow \"team online labeling\".\n[New Features Discussion](https://github.com/Cartucho/OpenLabeling/issues/3)\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Prerequisites](#prerequisites)\n- [Run project](#run-project)\n- [GUI usage](#gui-usage)\n- [Authors](#authors)\n\n## Quick start\n\nTo start using the YOLO Bounding Box Tool you need to [download the latest release](https://github.com/Cartucho/OpenLabeling/archive/v1.3.zip) or clone the repo:\n\n```\ngit clone --recurse-submodules git@github.com:Cartucho/OpenLabeling.git\n```\n\n### Prerequisites\n\nYou need to install:\n\n- [Python](https://www.python.org/downloads/)\n- [OpenCV](https://opencv.org/) version >= 3.0\n    1. `python -mpip install -U pip`\n    1. `python -mpip install -U opencv-python`\n    1. `python -mpip install -U opencv-contrib-python`\n- numpy, tqdm and lxml:\n    1. `python -mpip install -U numpy`\n    1. `python -mpip install -U tqdm`\n    1. `python -mpip install -U lxml`\n\nAlternatively, you can install everything at once by simply running:\n\n```\npython -mpip install -U pip\npython -mpip install -U -r requirements.txt\n```\n- [PyTorch](https://pytorch.org/get-started/locally/) \n    Visit the link for a configurator for your setup.\n    \n### Run project\n\nStep by step:\n\n  1. Open the `main/` directory\n  2. Insert the input images and videos in the folder **input/**\n  3. Insert the classes in the file **class_list.txt** (one class name per line)\n  4. Run the code:\n  5. You can find the annotations in the folder **output/**\n\n         python main.py [-h] [-i] [-o] [-t] [--tracker TRACKER_TYPE] [-n N_FRAMES]\n\n         optional arguments:\n          -h, --help                Show this help message and exit\n          -i, --input               Path to images and videos input folder | Default: input/\n          -o, --output              Path to output folder (if using the PASCAL VOC format it's important to set this path correctly) | Default: output/\n          -t, --thickness           Bounding box and cross line thickness (int) | Default: -t 1\n          --tracker tracker_type    tracker_type being used: ['CSRT', 'KCF','MOSSE', 'MIL', 'BOOSTING', 'MEDIANFLOW', 'TLD', 'GOTURN', 'DASIAMRPN']\n          -n N_FRAMES               number of frames to track object for\n  To use DASIAMRPN Tracker:\n  1. Install the [DaSiamRPN](https://github.com/foolwood/DaSiamRPN) submodule and download the model (VOT) from [google drive](https://drive.google.com/drive/folders/1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H)\n  2. copy it into 'DaSiamRPN/code/'\n  3. set default tracker in main.py or run it with --tracker DASIAMRPN\n\n\n#### How to use the deep learning feature\n\n- Download one or some deep learning models from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n  and put it into `object_detection/models` directory (you need to create the `models` folder by yourself). The outline of `object_detection` looks like that:\n  + `tf_object_detection.py`\n  + `utils.py`\n  + `models/ssdlite_mobilenet_v2_coco_2018_05_09`\n\nDownload the pre-trained model by clicking this link http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz and put it into `object_detection/models`. Create the `models` folder if necessary. Make sure to extract the model.\n\n  **Note**: Default model used in `main_auto.py` is `ssdlite_mobilenet_v2_coco_2018_05_09`. We can\n  set `graph_model_path` in file `main_auto.py` to change the pretrain model\n- Using `main_auto.py` to automatically label data first\n\n  TODO: explain how the user can \n\n### GUI usage\n\nKeyboard, press: \n\n<img src=\"https://github.com/Cartucho/OpenLabeling/blob/master/keyboard_usage.jpg\">\n\n| Key | Description |\n| --- | --- |\n| a/d | previous/next image |\n| s/w | previous/next class |\n| e | edges |\n| h | help |\n| q | quit |\n\nVideo:\n\n| Key | Description |\n| --- | --- |\n| p | predict the next frames' labels |\n\nMouse:\n  - Use two separate left clicks to do each bounding box\n  - **Right-click** -> **quick delete**!\n  - Use the middle mouse to zoom in and out\n  - Use double click to select a bounding box\n\n## Authors\n\n* **Jo\u00e3o Cartucho**\n\n    Feel free to contribute\n\n    [![GitHub contributors](https://img.shields.io/github/contributors/Cartucho/OpenLabeling.svg)](https://github.com/Cartucho/OpenLabeling/graphs/contributors)\n"
 },
 {
  "repo": "mbeyeler/opencv-machine-learning",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine Learning for OpenCV\n\n[![Google group](https://img.shields.io/badge/Google-Discussion%20group-lightgrey.svg)](https://groups.google.com/d/forum/machine-learning-for-opencv)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.833523.svg)](https://doi.org/10.5281/zenodo.833523)\n\nThis is the Jupyter notebook version of the following book:\n\n<img src=\"https://images-na.ssl-images-amazon.com/images/I/41CKBKW8y4L.jpg\" width=\"200\" align=\"left\" style=\"padding: 1px; border: 1px solid black; margin-right: 5px\"/> <br/>\nMichael Beyeler <br/>\n<a href=\"https://www.amazon.com/Machine-Learning-OpenCV-Michael-Beyeler/dp/1783980281\" target=\"_blank\"><b>Machine Learning for OpenCV</b></a> <br/>\nIntelligent Image Processing with Python\n<br/><br/>\n14 July 2017 <br/>\nPackt Publishing Ltd., London, England <br/>\nPaperback: 382 pages <br/>\nISBN 978-178398028-4\n<br clear=\"both\"/><br/>\n\nThe content is available on [GitHub](https://github.com/mbeyeler/opencv-machine-learning).\nThe code is released under the [MIT license](https://opensource.org/licenses/MIT).\n\nThe book is also available as a two-part video course:\n- [Part I: Supervised Learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-supervised-learning-video) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789347357/59952/59953/the-course-overview))\n- [Part II: Advanced methods and deep learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-%E2%80%93-advanced-methods-and-deep-learning-vide) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789340525/62127/62128/the-course-overview))\n\nFor questions, discussions, and more detailed help please refer to the [Google group](https://groups.google.com/d/forum/machine-learning-for-opencv).\n\nIf you use either book or code in a scholarly publication, please cite as:\n\n> M. Beyeler, (2017). Machine Learning for OpenCV. Packt Publishing Ltd., London, England, 380 pages, ISBN 978-178398028-4.\n\nOr use the following bibtex:\n\n```\n@book{MachineLearningOpenCV,\n\ttitle = {{Machine Learning for OpenCV}},\n\tsubtitle = {{Intelligent image processing with Python}},\n\tauthor = {Michael Beyeler},\n\tyear = {2017},\n\tpages = {380},\n\tpublisher = {Packt Publishing Ltd.},\n\tisbn = {978-178398028-4}\n}\n```\n\nScholarly work referencing this book:\n- S Lynch (2018). Image Processing with Python. *Dynamical Systems with Applications using Python*, Springer.\n- MQG Quiroz (2018). Inductive Machine Learning with Image Processing for Objects Detection of a Robotic Arm with Raspberry PI. *International Conference on Technology Trends*.\n- A Konate (2018). Un aper\u00e7u sur quelques m\u00e9thodes en apprentissage automatique supervis\u00e9. *HAL* 01946237.\n\n\n## Table of Contents\n\n[Preface](notebooks/00.00-Preface.ipynb)\n\n[Foreword by Ariel Rokem](notebooks/00.01-Foreword-by-Ariel-Rokem.ipynb)\n\n1. [A Taste of Machine Learning](notebooks/01.00-A-Taste-of-Machine-Learning.ipynb)\n\n2. [Working with Data in OpenCV](notebooks/02.00-Working-with-Data-in-OpenCV.ipynb)\n   - [Dealing with Data Using Python's NumPy Package](notebooks/02.01-Dealing-with-Data-Using-Python-NumPy.ipynb)\n   - [Loading External Datasets in Python](notebooks/02.02-Loading-External-Datasets-in-Python.ipynb)\n   - [Visualizing Data Using Matplotlib](notebooks/02.03-Visualizing-Data-Using-Matplotlib.ipynb)\n   - [Dealing with Data Using OpenCV's TrainData container](notebooks/02.05-Dealing-with-Data-Using-the-OpenCV-TrainData-Container-in-C%2B%2B.ipynb)\n\n3. [First Steps in Supervised Learning](notebooks/03.00-First-Steps-in-Supervised-Learning.ipynb)\n   - [Measuring Model Performance with Scoring Functions](notebooks/03.01-Measuring-Model-Performance-with-Scoring-Functions.ipynb)\n   - [Understanding the k-NN Algorithm](notebooks/03.02-Understanding-the-k-NN-Algorithm.ipynb)\n   - [Using Regression Models to Predict Continuous Outcomes](notebooks/03.03-Using-Regression-Models-to-Predict-Continuous-Outcomes.ipynb)\n   - [Applying Lasso and Ridge Regression](notebooks/03.04-Applying-Lasso-and-Ridge-Regression.ipynb)\n   - [Classifying Iris Species Using Logistic Regression](notebooks/03.05-Classifying-Iris-Species-Using-Logistic-Regression.ipynb)\n\n4. [Representing Data and Engineering Features](notebooks/04.00-Representing-Data-and-Engineering-Features.ipynb)\n   - [Preprocessing Data](notebooks/04.01-Preprocessing-Data.ipynb)\n   - [Reducing the Dimensionality of the Data](notebooks/04.02-Reducing-the-Dimensionality-of-the-Data.ipynb)\n   - [Representing Categorical Variables](notebooks/04.03-Representing-Categorical-Variables.ipynb)\n   - [Representing Text Features](notebooks/04.04-Represening-Text-Features.ipynb)\n   - [Representing Images](notebooks/04.05-Representing-Images.ipynb)\n\n5. [Using Decision Trees to Make a Medical Diagnosis](notebooks/05.00-Using-Decision-Trees-to-Make-a-Medical-Diagnosis.ipynb)\n   - [Building Our First Decision Tree](notebooks/05.01-Building-Our-First-Decision-Tree.ipynb)\n   - [Using Decision Trees to Diagnose Breast Cancer](notebooks/05.02-Using-Decision-Trees-to-Diagnose-Breast-Cancer.ipynb)\n   - [Using Decision Trees for Regression](notebooks/05.03-Using-Decision-Trees-for-Regression.ipynb)\n\n6. [Detecting Pedestrians with Support Vector Machines](notebooks/06.00-Detecting-Pedestrians-with-Support-Vector-Machines.ipynb)\n   - [Implementing Your First Support Vector Machine](notebooks/06.01-Implementing-Your-First-Support-Vector-Machine.ipynb)\n   - [Detecting Pedestrians in the Wild](notebooks/06.02-Detecting-Pedestrians-in-the-Wild.ipynb)\n   - [Additional SVM Exercises](notebooks/06.03-Additional-SVM-Exercises.ipynb)\n\n7. [Implementing a Spam Filter with Bayesian Learning](notebooks/07.00-Implementing-a-Spam-Filter-with-Bayesian-Learning.ipynb)\n   - [Implementing Our First Bayesian Classifier](notebooks/07.01-Implementing-Our-First-Bayesian-Classifier.ipynb)\n   - [Classifying E-Mails Using Naive Bayes](notebooks/07.02-Classifying-Emails-Using-Naive-Bayes.ipynb)\n\n8. [Discovering Hidden Structures with Unsupervised Learning](notebooks/08.00-Discovering-Hidden-Structures-with-Unsupervised-Learning.ipynb)\n   - [Understanding k-Means Clustering](notebooks/08.01-Understanding-k-Means-Clustering.ipynb)\n   - [Compressing Color Images Using k-Means](notebooks/08.02-Compressing-Color-Images-Using-k-Means.ipynb)\n   - [Classifying Handwritten Digits Using k-Means](notebooks/08.03-Classifying-Handwritten-Digits-Using-k-Means.ipynb)\n   - [Implementing Agglomerative Hierarchical Clustering](notebooks/08.04-Implementing-Agglomerative-Hierarchical-Clustering.ipynb)\n\n9. [Using Deep Learning to Classify Handwritten Digits](notebooks/09.00-Using-Deep-Learning-to-Classify-Handwritten-Digits.ipynb)\n   - [Understanding Perceptrons](notebooks/09.01-Understanding-Perceptrons.ipynb)\n   - [Implementing a Multi-Layer Perceptron in OpenCV](notebooks/09.02-Implementing-a-Multi-Layer-Perceptron-in-OpenCV.ipynb)\n   - [Getting Acquainted with Deep Learning](notebooks/09.03-Getting-Acquainted-with-Deep-Learning.ipynb)\n   - [Training an MLP in OpenCV to Classify Handwritten Digits](notebooks/09.04-Training-an-MLP-in-OpenCV-to-Classify-Handwritten-Digits.ipynb)\n   - [Training a Deep Neural Net to Classify Handwritten Digits Using Keras](notebooks/09.05-Training-a-Deep-Neural-Net-to-Classify-Handwritten-Digits-Using-Keras.ipynb)\n\n10. [Combining Different Algorithms Into an Ensemble](notebooks/10.00-Combining-Different-Algorithms-Into-an-Ensemble.ipynb)\n    - [Understanding Ensemble Methods](notebooks/10.01-Understanding-Ensemble-Methods.ipynb)\n    - [Combining Decision Trees Into a Random Forest](notebooks/10.02-Combining-Decision-Trees-Into-a-Random-Forest.ipynb)\n    - [Using Random Forests for Face Recognition](notebooks/10.03-Using-Random-Forests-for-Face-Recognition.ipynb)\n    - [Implementing AdaBoost](notebooks/10.04-Implementing-AdaBoost.ipynb)\n    - [Combining Different Models Into a Voting Classifier](notebooks/10.05-Combining-Different-Models-Into-a-Voting-Classifier.ipynb)\n\n11. [Selecting the Right Model with Hyper-Parameter Tuning](notebooks/11.00-Selecting-the-Right-Model-with-Hyper-Parameter-Tuning.ipynb)\n    - [Evaluating a Model](notebooks/11.01-Evaluating-a-Model.ipynb)\n    - [Understanding Cross-Validation, Bootstrapping, and McNemar's Test](notebooks/11.02-Understanding-Cross-Validation-Bootstrapping-and-McNemar's-Test.ipynb)\n    - [Tuning Hyperparameters with Grid Search](notebooks/11.03-Tuning-Hyperparameters-with-Grid-Search.ipynb)\n    - [Chaining Algorithms Together to Form a Pipeline](notebooks/11.04-Chaining-Algorithms-Together-to-Form-a-Pipeline.ipynb)\n\n12. [Wrapping Up](notebooks/12.00-Wrapping-Up.ipynb)\n\n\n\n## Running the Code\n\nThere are at least two ways you can run the code:\n- Using [Binder](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master) (no installation required).\n- Using Jupyter Notebook on your local machine.\n\nThe code in this book was tested with Python 3.5, although Python 3.6 and 2.7 should work as well. \n\n\n### Using Binder\n\n[Binder](http://www.mybinder.org) allows you to run Jupyter notebooks in an interactive Docker container.\nNo installation required!\n\nLaunch the project: [mbeyeler/opencv-machine-learning](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n\n\n\n### Using Jupyter Notebook\n\nYou basically want to follow the installation instructions in Chapter 1 of the book.\n\nIn short:\n\n1. Download and install [Python Anaconda](https://www.continuum.io/downloads).\n   On Unix, when asked if the Anaconda path should be added to your `PATH` variable, choose yes. Then either open a new terminal or run `$ source ~/.bashrc`.\n\n2. Fork and clone the GitHub repo:\n   - Click the\n     [`Fork`](https://github.com/mbeyeler/opencv-machine-learning#fork-destination-box)\n     button in the top-right corner of this page.\n   - Clone the repo, where `YourUsername` is your actual GitHub user name:\n\n   ```\n   $ git clone https://github.com/YourUsername/opencv-machine-learning\n   $ cd opencv-machine-learning\n   ```\n   \n   - Add the following to your remotes:\n   ```\n   $ git remote add upstream https://github.com/mbeyeler/opencv-machine-learning\n   ```\n   \n3. Add Conda-Forge to your trusted channels (to simplify installation of OpenCV on Windows platforms):\n\n   ```\n   $ conda config --add channels conda-forge\n   ```\n\n4. Create a conda environment for Python 3 with all required packages:\n\n   ```\n   $ conda create -n Python3 python=3.6 --file requirements.txt\n   ```\n\n5. Activate the conda environment.\n   On Linux / Mac OS X:\n\n   ```\n   $ source activate Python3\n   ```\n\n   On Windows:\n\n   ```\n   $ activate Python3\n   ```\n\n   You can learn more about conda environments in the\n   [Managing Environments](http://conda.pydata.org/docs/using/envs.html)\n   section of the conda documentation.\n\n6. Launch Jupyter notebook:\n\n   ```\n   $ jupyter notebook\n   ```\n\n   This will open up a browser window in your current directory.\n   Navigate to the folder `opencv-machine-learning`.\n   The README file has a table of contents.\n   Else navigate to the `notebooks` folder, click on the notebook of your choice,\n   and select `Kernel > Restart & Run All` from the top menu.\n   \n   \n## Getting the latest code\n\nIf you followed the instructions above and:\n- forked the repo,\n- cloned the repo,\n- added the `upstream` remote repository,\n\nthen you can always grab the latest changes by running a git pull:\n\n```\n$ cd opencv-machine-learning\n$ git pull upstream master\n```\n\n## Errata\n\nThe following errata have been reported that apply to the print version of the book. Some of these are typos, others are bugs in the code. Please note that all known bugs have been fixed in the code of this repository.\n- p.32: `Out[15]` should read '3' instead of 'int_arr[3]'.\n- p.32: `Out[22]` should read `array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])` instead of `array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])`.\n- p.33: In the sentence: \"Here, the first dimension defines the color channel...\", the order of color channels should read \"blue, green, and red in OpenCV\" instead of \"red, green, blue, green, and red\".\n- p.36: The range of x values should read \"0 <= x <= 10\" instead of \"0 <= x < 10\", since `np.linspace` by default includes the endpoint.\n- p.51: `In [15]` shoud read `precision = true_positive / (true_positive + false_positive)` instead of `precision = true_positive / (true_positive + true_negative)`.\n- p.51: `Out[15]` should read 0.2 instead of 1.0.\n- p.72: `In [6]` should read `ridgereg = linear_model.Ridge()` instead of `ridgereg = linear_model.RidgeRegression()`.\n- p.85: The first line of `In [8]` should read `min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-10,10))` instead of `min_max_scaler = preprocessing.MinMaxScaler(feature_range (-10,10))`.\n- p.91:  The last paragraph should read `We also specify an empty array, np.array([]), for the mean argument, which tells OpenCV to  compute the mean from the data:` instead of `We also specify an empty array, np.array([]), for the mask argument, which tells OpenCV to use all data points in the feature matrix:`.\n- p.112: `In [3]` should read `vec.get_feature_names()[:5]` instead of `function:vec.get_feature_names()[:5]`.\n- p.120: `In [16]` should read `dtree = cv2.ml.DTrees_create()` instead of `dtree = cv2.ml.dtree_create()`.\n- p.122: `In [26]` should read `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(dtc, out_file=f, feature_names=vec.get_feature_names(), class_names=['A', 'B', 'C', 'D'])` instead of `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(clf, out_file=f)`. Also, the second line should be indented.\n- p.147: The first occurrences of `X_hypo = np.c_[xx.ravel().astype(np.float32), yy.ravel().astype(np.float32)]` and `_, zz = svm.predict(X_hypo)` should be removed, as they mistakenly appear twice.\n- p.193: `In [28]` is missing `from sklearn import metrics`.\n- p.197: The sentence right below `In [3]` should read \"Then we can pass the preceding data matrix (`X`) to `cv2.kmeans`\", not `cv2.means`.\n- p.201: Indentation in bullet points 2-4 are wrong. Please refer to the Jupyter notebook for the correct indentation.\n- p.228: The last sentence in the middle paragraph should read \"[...] thus hopefully classifying the sample as y_{hat}=+1\" instead of \"[...] thus hopefully classifying the sample as y_{hat}=-1\".\n- p.230: `In [2]` has wrong indentation: `class Perceptron(object)` correctly has indentation level 1, but `def __init__` should have indentation level 2, and the two commands `self.lr = lr; self.n_iter = n_iter` should have indentation level 3.\n- p.260: `In [5]` should read `from keras.models import Sequential` instead of `from keras.model import Sequential`.\n- p.260: `In [6]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1]), padding='valid', input_shape=input_shape))` instead of `model.add(Convolution2D(n_filters, kernel_size[0], kernel_size[1], border_mode='valid', input_shape=input_shape))`.\n- p.260: `In [8]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1])))` instead of `model.add(Convolution2D(n_filters, (kernel_size[0], kernel_size[1])))`.\n- p.261: `In [12]` should read `model.fit(X_train, Y_train, batch_size=128, epochs=12, verbose=1, validation_data=(X_test, Y_test))` instead of `model.fit(X_train, Y_train, batch_size=128, nb_epoch=12, verbose=1, validation_data=(X_test, Y_test))`.\n- p.275, in bullet point 2 it should say `ret = classifier.predict(X_hypo)` instead of `zz = classifier.predict(X_hypo); zz = zz.reshape(xx.shape)`.\n- p.285: `plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')` should be indented so that it is aligned with the previous line.\n- p.288: `In [14]` should read `_, y_hat = rtree.predict(X_test)` instead of `_, y_hat = tree.predict(X_test)`.\n- p.305: The first paragraph should read \"...and the remaining folds (1, 2, and 4) for training\" instead of \"...and the remaining folds (1, 2, and 4) for testing\".\n- p.306: `In [2]` should read `from sklearn.model_selection import train_test_split` instead of `from sklearn.model_selection import model_selection`.\n- p.310: `In [18]` should read `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_train, cv2.ml.ROW_SAMPLE, y_boot)`.\n- p.311: `In [20]` should have a line `model.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)`, as well as `_, y_hat = model.predict(X_oob)` instead of `_, y_hat = knn.predict(X_oob)`.\n- p.328: `In [5]` is missing the statement `from sklearn.preprocessing import MinMaxScaler`.\n- p.328: `In [5]` should have a line `pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])` instead of `pipe = Pipeline([\"scaler\", MinMaxScaler(), (\"svm\", SVC())])`.\n\n\n## Acknowledgment\n\nThis book was inspired in many ways by the following authors and their corresponding publications:\n- Jake VanderPlas, Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly, ISBN 978-149191205-8, 2016, https://github.com/jakevdp/PythonDataScienceHandbook\n- Andreas Muller and Sarah Guido, Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly, ISBN\n978-144936941-5, 2016, https://github.com/amueller/introduction_to_ml_with_python\n- Sebastian Raschka, Python Machine Learning. Packt, ISBN 978-178355513-0, 2015, https://github.com/rasbt/python-machine-learning-book\n\nThese books all come with their own open-source code - check them out when you get a chance!\n"
 },
 {
  "repo": "kylemcdonald/ofxCv",
  "language": "C++",
  "readme_contents": "# Introduction\n\nofxCv represents an alternative approach to wrapping OpenCV for openFrameworks.\n\n# Installation\n\nFirst, pick the branch that matches your version of openFrameworks:\n\n* OF [stable](https://github.com/openframeworks/openFrameworks/tree/stable) (0.9.8): use [ofxCv/stable](https://github.com/kylemcdonald/ofxCv/tree/stable)\n* OF [master](https://github.com/openframeworks/openFrameworks) (0.10.0): use [ofxCv/master](https://github.com/kylemcdonald/ofxCv/)\n\nEither clone out the source code using git:\n\n\t> cd openFrameworks/addons/\n\t> git clone https://github.com/kylemcdonald/ofxCv.git\n\nOr download the source from GitHub [here](https://github.com/kylemcdonald/ofxCv/archive/master.zip), unzip the folder, rename it from `ofxCv-master` to `ofxCv` and place it in your `openFrameworks/addons` folder.\n\nTo run the examples, import them into the project generator, create a new project, and open the project file in your IDE.\n\n# Goals\n\nofxCv has a few goals driving its development.\n\n### Wrap complex things in a helpful way\n\nSometimes this means: providing wrapper functions that require fewer arguments than the real CV functions, providing a smart interface that handles dynamic memory allocation to make things faster for you, or providing in place and out of place alternatives.\n\n### Present the power of OpenCv clearly\n\nThis means naming things in an intuitive way, and, more importantly, providing classes that have methods that transform the data represented by that class. It also means providing demos of CV functions, and generally being more useful than ofxOpenCv.\n\n### Interoperability of openFrameworks and OpenCv\n\nMaking it easy to work directly with CV by providing lightweight conversion functions, and providing wrappers for CV functions that do the conversions for you.\n\n### Elegant internal OpenCv code\n\nProvide clean implementations of all functions in order to provide a stepping stone to direct OpenCV use. This means using function names and variable names that follow the OpenCV documentation, and spending the time to learn proper CV usage so I can explain it clearly to others through code. Sometimes there will be heavy templating in order to make OF interoperable with OpenCV, but this should be avoided in favor of using straight OpenCV as often as possible.\n\n# Usage\n\nSometimes this readme will fall out of date. Please refer to the examples as the primary reference in that case.\n\n## Project setup\n\nUsing ofxCv requires:\n\n* ofxCv/libs/ofxCv/include/ Which contains all the ofxCv headers.\n* ofxCv/libs/ofxCv/src/ Which contains all the ofxCv source.\n* ofxCv/src/ Which ties together all of ofxCv into a single include.\n* opencv/include/ The OpenCv headers, located in addons/ofxOpenCv/\n* opencv/lib/ The precompiled static OpenCv libraries, located in addons/ofxOpenCv/\n\nYour linker will also need to know where the OpenCv headers are. In XCode this means modifying one line in Project.xconfig:\n\n\tHEADER_SEARCH_PATHS = $(OF_CORE_HEADERS) \"../../../addons/ofxOpenCv/libs/opencv/include/\" \"../../../addons/ofxCv/libs/ofxCv/include/\"\n\nAlternatively, I recommend using [OFXCodeMenu](https://github.com/openframeworks/OFXcodeMenu) to add ofxCv to your project.\n\n## Including ofxCv\n\nInside your ofApp.h you will need one include:\n\n\t#include \"ofxCv.h\"\n\nOpenCv uses the `cv` namespace, and ofxCv uses the `ofxCv` namespace. You can automatically import them by writing this in your `.cpp` files:\n\n\tusing namespace cv;\n\tusing namespace ofxCv;\n\nIf you look inside the ofxCv source, you'll find lots of cases of `ofxCv::` and `cv::`. In some rare cases, you'll need to write `cv::` in your code. For example, on OSX `Rect` and `Point` are defined by OpenCv, but also `MacTypes.h`. So if you're using an OpenCv `Rect` or `Point` you'll need to say so explicitly with `cv::Rect` or `cv::Point` to disambiguate.\n\nofxCv takes advantage of namespaces by using overloaded function names. This means that the ofxCv wrapper for `cv::Canny()` is also called `ofxCv::Canny()`. If you write simply `Canny()`, the correct function will be chosen based on the arguments you pass.\n\n## Working with ofxCv\n\nUnlike ofxOpenCv, ofxCv encourages you to use either native openFrameworks types or native OpenCv types, rather than introducing a third type like `ofxCvImage`. To work with OF and OpenCv types in a fluid way, ofxCv includes the `toCv()` and `toOf()` functions. They provide the ability to convert openFrameworks data to OpenCv data and vice versa. For large data, like images, this is done by wrapping the data rather than copying it. For small data, like vectors, this is done by copying the data.\n\nThe rest of ofxCv is mostly helper functions (for example, `threshold()`) and wrapper classes (for example, `Calibration`).\n\n### toCv() and copy()\n\n`toCv()` is used to convert openFrameworks data to OpenCv data. For example:\n\n\tofImage img;\n\timg.load(\"image.png\");\n\tMat imgMat = toCv(img);\n\nThis creates a wrapper for `img` called `imgMat`. To create a deep copy, use `clone()`:\n\n\tMat imgMatClone = toCv(img).clone();\n\nOr `copy()`, which works with any type supported by `toCv()`:\n\n\tMat imgCopy;\n\tcopy(img, imgCopy);\n\n`toCv()` is similar to ofxOpenCv's `ofxCvImage::getCvImage()` method, which returns an `IplImage*`. The biggest difference is that you can't always use `toCv()` \"in place\" when calling OpenCv code directly. In other words, you can always write this:\n\n\tMat imgMat = toCv(img);\n\tcv::someFunction(imgMat, ...);\n\nBut you should avoid using `toCv()` like this:\n\n\tcv::someFunction(toCv(img), ...);\n\nBecause there are cases where in place usage will cause a compile error. More specifically, calling `toCv()` in place will fail if the function requires a non-const reference for that parameter.\n\n### imitate()\n\n`imitate()` is primarily used internally by ofxCv. When doing CV, you regularly want to allocate multiple buffers of similar dimensions and channels. `imitate()` follows a kind of prototype pattern, where you pass a prototype image `original` and the image to be allocated `mirror` to `imitate(mirror, original)`. `imitate()` has two big advantages:\n\n* It works with `Mat`, `ofImage`, `ofPixels`, `ofVideoGrabber`, and anything else that extends `ofBaseHasPixels`.\n* It will only reallocate memory if necessary. This means it can be used liberally.\n\nIf you are writing a function that returns data, the ofxCv style is to call `imitate()` on the data to be returned from inside the function, allocating it as necessary.\n\n### drawMat() vs. toOf()\n\nSometimes you want to draw a `Mat` to the screen directly, as quickly and easily as possible, and `drawMat()` will do this for you. `drawMat()` is not the most optimal way of drawing images to the screen, because it creates a texture every time it draws. If you want to draw things efficiently, you should allocate a texture using `ofImage img;` *once* and draw it using `img.draw()`.\n\n1. Either use `Mat mat = toCv(img);` to treat the `ofImage` as a `Mat`, modify the `mat`, then `img.update()` to upload the modified pixels to the GPU.\n2. Alternatively; call `toOf(mat, img)` each time after modifying the `Mat`. This will only reallocate the texture if necessary, e.g. when the size has changed.\n\n\n# Working with OpenCv 2\n\nOpenCv 2 is an incredibly well designed API, and ofxCv encourages you to use it directly. Here are some hints on using OpenCv.\n\n### OpenCv Types\n\nOpenCv 2 uses the `Mat` class in place of the old `IplImage`. Memory allocation, copying, and deallocation are all handled automatically. `operator=` is a shallow, reference-counted copy. A `Mat` contains a collection of `Scalar` objects. A `Scalar` contains a collection of basic types (unsigned char, bool, double, etc.). `Scalar` is a short vector for representing color or other multidimensional information. The hierarchy is: `Mat` contains `Scalar`, `Scalar` contains basic types.\n\nDifferent functions accept `Mat` in different ways:\n\n* `Mat` will create a lightweight copy of the underlying data. It's easy to write, and it allows you to use `toCv()` \"in-place\" when passing arguments to the function.\n* `Mat&` allows the function to modify the header passed in. This means the function can allocate if necessary.\n* `const Mat&` means that the function isn't going to modify the underlying data. This should be used instead of `Mat` when possible. It also allows \"in-place\" `toCv()` usage.\n\n### Mat creation\n\nIf you're working with `Mat` directly, it's important to remember that OpenCv talks about `rows` and `cols` rather than `width` and `height`. This means that the arguments are \"backwards\" when they appear in the `Mat` constructor. Here's an example of creating a `Mat` wrapper for some grayscale `unsigned char* pixels` for which we know the `width` and `height`:\n\n\tMat mat = Mat(height, width, CV_8UC1, pixels, 0);\n\n### Mat operations\n\nBasic mathematical operations on `Mat` objects of the same size and type can be accomplished with matrix expressions. Matrix expressions are a collection of overloaded operators that accept `Mat`, `Scalar`, and basic types. A normal mathematical operation might look like:\n\n\tfloat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nA matrix operation looks similar:\n\n\tMat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nThis will add every element of `a` and `b`, then multiply the results by 10, and finally assign the result to `x`.\n\nAvailable matrix expressions include mathematical operators `+`, `-`, `/` (per element division), `*` (matrix multiplication), `.mul()` (per-element multiplication). As well as comparison operators `!=`, `==`, `<`, `>`, `>=`, `<=` (useful for thresholding). Binary operators `&`, `|`, `^`, `~`. And a few others like `abs()`, `min()`, and `max()`. For the complete listing see the OpenCv documention or `mat.hpp`.\n\n# Code Style\n\nofxCv tries to have a consistent code style. It's most similar to the K&R variant used for Java, and the indentation is primarily determined by XCode's auto-indent feature.\n\nMultiline comments are used for anything beyond two lines.\n\nCase statements have a `default:` fall-through with the last case.\n\nWhen two or three similar variables are initialized, commas are used instead of multiple lines. For example `Mat srcMat = toCv(src), dstMat = toCv(dst);`. This style was inherited from reading Jason Saragih's FaceTracker.\n\n- - --\n\n*ofxCv was developed with support from [Yamaguchi Center for Arts and Media](http://ycam.jp/).*\n"
 },
 {
  "repo": "yinguobing/head-pose-estimation",
  "language": "Python",
  "readme_contents": "# Head pose estimation\n\nThis repo shows how to estimate human head pose from videos using TensorFlow and OpenCV.\n\n![demo](doc/demo.gif)\n![demo](doc/demo1.gif)\n\n## Getting Started\n\n:information_source:  *Checkout branch `tf2` if you are using TensorFlow 2*\n\nThe following packages are required:\n\n- TensorFlow 1.14.\n- OpenCV 3.3 or higher.\n- Python 3.5\n\nThe code is tested on Ubuntu 16.04.\n\n## Installing\n\nThis repository comes with a pre-trained model for facial landmark detection. Just git clone then you are good to go.\n\n```bash\n# From your favorite development directory:\ngit clone https://github.com/yinguobing/head-pose-estimation.git\n```\n\n## Running\n\nA video file or a webcam index should be assigned through arguments. If no source provided, the default webcam will be used.\n\n### For video file\n\nFor any video format that OpenCV supported (`mp4`, `avi` etc.):\n\n```bash\npython3 estimate_head_pose.py --video /path/to/video.mp4\n```\n\n### For webcam\n\nThe webcam index should be assigned:\n\n```bash\npython3 estimate_head_pose.py --cam 0\n``` \n\n## How it works\n\nThere are three major steps:\n\n1. Face detection. A face detector is adopted to provide a face box containing a human face. Then the face box is expanded and transformed to a square to suit the needs of later steps.\n\n2. Facial landmark detection. A custom trained facial landmark detector based on TensorFlow is responsible for output 68 facial landmarks.\n\n3. Pose estimation. Once we got the 68 facial landmarks, a mutual PnP algorithms is adopted to calculate the pose.\n\nThe marks is detected frame by frame, which result in small variance between adjacent frames. This makes the pose unstable. A Kalman filter is used to solve this problem, you can draw the original pose to observe the difference.\n\n## Retrain the model\n\nTo reproduce the facial landmark detection model, you can refer to this [series](https://yinguobing.com/deeplearning/) of posts(in Chinese only). And the training code is also open sourced: https://github.com/yinguobing/cnn-facial-landmark\n\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Authors\nYin Guobing (\u5c39\u56fd\u51b0) - [yinguobing](https://yinguobing.com)\n\n![](doc/wechat_logo.png)\n\n## Acknowledgments\nThe pre-trained TensorFlow model file is trained with various public data sets which have their own licenses. Please refer to them before using this code.\n\n- 300-W: https://ibug.doc.ic.ac.uk/resources/300-W/\n- 300-VW: https://ibug.doc.ic.ac.uk/resources/300-VW/\n- LFPW: https://neerajkumar.org/databases/lfpw/\n- HELEN: http://www.ifp.illinois.edu/~vuongle2/helen/\n- AFW: https://www.ics.uci.edu/~xzhu/face/\n- IBUG: https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/\n\nThe 3D model of face comes from OpenFace, you can find the original file [here](https://github.com/TadasBaltrusaitis/OpenFace/blob/master/lib/local/LandmarkDetector/model/pdms/In-the-wild_aligned_PDM_68.txt).\n\nThe build in face detector comes from OpenCV. \nhttps://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n"
 },
 {
  "repo": "Mjrovai/OpenCV-Face-Recognition",
  "language": "Python",
  "readme_contents": "# OpenCV-Face-Recognition\nReal-time face recognition project with OpenCV and Python\n<br><br>\nLinks for complete Tutorial:\n<br>\nhttps://www.hackster.io/mjrobot/real-time-face-recognition-an-end-to-end-project-a10826\nhttps://www.instructables.com/id/Real-time-Face-Recognition-an-End-to-end-Project/\n<br>\n<p><img src=\"https://github.com/Mjrovai/OpenCV-Face-Recognition/blob/master/FaceRecogBlock.png?raw=true\"></p>\n"
 },
 {
  "repo": "nomacs/nomacs",
  "language": "C++",
  "readme_contents": "# nomacs - Image Lounge \ud83c\udf78\n\nnomacs is a free, open source image viewer, which supports multiple platforms. You can use it for viewing all common image formats including RAW and psd images. nomacs is licensed under the GNU General Public License v3 and available for Windows, Linux, FreeBSD, Mac, and OS/2.\n\n[![Build Status](https://travis-ci.org/nomacs/nomacs.svg?branch=master)](https://travis-ci.org/nomacs/nomacs)\n[![Build status](https://ci.appveyor.com/api/projects/status/0lw27jchw3ymaqd4?svg=true)](https://ci.appveyor.com/project/diemmarkus/nomacs)\n[![Downloads](https://img.shields.io/github/downloads/nomacs/nomacs/total.svg)](https://github.com/nomacs/nomacs/releases/latest)\n[![Crowdin](https://badges.crowdin.net/nomacs/localized.svg)](http://translate.nomacs.org/project/nomacs)\n\n## Build nomacs (Windows)\n\nWe assume you have an IDE (i.e. Visual Studio), python, git, and [Qt](https://www.qt.io/download-open-source) installed.  \n\nGet all dependencies:\n```bash\ngit submodule init\ngit submodule update\n```\nProject folders in ``3rd-party`` will not be empty anymore. Now call:\n```bash\npython scripts/make.py \"qtpath/bin\"\n```\n\nThis will build nomacs into `build/nomacs`. If you are using Visual Studio, you can then double-click `build/nomacs/nomacs.sln`. Right-click the nomacs project and choose `Set as StartUp Project`.\n\nBuild individual projects using:\n```bash\npython scripts/make.py \"qt/bin\" --project quazip,libraw --force\n```\n\n### Developer Build\nI like having a separate developer build (without submodules) that uses 3rd party libs already compiled. To do so you need to: \n```bash\ngit submodule update --init --remote scripts \n\n# python scripts/make.py \"C:\\Qt\\Qt-5.14.1-installer\\5.14.2\\msvc2017_64\\bin\" --lib-path C:\\coding\\nomacs\\nomacs\\3rd-party\\build\npython scripts/make.py \"qt/bin\" --lib-path \"nomacs/3rd-party/build\"\n```\n\n### If anything did not work\n\n- check if you have setup opencv (otherwise uncheck ENABLE_OPENCV)\n- check if your Qt is set correctly (otherwise set the path to `qt_install_dir/qtbase/bin/qmake.exe`)\n- check if your builds proceeded correctly\n\n## Build nomacs (Ubuntu)\n\nGet the required packages:\n\n``` console\nsudo apt-get install debhelper cdbs qt5-qmake qttools5-dev-tools qt5-default qttools5-dev libqt5svg5-dev qt5-image-formats-plugins libexiv2-dev libraw-dev libopencv-dev cmake libtiff-dev libquazip5-dev libwebp-dev git build-essential lcov libzip-dev\n```\n\nGet the nomacs sources from github:\n``` console\ngit clone https://github.com/nomacs/nomacs.git\n```\n\nThis will by default place the source into ~/nomacs\nGo to the nomacs/ImageLounge directory and run `cmake` to get the Makefiles:\n``` console\nmkdir build\ncd build\ncmake ../ImageLounge/.\n```\n\nCompile nomacs:\n``` console\nmake\n```\n\nYou will now have a binary (~/nomacs/build/nomacs), which you can test (or use directly). To install it to /usr/local/bin, use:\n``` console\nsudo make install\n```\n\nnote that you have to execute\n``` console\nsudo ldconfig\n```\nafter a successful install.\n\nInstall the [heif plugin](https://github.com/jakar/qt-heif-image-plugin) for HEIF support.\n\n### For Package Maintainers\n\n- Set `ENABLE_TRANSLATIONS` to `true` (default: `false`)\n- Build all officially supported [plugins](https://github.com/nomacs/nomacs-plugins/)\n\n## Build nomacs (MacOS)\n\nInstall [Homebrew](http://brew.sh/) for easier installation of dependencies.\nInstall required dependencies:\n\n``` console\n$ brew install qt5 exiv2 opencv libraw quazip cmake pkg-config\n```\n\nGo to the `nomacs` directory and run cmake to get the Makefiles:\n\n``` console\n$ mkdir build\n$ cd build\n$ Qt5_DIR=/usr/local/opt/qt5/ cmake -DQT_QMAKE_EXECUTABLE=/usr/local/opt/qt5/bin ../ImageLounge/.\n```\n\nRun make:\n\n```console\n$ make\n```\n\nYou will now have a binary (`nomacs.app`), which you can test (or use directly). To install it to `/usr/local/bin`, use\n\n```console\n$ sudo make install\n```\n\n## Build in Docker\nWe have created a docker image that best simulates the travis system (currently it's ubuntu xenial 16.04). To build nomacs in a docker, you have to create the image:\n````bash\ndocker build --rm -f \"Dockerfile\" -t nomacs:latest empty-docker-dir\n`````\nTo deploy nomacs in a docker on your system, you can mount this directory using:\n````bash\ndocker run --rm -it -v C:\\\\coding\\\\nomacs:/usr/nomacs nomacs:latest\n````\nIf needed, you can upload the image:\n````bash\ndocker login\ndocker tag nomacs diemmarkus/nomacs\ndocker push diemmarkus/nomacs:latest\n````\n\n## Links\n\n- [nomacs.org](https://nomacs.org)\n- [GitHub](https://github.com/nomacs)\n\n[![nomacs-icon](https://nomacs.org/startpage/nomacs.svg)](https://nomacs.org)\n"
 },
 {
  "repo": "DingProg/Makeup",
  "language": "Java",
  "readme_contents": "# \u9879\u76ee\u4ecb\u7ecd  \n\n\u672c\u9879\u76ee\u662f\u4e00\u4e2aAndroid Project\uff0c\u7528Canvas\u7ed9\u4eba\u8138\u5316\u5986(\u753b\u5986)\u7684APP\u6f14\u793a\u9879\u76ee  \n\n\u4e3b\u8981\u5185\u5bb9\u5305\u62ec\uff1a\n- \u5507\u5f69\uff0c\u7f8e\u77b3\uff0c\u7c89\u5e95\uff0c\u773c\u5f71\uff0c\u816e\u7ea2\uff0c\u773c\u7ebf\uff0c\u53cc\u773c\u76ae\uff0c\u7709\u6bdb\u7b49\uff0c\u80fd\u753b\u7684\u5986\uff0c\u90fd\u753b\u4e86\n- \u5229\u7528\u56fe\u5f62\u5c40\u90e8\u53d8\u5f62\u7b97\u6cd5\u8fdb\u884c \u5927\u773c\uff0c\u7626\u8138\uff0c\u4e30\u80f8\uff0c\u5927\u957f\u817f\u7b49\n- \u78e8\u5e73/\u7f8e\u767d\n\n# \u90e8\u5206\u6548\u679c\u5c55\u793a\n\u7f8e\u5986  \n![](https://github.com/DingProg/Makeup/blob/master/doc/3.png)\n![](https://github.com/DingProg/Makeup/blob/master/doc/5.png)      \n\u5927\u773c  \n![](https://github.com/DingProg/Makeup/blob/master/doc/1.png)  \n\u7626\u8138  \n![](https://github.com/DingProg/Makeup/blob/master/doc/2.png)  \n\u5927\u957f\u817f  \n![](https://github.com/DingProg/Makeup/blob/master/doc/4.png)   \n\n\n![](https://github.com/DingProg/Makeup/blob/master/doc/smallface.gif)\n\n\u66f4\u591a\u6f14\u793a\u6548\u679c\u8bf7\u76f4\u63a5\u67e5\u770b\u4e0b\u65b9\u539f\u7406\u6587\u7ae0\uff0c\u6216\u8005\u76f4\u63a5\u4e0b\u8f7d [\u6f14\u793aAPP Release V1.0.0\u7248\u672c](https://github.com/DingProg/Makeup/releases)   \n\n\u5982\u679c\u4f60\u8981\u770bOpenCV\u76f8\u5173\u7684(\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d)\uff0c\u53ef\u4ee5\u5207\u6362\u5230\u5206\u652f[with-photo-changecolor](https://github.com/DingProg/Makeup/tree/with-photo-changecolor)   \n\u76f8\u5173\u7684\u6f14\u793aAPP\u4e3a [\u5e26\u66ff\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d\u7248\u672c](https://github.com/DingProg/Makeup/releases)\n\n# \u6f14\u793aAPP \u4e3b\u8981\u5b9e\u73b0\u4e86\u7684\u90e8\u5206\u4e3a\n```java\npublic enum Region {\n\n    FOUNDATION(\"\u7c89\u5e95\"),\n    BLUSH(\"\u816e\u7ea2\"),\n    LIP(\"\u5507\u5f69\"),\n    BROW(\"\u7709\u6bdb\"),\n\n    EYE_LASH(\"\u776b\u6bdb\"),\n    EYE_CONTACT(\"\u7f8e\u77b3\"),\n    EYE_DOUBLE(\"\u53cc\u773c\u76ae\"),\n    EYE_LINE(\"\u773c\u7ebf\"),\n    EYE_SHADOW(\"\u773c\u5f71\");\n\n    private String name;\n    Region(String name) {\n        this.name = name;\n    }\n}\n\npublic enum BeautyType {\n\n    SMALLFACE(2,\"\u7626\u8138\"),\n    LONGLEG(3,\"\u5927\u957f\u817f\u589e\u9ad8\"),\n    EYE(4,\"\u773c\u775b\u653e\u5927\"),\n    BREST(5,\"\u4e30\u80f8\"),\n    WHITE(7,\"\u7f8e\u767d\"),\n    SMALLBODY(9,\"\u7626\u8138\u7626\u8eab\");\n\n    private int type;\n    private String name;\n\n    BeautyType(int type, String name) {\n        this.type = type;\n        this.name = name;\n    }\n}\n```\n\n# \u539f\u7406\n\n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986\uff08\u753b\u5986)](https://github.com/DingProg/Makeup/blob/master/doc/doc1.md)  \n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986 2\uff08\u5927\u773c\uff0c\u7626\u8138\uff0c\u5927\u957f\u817f\uff09](https://github.com/DingProg/Makeup/blob/master/doc/doc2.md)\n\n# \u58f0\u660e  \n\u672c\u9879\u76ee\u662f\u6f14\u793a\u6027\u53ca\u5b66\u4e60\u6027\u9879\u76ee\uff0c\u9879\u76ee\u4e2d\u6240\u7528\u7d20\u6750\u5bf9\u4e8e\u76f4\u63a5\u62ff\u53bb\u5546\u7528\u6240\u9020\u6210\u7684\u4fb5\u6743\uff0c\u6982\u4e0d\u8d1f\u8d23."
 },
 {
  "repo": "bytefish/opencv",
  "language": "C++",
  "readme_contents": "# bytefish/opencv #\n\nThis repository contains OpenCV code and documents.\n\nMore (maybe) here: [https://www.bytefish.de](https://www.bytefish.de).\n\n## colormaps ##\n\nAn implementation of various colormaps for OpenCV2 C++ in order to enhance visualizations. Feel free to fork and add your own colormaps.\n\n### Related posts ###\n\n* https://bytefish.de/blog/colormaps_in_opencv\n  \n## misc ##\n\nSample code that doesn't belong to a specific project. \n\n* Skin Color detection\n* PCA\n* TanTriggs Preprocessing\n\n## machinelearning ##\n\nDocument and sourcecode about OpenCV C++ machine learning API including:\n\n* Support Vector Machines\n* Multi Layer Perceptron\n* Normal Bayes\n* k-Nearest-Neighbor\n* Decision Tree\n\n### Related posts ###\n  \n* https://www.bytefish.de/blog/machine_learning_opencv\n\n## eigenfaces ##\n\nEigenfaces implementation using the OpenCV2 C++ API. There's a very basic function for loading the dataset, you probably want to make this a bit more sophisticated. The dataset is available at [http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html).\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/pca_in_opencv\n* https://www.bytefish.de/blog/eigenfaces\n* https://www.bytefish.de/blog/fisherfaces\n  \n## lbp ##\n\nImplements various Local Binary Patterns with the OpenCV2 C++ API:\n  \n* Original LBP\n* Circular LBP (also known as Extended LBP)\n* Variance-based LBP\n\nBasic code for spatial histograms and histogram matching with a chi-square distance is included, but it's not finished right now. There's a tiny demo application you can experiment with.\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/local_binary_patterns\n* https://www.bytefish.de/blog/numpy_performance/\n  \n## lda ##\n\nFisherfaces implementation with the OpenCV2 C++ API. \n\n### Related posts ###\n\n* https://www.bytefish.de/blog/fisherfaces\n* https://www.bytefish.de/blog/lda_in_opencv\n* https://www.bytefish.de/blog/fisherfaces_in_opencv\n"
 },
 {
  "repo": "nladuo/captcha-break",
  "language": "C++",
  "readme_contents": "# captcha-break\ncaptcha break based on opencv2, tesseract-ocr and some machine learning algorithm.\n\n## Types\n### Basic[[cpp](./basic/cpp)][[python](./basic/python)]\n![](./basic/basic.jpg)  \nThe simplest captcha breaking.\n\n### CSDN[[cpp](./csdn/cpp)][[python](./csdn/python)]\n![](./csdn/csdn.png)  \nCAPTCHA from http://download.csdn.net/\n\n### SubMail[[cpp](./submail/cpp)]\n![](./submail/submail.png)   \nCAPTCHA from http://submail.cn/sms\n\n### Weibo.cn[[cpp](./weibo.cn/cpp)][[python](./weibo.cn/python)]\n![](./weibo.cn/weibo.cn.png)  \nCAPTCHA from http://login.weibo.cn/login/.  \n(Note: This website has changed now, and the captcha is not available!)\n\n### JiKeXueYuan[[python](./jikexueyuan/python)]\n![](./jikexueyuan/jikexueyuan.png)   \nCAPTCHA of http://passport.jikexueyuan.com/sso/verify\n\n### Weibo.com[[python3](./weibo.com)]\n![](./weibo.com/weibo.com.png)  \nCAPTCHA of [http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0](http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0)\n\n\n## License\nMIT\n"
 }
]