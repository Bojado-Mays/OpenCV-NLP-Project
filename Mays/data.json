[
 {
  "repo": "openframeworks/openFrameworks",
  "language": "C++",
  "readme_contents": "[openFrameworks](http://openframeworks.cc/)\n================\n\nopenFrameworks is a C++ toolkit for creative coding.  If you are new to OF, welcome!\n\n[![Slack Status](https://ofslack.herokuapp.com/badge.svg)](https://ofslack.herokuapp.com)\n\n## Build status\n\n* The **master** branch contains the newest, most recently updated code. This code is packaged and available for download in the \"Nightly Builds\" section of [openframeworks.cc/download](https://openframeworks.cc/download/).\n* The **stable** branch contains the code corresponding to the last stable openFrameworks release. This stable code is packaged and available for download at [openframeworks.cc/download](https://openframeworks.cc/download/).\n\nPlatform                     | Master branch  | Stable branch\n-----------------------------|:---------|:---------\nWindows MSYS2 32bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/1)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows MSYS2 64bits         | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | N/A\nWindows Visual Studio 32bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/2)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nWindows Visual Studio 64bits | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/master/4)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/master) | [![Build status](https://appveyor-matrix-badges.herokuapp.com/repos/arturoc/openFrameworks/branch/stable/3)](https://ci.appveyor.com/project/arturoc/openFrameworks/branch/stable)\nLinux 64                     | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux 64 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linux64\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv6l                 | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv6l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv6l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nLinux armv7l                 | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Linux armv7l Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"linuxarmv7l\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nEmscripten                   | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Emscripten Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"emscripten\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos                        | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"osx\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nmacos makefiles              | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![macos makefiles Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=OPT=\"makefiles\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\niOS                          | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![iOS Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"ios\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\ntvos                         | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![tvos Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=TARGET=\"tvos\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid Arm7                 | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android Arm7 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileArm7DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\nAndroid X86                  | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=master)](https://travis-ci.org/openframeworks/openFrameworks) | [![Android X86 Build Status](http://badges.herokuapp.com/travis/openframeworks/openFrameworks?env=GRADLE_TARGET=\"compileX86DebugSources\"&label=build&branch=stable)](https://travis-ci.org/openframeworks/openFrameworks)\n\n\n## folder structure\n\nThis release of OF comes with several folders:\n\n* addons\n* apps\n* docs\n* examples\n* export (on some systems)\n* libs\n* other\n* scripts\n* project generator\n\n\n`docs` has some documentation around OF usage, per platform things to consider, etc. You should definitely take a look in there; for example, if you are on OSX, read the osx.md.   `apps` and `examples` are where projects go -- `examples` contains a variety of projects that show you how to use OF, and `apps` is where your own projects will go.  `libs` contains the libraries that OF uses, including the openframeworks core itself.  `addons` are for additional functionality that's not part of the core.  `export` is for DLLs and dylibs that need to be put in each compiled project.  The `scripts` folder has the templates and small scripts for automating OF per platform. `project generator` is a GUI based tool for making new projects - this folder is only there in packaged releases.  \n\nOne idea that's important is that OF releases are designed to be self-contained.  You can put them anywhere on your hard drive, but it's not possible to mix different releases of OF together, so please keep each release (0.8.0, 0.8.1) separate.  Projects may generally work from release to release, but this is not guaranteed.  Because OF is self-contained, there's extensive use of local file paths (ie, ../../../) throughout OF.  It's important to be aware of how directories are structured.  A common error is to take a project and move it so that it's a level below or above where it used to be compared to the root of OF.  This means that links such as ../../../libs will break.  \n\n## Get involved\n\nThe openframeworks forum:\n\n[http://forum.openframeworks.cc/](http://forum.openframeworks.cc/)\n\nis a warm and friendly place.  Please ask or answer a question.  The most important part of this project is that it's a community, more than just a tool, so please join us!  Also, this is free software, and we learn so much about what is hard, what doesn't make sense, what is useful, etc. The most basic questions are acceptable here!  Don't worry, just join the conversation.  Learning in OF is social, it's hard to do it alone, but together we can get far!\n\nOur GitHub site is active:\n\n[https://github.com/openframeworks/openFrameworks](https://github.com/openframeworks/openFrameworks)\n\nif you have bugs or feature requests, consider opening an issue.  If you are a developer and want to help, pull requests are warmly welcome.  Please read the contributing guide for guidelines:\n\n[https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md](https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md\n)\n\nWe also have a developer's mailing list, which is useful for discussing issues around the development and future of OF.\n\n## Developers\n\nTo grab a copy of openFrameworks for your platform, check the [download page](http://openframeworks.cc/download) on the main site.  \n\nIf you are working with the Git repository, the `stable` branch of the OF repository corresponds to the most recent release, with a few important differences:  \n\n1. The release includes a simple openFrameworks project generator.\n2. This GitHub repository contains code and libs for all the platforms, but the releases are done on a per-platform basis.\n3. This GitHub repository has no project files for the different examples. They are generated automatically for each release using a tool in `apps/projectGenerator/`.\n4. There are no external dependencies in this repository, you can download them using the download_libs.sh script for each platform in the particular platform folder inside scripts.\n\nIf you want to work with the openFrameworks GitHub repository, you need to download the external dependencies and you should use the project generator to create project files for all the code in `examples/`.  To generate the project files with the project generator enable the 'Advanced Options' in the settings tab, then use 'Update Multiple' to update the projects for the `examples/` folder path in the repo.\n\nTo set up the project generator submodule within the OF repo, use the command `git submodule init` then `git submodule update` whilst inside the openFrameworks repo.\n\nFor more info on working with the Project Generator, for per-platform readmes, and more information, see the [documentation](docs/table_of_contents.md).\n\n## Versioning\n\nopenFrameworks uses [Semantic Versioning](http://semver.org/), although strict adherence will only come into effect at version 1.0.0.\n"
 },
 {
  "repo": "opencv/opencv_contrib",
  "language": "C++",
  "readme_contents": "## Repository for OpenCV's extra modules\n\nThis repository is intended for the development of so-called \"extra\" modules,\ncontributed functionality. New modules quite often do not have stable API,\nand they are not well-tested. Thus, they shouldn't be released as a part of\nofficial OpenCV distribution, since the library maintains binary compatibility,\nand tries to provide decent performance and stability.\n\nSo, all the new modules should be developed separately, and published in the\n`opencv_contrib` repository at first. Later, when the module matures and gains\npopularity, it is moved to the central OpenCV repository, and the development team\nprovides production-quality support for this module.\n\n### How to build OpenCV with extra modules\n\nYou can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV.\n\nHere is the CMake command for you:\n\n```\n$ cd <opencv_build_directory>\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory>\n$ make -j5\n```\n\nAs the result, OpenCV will be built in the `<opencv_build_directory>` with all\nmodules from `opencv_contrib` repository. If you don't want all of the modules,\nuse CMake's `BUILD_opencv_*` options. Like in this example:\n\n```\n$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory>\n```\n\nIf you also want to build the samples from the \"samples\" folder of each module, also include the \"-DBUILD_EXAMPLES=ON\" option.\n\nIf you prefer using the gui version of cmake (cmake-gui), then, you can add `opencv_contrib` modules within `opencv` core by doing the following:\n\n1. Start cmake-gui.\n\n2. Select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface).\n\n3. Press the `configure` button. You will see all the opencv build parameters in the central interface.\n\n4. Browse the parameters and look for the form called `OPENCV_EXTRA_MODULES_PATH` (use the search form to focus rapidly on it).\n\n5. Complete this `OPENCV_EXTRA_MODULES_PATH` by the proper pathname to the `<opencv_contrib>/modules` value using its browse button.\n\n6. Press the `configure` button followed by the `generate` button (the first time, you will be asked which makefile style to use).\n\n7. Build the `opencv` core with the method you chose (make and make install if you chose Unix makefile at step 6).\n\n8. To run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, \"-lopencv_aruco\" flag will be added.\n\n### Update the repository documentation\n\nIn order to keep a clean overview containing all contributed modules, the following files need to be created/adapted:\n\n1. Update the README.md file under the modules folder. Here, you add your model with a single line description.\n\n2. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also.\n"
 },
 {
  "repo": "oarriaga/face_classification",
  "language": "Python",
  "readme_contents": "# This repository is deprecated for at TF-2.0 rewrite visit:\n# https://github.com/oarriaga/paz\n------------------------------------------------\n# Face classification and detection.\nReal-time face detection and emotion/gender classification using fer2013/IMDB datasets with a keras CNN model and openCV.\n* IMDB gender classification test accuracy: 96%.\n* fer2013 emotion classification test accuracy: 66%.\n\nFor more information please consult the [publication](https://github.com/oarriaga/face_classification/blob/master/report.pdf)\n\n# Emotion/gender examples:\n\n![alt tag](images/demo_results.png)\n\nGuided back-prop\n![alt tag](images/gradcam_results.png)\n\nReal-time demo:\n<div align='center'>\n  <img src='images/color_demo.gif' width='400px'>\n</div>\n\n[B-IT-BOTS](https://mas-group.inf.h-brs.de/?page_id=622) robotics team :)\n![alt tag](images/robocup_team.png)\n\n## Instructions\n\n### Run real-time emotion demo:\n> python3 video_emotion_color_demo.py\n\n### Run real-time guided back-prop demo:\n> python3 image_gradcam_demo.py\n\n### Make inference on single images:\n> python3 image_emotion_gender_demo.py <image_path>\n\ne.g.\n\n> python3 image_emotion_gender_demo.py ../images/test_image.jpg\n\n### Running with Docker\n\nWith a few steps one can get its own face classification and detection running. Follow the commands below:\n\n* ```docker pull ekholabs/face-classifier```\n* ```docker run -d -p 8084:8084 --name=face-classifier ekholabs/face-classifier```\n* ```curl -v -F image=@[path_to_image]  http://localhost:8084/classifyImage > image.png```\n\n### To train previous/new models for emotion classification:\n\n\n* Download the fer2013.tar.gz file from [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xzf fer2013.tar\n\n* Run the train_emotion_classification.py file\n> python3 train_emotion_classifier.py\n\n### To train previous/new models for gender classification:\n\n* Download the imdb_crop.tar file from [here](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) (It's the 7GB button with the tittle Download faces only).\n\n* Move the downloaded file to the datasets directory inside this repository.\n\n* Untar the file:\n> tar -xfv imdb_crop.tar\n\n* Run the train_gender_classification.py file\n> python3 train_gender_classifier.py\n\n"
 },
 {
  "repo": "opencv/opencv",
  "language": "C++",
  "readme_contents": "## OpenCV: Open Source Computer Vision Library\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/master/>\n* Q&A forum: <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib> \n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n"
 },
 {
  "repo": "Ewenwan/MVision",
  "language": "C++",
  "readme_contents": "# MVision\u3000Machine Vision \u673a\u5668\u89c6\u89c9\n[AI\u7b97\u6cd5\u5de5\u7a0b\u5e08\u624b\u518c \u6570\u5b66\u57fa\u7840 \u7edf\u8ba1\u5b66\u4e60 \u6df1\u5ea6\u5b66\u4e60 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5de5\u5177\u4f7f\u7528](http://www.huaxiaozhuan.com/)\n\n[AI \u5b89\u5168\u6570\u636e\u79d1\u5b66\u548c\u7b97\u6cd5 ](https://github.com/Ewenwan/AI-Security-Learning)\n\n[\u6fb3\u5927\u5229\u4e9a\u673a\u5668\u4eba\u89c6\u89c9\u7814\u7a76\u4e2d\u5fc3](https://www.roboticvision.org/)\n\n[NIPS Neural Information Processing Systems](https://papers.nips.cc/)\n\n[icml Proceedings of Machine Learning Research PMLR](http://proceedings.mlr.press/index.html)\n\n[ICDM IEEE International Conference on Data Mining](http://www.cs.uvm.edu/~icdm/)\n\n[Computer Vision and Pattern Recognition arxiv.org \u6700\u65b0\u63d0\u4ea4\u7684\u8bba\u6587](https://arxiv.org/list/cs.CV/recent)\n\n[papercept \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://controls.papercept.net/conferences/scripts/start.pl)\n\n[easychair \u4f1a\u8bae\u8bba\u6587\u6295\u9012](https://easychair.org/my/roles.cgi?welcome=1)\n\n[DBLP \u8ba1\u7b97\u673a\u6838\u5fc3\u6280\u672f\u6587\u732e](https://dblp.uni-trier.de/)\n\n[\u6280\u672f\u5218 \u589e\u5f3a\u73b0\u5b9e\u3001\u56fe\u50cf\u8bc6\u522b\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u4eba](http://liuxiao.org/category/robots/)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0a\uff09](https://cloud.tencent.com/developer/article/1005894)\n\n[\u6f2b\u8c08 SLAM \u6280\u672f\uff08\u4e0b\uff09](https://cloud.tencent.com/developer/article/1005893)\n\n[\u4f18\u79c0\u7684\u535a\u5ba2\u8bba\u6587\u7b14\u8bb0](https://github.com/Ewenwan/antkillerfarm.github.com)\n\n[CSCI 1430: Introduction to Computer Vision \u8ba1\u7b97\u673a\u89c6\u89c9\u8bfe\u7a0b](http://cs.brown.edu/courses/csci1430/#schedule)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u7b97\u6cd5 \u4e66\u7c4d](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf)\n\n[Computer vision:models, learning and inference \u4e66\u7c4d](http://web4.cs.ucl.ac.uk/staff/s.prince/book/book.pdf)\n\n[TLD\uff1atracking-learning-detection \u8ddf\u8e2a\u7b97\u6cd5](https://github.com/Ewenwan/opencv_TLD)\n\n[\u673a\u5668\u4eba\u9009\u4fee\u8bfe](http://www.diag.uniroma1.it/%7Elanari/EIR/)\n\n[Andrew Davison\u7684\u8bfe\u7a0b\uff1a Robotics Lecture Course (course code 333)](http://www.doc.ic.ac.uk/~ajd/Robotics/index.html)\n\n[Simultaneous Localization and Mapping: Part I ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial1.pdf)\n\n[Simultaneous Localization and Mapping: Part II ](http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial2.pdf)\n\n[\u745e\u58eb\u82cf\u9ece\u4e16\u7406\u5de5\u7684\u5b66\u751f\u7ec3\u4e60](http://www.csc.kth.se/~kootstra/index.php?item=313&menu=300)\n\n[\u4e66\u7c4d Robotics, Vision & Control \u63a8\u8350\uff01\uff01\uff01\uff01](http://petercorke.com/wordpress/)\n\n[Robotics, Vision & Control.PDF \u767e\u5ea6\u7f51\u76d8](https://pan.baidu.com/s/1c1TcEgo)\n\n[Robotics, Vision and Control csdn](https://download.csdn.net/download/u013834525/10169878)\n\n[\u4f18\u8fbe\u5b66\u57ce \u673a\u5668\u4eba\u4eba\u5de5\u667a\u80fd\u8bfe\u7a0b](https://classroom.udacity.com/courses/cs373)\n\n[\u5b66\u4e60\u65e0\u4eba\u9a7e\u9a76\u8f66\uff0c\u4f60\u6240\u5fc5\u987b\u77e5\u9053\u7684](https://zhuanlan.zhihu.com/p/27686577)\n\n[\u5f3a\u5316\u5b66\u4e60\u4ece\u5165\u95e8\u5230\u653e\u5f03\u7684\u8d44\u6599](https://zhuanlan.zhihu.com/p/34918639?utm_source=wechat_session&utm_medium=social&wechatShare=1&from=singlemessage&isappinstalled=0)\n\n[\u53f0\u5927 \u673a\u5668\u5b66\u4e60\u6df1\u5ea6\u5b66\u4e60\u8bfe\u7a0b](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)\n\n[\u65af\u5766\u798fCS231\u8ba1\u7b97\u673a\u89c6\u89c92017](http://www.mooc.ai/course/268/learn?lessonid=1819#lesson/1819)\n\n[\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u793a\u4f8b cs231n\u7b49](https://github.com/autoliuweijie/DeepLearning)\n\n[2018 MIT 6.S094 \u9ebb\u7701\u7406\u5de5\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u4e2d\u6587](http://www.mooc.ai/course/483/notes)\n\n[MIT  \u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u8bfe\u7a0b \u82f1\u6587](https://selfdrivingcars.mit.edu/)\n\n[DeepTraffic](https://selfdrivingcars.mit.edu/deeptraffic/)\n\n[SegFuse: Dynamic Driving Scene Segmentation](https://selfdrivingcars.mit.edu/segfuse/)\n\n[DeepTesla - End-to-End Steering Model](https://selfdrivingcars.mit.edu/deeptesla/)\n\n[\u4e2d\u6587slam\u9996\u9875](http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5)\n\n[ORB-LSD-SVO\u6bd4\u8f83-\u5218\u6d69\u654f_bilibili](https://www.bilibili.com/video/av5934066/)\n\n[LSD\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7136033.html)\n\n[SVO\u7b97\u6cd5\u4ee3\u7801\u89e3\u6790](http://www.cnblogs.com/shhu1993/p/7135847.html)\n\n[DSO \u534a\u95f2\u5c45\u58eb \u89e3\u6790](https://zhuanlan.zhihu.com/p/29177540)\n\n[\u8def\u5f84\u89c4\u5212A*\u7b97\u6cd5\u53caSLAM\u81ea\u4e3b\u5730\u56fe\u521b\u5efa\u5bfc\u822a\u7b97\u6cd5](http://www.voidcn.com/article/p-yfjpnwte-tz.html)\n\n[\u51af\u5175\u7684blog slam](http://www.fengbing.net)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801\uff08EKF)](https://github.com/ethz-asl/rovio)\n\n[imu\u548c\u5355\u76ee\u7684\u6570\u636e\u878d\u5408\u5f00\u6e90\u4ee3\u7801(\u975e\u7ebf\u6027\u4f18\u5316\uff09](https://github.com/ethz-asl/okvis_ros)\n\n[\u53cc\u76ee\u7acb\u4f53\u5339\u914d](https://wenku.baidu.com/view/08f86102e518964bcf847c6c.html)\n\n[\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e00\u4e9b\u5e93\u6587\u4ef6](https://blog.csdn.net/garfielder007/article/details/50533052)\n\n[\u4eba\u8138\u68c0\u6d4b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/52983093)\n\n[\u884c\u4e3a\u8bc6\u522b\u603b\u7ed3](https://blog.csdn.net/neu_chenguangq/article/details/79504214)\n\n[Free-SpaceEstimation \u65e0\u969c\u788d\u7269\u7a7a\u95f4\u4f30\u8ba1 \u7a20\u5bc6\u5730\u56fe \u6805\u683c\u5730\u56fe \u52a8\u6001\u89c4\u5212 \u9ad8\u5ea6\u5206\u5272 \u8def\u9762\u4fe1\u606f\u63d0\u53d6](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_free_space.pdf)\n\n[2D Object Detection 2d\u76ee\u6807\u68c0\u6d4b RCNN ](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/05_2D_detection.pdf)\n\n[3D Object Detection 3D\u76ee\u6807\u68c0\u6d4b \u52a8\u673a](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf)\n\n[Semantic Segmentation \u8bed\u4e49\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/07_segmentation.pdf)\n\n[Instance-level Segmentation \u5b9e\u4f8b\u5206\u5272](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/08_instance.pdf)\n\n[Tracking \u8ddf\u8e2a ]()\n\n[Kalibr calibration toolbox \u6807\u5b9a\u591a\u76ee\u76f8\u673a\u7cfb\u7edf\u3001\u76f8\u673a IMU \u76f8 \u5bf9 \u4f4d \u59ff \u548c \u5377 \u5e18 \u5feb \u95e8 \u76f8 \u673a  ](https://github.com/Ewenwan/kalibr)\n\n[\u970d\u592b\u68ee\u6797(Hough Forest) \u968f\u673a\u68ee\u6797\u548c\u970d\u592b\u6295\u7968\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u7528\u5728\u7269\u4f53\u68c0\u6d4b\uff0c\u8ddf\u8e2a\u548c\u52a8\u4f5c\u8bc6\u522b](https://github.com/Ewenwan/HoughForest)\n\n[\u767e\u5ea6\u81ea\u52a8\u9a7e\u9a76\u5f00\u6e90\u6846\u67b6 apollo](https://github.com/Ewenwan/apollo)\n\n[\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u8bc6\u522b \u6570\u636e\u96c6](https://cg.cs.tsinghua.edu.cn/traffic-sign/)\n\n[Halcon \u4f7f\u7528\u53c2\u8003](https://blog.csdn.net/maweifei/article/details/52613392)\n\n[\u6709\u4ee3\u7801\u7684\u8bba\u6587](https://github.com/Ewenwan/pwc)\n\n[\u56fe\u50cf\u5904\u7406\u57fa\u672c\u7b97\u6cd5\u4ee3\u7801](http://www.cnblogs.com/Imageshop/p/3430742.html)\n\n# \u611f\u8c22\u652f\u6301\n\n![](https://github.com/Ewenwan/EwenWan/blob/master/zf.jpg)\n\n# \u65e0\u4eba\u9a7e\u9a76\u7684\u5404\u4e2a\u65b9\u9762\u77e5\u8bc6\n[\u53c2\u8003](https://blog.csdn.net/qq_40027052/article/details/78485120)\n\n    1. \u611f\u77e5\uff08Perception\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u573a\u666f\u7406\u89e3\u3001\u4ea4\u901a\u72b6\u51b5\u5206\u6790\u3001\u8def\u9762\u68c0\u6d4b\u3001\u7a7a\u95f4\u68c0\u6d4b\u3001\n        \u969c\u788d\u7269\u68c0\u6d4b\u3001\u884c\u4eba\u68c0\u6d4b\u3001\u8def\u6cbf\u68c0\u6d4b\u3001\u8f66\u9053\u68c0\u6d4b\u3002\u8fd8\u6709\u4e00\u4e2a\u6bd4\u8f83\u65b0\u9896\u6709\u8da3\u7684\u662f\u901a\u8fc7\u80ce\u538b\u53bb\u68c0\u6d4b\u9053\u8def\u8d28\u91cf\u3002\n        \u5728\u65e0\u4eba\u9a7e\u9a76\u884c\u4e1a\uff0c\u6709\u4e00\u5957\u901a\u7528\u7684\u6570\u636e\u96c6\u2014\u2014KITTI\u6570\u636e\u96c6\uff0c\u91cc\u9762\u6709\u4e0d\u540c\u7684\u6570\u636e\uff0c\u5305\u62ec\u53cc\u76ee\u89c6\u89c9\u7684\u6570\u636e\u3001\u5b9a\u4f4d\u5bfc\u822a\u7684\u6570\u636e\u7b49\u3002\n        \u7269\u4f53\u68c0\u6d4b\uff08Object Detection\uff09\uff1a\n            \u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u662f\u9488\u5bf9\u56fa\u5b9a\u7269\u4f53\u7684\u68c0\u6d4b\u3002\u4e00\u822c\u7684\u65b9\u6cd5\u662fHOG\uff08 \u65b9\u5411\u68af\u5ea6\u76f4\u65b9\u56fe\uff09\uff0c\u7136\u540e\u518d\u52a0\u4e00\u4e2aSVM\u7684\u5206\u7c7b\u5668\u3002\n            \u800c\u5bf9\u4e8e\u52a8\u6001\u7269\u4f53\u7684\u68c0\u6d4b\uff0c\u4e3b\u8981\u4f7f\u7528\u7684\u662fDPM\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5148\u628a\u624b\u548c\u811a\u8bc6\u522b\u51fa\u6765\uff0c\u518d\u8fdb\u884c\u7ec4\u5408\u3002\n            \u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 RCNN YOLO\n       \u573a\u666f\u5206\u5272\uff08Segmentation\uff09 \u00a0\uff1a\n            \u4eba\u884c\u9053\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u9053\u8def\u662f\u4e00\u4e2a\u573a\u666f\uff0c\u5728\u573a\u666f\u4e2d\u5bf9\u4e0d\u540c\u7684\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u662f\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u95ee\u9898\u3002\n            \u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u91c7\u7528CRF\uff08 \u6761\u4ef6\u968f\u673a\u573a\uff09\uff0c\u57fa\u672c\u539f\u7406\u5728\u4e8e\u56fe\u50cf\u90fd\u662f\u7531\u50cf\u7d20\u70b9\u7ec4\u6210\u7684\uff0c\n            \u82e5\u4e24\u4e2a\u50cf\u7d20\u70b9\u90fd\u6bd4\u8f83\u50cf\u8f66\uff0c\u90a3\u5c31\u628a\u4e8c\u8005\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u5bf9\u8f66\u8f86\u7684\u8bc6\u522b\u3002\n\n            \u8fd0\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5219\u4f7f\u7528\u7684\u662f\u53e6\u4e00\u79cd\u6a21\u578b\uff0c\u88ab\u79f0\u4e3aPSPnet\uff08\u8bed\u4e49\u5206\u5272\uff09\u3002\n            \u8fd9\u662f\u91d1\u5b57\u5854\u578b\u7684\u573a\u666f\u5206\u89e3\u6a21\u578b\uff0c\u5c06\u4e00\u4e2a\u573a\u666f\u4e0d\u65ad\u5730\u538b\u7f29\uff0c\u628a\u7c7b\u4f3c\u7684\u7269\u4f53\u805a\u7c7b\uff0c\u7136\u540e\u518d\u505a\u5224\u65ad\u3002\n       \u53cc\u76ee \u5149\u6d41\uff08Optical Flow\uff09\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff1a\n            \u5149\u6d41\u662f\u9488\u5bf92D\u56fe\u50cf\u6765\u8bf4\u7684\uff0c\u5982\u679c\u8bf4\u4e00\u4e2a\u56fe\u7247\u6d41\u5230\u53e6\u5916\u4e00\u4e2a\u56fe\u7247\uff0c\u90fd\u662f2D\u7684\u7269\u4f53\u79fb\u52a8\uff0c\u90a3\u5c31\u7528\u5149\u6d41\u6765\u505a\u3002\n            \u5982\u679c\u662f3D\u7684\u7269\u4f53\u6d41\u52a8\uff0c\u90a3\u6211\u4eec\u5c31\u7528\u573a\u666f\u6d41\uff08Scene Flow\uff09\uff0c\u573a\u666f\u6d41\u5728\u4f20\u7edf\u7684\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528\u7684\u662fSGBM\uff0c\n            \u5229\u7528\u7684\u662f\u53cc\u76ee\u6210\u50cf\u7684\u6280\u672f\uff0c\u628a\u5de6\u56fe\u548c\u53f3\u56fe\u5408\u8d77\u6765\u63d0\u53d6\u51fa\u7a7a\u95f4\u7684\u70b9\uff0c\u7528\u5149\u6d41\u5728\u4e0a\u9762\u505a\uff0c\u5c31\u80fd\u628a\u573a\u666f\u7684\u6d41\u52a8\u5206\u6790\u51fa\u6765\u3002\n\n            \u5149\u6d41\u4e5f\u53ef\u4ee5\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u6765\u505a\uff0c\u628a\u5de6\u53f3\u4e24\u56fe\u7528\u540c\u6837\u7684\u6a21\u578b\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u7ecf\u8fc7\u8ba1\u7b97\u5c31\u80fd\u5f97\u51fa\u4e00\u4e2a\u6df1\u5ea6\u7684\u4fe1\u606f\u3002\n            \u4f46\u662f\u8fd9\u4e2a\u65b9\u5f0f\u7684\u8ba1\u7b97\u91cf\u975e\u5e38\u5927\u3002\n\n       \u7269\u4f53\u8ffd\u8e2a\uff08Object Tracking\uff09\uff1a \u00a0 \u00a0\n            \u8fd9\u4e5f\u662f\u65e0\u4eba\u9a7e\u9a76\u4e2d\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u6280\u672f\u3002\u5982\u4f55\u9884\u6d4b\u884c\u4eba\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3001\u600e\u4e48\u53bb\u8ddf\u8e2a\u8fd9\u4e2a\u884c\u4eba\uff0c\u4e5f\u6709\u4e00\u7cfb\u5217\u95ee\u9898\u3002\n            \u91cc\u9762\u7528\u5230\u7684\u662f\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e2a\u6280\u672f\u53eb\u505aMDP\uff0c\u8ddf\u8e2a\u4e00\u4e2a\u4eba\uff0c\u968f\u65f6\u8ddf\u8e2a\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u9884\u6d4b\u5176\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3002\n            \u4ee5\u4e0a\u5176\u5b9e\u90fd\u662f\u4e00\u4e9b\u4f20\u7edf\u7684\u611f\u77e5\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u5e74\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u5e94\u7528\u4e5f\u975e\u5e38\u5e7f\u6cdb\u3002\n            \n    2. \u8fd0\u52a8\u89c4\u5212\uff08Motion Planning\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u7684\u6280\u672f\u70b9\u5305\u62ec\u8fd0\u52a8\u89c4\u5212\u3001\u8f68\u8ff9\u89c4\u5212\u3001\u901f\u5ea6\u89c4\u5212\u3001\u8fd0\u52a8\u6a21\u578b\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u901a\u8fc7\u8d5b\u8f66\u6e38\u620f\u53bb\u5b66\u4e60\u57fa\u4e8e\u7f51\u683c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u91cd\u91cf\u7ea7\u8d27\u8f66\u7684\u907f\u969c\u89c4\u5212\uff0c\u666e\u4e16\u7684\u9002\u7528\u4e8e\u65e0\u4eba\u9a7e\u9a76\u7684\u53cc\u8f6e\u6a21\u578b\u7b49\u7b49\u3002\n\n    3. \u9632\u78b0\u649e\uff08CollisionAvoidance\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u8f66\u5185\u7684\u611f\u77e5\u7cfb\u7edf\u4ee5\u53caV2X \u7cfb\u7edf\u53bb\u8f85\u52a9\u9632\u78b0\u649e\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5b9e\u65f6\u5730\u53bb\u8bc4\u4f30\u5f53\u524d\u9a7e\u9a76\u884c\u4e3a\u7684\u5371\u9669\u6027\uff0c\u5982\u4f55\u901a\u8fc7\u5f53\u524d\u9053\u8def\u7684\u62d3\u6251\u53bb\u589e\u5f3a\u81ea\u884c\u8f66\u9a91\u58eb\u7684\u5b89\u5168\u6027\u7b49\u7b49\u3002\n\n    4. \u5730\u56fe\u4e0e\u5b9a\u4f4d\uff08Mapping andLocalization\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7684\u4f20\u611f\u5668\uff0c\u5305\u62ec\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\u3001GNSS\uff0c\u4ee5\u53ca V2X \u53bb\u5efa\u56fe\u4e0e\u5b9a\u4f4d\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u8fdb\u5c55\u5305\u62ec\u5982\u4f55\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u53bb\u5b9a\u4f4d\uff0c\u6bd4\u5982\u5728\u957f\u96a7\u9053\u91cc\u9762\uff0c\u65e2\u6ca1\u6709 GNSS \u4fe1\u53f7\uff0c\u4e5f\u6ca1\u6709\u592a\u597d\u7684\u6fc0\u5149\u6216\u8005\u89c6\u89c9\u7279\u5f81\u7684\u65f6\u5019\u5982\u4f55\u5b9a\u4f4d\u3002\n\n    5. \u5408\u4f5c\u7cfb\u7edf\uff08CooperativeSystems\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u534f\u540c\u591a\u4e2a\u65e0\u4eba\u8f66\u53bb\u5b8c\u6210\u4e00\u4e9b\u4efb\u52a1\uff0c\u6bd4\u5982\u5728\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u51fa\u73b0\u65f6\u5982\u4f55\u8c03\u5ea6\uff0c\n        \u8fd8\u6709\u5c31\u662f\u5f53\u6709\u591a\u4e2a\u65e0\u4eba\u8f66\u540c\u65f6\u5728\u505c\u8f66\u573a\u8bd5\u5982\u4f55\u6709\u5e8f\u7684\u505c\u8f66\u3002\n\n    6. \u63a7\u5236\u7b56\u7565\uff08Control Strategy\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5728\u4e0d\u540c\u7684\u7ec6\u5206\u573a\u666f\u4e0b\u7684\u63a7\u5236\u7b56\u7565\uff0c\u6bd4\u5982\u5728\u5341\u5b57\u8def\u53e3\u5982\u4f55\u63a7\u5236\uff0c\u8f6c\u7ebf\u5982\u4f55\u63a7\u5236\uff0c\u5728\u611f\u77e5\u6570\u636e\u4e0d\u53ef\u9760\u65f6\u5982\u4f55\u5c3d\u91cf\u5b89\u5168\u7684\u63a7\u5236\u7b49\u7b49\u3002\n\n    7. \u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff08VehicleDetection and Tracking\uff09\uff1a\n        \u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u3001\u89c6\u89c9\uff0c\u4ee5\u53ca\u6beb\u7c73\u6ce2\u96f7\u8fbe\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u5de5\u4f5c\u5305\u62ec\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6df1\u5ea6\u89c6\u89c9\u7684\u7ed3\u5408\u8fdb\u884c\u8f66\u8f86\u8ddf\u8e2a\uff0c\n        \u901a\u8fc7\u5355\u76ee\u89c6\u89c9\u6df1\u5ea6\u5b66\u4e60\u53bb\u5c3d\u91cf\u4f30\u8ba1\u8f66\u4f53\u5927\u5c0f\uff0c\u901a\u8fc7\u4f20\u7edf\u89c6\u89c9\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u53bb\u5224\u65ad\u662f\u5426\u8f66\u4f53\u7b49\u7b49\u3002\n\n    8. \u9759\u6001\u7269\u4f53\u68c0\u6d4b\uff08Static ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u4ee5\u53ca\u6fc0\u5149\u96f7\u8fbe\u53bb\u68c0\u6d4b\u4e00\u4e9b\u9759\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u4ea4\u901a\u706f\u3001\u4ea4\u901a\u6307\u793a\u724c\u3001\u8def\u6cbf\u3001\u8def\u9762\u7b49\u7b49\uff0c\u6bcf\u4e2a\u7269\u4f53\u54c1\u7c7b\u7684\u68c0\u6d4b\u90fd\u662f\u4e00\u4e2a\u7ec6\u5206\u65b9\u5411\u3002\n\n    9. \u52a8\u6001\u7269\u4f53\u68c0\u6d4b\uff08Moving ObjectDetection\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u901a\u8fc7\u89c6\u89c9\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u4ee5\u53ca\u4f20\u611f\u5668\u878d\u5408\u7684\u65b9\u6cd5\u53bb\u68c0\u6d4b\u4e00\u4e9b\u52a8\u6001\u7684\u7269\u4f53\uff0c\n        \u5305\u62ec\u884c\u4eba\u3001\u8f66\u8f86\u3001\u81ea\u884c\u8f66\u9a91\u58eb\u7b49\u7b49\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u52a8\u6001\u7269\u4f53\u7684\u52a8\u4f5c\u53bb\u9884\u6d4b\u884c\u4e3a\u3002\n\n    10. \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\uff08Road andIntersection Detection\uff09\uff1a\n        \u9053\u8def\u4e0e\u8def\u53e3\u68c0\u6d4b\u7531\u4e8e\u5176\u7279\u6b8a\u6027\u4ee5\u53ca\u5bf9\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u88ab\u5355\u72ec\u5217\u51fa\u4f5c\u4e3a\u4e00\u4e2a\u7ec6\u5206\u7684\u5c0f\u65b9\u5411\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u4e00\u4e9b\u7ec6\u5206\u573a\u666f\uff0c\u6bd4\u5982\u5efa\u7b51\u5de5\u5730\u7684\u68c0\u6d4b\u3001\u505c\u8f66\u4f4d\u7684\u68c0\u6d4b\u7b49\u7b49\u3002\n\n    11. \u51b3\u7b56\u7cfb\u7edf\uff08Planning andDecision\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u6bcf\u4e2a\u65e0\u4eba\u8f66\u7684\u52a8\u4f5c\u7684\u51b3\u7b56\uff0c\u6bd4\u5982\u52a0\u901f\u3001\u5239\u8f66\u3001\u6362\u7ebf\u3001\u8d85\u8f66\u3001\u8c03\u5934\u7b49\u7b49\u3002\n        \u7814\u7a76\u7684\u524d\u6cbf\u4e00\u822c\u6d89\u53ca\u5728\u9ad8\u901f\u884c\u9a76\u4e2d\u5982\u4f55\u5b89\u5168\u7684\u6362\u7ebf\uff0c\u5728\u901a\u8fc7\u89c6\u89c9\u7406\u89e3\u4e86\u573a\u666f\u540e\u5982\u4f55\u51b3\u7b56\uff0c\u5728\u611f\u77e5\u4fe1\u606f\u7f3a\u5931\u7684\u65f6\u5019\uff08\u6bd4\u5982\u5728\u96a7\u9053\u91cc\u9762\uff09\u5982\u4f55\u51b3\u7b56\u7b49\u7b49\u3002\n\n    12. \u4e3b\u52a8\u4e0e\u88ab\u52a8\u5b89\u5168\uff08Active andPassive Safety\uff09\uff1a\n        \u4e3b\u8981\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u4f20\u611f\u5668\u7684\u611f\u77e5\u53bb\u786e\u4fdd\u65e0\u4eba\u9a7e\u9a76\u4ee5\u53ca\u884c\u4eba\u5b89\u5168\uff0c\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u901a\u8fc7\u5bf9 CAN \u603b\u7ebf\u7684\u5f02\u5e38\u68c0\u6d4b\u53bb\u8bc4\u4f30\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5bf9\u505c\u8f66\u573a\u7684\u89c6\u9891\u76d1\u63a7\u53bb\u8bad\u7ec3\u81ea\u52a8\u6cca\u8f66\u6a21\u578b\u7b49\u7b49\u3002\n\n    13. \u65e0\u4eba\u8f66\u4e0e\u4ea4\u901a\u7684\u4ea4\u4e92\uff08AutonomousVehicles: Interaction with Traffic\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u65e0\u4eba\u8f66\u5982\u4f55\u4e0e\u73b0\u6709\u7684\u4ea4\u901a\u751f\u6001\u5171\u5b58\uff0c\u7279\u522b\u662f\u4f20\u7edf\u8f66\u4e0e\u65e0\u4eba\u8f66\u7684\u5171\u5b58\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec V2X \u865a\u62df\u4ea4\u901a\u6807\u5fd7\uff0c\u901a\u8fc7\u89c6\u89c9\u53bb\u8bc4\u4f30\u65c1\u8fb9\u8f66\u9053\u53f8\u673a\u7684\u9a7e\u9a76\u884c\u4e3a\u7b49\u7b49\u3002\n\n    14. \u89c6\u89c9\u5b9a\u4f4d\uff08SLAM and VisualOdometry\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u7528\u89c6\u89c9\u4e0e\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002\n        \u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u89c6\u89c9\u7684\u7ebf\u4e0a\u6821\u51c6\uff0c\u4f7f\u7528\u8f66\u9053\u7ebf\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7b49\u7b49\u3002\n\n    15. \u73af\u5883\u5b66\u4e60\u4e0e\u5efa\u56fe\uff08Mapping andLearning the Environment\uff09\uff1a\n        \u4e3b\u8981\u7814\u7a76\u5982\u4f55\u5efa\u7acb\u7cbe\u51c6\u7684\u73af\u5883\u4fe1\u606f\u56fe\u3002\u6bd4\u8f83\u6709\u8da3\u7684\u4e00\u4e9b\u7814\u7a76\u5305\u62ec\u4f7f\u7528\u4f4e\u7a7a\u65e0\u4eba\u673a\u53bb\u521b\u5efa\u7ed9\u65e0\u4eba\u9a7e\u9a76\u4f7f\u7528\u7684\u5730\u56fe\uff0c\n        \u4ee5\u53ca\u901a\u8fc7\u505c\u8f66\u573a\u76d1\u63a7\u6444\u50cf\u5934\u5efa\u7acb\u8f85\u52a9\u81ea\u52a8\u6cca\u8f66\u7684\u5730\u56fe\u7b49\u7b49\u3002\n\n## \u65e0\u4eba\u9a7e\u9a76\u9762\u8bd5\u77e5\u8bc6\u70b9\n[\u53c2\u8003\u535a\u5ba2](https://blog.csdn.net/xiangxianghehe/article/details/82528180)\n```\n1. \u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\n    \u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u533a\u522b\uff0c\u5404\u81ea\u9002\u7528\u4e8e\u4ec0\u4e48\u95ee\u9898\n    CNN\u57fa\u672c\u539f\u7406\uff0cCNN\u7684\u90a3\u4e9b\u90e8\u5206\u662f\u795e\u7ecf\u5143\n    CNN\u53bb\u6389\u6fc0\u6d3b\u51fd\u6570\u4f1a\u600e\u4e48\u6837\n    \u4ecb\u7ecdYOLO/SSD/RCNN/Faster-RCNN/Mask-RCNN\u7b97\u6cd5\n    YOLO v1/v2/v3 \u533a\u522b\u7ec6\u8282\uff0cSSD\u5982\u4f55\u6539\u8fdb\u6709\u601d\u8003\u8fc7\u561b\uff0c\u77e5\u9053DSSD\u548cFSSD\u561b\n    \u662f\u5426\u4e86\u89e3RPN\uff0cRoI pooling,\u548cRoIAlign\n    YOLO/SSD\u91cc\u9762\u6709\u5168\u8fde\u63a5\u5c42\u561b\n    YOLO/SSD\u7b97\u6cd5\u601d\u60f3\u5982\u4f55\u7528\u5230\u4e09\u7ef4\u70b9\u4e91\u76ee\u6807\u68c0\u6d4b\n    \u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5one-stage\u548ctwo-stage\u533a\u522b\u70b9\u5728\u54ea\u91cc\n    two-stage\u7b97\u6cd5\u76f8\u6bd4\u4e8eone-stage\u6709\u4f55\u4f18\u52bf\n    \u5355\u5f20\u56fe\u7247\u7269\u4f53\u8d8a\u591a\u8d8a\u5bc6\u96c6\uff0cYOLO/SSD/Faster-RCNN\u4e2d\u8ba1\u7b97\u91cf\u662f\u5426\u4e5f\u968f\u7740\u589e\u52a0\n    CVPR/ECCV 2018 \u6700\u65b0\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u6709\u4e86\u89e3\u8fc7\u561b\n    \u5982\u4f55\u7406\u89e3\u4e0a\u91c7\u6837\uff0c\u548c\u4e0b\u91c7\u6837\u7684\u533a\u522b\u662f\u4ec0\u4e48\n    \u4e0a\u91c7\u6837(UNSampling)\u4e0e\u4e0a\u6c60\u5316(UnPooling)\u533a\u522b\n    \u5168\u8fde\u63a5\u5c42\u7406\u8bba\u4e0a\u53ef\u4ee5\u66ff\u4ee3\u5377\u79ef\u5c42\u561b\n    \u795e\u7ecf\u7f51\u7edc\u91cc\u9762\u53ef\u4ee5\u7528\u4ec0\u4e48\u65b9\u6cd5\u66ff\u6362\u6389pooling\n    \u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\u7684\u65b9\u5f0f\u6709\u54ea\u4e9b\n    \u4ecb\u7ecd\u4e0b\u4f60\u4e86\u89e3\u7684\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\n    \u7f51\u7edc\u6a21\u578b\u538b\u7f29\u65b9\u9762\u7684\u526a\u679d\uff0c\u91cf\u5316\u548c\u4e8c\u503c\u5316\u7f16\u7801\n    \u57fa\u4e8e\u89c6\u9891\u7684C3D\u4e09\u7ef4\u7f51\u7edc\u6a21\u578b\u6709\u542c\u8bf4\u8fc7\u561b\n    2.5D\u5377\u79ef\u5462\n    \u4ec0\u4e48\u662f\u7a7a\u6d1e\u5377\u79ef\uff0c\u4ec0\u4e48\u662f\u53cd\u5377\u79ef\uff0c\u4f5c\u7528\u662f\u4ec0\u4e48\n    \u5982\u4f55\u4e00\u5f20RGB\u56fe\u7247\u751f\u6210\u4e09\u7ef4\u6a21\u578b\n    PNG/JPG\u5b58\u50a8\u56fe\u50cf\u7684\u539f\u7406\n    global average pooling \u548caverage pooling\u533a\u522b\n    FPN\u7684\u539f\u7406\uff0c\u4e3a\u4ec0\u4e48\u4e0d\u540c\u5c3a\u5ea6feature map\u878d\u5408\u4f1a\u6709\u6548\u679c\u63d0\u5347\n    \u65e0\u76d1\u7763/\u534a\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6709\u4e86\u89e3\u8fc7\u561b\n    GAN\u7684\u539f\u7406\n    \u57fa\u4e8eRGB\u56fe\u7684\u6df1\u5ea6\u4fe1\u606f\u4f30\u8ba1\u6709\u4e86\u89e3\u8fc7\u561b\n    MobileNet V1/V2\u533a\u522b\n    ShuffleNet\u548cSqueezeNet\n    \u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u6709\u54ea\u4e9b\n    \u53cc\u7ebf\u6027\u63d2\u503c\uff0c\u91cf\u5316\u5bf9\u9f50\n    Relu\u4e3a\u4ec0\u4e48\u6bd4sigmod\u597d\n    \u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u5e38\u7528\u8bc4\u6d4b\u65b9\u5f0f\n    IOU\u548cmAP\uff0cAUC\u548cROC\u5206\u522b\u662f\u4ec0\u4e48\n    \u4ecb\u7ecd\u4e0b\u5e38\u89c1\u635f\u5931\u51fd\u6570\uff0csoftmax\u4e00\u822c\u548c\u54ea\u4e2a\u6fc0\u6d3b\u51fd\u6570\u4f7f\u7528\n    \u4ecb\u7ecd\u4e0bPointNet/PointNet++/VoxelNet\u4ee5\u53ca\u4ed6\u4eec\u7684\u4f18\u7f3a\u70b9\n    PointCNN\u4ecb\u7ecd\u4e00\u4e0b\n    \u65cb\u8f6c\u77e9\u9635\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u6027\u8d28\uff0cPointNet\u4e2dT-Net\u65cb\u8f6c\u77e9\u9635\u7684\u635f\u5931\u51fd\u6570\u5982\u4f55\u8bbe\u8ba1\n    \u5982\u4f55\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\n    \u4ecb\u7ecd\u4e0b\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u53c2\u6570\u7c7b\u7b97\u6cd5\u548c\u975e\u53c2\u6570\u7c7b\u7b97\u6cd5\n    \u968f\u673a\u68af\u5ea6\u4e0b\u964d\n    \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5982\u4f55\u89e3\u51b3\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\n    L1\u6b63\u5219\u5316\u548cL2\u6b63\u5219\u5316\u533a\u522b\uff0c\u5177\u4f53\u6709\u4f55\u7528\u9014\n    L1\u6b63\u5219\u5316\u76f8\u6bd4\u4e8e L2\u6b63\u5219\u5316\u4e3a\u4f55\u5177\u6709\u7a00\u758f\u89e3\n    \n2. C++\u5f00\u53d1\u76f8\u5173\n    c++\u5e38\u89c1\u5bb9\u5668\uff0cvector\u5bb9\u5668capacity\u548csize\u533a\u522b\uff0c\u5982\u4f55\u52a8\u6001\u589e\u957f\n    vector\u904d\u5386\u6709\u54ea\u51e0\u79cd\u65b9\u5f0f\uff08\u5c3d\u53ef\u80fd\u591a\uff09\n    cv:Mat \u6709\u51e0\u79cd\u8bbf\u95ee\u65b9\u5f0f\n    map\u5bb9\u5668\u589e\u5220\u6539\u67e5\uff0c\u548cunorder_map\u533a\u522b\uff0cmap\u5e95\u5c42\u5982\u4f55\u5b9e\u73b0\n    c++\u667a\u80fd\u6307\u9488\n    c++14/17\u65b0\u7279\u6027\n    c++\u548cc\u8bed\u8a00\u533a\u522b\n    c++\u5982\u4f55\u5b9e\u73b0\u591a\u6001\uff0c\u6709\u51e0\u79cd\u65b9\u5f0f\uff0c\u52a8\u6001\u591a\u6001\u548c\u9759\u6001\u591a\u6001\u533a\u522b\n    \u6a21\u677f\u4e86\u89e3\u561b\n    c++\u7ee7\u627f\u591a\u6001\n    c++\u6df1\u62f7\u8d1d\u4e0e\u6d45\u62f7\u8d1d\n    \u62f7\u8d1d\u6784\u9020\u51fd\u6570\u548c\u59d4\u6258\u6784\u9020\u51fd\u6570\n    c++\u9762\u5411\u5bf9\u8c61\n    \u53f3\u503c\u5f15\u7528\uff0cmove\u8bed\u4e49\uff0c\u5b8c\u7f8e\u8f6c\u53d1\n    emplace_back\u548cpush_back\u533a\u522b\n    Eigen\u5e93\u4e86\u89e3\u561b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2ac++\u7684\u5355\u4f8b\u6a21\u5f0f\n    \u5185\u8054\u51fd\u6570\u548c\u5b8f\u7684\u533a\u522b\n    \u5982\u4f55\u5b9e\u73b0\u4e00\u4e2a\u53ea\u5728\u5806\u6216\u8005\u6808\u4e0a\u521d\u59cb\u5316\u7684\u7c7b\n    \u5982\u4f55\u67e5\u627e\u5bb9\u5668\u5185\u6240\u6709\u7b26\u5408\u6761\u4ef6\u7684\u5143\u7d20\n    \n3. Python\u5f00\u53d1\u76f8\u5173\n    list tuple\u533a\u522b\n    \u751f\u6210\u5668\u548c\u8fed\u4ee3\u5668\n    Python\u7c7b\u7684\u5b9a\u4e49\u548c\u5b9e\u4f8b\u5316\u65b9\u6cd5\n    \n4. \u6570\u636e\u7ed3\u6784\u76f8\u5173\n    \u7ea2\u9ed1\u6811\u7ed3\u6784\uff0c\u67e5\u627e\u65f6\u95f4\u590d\u6742\u5ea6\n    \u5806\u6392\u5e8f\u7684\u65f6\u95f4\u590d\u6742\u5ea6\n    Top K\u6392\u5e8f\n    \u5982\u4f55\u7528O(1)\u590d\u6742\u5ea6\u67e5\u627e\u5230stack\u91cc\u9762\u7684\u6700\u5c0f\u503c\n    \u516b\u7687\u540e\n    C++\u81ea\u5df1\u5b9e\u73b0\u4e00\u4e2a\u961f\u5217\n    \u6570\u7ec4\u548c\u94fe\u8868\u7684\u533a\u522b\n    \u4ec0\u4e48\u662fkd-tree\uff0c\u5982\u4f55\u5b9e\u73b0\n    \u9752\u86d9\u8df3\u53f0\u9636\u7684\u9012\u5f52\u548c\u975e\u9012\u5f52\u5b9e\u73b0\n    \n5. \u64cd\u4f5c\u7cfb\u7edf\u76f8\u5173\n    \u5982\u4f55\u8c03\u8bd5\u6808\u6ea2\u51fa\n    \u8ba1\u7b97\u673a\u5185\u5b58\u5806\u548c\u6808\u7684\u533a\u522b\n    \u7ebf\u7a0b\u540c\u6b65\u7684\u65b9\u5f0f\uff0c\u4e92\u65a5\u9501\u548c\u4fe1\u53f7\u91cf\u7684\u5bf9\u6bd4\n    \u8fdb\u7a0b\u548c\u7ebf\u7a0b\u7684\u533a\u522b\n    \u56fe\u7247\u5b58\u50a8\u539f\u7406\u4ecb\u7ecd\u4e00\u4e0b\n    \n6. \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u5173\n    Tensorflow\u7ed3\u6784\u6846\u67b6\uff0c\u5982\u4f55\u7528Tensorflow\u5b9e\u73b0\u4e00\u4e2a\u53cd\u5411\u6c42\u68af\u5ea6\n    Tensorflow\u5982\u4f55\u5408\u5e76\u4e24\u4e2aTensor\n    caffe\u548cPytorch\u4e86\u89e3\u561b\n    caffe\u548cTensorflow\u533a\u522b\u5728\u4ec0\u4e48\u5730\u65b9\n    Tensorflow serving\u548cTensorRT\u6709\u4e86\u89e3\u8fc7\u561b\n    caffe\u7ed3\u6784\u6846\u67b6\n    \n7. \u89c6\u89c9SLAM\u76f8\u5173\n    SLAM\u4e3b\u8981\u5206\u4e3a\u54ea\u51e0\u4e2a\u6a21\u5757\n    ORB-SLAM2\u7684\u4f18\u7f3a\u70b9\u5206\u6790\uff0c\u5982\u4f55\u6539\u8fdb\n    ORB\u548cFAST\u5bf9\u6bd4\n    BA\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\n    ORB-SLAM2\u7684\u4e09\u4e2a\u7ebf\u7a0b\u662f\u4ec0\u4e48\n    ORB-SLAM2\u7684\u5b9a\u4f4d\u5982\u4f55\u5b9e\u73b0\n    \u5982\u4f55\u7406\u89e3ORB-SLAM2\u7684\u56fe\u4f18\u5316\n    \u7ed3\u6784\u5149\u3001TOF\u3001\u53cc\u76ee\u89c6\u89c9\u539f\u7406\n    \u76f4\u63a5\u6cd5\u3001\u534a\u76f4\u63a5\u6cd5\u3001\u7279\u5f81\u70b9\u6cd5\u533a\u522b\u4e0e\u8054\u7cfb\n    Apollo\u7684\u611f\u77e5\u6a21\u5757\u539f\u7406\n    Apollo\u76842D\u548c3D\u8ddf\u8e2a\n    \u5982\u4f55\u6c42\u89e3\u65cb\u8f6c\u77e9\u9635\n    \u5982\u679c\u53ea\u670932\u7ebf\u96f7\u8fbe\uff0c\u4e2a\u6570\u4e0d\u9650\uff0c\u80fd\u5b9e\u73b0360\u5ea6\u89c6\u89d2\u8986\u76d6\u5417\uff0c\u5982\u4f55\u5b9e\u73b0\uff0c64\u7ebf\u5462\uff1f\n```\n\n##  \u516c\u53f8\n[\u89c6\u89c9\u9886\u57df\u7684\u90e8\u5206\u56fd\u5185\u516c\u53f8](http://www.ipcv.org/cvcom/)\n###  \u521d\u521b\u516c\u53f8\uff1a\n[\u56fe\u666e\u79d1\u6280](http://www.tuputech.com/)---[Face++](http://www.faceplusplus.com.cn/)---[Linkface](http://www.linkface.cn/index.html)---[Minieye](http://www.minieye.cc/cn/)---[\u77e5\u56feCogtu](http://www.cogtu.com/?lang=zh)---[\u5546\u6c64\u79d1\u6280Sensetime](http://www.sensetime.com/cn)---[\u4eae\u98ce\u53f0Hiscene](http://www.hiscene.com/)---[\u638c\u8d62\u79d1\u6280](http://www.zhangying.mobi/index.html)---[\u683c\u7075\u6df1\u77b3DeepPG](http://www.deepglint.com/)---[\u51cc\u611f\u79d1\u6280usens](http://www.lagou.com/gongsi/j114187.html)---[\u56fe\u68eeTuSimple](http://www.tusimple.com/)---[\u4e2d\u79d1\u89c6\u62d3Seetatech(\u5c71\u4e16\u5149)](http://www.seetatech.com/)---[\u7b2c\u56db\u8303\u5f0f](https://www.4paradigm.com/product/prophet)\n\n### \u4e0a\u5e02\u516c\u53f8\uff1a\n[\u767e\u5ea6DL\u5b9e\u9a8c\u5ba4](http://idl.baidu.com/)---[\u817e\u8baf\u4f18\u56fe](http://youtu.qq.com/)---[\u963f\u91cc\u9ad8\u5fb7](http://www.newsmth.net/nForum/#!article/Career_Upgrade/429476)---[\u66b4\u98ce\u9b54\u955c](http://www.newsmth.net/nForum/#!article/Career_PHD/225254)---[\u641c\u72d7](http://www.newsmth.net/nForum/#!article/Career_PHD/224449)---[\u4e50\u89c6tv](http://www.newsmth.net/nForum/#!article/Career_PHD/222651)---[\u5947\u864e360](http://www.newsmth.net/nForum/#!article/Career_PHD/222379)---[\u4eac\u4e1c\u5b9e\u9a8c\u5ba4](http://www.newsmth.net/nForum/#!article/Career_PHD/223133/a>)---[\u963f\u91cc\u5df4\u5df4](http://www.newsmth.net/nForum/#!article/Career_PHD/222007)---[\u8054\u60f3\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/220225)---[\u534e\u4e3a\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/225976)\n\n### \u77e5\u540d\u5916\u4f01\uff1a\n[\u4f73\u80fd\u4fe1\u606f](http://www.newsmth.net/nForum/#!article/Career_PHD/222548)---[\u7d22\u5c3c\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/223437)---[\u5bcc\u58eb\u901a\u7814\u53d1\u4e2d\u5fc3](http://www.newsmth.net/nForum/#!article/Career_PHD/220654)---[\u5fae\u8f6f\u7814\u7a76\u9662](https://careers.microsoft.com/?rg=cn)---[\u82f1\u7279\u5c14\u7814\u7a76\u9662](http://www.newsmth.net/nForum/#!article/Career_PHD/221175)---[\u4e09\u661f\u7814\u7a76\u9662](http://www.yingjiesheng.com/job-001-742-124.html)\n\n\n\n## 0 \u8ba1\u7b97\u6444\u5f71\u3000\u6444\u5f71\u51e0\u4f55\n[\u8ba1\u7b97\u6444\u5f71\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/cp-lecture/)\n[\u76f8\u673a\u5185\u90e8\u56fe\u50cf\u5904\u7406\u6d41\u7a0b](http://www.comp.nus.edu.sg/~brown/ICIP2013_Brown.html)\n[pdf](http://www.comp.nus.edu.sg/~brown/ICIP2013_Tutorial_Brown.pdf)\n\n    \u76f8\u673a = \u5149\u6d4b\u91cf\u88c5\u7f6e(Camera = light-measuring device)\n        \u7167\u660e\u5149\u6e90(Illumination source)\uff08\u8f90\u5c04(radiance)\uff09 --> \n        \u573a\u666f\u5143\u7d20(Scene Element)   --->\n        \u6210\u50cf\u7cfb\u7edf(Imaging System)  --->\n        \u5185\u90e8\u56fe\u50cf\u5e73\u9762(Internal Image Plane) --->\n        \u8f93\u51fa\uff08\u6570\u5b57\uff09\u56fe\u50cf(Output (digital) image) \n    \u56fe\u50cf = \u8f90\u5c04\u80fd\u91cf\u6d4b\u91cf(Image = radiant-energy measurement)  \n    \n    \n    \u73b0\u4ee3\u6444\u5f71\u6d41\u6c34\u7ebf\u3000Modern photography pipeline \n    \u573a\u666f\u8f90\u5c04\u3000--->\u3000\u76f8\u673a\u524d\u7aef(\u955c\u5934\u8fc7\u6ee4\u5668 \u955c\u5934Lens \u5feb\u95e8Shutter \u5b54\u5f84)\u3000--->\u3000\n    \u76f8\u673a\u5185\u90e8(ccd\u54cd\u5e94response\uff08RAW\uff09 CCD\u63d2\u503cDemosaicing \uff08\u539f\uff09)\u3000--->\u3000\n    \u76f8\u673a\u540e\u7aef\u5904\u7406(\u76f4\u65b9\u56fe\u5747\u8861Hist equalization\u3001\u7a7a\u95f4\u626d\u66f2Spatial warping)--->\u3000\u8f93\u51fa\n    \n\n\n    \u900f\u8fc7\u68f1\u955c\u7684\u767d\u5149 \u3000\u201cWhite light\u201d through a prism  ------> \u6298\u5c04\u5149(Refracted light)----> \u5149\u8c31 Spectral \u3000\n    \u6211\u4eec\u7684\u773c\u775b\u6709\u4e09\u4e2a\u53d7\u4f53\uff08\u9525\u7ec6\u80de\uff09\uff0c\u5b83\u4eec\u5bf9\u53ef\u89c1\u5149\u4f5c\u51fa\u53cd\u5e94\u5e76\u4ea7\u751f\u989c\u8272\u611f\u3002\n    \n[CSC320S: Introduction to Visual Computing \u89c6\u89c9\u8ba1\u7b97\u5bfc\u8bba ](http://www.cs.toronto.edu/~kyros/courses/320/)\n\n[Facebook surround 360 \u300a\u5168\u666f\u56fe\u62fc\u63a5\u300b](https://github.com/facebook/Surround360)\n\n        \u8f93\u5165\uff1a17\u5f20raw\u56fe\u50cf\uff0c\u5305\u62ec14\u5f20side images\u30012\u5f20top images\u30011\u5f20bottom image\n        \u8f93\u51fa\uff1a3D\u7acb\u4f53360\u5ea6\u5168\u666f\u56fe\u50cf  \n[\u535a\u5ba2\u7b14\u8bb0](https://blog.csdn.net/electech6/article/details/53618965)   \n        \n\n[\u6df1\u5ea6\u6444\u5f71\u98ce\u683c\u8f6c\u6362 Deep Photo Style Transfer](https://github.com/luanfujun/deep-photo-styletransfer)\n\n### \u56fe\u50cf\u5f62\u53d8 Image warping\n[\u53c2\u8003](http://www.ipcv.org/image-warping/)\n### \u8272\u5f69\u589e\u5f3a/\u8f6c\u6362\u3000Color transfer\n[\u53c2\u8003](http://www.ipcv.org/colortransfer/)\n### \u56fe\u50cf\u4fee\u8865 Image repair\n[\u53c2\u8003](http://www.ipcv.org/imagerepair/)\n### \u56fe\u50cf\u53bb\u566a Image denoise\n[\u53c2\u8003](http://www.ipcv.org/imagedenoise/)\n### \u56fe\u50cf\u53bb\u6a21\u7cca Image deblur \n[\u53c2\u8003](http://www.ipcv.org/imagedeblur/)\n###  \u56fe\u50cf\u6ee4\u6ce2 Image filter\n[\u53c2\u8003](http://www.ipcv.org/imagefilter/)\n\n###  \u8d85\u5206\u8fa8\u7387 Super-resolution\n[\u53c2\u8003](http://www.ipcv.org/code-superresolution/)  \n\n\n## 1\u3000\u4e09\u7ef4\u91cd\u5efa 3D Modeling\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/3dd/)\n### \u76f8\u673a\u77eb\u6b63\u3000Camera calibration\n[\u53c2\u8003](http://www.ipcv.org/poseestimation/)\n### \u975e\u521a\u4f53\u91cd\u5efa\u3000Non-rigid modeling\u3000\n[\u53c2\u8003](http://www.ipcv.org/nonnigitreco/)\n### \u4e09\u7ef4\u91cd\u6784 3D modeling\n[\u53c2\u8003](http://www.ipcv.org/3dmodeling/)\n[\u89c6\u89c9SLAM](http://www.ipcv.org/on-visual-slam/)\n\n[Self-augmented Convolutional Neural Networks](https://github.com/msraig/self-augmented-net)\n\n[\u8fd0\u52a8\u4f30\u8ba1 motion estimation](http://www.ipcv.org/on-motion-estimation/)\n\n[\u9762\u90e8\u53d8\u5f62\u3000face morphing\u3000](http://www.ipcv.org/about-face-morphing/)\n\n[\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u7684\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/people-3d-modeling/)\n\n\n## 2  \u5339\u914d/\u8ddf\u8e2a Matching & Tracking\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/tracking/)\n\n### 2.a \u7279\u5f81\u63d0\u53d6 Feature extraction\n[\u53c2\u8003](http://www.ipcv.org/featureextraction/)\n\n### 2.b \u7279\u5f81\u5339\u914d Feature matching\n[\u53c2\u8003](http://www.ipcv.org/code-featmatching/)\n\n### 2.c \u65f6\u7a7a\u5339\u914d Space-time matching\n[\u53c2\u8003](http://www.ipcv.org/code-spacetimematching/)\n\n### 2.d \u533a\u57df\u5339\u914d Region matching\n[\u53c2\u8003](http://www.ipcv.org/code-regionmatching/)\n\n### 2.e \u8f6e\u5ed3\u5339\u914d Contour matching\n[\u53c2\u8003](http://www.ipcv.org/code-coutourmatching/)\n\n### 2.f \u7acb\u4f53\u5339\u914d Stereo matching \n[\u53c2\u8003](http://www.ipcv.org/code-stereomatching/)\n\n[\u53cc\u76ee\u89c6\u89c9\u81ea\u52a8\u9a7e \u573a\u666f\u7269\u4f53\u8ddf\u8e2apaper](http://www.cvlibs.net/publications/Menze2015CVPR.pdf)\n\n[kitti\u53cc\u76ee\u6570\u636e\u96c6\u89e3\u51b3\u65b9\u6848](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo)\n\n[\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5c062D\u7535\u5f71\u8f6c\u6362\u4e3a3D\u7535\u5f71\u7684\u8f6c\u6362](https://github.com/piiswrong/deep3d)\n\n[\u795e\u7ecf\u7f51\u7edc\u3000\u53cc\u76ee\u5339\u914d](https://github.com/jzbontar/mc-cnn)\n\n\n[\u4e2d\u5c71\u5927\u5b66\u5f20\u5f1b\u535a\u58eb](http://chizhang.me/)\n\n    MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation\n    1\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\uff08Stereo Matching\uff09\u95ee\u9898\u7684\u5b9a\u4e49\u53ca\u5176\u4e0e\u4eba\u773c\u611f\u77e5\u6df1\u5ea6\u7684\u5173\u7cfb\uff1b\n    2\u3001\u5bf9Matching Cost Volume\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u5206\u6790\uff0c\u4ee5\u671f\u671b\u8fbe\u5230\u542c\u8005\u5bf9\u5176\u7684\u76f4\u89c2\u4e14\u672c\u8d28\u7684\u7406\u89e3\uff1b\n    3\u3001\u8ba8\u8bba\u4e86\u7acb\u4f53\u89c6\u89c9\u5339\u914d\u95ee\u9898\u4e2d\u7684\u56db\u4e2a\u7ecf\u5178\u65b9\u6cd5\uff08\n        Graph Cut\uff0cAdaptive Support Weight Aggregation, \n        Semi-Global Matching, \n        \u4ee5\u53ca PatchMatch Stereo\uff09\uff1b\n    4\u3001\u8ba8\u8bba\u4e86MeshStereo\u7684\u8bd5\u56fe\u7edf\u4e00disparity\u6c42\u89e3\u4ee5\u53ca\u7f51\u683c\u751f\u6210\u4e24\u4e2a\u6b65\u9aa4\u7684motivation\uff0c\n        \u4ee5\u53caformulate\u8fd9\u6837\u4e00\u4e2aunified model\u4f1a\u9047\u5230\u7684\u56f0\u96be\uff1b\n    5\u3001\u8ba8\u8bba\u4e86MeshStereo\u5f15\u5165splitting probability\u7684\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u4f18\u5316\u8fc7\u7a0b\u3002\n    \n    Webinar\u6700\u540e\u5c55\u793a\u4e86MeshStereo\u5728\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca\u65b0\u89c6\u89d2\u6e32\u67d3\u4e24\u4e2a\u4efb\u52a1\u4e2d\u7684\u7ed3\u679c\u3002\n\n\n[Stereo Matching Using Tree Filtering non-local\u7b97\u6cd5\u5728\u53cc\u76ee\u7acb\u4f53\u5339\u914d\u4e0a\u7684\u5e94\u7528 ](https://blog.csdn.net/wsj998689aa/article/details/45584725)\n\n\n\n\n### 2.g \u6df1\u5ea6\u5339\u914d depth matching \n[\u6df1\u5ea6\u5339\u914d depth matching ](http://www.ipcv.org/on-depth-matching/)\n\n### 2.h \u59ff\u6001\u8ddf\u8e2a Pose tracking \n[\u53c2\u8003](http://www.ipcv.org/code-posetracking/)\n\n### 2.i \u7269\u4f53\u8ddf\u8e2a Object tracking\n[\u53c2\u8003](http://www.ipcv.org/code-objtracking/)\n\n### 2.j \u7fa4\u4f53\u5206\u6790 Crowd analysis\n[\u53c2\u8003](http://www.ipcv.org/code-crowdanalysis/)\n[\u7fa4\u4f53\u8fd0\u52a8\u5ea6\u91cf](https://github.com/metalbubble/collectiveness)\n\n### 2.k \u5149\u6d41\u573a\u8ddf\u8e2a Optical flow\n[\u53c2\u8003](http://www.ipcv.org/code-opticalflow/)\n\n\n## 3 \u8bed\u4e49/\u5b9e\u4f8b\u5206\u5272&\u89e3\u6790\u3000Segmentation & Parsing\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/parsing/)\n### 3.a \u89c6\u9891\u5206\u5272  Video  segmentation\n[\u53c2\u8003](http://www.ipcv.org/video-segmentation/)\n\n### 3.b \u4eba\u4f53\u89e3\u6790  Person parsing\n[\u53c2\u8003](http://www.ipcv.org/code-poseparsing/)\n\n[person parsing](http://www.ipcv.org/about-person-parsing/)\n\n### 3.c \u573a\u666f\u89e3\u6790  Scene  parsing\n[scene parsing](http://www.ipcv.org/about-scene-parsing/)\n[\u53c2\u8003](http://www.ipcv.org/code-sceneparsing/)\n\n### 3.d \u8fb9\u7f18\u68c0\u6d4b  Edge   detection\n[\u53c2\u8003](http://www.ipcv.org/code-edgedetection/)\n[\u8fb9\u7f18\u68c0\u6d4b](http://www.ipcv.org/on-edge-detection/)\n\n\n### 3.e \u56fe\u50cf\u7269\u4f53\u5206\u5272 Image object segmentation \n[\u53c2\u8003](http://www.ipcv.org/code-imobjseg/)\n \n### 3.f \u89c6\u9891\u7269\u4f53\u5206\u5272 Video object segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-viobseg/)\n[object segmentation](http://www.ipcv.org/about-video-object-segmentation/)\n\n\n### 3.g \u4ea4\u4e92\u5f0f\u5206\u5272   Interactive segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-intseg/)\n\n### 3.h \u5171\u5206\u5272      Co-segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-cosegmentation/)\n\n### 3.i \u80cc\u666f\u5dee      Background subtraction \n[\u53c2\u8003](http://www.ipcv.org/code-backsub/)\n\n### 3.j \u56fe\u50cf\u5206\u5272\u65b9\u9762 Image segmentation\n[\u53c2\u8003](http://www.ipcv.org/code-imgseg/)\n \n\n## 4 \u8bc6\u522b/\u68c0\u6d4b\u3000Recognition & Detection\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/detect/)\n###  4.a \u5176\u4ed6\u8bc6\u522b Other recognition\n[\u53c2\u8003](http://www.ipcv.org/otherrecog/)\n\n### 4.b \u56fe\u50cf\u68c0\u7d22 Image retrieval\n[\u53c2\u8003](http://www.ipcv.org/%e5%9b%be%e5%83%8f%e6%a3%80%e7%b4%a2/)\n\n### 4.c \u663e\u8457\u68c0\u6d4b Saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saldetection/)\n\n### 4.d \u901a\u7528\u7269\u4f53\u68c0\u6d4b Object proposal\n[\u53c2\u8003](http://www.ipcv.org/code-objproposal/)\n\n### 4.e \u884c\u4e3a\u8bc6\u522b Action recognition\n[\u53c2\u8003](http://www.ipcv.org/code-actionrecogntion/)\n\n### 4.f \u7269\u4f53\u8bc6\u522b Object recognition\n[\u53c2\u8003](http://www.ipcv.org/code-objrecogntion/)\n\n### 4.g \u884c\u4eba\u68c0\u6d4b Human detection\n[\u53c2\u8003](http://www.ipcv.org/code-humandetection/)\n\n### 4.h \u4eba\u8138\u89e3\u6790 Face Parsing\n[\u53c2\u8003](http://www.ipcv.org/code-facerecog/)\n\n### 4.i \u7eb9\u7406\u5206\u6790 Texture Analysis\n[\u7eb9\u7406\u5206\u6790 Texture Analysis](http://www.ipcv.org/on-texture-analysis/)\n[\u76f8\u5173\u4eba\u7269](http://www.ipcv.org/people-reidentity/)\n\n## 5 \u673a\u5668\u5b66\u4e60 Maching Learning\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/ml/)\n\n### 5.a \u751f\u6210\u5bf9\u6297\u7f51\u7edc GAN Generative Adversarial Networks\n[\u53c2\u8003](http://www.ipcv.org/adversarial-networks/)\n\n### 5.b \u6df1\u5ea6\u5b66\u4e60    Deep learning \n[\u53c2\u8003](http://www.ipcv.org/deeplearning/)\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u8bfe\u7a0b\u8bb2\u4e49](http://www.ipcv.org/lecture-deeplearning/)\n\n[CNN Models \u5377\u79ef\u7f51\u7edc\u6a21\u578b](http://www.ipcv.org/on-object-detection/)\n\n[Deep Learning Libraries\u3000\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u5e93](http://www.ipcv.org/deep_learning_libraries/)\n\n[\u6df1\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u90e8\u5206\u89c6\u89c9\u4eba\u7269](http://www.ipcv.org/dl-researcher/)\n\n### 5.c \u80fd\u91cf\u4f18\u5316    Energy optimization \n[\u53c2\u8003](http://www.ipcv.org/energyopt/)\n\n### 5.d \u6a21\u578b\u8bbe\u8ba1    Model design\n[\u53c2\u8003](http://www.ipcv.org/modelbuilding/)\n\n### 5.e \u7a7a\u95f4\u964d\u7ef4    Dimention reduction \n[\u53c2\u8003](http://www.ipcv.org/dimention/)\n\n### 5.f \u805a\u7c7b       Clustering \n[\u53c2\u8003](http://www.ipcv.org/clustering/)\n\n### 5.g \u5206\u7c7b\u5668     Classifier\n[\u53c2\u8003](http://www.ipcv.org/classifier/)\n\n## 6 \u5f00\u6e90\u5e93\u3000Open library\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/lib/)\n###\n###\n\n\n## 7 \u6570\u636e\u96c6\u3000Public dataset\n[\u53c2\u8003](http://www.ipcv.org/category/code-data/dataset/)\n### 7.a \u5176\u4ed6\u65b9\u9762 Other datasets\n[\u53c2\u8003](http://www.ipcv.org/otherdb/)\n[\u4eba\u8138\u68c0\u6d4bFace tracking and recognition database ](http://seqam.rutgers.edu/site/index.php?option=com_content&view=article&id=65&Itemid=76)\n\n[\u4eba\u8138\u68c0\u6d4bCaltech 10,000 Web Faces](http://vision.caltech.edu/archive.html)\n\n[\u4eba\u8138\u68c0\u6d4bHelen dataset](http://www.ifp.illinois.edu/~vuongle2/helen/)\n\n[\u6df1\u5ea6\u56fe RGB-D dataset](http://mobilerobotics.cs.washington.edu/projects/kdes/)\n\n[\u89c6\u9891\u5206\u5272 2010 ECCV Efficient Hierarchical Graph Based Video Segmentation](http://www.cc.gatech.edu/cpl/projects/videosegmentation/)\n\n[\u624b\u52bf\u8ddf\u8e2a Hand dataset](https://engineering.purdue.edu/RVL/Database.html)\n\n[\u624b\u52bf\u8ddf\u8e2a2](http://www.robots.ox.ac.uk/~vgg/research/hands/index.html)\n\n[\u8f66\u8f86\u68c0\u6d4b 2002 ECCV Learning a sparse representation for object detection](http://cogcomp.cs.illinois.edu/Data/Car/)\n\n### 7.b \u4eba\u4f53\u68c0\u6d4b dataset on human annotation\n[\u53c2\u8003](http://www.ipcv.org/humandetection/)\n[Caltech Pedestrian Detection Benchmark](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n\n[Ethz](http://www.vision.ee.ethz.ch/~aess/dataset/)\n[bleibe](http://www.vision.ee.ethz.ch/~bleibe/data/datasets.html)\n\n[RGB-D People Dataset](http://www.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)\n\n[TUD Campus](http://www.d2.mpi-inf.mpg.de/tud-brussels)\n[382](https://www.d2.mpi-inf.mpg.de/node/382)\n\n[PSU HUB Dataset](http://www.cse.psu.edu/~rcollins/software.html)\n\n[Pedestrian parsing](http://vision.ics.uci.edu/datasets/)\n\n[Human Eva](http://vision.cs.brown.edu/humaneva/)\n\n\n### 7.c \u7269\u4f53\u8bc6\u522b dataset on object recognition\n\n[\u53c2\u8003](http://www.ipcv.org/objectrecognition/)\n\n[ e-Lab Video Data Set](https://engineering.purdue.edu/elab/eVDS/)\n\n[Image Net](http://www.image-net.org/)\n\n[Places2 Database](http://places2.csail.mit.edu)\n\n[Microsoft CoCo: Common Objects in Context](http://mscoco.org/)\n\n[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n[MIT\u2019s Place2]( http://places2.csail.mit.edu/)\n\n### 7.d \u663e\u8457\u68c0\u6d4b\u65b9\u9762 dataset on saliency detection\n[\u53c2\u8003](http://www.ipcv.org/saliencydetection/)\n\n[\u89c6\u89c9\u663e\u8457\u6027\u68c0\u6d4b\u6280\u672f\u53d1\u5c55\u60c5\u51b5](http://blog.csdn.net/anshan1984/article/details/8657176)\n\n[2012 ECCV Salient Objects Dataset (SOD)](http://elderlab.yorku.ca/SOD/)\n\n[2012 ECCV Neil D. B. Bruce Eye Tracking Data](http://cs.umanitoba.ca/~bruce/datacode.html)\n\n[2012 ECCV DOVES:A database of visual eye movements](http://live.ece.utexas.edu/research/doves/)\n\n[2012 ECCV MSRA:Salient Object Database](http://research.microsoft.com/en-us/um/people/jiansun/SalientObject/salient_object.htm)\n\n[2012 ECCV NUS: Predicting Saliency Beyond Pixels](http://www.ece.nus.edu.sg/stfpage/eleqiz/predicting.html)\n\n[2012 ECCV saliency benchmark](http://people.csail.mit.edu/tjudd/SaliencyBenchmark/index.html)\n\n[2010 ECCV The DUT-OMRON Image Dataset](http://ice.dlut.edu.cn/lu/DUT-OMRON/Homepage.htm)\n\n[2010 ECCV An eye fixation database for saliency detection in images](http://mmas.comp.nus.edu.sg/NUSEF.html)\n\n### 7.e \u884c\u4e3a\u8bc6\u522b dataset on action recognition\n[\u53c2\u8003](http://www.ipcv.org/actionrecognition/)\n\n[UCF](http://www.cs.ucf.edu/~liujg/YouTube_Action_dataset.html)\n[ChaoticInvariants](http://www.cs.ucf.edu/~sali/Projects/ChaoticInvariants/index.html)\n[datasetsActions](http://vision.eecs.ucf.edu/datasetsActions.html)\n\n[Hollywood Human Actions dataset](http://www.di.ens.fr/~laptev/download.html)\n[data](http://lear.inrialpes.fr/data)\n\n[Weizmann: Actionsas Space-Time Shapes](http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html)\n\n[KTH](http://www.nada.kth.se/cvap/actions/)\n\n[UMD](http://www.umiacs.umd.edu/~zhuolin/Keckgesturedataset.html)\n\n[HMDB: A Large Video Database for Human Motion Recognition](http://serre-lab.clps.brown.edu/resources/HMDB/related_data/)\n\n[Collective Activity Dataset](http://www.eecs.umich.edu/vision/activity-dataset.html)\n\n[MSR Action Recognition Datasets and Codes](http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/default.htm)\n\n[Visual Event Recognition in Videos](http://vc.sce.ntu.edu.sg/index_files/VisualEventRecognition/VisualEventRecognition.html)\n\n\n\n### 7.f \u7269\u4f53\u5206\u5272 dataset on object segmentation\n[\u53c2\u8003](http://www.ipcv.org/objectsegmentation/)\n[microsoft MSRC-V2](http://research.microsoft.com/en-us/projects/objectclassrecognition/)\n\n[2010 CVPR iCoseg: Interactive cosegmentation by touch](http://chenlab.ece.cornell.edu/projects/touch-coseg/)\n\n[2010 CVPR Caltech-UCSD Birds 200](http://www.vision.caltech.edu/visipedia/CUB-200.html)\n\n[2010 CVPR Flower Datasets](http://www.robots.ox.ac.uk/~vgg/data/flowers/)\n\n[2009 ICCV An efficient algorithm for co-segmentation](http://www.biostat.wisc.edu/~vsingh/)\n\n[2008 CVPR Unsupervised Learning of Probabilistic Object Models (POMs) for Object Classification, Segmentation and Recognition](http://people.csail.mit.edu/leozhu/)\n\n[2008 CVPR Caltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)\n\n[2004 ECCV The Weizmann Horse Database](http://www.msri.org/people/members/eranb/)\n\n\n### 7.g \u573a\u666f\u89e3\u6790 dataset on scene parsing\n[\u53c2\u8003](http://www.ipcv.org/sceneparsing/)\n\n[ImageNet](http://www.image-net.org/)\n\n[ADE 20k](http://sceneparsing.csail.mit.edu/)\n\n[Cityscapes](https://www.cityscapes-dataset.com/)\n\n[COCO](http://cocodataset.org/#home)\n\n[Lab, Koch](http://www.mis.tu-darmstadt.de/tudds)\n\n[uiuc, D hoiem](http://www.cs.illinois.edu/homes/dhoiem/)\n\n[mit, cbcl](http://cbcl.mit.edu/software-datasets/streetscenes/)\n\n[mit LabelMeVideo](http://labelme.csail.mit.edu/LabelMeVideo/)\n\n[2013 BMVC Hierarchical Scene Annotation](http://www.vision.caltech.edu/~mmaire/)\n\n[2010 ECCV SuperParsing: Scalable Nonparametric Image Parsing with Superpixels](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/)\n\n[2009 CVPR Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment](ttp://people.csail.mit.edu/celiu/CVPR2009/)\n\n[2009 Scene Understanding Datasets](http://dags.stanford.edu/projects/scenedataset.html)\n\n[2008 IJCV 6D-Vision](http://www.6d-vision.com/scene-labeling)\n\n[2008 IJCV The Daimler Urban Segmentation Dataset](http://www.6d-vision.com/scene-labeling)\n\n[2008 ECCV Motion-based Segmentation and Recognition Dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec))/CamVid/\n\n[2008 York Urban Dataset](http://www.elderlab.yorku.ca/YorkUrbanDB/)\n\n[2008 IJCV LabelMe](http://labelme.csail.mit.edu/LabelMeToolbox/index.html)\n\n\n## 8 \u4f1a\u8bae\u3000\u671f\u520a\u3000\n### CVPR Computer vision  and  Pattern Reconition \u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6a21\u5f0f\u8bc6\u522b\n### ECCV European Conference on Computer Vision   \u6b27\u6d32\u8ba1\u7b97\u673a\u89c6\u89c9\u56fd\u9645\u4f1a\u8bae \n### ICCV IEEE International Conference on Computer Vision  \u56fd\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u5927\u4f1a\n### \u5176\u4ed6\n[\u5176\u4ed6](http://www.ipcv.org/otherpaper/)\n###\n###\n###\n###\n\n\n## AR&VR\n[\u53c2\u8003](http://www.ipcv.org/category/top-dir/arvr/)\n\n\n## \n"
 },
 {
  "repo": "spmallick/learnopencv",
  "language": "Jupyter Notebook",
  "readme_contents": "# LearnOpenCV\nThis repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog [LearnOpenCV.com](https://www.LearnOpenCV.com).\n\nWant to become an expert in AI? [AI Courses by OpenCV](https://opencv.org/courses/) is a great place to start.\n\n<a href=\"https://opencv.org/courses/\">\n<p align=\"center\">\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/04/AI-Courses-By-OpenCV-Github.png\">\n</p>\n</a>\n\n## List of Blog Posts\n\n| Blog Post | |\n| ------------- |:-------------|\n|[Image Classification with OpenCV for Android](https://www.learnopencv.com/image-classification-with-opencv-for-android/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android) |\n|[Image Classification with OpenCV Java](https://www.learnopencv.com/image-classification-with-opencv-java)|[Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java) |\n|[PyTorch to Tensorflow Model Conversion](https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion) |\n|[Snake Game with OpenCV Python](https://www.learnopencv.com/snake-game-with-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SnakeGame) |\n|[Stanford MRNet Challenge: Classifying Knee MRIs](https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/)|[Code](https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model) |\n|[Experiment Logging with TensorBoard and wandb](https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging) |\n|[Understanding Lens Distortion](https://www.learnopencv.com/understanding-lens-distortion/)|[Code](https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion) |\n|[Image Matting with state-of-the-art Method \u201cF, B, Alpha Matting\u201d](https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FBAMatting) |\n|[Bag Of Tricks For Image Classification - Let's check if it is working or not](https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification) |\n|[Getting Started with OpenCV CUDA Module](https://www.learnopencv.com/getting-started-opencv-cuda-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module) |\n|[Training a Custom Object Detector with DLIB & Making Gesture Controlled Applications](https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib) |\n|[How To Run Inference Using TensorRT C++ API](https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP) |\n|[Using Facial Landmarks for Overlaying Faces with Medical Masks](https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay) |\n|[Tensorboard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning) |\n|[Otsu's Thresholding with OpenCV](https://www.learnopencv.com/otsu-thresholding-with-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/otsu-method) |\n|[PyTorch-to-CoreML-model-conversion](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion) |\n|[Playing Rock, Paper, Scissors with AI](https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI) |\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[CNN Fully Convolutional Image Classification with TensorFlow](https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow) | [Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification) |\n|[How to convert a model from PyTorch to TensorRT and speed up inference](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT) |\n|[Efficient image loading](https://www.learnopencv.com/efficient-image-loading/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading) |\n|[Graph Convolutional Networks: Model Relations In Data](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data)|\n|[Getting Started with Federated Learning with PyTorch and PySyft](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro)|\n|[Creating a Virtual Pen & Eraser](http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser) |\n|[Getting Started with PyTorch Lightning](https://www.learnopencv.com/getting-started-with-pytorch-lightning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning)|\n|[Multi-Label Image Classification with PyTorch: Image Tagging](https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging)|\n|[Funny Mirrors Using OpenCV](https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/)|[code](https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors)|\n|[t-SNE for ResNet feature visualization](https://www.learnopencv.com/t-sne-for-resnet-feature-visualization/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TSNE)|\n|[Multi-Label Image Classification with Pytorch](https://www.learnopencv.com/multi-label-image-classification-with-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification)|\n|[CNN Receptive Field Computation Using Backprop](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop)|\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[Augmented Reality using AruCo Markers in OpenCV(C++ and Python)](https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/) |[Code](https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers)|\n|[Fully Convolutional Image Classification on Arbitrary Sized Image](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification)|\n|[Camera Calibration using OpenCV](https://www.learnopencv.com/camera-calibration-using-opencv/) |[Code](https://github.com/spmallick/learnopencv/tree/master/CameraCalibration)|\n|[Geometry of Image Formation](https://www.learnopencv.com/geometry-of-image-formation/) ||\n|[Ensuring Training Reproducibility in Pytorch](https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch) ||\n|[Gaze Tracking](https://www.learnopencv.com/gaze-tracking/) ||\n|[Simple Background Estimation in Videos Using OpenCV](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation)|\n|[Applications of Foreground-Background separation with Semantic Segmentation](https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg) |\n|[EfficientNet: Theory + Code](https://www.learnopencv.com/efficientnet-theory-code) | [Code](https://github.com/spmallick/learnopencv/tree/master/EfficientNet) |\n|[PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch](https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/) | [Code](./PyTorch-Mask-RCNN) |\n|[PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN) |\n|[PyTorch for Beginners: Semantic Segmentation using torchvision](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision) |\n|[PyTorch for Beginners: Comparison of pre-trained models for Image Classification](https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb) |\n|[PyTorch for Beginners: Basics](https://www.learnopencv.com/pytorch-for-beginners-basics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb) |\n|[PyTorch Model Inference using ONNX and Caffe2](https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2) |\n|[Image Classification Using Transfer Learning in PyTorch](https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch) |\n|[Hangman: Creating games in OpenCV](https://www.learnopencv.com/hangman-creating-games-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hangman) |\n|[Image Inpainting with OpenCV (C++/Python)](https://www.learnopencv.com/image-inpainting-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting) |\n|[Hough Transform with OpenCV (C++/Python)](https://www.learnopencv.com/hough-transform-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hough-Transform) |\n|[Xeus-Cling: Run C++ code in Jupyter Notebook](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/) | [Code](https://github.com/spmallick/learnopencv/tree/master/XeusCling) |\n|[Gender & Age Classification using OpenCV Deep Learning ( C++/Python )](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AgeGender) |\n|[Invisibility Cloak using Color Detection and Segmentation with OpenCV](https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak) |\n|[Fast Image Downloader for Open Images V4 (Python)](https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/) | [Code](https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages) |\n|[Deep Learning based Text Detection Using OpenCV (C++/Python)](https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST) |\n|[Video Stabilization Using Point Feature Matching in OpenCV](https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoStabilization) |\n|[Training YOLOv3 : Deep Learning based Custom Object Detector](https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector\t) |\n|[Using OpenVINO with OpenCV](https://www.learnopencv.com/using-openvino-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV) |\n|[Duplicate Search on Quora Dataset](https://www.learnopencv.com/duplicate-search-on-quora-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search) |\n|[Shape Matching using Hu Moments (C++/Python)](https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HuMoments) |\n|[Install OpenCV 4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh) |\n|[Install OpenCV 4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh) |\n|[Install OpenCV 4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh) |\n|[Install OpenCV 3.4.4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh) |\n|[Install OpenCV 3.4.4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh) |\n|[OpenCV QR Code Scanner (C++ and Python)](https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV) |\n|[Install OpenCV 3.4.4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3) |\n|[Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh) |\n|[Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh) |\n|[Universal Sentence Encoder](https://www.learnopencv.com/universal-sentence-encoder) | [Code](https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder) |\n|[Install OpenCV 4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh) |\n|[Install OpenCV 4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4) |\n|[Hand Keypoint Detection using Deep Learning and OpenCV](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HandPose)|\n|[Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN) |\n|[Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh) |\n|[Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh) |\n|[Multi-Person Pose Estimation in OpenCV using OpenPose](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person) |\n|[Heatmap for Logo Detection using OpenCV (Python)](https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/heatmap)|\n|[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO)|\n|[Convex Hull using OpenCV in Python and C++](https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ConvexHull)|\n|[MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)](https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker) |\n|[Convolutional Neural Network based Image Colorization using OpenCV](https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colorization)|\n|[SVM using scikit-learn](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[GOTURN: Deep Learning based Object Tracking](https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/) | [Code](https://github.com/spmallick/learnopencv/tree/master/GOTURN)|\n|[Find the Center of a Blob (Centroid) using OpenCV (C++/Python)](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CenterofBlob)|\n|[Support Vector Machines (SVM)](https://www.learnopencv.com/support-vector-machines-svm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[Batch Normalization in Deep Networks](https://www.learnopencv.com/batch-normalization-in-deep-networks/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BatchNormalization)|\n|[Deep Learning based Character Classification using Synthetic Dataset](https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CharClassification)|\n|[Image Quality Assessment : BRISQUE](https://www.learnopencv.com/image-quality-assessment-brisque/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageMetrics)|\n|[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)|\n|[Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/OCR)|\n|[Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose)|\n|[Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)](https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/)| |\n|[How to convert your OpenCV C++ code into a Python module](https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/pymodule)|\n|[CV4Faces : Best Project Award 2018](https://www.learnopencv.com/cv4faces-best-project-award-2018/)| |\n|[Facemark : Facial Landmark Detection using OpenCV](https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/)|[ Code](https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection)|\n|[Image Alignment (Feature Based) using OpenCV (C++/Python)](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased)|\n|[Barcode and QR code Scanner using ZBar and OpenCV](https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner)|\n|[Keras Tutorial : Fine-tuning using pre-trained models](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning)|\n|[OpenCV Transparent API](https://www.learnopencv.com/opencv-transparent-api/)| |\n|[Face Reconstruction using EigenFaces (C++/Python)](https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces) |\n|[Eigenface using OpenCV (C++/Python)](https://www.learnopencv.com/eigenface-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/EigenFace)|\n|[Principal Component Analysis](https://www.learnopencv.com/principal-component-analysis/)| |\n|[Keras Tutorial : Transfer Learning using pre-trained models](https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning) |\n|[Keras Tutorial : Using pre-trained Imagenet models](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models) |\n|[Technical Aspects of a Digital SLR](https://www.learnopencv.com/technical-aspects-of-a-digital-slr/) | |\n|[Using Harry Potter interactive wand with OpenCV to create magic](https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/)| |\n|[Install OpenCV 3 and Dlib on Windows ( Python only )](https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/)| |\n|[Image Classification using Convolutional Neural Networks in Keras](https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR)|\n|[Understanding Autoencoders using Tensorflow (Python)](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder)|\n|[Best Project Award : Computer Vision for Faces](https://www.learnopencv.com/best-project-award-computer-vision-for-faces/) | |\n|[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)      | |\n|[Image Classification using Feedforward Neural Network in Keras](https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasMLP-MNIST)|\n|[Exposure Fusion using OpenCV (C++/Python)](https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/ExposureFusion)|\n|[Understanding Feedforward Neural Networks](https://www.learnopencv.com/understanding-feedforward-neural-networks/)      | |\n|[High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)](http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python)      | [Code](https://github.com/spmallick/learnopencv/tree/master/hdr)|\n|[Deep learning using Keras \u2013 The Basics](http://www.learnopencv.com/deep-learning-using-keras-the-basics)      | [Code](https://github.com/spmallick/learnopencv/tree/master/keras-linear-regression)|\n|[Selective Search for Object Detection (C++ / Python)](http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch) |\n|[Installing Deep Learning Frameworks on Ubuntu with CUDA support](http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/) | |\n|[Parallel Pixel Access in OpenCV using forEach](http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/) | [Code](https://github.com/spmallick/learnopencv/tree/master/forEach) |\n|[cvui: A GUI lib built on top of OpenCV drawing primitives](http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/) | [Code](https://github.com/spmallick/learnopencv/tree/master/UI-cvui) |\n|[Install Dlib on Windows](http://www.learnopencv.com/install-dlib-on-windows/) | |\n|[Install Dlib on Ubuntu](http://www.learnopencv.com/install-dlib-on-ubuntu/) | |\n|[Install OpenCV3 on Ubuntu](http://www.learnopencv.com/install-opencv3-on-ubuntu/) | |\n|[Read, Write and Display a video using OpenCV ( C++/ Python )](http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay) |\n|[Install Dlib on MacOS](http://www.learnopencv.com/install-dlib-on-macos/) | |\n|[Install OpenCV 3 on MacOS](http://www.learnopencv.com/install-opencv3-on-macos/) | |\n|[Install OpenCV 3 on Windows](http://www.learnopencv.com/install-opencv3-on-windows/) | |\n|[Get OpenCV Build Information ( getBuildInformation )](http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/) | |\n|[Color spaces in OpenCV (C++ / Python)](http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ColorSpaces)|\n|[Neural Networks : A 30,000 Feet View for Beginners](http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/) | |\n|[Alpha Blending using OpenCV (C++ / Python)](http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AlphaBlending) |\n|[User stories : How readers of this blog are applying their knowledge to build applications](http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/) | |\n|[How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?](http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/) | |\n|[Automatic Red Eye Remover using OpenCV (C++ / Python)](http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover) |\n|[Bias-Variance Tradeoff in Machine Learning](http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/) | |\n|[Embedded Computer Vision: Which device should you choose?](http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/) | |\n|[Object Tracking using OpenCV (C++/Python)](http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/tracking) |\n|[Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/digits-classification) |\n|[Training a better Haar and LBP cascade based Eye Detector using OpenCV](http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/) | |\n|[Deep Learning Book Gift Recipients](http://www.learnopencv.com/deep-learning-book-gift-recipients/) | |\n|[Minified OpenCV Haar and LBP Cascades](http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector)|\n|[Deep Learning Book Gift](http://www.learnopencv.com/deep-learning-book-gift/) | |\n|[Histogram of Oriented Gradients](http://www.learnopencv.com/histogram-of-oriented-gradients/) | |\n|[Image Recognition and Object Detection : Part 1](http://www.learnopencv.com/image-recognition-and-object-detection-part1/) | |\n|[Head Pose Estimation using OpenCV and Dlib](http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HeadPose) |\n|[Live CV : A Computer Vision Coding Application](http://www.learnopencv.com/live-cv/) | |\n|[Approximate Focal Length for Webcams and Cell Phone Cameras](http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) | |\n|[Configuring Qt for OpenCV on OSX](http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qt-test) |\n|[Rotation Matrix To Euler Angles](http://www.learnopencv.com/rotation-matrix-to-euler-angles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles) |\n|[Speeding up Dlib\u2019s Facial Landmark Detector](http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/) | |\n|[Warp one triangle to another using OpenCV ( C++ / Python )](http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/WarpTriangle) |\n|[Average Face : OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/average-face-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceAverage) |\n|[Face Swap using OpenCV ( C++ / Python )](http://www.learnopencv.com/face-swap-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceSwap) |\n|[Face Morph Using OpenCV \u2014 C++ / Python](http://www.learnopencv.com/face-morph-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceMorph) |\n|[Deep Learning Example using NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/) | |\n|[NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/nvidia-digits-3-on-ec2/) | |\n|[Homography Examples using OpenCV ( Python / C ++ )](http://www.learnopencv.com/homography-examples-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Homography) |\n|[Filling holes in an image using OpenCV ( Python / C++ )](http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Holes) |\n|[How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?](http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FPS) |\n|[Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) ](http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Delaunay) |\n|[OpenCV (C++ vs Python) vs MATLAB for Computer Vision](http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/) | |\n|[Facial Landmark Detection](http://www.learnopencv.com/facial-landmark-detection/) | |\n|[Why does OpenCV use BGR color format ?](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/) | |\n|[Computer Vision for Predicting Facial Attractiveness](http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness) |\n|[applyColorMap for pseudocoloring in OpenCV ( C++ / Python )](http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colormap) |\n|[Image Alignment (ECC) in OpenCV ( C++ / Python )](http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment) |\n|[How to find OpenCV version in Python and C++ ?](http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/) | |\n|[Baidu banned from ILSVRC 2015](http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/) | |\n|[OpenCV Transparent API](http://www.learnopencv.com/opencv-transparent-api/) | |\n|[How Computer Vision Solved the Greatest Soccer Mystery of All Time](http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/) | |\n|[Embedded Vision Summit 2015](http://www.learnopencv.com/embedded-vision-summit-2015/) | |\n|[Read an Image in OpenCV ( Python, C++ )](http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/imread) |\n|[Non-Photorealistic Rendering using OpenCV ( Python, C++ )](http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering) |\n|[Seamless Cloning using OpenCV ( Python , C++ )](http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning) |\n|[OpenCV Threshold ( Python , C++ )](http://www.learnopencv.com/opencv-threshold-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Threshold) |\n|[Blob Detection Using OpenCV ( Python, C++ )](http://www.learnopencv.com/blob-detection-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BlobDetector) |\n|[Turn your OpenCV Code into a Web API in under 10 minutes \u2014 Part 1](http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/) | |\n|[How to compile OpenCV sample Code ?](http://www.learnopencv.com/how-to-compile-opencv-sample-Code/) | |\n|[Install OpenCV 3 on Yosemite ( OSX 10.10.x )](http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/) | |\n"
 },
 {
  "repo": "CMU-Perceptual-Computing-Lab/openpose",
  "language": "C++",
  "readme_contents": "<div align=\"center\">\n    <img src=\".github/Logo_main_black.png\", width=\"300\">\n</div>\n\n-----------------\n\n|                  |`Default Config`  |`CUDA (+Python)`  |`CPU (+Python)`   |`OpenCL (+Python)`| `Debug`          | `Unity`          |\n| :---:            | :---:            | :---:            | :---:            | :---:            | :---:            | :---:            |\n| **`Linux`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/1)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/2)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/3)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/4)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/5)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/6)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`MacOS`**   | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/8)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/9)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/10)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) | [![Status](https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/11)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose) |\n| **`Windows`** | [![Status](https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true)](https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master) | | | | |\n<!--\nNote: Currently using [travis-matrix-badges](https://github.com/bjfish/travis-matrix-badges) vs. traditional [![Build Status](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose.svg?branch=master)](https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose)\n-->\n\n[**OpenPose**](https://github.com/CMU-Perceptual-Computing-Lab/openpose) has represented the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**.\n\nIt is **authored by [Gines Hidalgo](https://www.gineshidalgo.com), [Zhe Cao](https://people.eecs.berkeley.edu/~zhecao), [Tomas Simon](http://www.cs.cmu.edu/~tsimon), [Shih-En Wei](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [Hanbyul Joo](https://jhugestar.github.io), and [Yaser Sheikh](http://www.cs.cmu.edu/~yaser)**, and **maintained by [Gines Hidalgo](https://www.gineshidalgo.com) and [Yaadhav Raaj](https://www.raaj.tech)**. OpenPose would not be possible without the [**CMU Panoptic Studio dataset**](http://domedb.perception.cs.cmu.edu). We would also like to thank all the people who helped OpenPose in any way ([doc/contributors.md](doc/contributors.md)).\n\n<!-- The [original CVPR 2017 repo](https://github.com/ZheC/Multi-Person-Pose-Estimation) includes Matlab and Python versions, as well as the training code. The body pose estimation work is based on [the original ECCV 2016 demo](https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose). -->\n\n\n\n<p align=\"center\">\n    <img src=\".github/media/pose_face_hands.gif\", width=\"480\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left) and <a href=\"https://jhugestar.github.io\" target=\"_blank\">Hanbyul Joo</a> (right) in front of the <a href=\"http://domedb.perception.cs.cmu.edu\" target=\"_blank\">CMU Panoptic Studio</a></sup>\n</p>\n\n\n\n## Contents\n1. [Results](#results)\n2. [Features](#features)\n3. [Related Work](#related-work)\n4. [Installation](#installation)\n5. [Quick Start](#quick-start)\n6. [Send Us Feedback!](#send-us-feedback)\n7. [Citation](#citation)\n8. [License](#license)\n\n\n\n## Results\n### Body and Foot Estimation\n<p align=\"center\">\n    <img src=\".github/media/dance_foot.gif\", width=\"360\">\n    <br>\n    <sup>Testing the <a href=\"https://www.youtube.com/watch?v=2DiQUX11YaY\" target=\"_blank\"><i>Crazy Uptown Funk flashmob in Sydney</i></a> video sequence with OpenPose</sup>\n</p>\n\n### 3D Reconstruction Module (Body, Foot, Face, and Hands)\n<p align=\"center\">\n    <img src=\".github/media/openpose3d.gif\", width=\"360\">\n    <br>\n    <sup>Testing the 3D Reconstruction Module of OpenPose</sup>\n</p>\n\n### Body, Foot, Face, and Hands Estimation\n<p align=\"center\">\n    <img src=\".github/media/pose_face.gif\", width=\"360\">\n    <img src=\".github/media/pose_hands.gif\", width=\"360\">\n    <br>\n    <sup>Authors <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> (left image) and <a href=\"http://www.cs.cmu.edu/~tsimon\" target=\"_blank\">Tomas Simon</a> (right image) testing OpenPose</sup>\n</p>\n\n### Unity Plugin\n<p align=\"center\">\n    <img src=\".github/media/unity_main.png\", width=\"240\">\n    <img src=\".github/media/unity_body_foot.png\", width=\"240\">\n    <img src=\".github/media/unity_hand_face.png\", width=\"240\">\n    <br>\n    <sup><a href=\"http://tianyizhao.com\" target=\"_blank\">Tianyi Zhao</a> and <a href=\"https://www.gineshidalgo.com\" target=\"_blank\">Gines Hidalgo</a> testing the <a href=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin\" target=\"_blank\">OpenPose Unity Plugin</a></sup>\n</p>\n\n### Runtime Analysis\nWe show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details [**here**](https://arxiv.org/abs/1812.08008).\n<p align=\"center\">\n    <img src=\".github/media/openpose_vs_competition.png\", width=\"360\">\n</p>\n\n\n\n## Features\n- **Main Functionality**:\n    - **2D real-time multi-person keypoint detection**:\n        - 15, 18 or **25-keypoint body/foot keypoint estimation**, including **6 foot keypoints**. **Runtime invariant to number of detected people**.\n        - **2x21-keypoint hand keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n        - **70-keypoint face keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n    - [**3D real-time single-person keypoint detection**](doc/advanced/3d_reconstruction_module.md):\n        - 3D triangulation from multiple single views.\n        - Synchronization of Flir cameras handled.\n        - Compatible with Flir/Point Grey cameras.\n    - [**Calibration toolbox**](doc/advanced/calibration_module.md): Estimation of distortion, intrinsic, and extrinsic camera parameters.\n    - **Single-person tracking** for further speedup or visual smoothing.\n- **Input**: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).\n- **Output**: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).\n- **OS**: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\n- **Hardware compatibility**: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.\n- **Usage Alternatives**:\n    - [**Command-line demo**](doc/demo_quick_start.md) for built-in functionality.\n    - [**C++ API**](examples/tutorial_api_cpp/) and [**Python API**](doc/python_api.md) for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.\n\nFor further details, check [all released features](doc/released_features.md) and [release notes](doc/release_notes.md).\n\n\n\n## Related Work\n- [**OpenPose training code**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)\n- [**OpenPose foot dataset**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/)\n- [**OpenPose Unity Plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin)\n- OpenPose papers published in [**IEEE TPAMI** and **CVPR**](#citation). [Cite them](#citation) in your publications if it helps your research!\n\n\n\n## Installation\nIf you want to use OpenPose without compiling or writing any code, simply [download and use the latest Windows portable version of OpenPose](doc/installation/README.md#windows-portable-demo))! Otherwise, you can also [build OpenPose from source](doc/installation/README.md#compiling-and-running-openpose-from-source-on-windows-ubuntu-and-mac).\n\nSee [doc/installation/README.md](doc/installation/README.md) for more details.\n\n\n\n## Quick Start\nMost users do not need to know C++ or Python, they can simply use the OpenPose Demo in their command-line tool (e.g., PowerShell/Terminal). E.g., this would run OpenPose on the webcam and display the body keypoints:\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin\n```\n```\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi\n```\n\nYou can also add any of the available flags in any order. Do you also want to add face and/or hands? Add the `--face` and/or `--hand` flags. Do you also want to save the output keypoints on JSON files on disk? Add the `--write_json` flag, etc.\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/\n```\n```\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi --face --hand --write_json output_json_folder/\n```\n\nAfter [installing](#installation) OpenPose, check [doc/README.md](doc/README.md) for a quick overview of all the alternatives and tutorials.\n\n\n\n## Send Us Feedback!\nOur library is open source for research purposes, and we want to continuously improve it! So let us know if you...\n1. Find any bug (in functionality or speed).\n2. Add some functionality on top of OpenPose which we might want to add.\n3. Know how to speed up or improve any part of OpenPose.\n4. Want to share your cool demo or project made on top of OpenPose (you can email it to us too!).\n\nJust create a new GitHub issue or a pull request and we will answer as soon as possible!\n\n\n\n## Citation\nPlease cite these papers in your publications if it helps your research. All of OpenPose is based on [OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1812.08008), while the hand and face detectors also use [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809) (the face detector was trained using the same procedure than the hand detector).\n\n    @article{8765346,\n      author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2019}\n    }\n\n    @inproceedings{simon2017hand,\n      author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\n      year = {2017}\n    }\n\n    @inproceedings{cao2017realtime,\n      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2017}\n    }\n\n    @inproceedings{wei2016cpm,\n      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Convolutional pose machines},\n      year = {2016}\n    }\n\nPaper links:\n- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\n    - [IEEE TPAMI](https://ieeexplore.ieee.org/document/8765346)\n    - [ArXiv](https://arxiv.org/abs/1812.08008)\n- [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809)\n- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)\n- [Convolutional Pose Machines](https://arxiv.org/abs/1602.00134)\n\n\n\n## License\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the [license](LICENSE) for further details. Interested in a commercial license? Check this [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740). For commercial queries, use the `Contact` section from the [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740) and also send a copy of that message to [Yaser Sheikh](mailto:yaser@cs.cmu.edu).\n"
 },
 {
  "repo": "vipstone/faceai",
  "language": "Python",
  "readme_contents": "[English Doc](README_en.md)\n# \u529f\u80fd #\n\n1. \u4eba\u8138\u68c0\u6d4b\u3001\u8bc6\u522b\uff08\u56fe\u7247\u3001\u89c6\u9891\uff09\n2. \u8f6e\u5ed3\u6807\u8bc6\n3. \u5934\u50cf\u5408\u6210\uff08\u7ed9\u4eba\u6234\u5e3d\u5b50\uff09\n4. \u6570\u5b57\u5316\u5986\uff08\u753b\u53e3\u7ea2\u3001\u7709\u6bdb\u3001\u773c\u775b\u7b49\uff09\n5. \u6027\u522b\u8bc6\u522b\n6. \u8868\u60c5\u8bc6\u522b\uff08\u751f\u6c14\u3001\u538c\u6076\u3001\u6050\u60e7\u3001\u5f00\u5fc3\u3001\u96be\u8fc7\u3001\u60ca\u559c\u3001\u5e73\u9759\u7b49\u4e03\u79cd\u60c5\u7eea\uff09\n7. \u89c6\u9891\u5bf9\u8c61\u63d0\u53d6\n8. \u56fe\u7247\u4fee\u590d\uff08\u53ef\u7528\u4e8e\u6c34\u5370\u53bb\u9664\uff09\n9. \u56fe\u7247\u81ea\u52a8\u4e0a\u8272\n10. \u773c\u52a8\u8ffd\u8e2a\uff08\u5f85\u5b8c\u5584\uff09\n11. \u6362\u8138\uff08\u5f85\u5b8c\u5584\uff09\n\n**\u67e5\u770b\u529f\u80fd\u9884\u89c8\u2193\u2193\u2193**\n\n# \u5f00\u53d1\u73af\u5883 #\n\n- Windows 10\uff08x64\uff09\n- Python 3.6.4\n- OpenCV 3.4.1\n- Dlib 19.8.1\n- face_recognition 1.2.2\n- keras 2.1.6\n- tensorflow 1.8.0\n- Tesseract OCR 4.0.0-beta.1\n\n\n# \u6559\u7a0b #\n\n[OpenCV\u73af\u5883\u642d\u5efa](doc/settingup.md)\n\n[Tesseract OCR\u6587\u5b57\u8bc6\u522b](doc/tesseractOCR.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/detectionOpenCV.md)\n\n[\u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/detectionDlib.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09](doc/videoOpenCV.md)\n\n[\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09](doc/videoDlib.md)\n\n[\u8138\u90e8\u8f6e\u5ed3\u7ed8\u5236](doc/faceRecognitionOutline.md)\n\n[\u6570\u5b57\u5316\u5986](doc/faceRecognitionMakeup.md)\n\n[\u89c6\u9891\u4eba\u8138\u8bc6\u522b](doc/faceRecognition.md)\n\n[\u5934\u50cf\u7279\u6548\u5408\u6210](doc/compose.md)\n\n[\u6027\u522b\u8bc6\u522b](doc/gender.md)\n\n[\u8868\u60c5\u8bc6\u522b](doc/emotion.md)\n\n[\u89c6\u9891\u5bf9\u8c61\u63d0\u53d6](https://github.com/vipstone/faceai/blob/master/doc/hsv-opencv.md)\n\n[\u56fe\u7247\u4fee\u590d](https://github.com/vipstone/faceai/blob/master/doc/inpaint.md)\n\n\n# \u5176\u4ed6\u6559\u7a0b #\n\n[Ubuntu apt-get\u548cpip\u6e90\u66f4\u6362](doc/ubuntuChange.md)\n\n[pip/pip3\u66f4\u6362\u56fd\u5185\u6e90\u2014\u2014Windows\u7248](doc/pipChange.md)\n\n[OpenCV\u6dfb\u52a0\u4e2d\u6587](doc/chinese.md)\n\n[\u4f7f\u7528\u9f20\u6807\u7ed8\u56fe\u2014\u2014OpenCV](https://github.com/vipstone/faceai/blob/master/doc/opencv/mouse.md)\n\n\n# \u529f\u80fd\u9884\u89c8 #\n\n**\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/face_recognition-outline.png\" width = \"250\" height = \"300\" alt=\"\u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3\" />\n\n----------\n\n**\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/dlib68.png\" width = \"230\" height = \"300\" alt=\"\u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6\" />\n\n----------\n\n**\u5934\u50cf\u7279\u6548\u5408\u6210**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/compose.png\" width = \"200\" height = \"300\" alt=\"\u5934\u50cf\u7279\u6548\u5408\u6210\"  />\n\n----------\n\n**\u6027\u522b\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/gender.png\" width = \"430\" height = \"220\" alt=\"\u6027\u522b\u8bc6\u522b\"  />\n\n----------\n\n**\u8868\u60c5\u8bc6\u522b**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/emotion.png\" width = \"250\" height = \"300\" alt=\"\u8868\u60c5\u8bc6\u522b\"  />\n\n----------\n\n**\u6570\u5b57\u5316\u5986**\n\n<img src=\"https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognitionMakeup-1.png\" width = \"450\" height = \"300\" alt=\"\u89c6\u9891\u4eba\u8138\u8bc6\u522b\"  />\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u68c0\u6d4b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/video-jiance.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](https://raw.githubusercontent.com/vipstone/faceai/master/res/faceRecognition.gif)\n\n----------\n\n**\u89c6\u9891\u4eba\u8138\u8bc6\u522b**\n\n![](http://icdn.apigo.cn/opencv-hsv.gif)\n\n----------\n\n**\u56fe\u7247\u4fee\u590d**\n\n![](http://icdn.apigo.cn/inpaint.png?2)\n\n----------\n\n**\u56fe\u7247\u81ea\u52a8\u4e0a\u8272**\n\n![](http://icdn.apigo.cn/colorize-faceai.png)\n\n----------\n\n# \u6280\u672f\u65b9\u6848 #\n\n\u6280\u672f\u5b9e\u73b0\u65b9\u6848\u4ecb\u7ecd\n\n\t\u4eba\u8138\u8bc6\u522b\uff1aOpenCV / Dlib\n\t\n\t\u4eba\u8138\u68c0\u6d4b\uff1aface_recognition\n\t\n\t\u6027\u522b\u8bc6\u522b\uff1akeras + tensorflow\n\t\n\t\u6587\u5b57\u8bc6\u522b\uff1aTesseract OCR\n\n\n### TODO ###\n\n\u6362\u8138\u2014\u2014\u5f85\u5b8c\u5584\n\n\u773c\u775b\u79fb\u52a8\u65b9\u5411\u68c0\u6d4b\u2014\u2014\u5f85\u5b8c\u5584\n\nDlib\u6027\u80fd\u4f18\u5316\u65b9\u6848\n\nDlib\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\nTesseract\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n\n# \u8d21\u732e\u8005\u540d\u5355\uff08\u7279\u522b\u611f\u8c22\uff09\t\n\n[archersmind](https://github.com/archersmind)\t\n\t\n[rishab-sharma](https://github.com/rishab-sharma)\n\n# \u5fae\u4fe1\u6253\u8d4f\n\n![\u5fae\u4fe1\u6253\u8d4f](http://icdn.apigo.cn/myinfo/wchat-pay.png)"
 },
 {
  "repo": "Hironsan/BossSensor",
  "language": "Python",
  "readme_contents": "# BossSensor\nHide your screen when your boss is approaching.\n\n## Demo\nThe boss stands up. He is approaching.\n\n![standup](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/standup.jpg)\n\nWhen he is approaching, the program fetches face images and classifies the image.\n \n![approaching](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/approach.jpg)\n\nIf the image is classified as the Boss, it will monitor changes.\n\n![editor](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/editor.jpg)\n\n## Requirements\n\n* WebCamera\n* Python3.5\n* OSX\n* Anaconda\n* Lots of images of your boss and other person image\n\nPut images into [data/boss](https://github.com/Hironsan/BossSensor/tree/master/data/boss) and [data/other](https://github.com/Hironsan/BossSensor/tree/master/data/other).\n\n## Usage\nFirst, Train boss image.\n\n```\n$ python boss_train.py\n```\n\n\nSecond, start BossSensor. \n\n```\n$ python camera_reader.py\n```\n\n## Install\nInstall OpenCV, PyQt4, Anaconda.\n\n```\nconda create -n venv python=3.5\nsource activate venv\nconda install -c https://conda.anaconda.org/menpo opencv3\nconda install -c conda-forge tensorflow\npip install -r requirements.txt\n```\n\nChange Keras backend from Theano to TensorFlow. \n\n## Licence\n\n[MIT](https://github.com/Hironsan/BossSensor/blob/master/LICENSE)\n\n## Author\n\n[Hironsan](https://github.com/Hironsan)\n"
 },
 {
  "repo": "PySimpleGUI/PySimpleGUI",
  "language": "Python",
  "readme_contents": "\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Logo%20with%20text%20for%20GitHub%20Top.png\" alt=\"\u4eba\u9593\u306e\u305f\u3081\u306ePythonGUI \">\n  <h2 align=\"center\">\u4eba\u9593\u306e\u305f\u3081\u306ePython\u306eGUI</h2>\n</p>\n\ntkinter\u3001Qt\u3001WxPython\u3001\u304a\u3088\u3073Remi(\u30d6\u30e9\u30a6\u30b6\u30d9\u30fc\u30b9)\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u3001\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\u306f\u521d\u5fc3\u8005\u304c\u7406\u89e3\u3059\u308bPython\u30b3\u30a2\u30c7\u30fc\u30bf\u578b (\u30ea\u30b9\u30c8\u3068\u8f9e\u66f8) \u3092\u4f7f\u7528\u3057\u3066\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u6e21\u3059\u30e2\u30c7\u30eb\u306b\u30a4\u30d9\u30f3\u30c8\u51e6\u7406\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3055\u3089\u306b\u5358\u7d14\u5316\u304c\u884c\u308f\u308c\u307e\u3059\u3002 \n\n\u30b3\u30fc\u30c9\u306f\u3088\u308a\u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b\u306e\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u6301\u3064*\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093*\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u7406\u89e3\u3057\u3084\u3059\u3044\u3082\u306e\u3067\u3059\u304c\u3001\u5fc5\u305a\u3057\u3082*\u5358\u7d14*\u306a\u554f\u984c\u3060\u3051\u306b\u5236\u9650\u3055\u308c\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u305f\u3060\u3057\u3001\u4e00\u90e8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fPySimpleGUI\u306b\u306f\u9069\u3057\u3066\u3044\u307e\u305b\u3093\u3002 \u5b9a\u7fa9\u4e0a\u3001PySimpleGUI \u306f\u57fa\u76e4\u3068\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304cPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u3066\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u9069\u3057\u3066\u3044\u306a\u3044\u304b\u3092\u6b63\u78ba\u306b\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3067\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8a73\u7d30\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\u3002\u30a8\u30af\u30bb\u30eb\u3092\u8a73\u7d30\u306b\u8907\u88fd\u3059\u308b\u3053\u3068\u306fPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u306a\u3044\u3082\u306e\u306e\u4f8b\u3067\u3059\u3002\n\n<hr>\n\n# \u7d71\u8a08 :chart_with_upwards_trend:\n\n## PyPI \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n<p align=\"center\">\ntkinter <img src=\"http://pepy.tech/badge/pysimplegui?color=blue&style=for-the-badge\" width=\"100px\"  align=\"center\">\ntkinter 2.7 <img src=\"https://pepy.tech/badge/pysimplegui27?color=blue&style=for-the-badge\"  align=\"center\"><br>\nQt <img src=\"https://pepy.tech/badge/pysimpleguiqt?color=blue&style=for-the-badge\"  align=\"center\">\nWxPython<img src=\"https://pepy.tech/badge/pysimpleguiwx?color=blue&style=for-the-badge\"  align=\"center\">\nWeb (Remi) <img src=\"https://pepy.tech/badge/pysimpleguiweb?color=blue&style=for-the-badge\"  align=\"center\">\n</p>\n\n\n## GitHub\n\n<p align=\"center\">\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\" width=\"180px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/issues-closed-raw/PySimpleGUI/PySimpleGUI?color=blue&style=for-the-badge\" alt=\"img\"  width=\"200px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/commit-activity/m/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"  width=\"260px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/last-commit/PySimpleGUI/PySimpleGUI.svg?color=blue&style=for-the-badge\" alt=\"img\"width=\"200px\"></a>\n<a href=\"\"><img src=\"http://ForTheBadge.com/images/badges/makes-people-smile.svg\" alt=\"img\"width=\"190px\"></a>\n<a href=\"\"><img src=\"https://img.shields.io/github/stars/PySimpleGUI/PySimpleGUI.svg?style=social&label=Star&maxAge=2592000\" alt=\"img\"width=\"140x\"></a>\n</p>\n\n\n<p align=\"center\">\n  <img src=\"https://github-readme-stats.vercel.app/api/?username=PySimpleGUI&bg_color=3e7bac&title_color=ffdd55&icon_color=ffdd55&text_color=ffdd55&show_icons=true&count_private=true\">\n</p>\n\n## \u6700\u65b0\u306e PyPI \u30d0\u30fc\u30b8\u30e7\u30f3\n\n\n<p align=\"center\">\ntkinter\n<a href=\"pypi tkinter\"><img src=\"https://img.shields.io/pypi/v/pysimplegui.svg?style=for-the-badge&color=red\" alt=\"img\" align=\"center\" width=\"150px\"></a>\nQt\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiqt.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWeb\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiweb.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\nWxPython\n<a href=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"><img src=\"https://img.shields.io/pypi/v/pysimpleguiwx.svg?style=for-the-badge\"  alt=\"img\" align=\"center\" width=\"150px\"></a>\n</p>\n\n<hr>\n\n# PySimpleGUI\u3068\u306f\u4f55\u3067\u3059\u304b:question:\n\nPySimpleGUI\u306f\u3042\u3089\u3086\u308b\u30ec\u30d9\u30eb\u306ePython\u30d7\u30ed\u30b0\u30e9\u30de\u304cGUI\u3092\u4f5c\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308bPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080 \u300c\u30ec\u30a4\u30a2\u30a6\u30c8\u300d\u3092\u4f7f\u7528\u3057\u3066 GUI \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u6307\u5b9a\u3057\u307e\u3059 (PySimpleGUI \u3067\u306f\u300c\u30a8\u30ec\u30e1\u30f3\u30c8\u300d\u3068\u547c\u3073\u307e\u3059)\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b4\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u8868\u793a\u3084\u64cd\u4f5c\u3059\u308b\u306e\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u30b5\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001tkinter\u3001Qt\u3001WxPython\u3001WxPython\u307e\u305f\u306fRemi\u304c\u542b\u307e\u308c\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u300c\u30e9\u30c3\u30d1\u30fc\u300d\u3068\u3044\u3046\u7528\u8a9e\u304c\u4f7f\u308f\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\n\nPySimpleGUI\u306f\u300c\u30dc\u30a4\u30e9\u30fc\u30d7\u30ec\u30fc\u30c8\u30b3\u30fc\u30c9\u300d\u306e\u591a\u304f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u57fa\u3068\u306a\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082\u5358\u7d14\u3067\u77ed\u304b\u3044\u30b3\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002\n\u3055\u3089\u306b\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306f\u3001\u671b\u3093\u3060\u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u3067\u304d\u308b\u3060\u3051\u5c11\u306a\u304f\u3059\u308b\u3088\u3046\u306b\u5358\u7d14\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u3082\u3088\u308a\u307e\u3059\u304c\u3001PySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u76f4\u63a5\u4f7f\u7528\u3057\u3066\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u3088\u308a\u3082\u3001\u30b3\u30fc\u30c9\u306e\u91cf\u306f1/2\u304b\u30891/10\u7a0b\u5ea6\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u76ee\u6a19\u306f\u4f7f\u7528\u3057\u3066\u3044\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u4e0a\u306e\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3084\u30b3\u30fc\u30c9\u3092\u30ab\u30d7\u30bb\u30eb\u5316/\u975e\u8868\u793a\u306b\u3059\u308b\u3053\u3068\u3067\u3059\u304c\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3084\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u30a2\u30af\u30bb\u30b9\u3067\u304d\u307e\u3059\u3002\n\u8a2d\u5b9a\u3084\u6a5f\u80fd\u304c\u307e\u3060\u516c\u958b\u3055\u308c\u3066\u304a\u3089\u305a\u3001PySimpleGUI API\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u906e\u65ad\u3055\u308c\u3066\u307e\u305b\u3093\u3002PySimpleGUI\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u81ea\u4f53\u3092\u76f4\u63a5\u5909\u66f4\u305b\u305a\u306b\u6a5f\u80fd\u3092\u62e1\u5f35\u3067\u304d\u307e\u3059\u3002\n## \u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7\u300d\u3092\u57cb\u3081\u308b\n\nPython \u306f\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u591a\u304f\u306e\u4eba\u3005\u3092\u62db\u3044\u3066\u3044\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6570\u3068\u6271\u3046\u9818\u57df\u306e\u7bc4\u56f2\u306f\u6c17\u304c\u9060\u304f\u306a\u308a\u307e\u3059 \u3057\u304b\u3057\u591a\u304f\u306e\u5834\u5408\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3068\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u4e00\u63e1\u308a\u306e\u4eba\u3005\u4ee5\u5916\u306e\u624b\u306e\u5c4a\u304b\u306a\u3044\u3068\u3053\u308d\u306b\u3042\u308a\u307e\u3059\u3002Python \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u5927\u534a\u306f\"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\"\u30d9\u30fc\u30b9\u3067\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u7cfb\u306e\u4eba\u306f\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3092\u4ecb\u3057\u3066\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u3053\u3068\u306b\u6163\u308c\u3066\u3044\u3066\u3001\u3053\u306e\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u307b\u3068\u3093\u3069\u306e\u300c\u666e\u901a\u306e\u4eba\u300d\u306f\u554f\u984c\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30b8\u30bf\u30eb\u30fb\u30c7\u30a3\u30d0\u30a4\u30c9\u3001\u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7 \u300d\u304c\u751f\u307f\u51fa\u3055\u308c\u307e\u3059\u3002\n\u30d7\u30ed\u30b0\u30e9\u30e0\u306bGUI\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u591a\u304f\u306e\u4eba\u306b\u77e5\u3063\u3066\u3082\u3089\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u89aa\u3057\u307f\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002GUI\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u6163\u308c\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u3067\u3042\u3063\u3066\u3082\u3001\u3044\u304f\u3064\u304b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u64cd\u4f5c\u3092\u7c21\u5358\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u305d\u3057\u3066\u6700\u5f8c\u306bGUI\u3092\u5fc5\u8981\u3068\u3059\u308b\u554f\u984c\u3082\u3042\u308a\u307e\u3059\u3002   \n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/GUI%20Gap%202020.png\" width=\"600px\">\n</p>\n\n\n<hr>\n\n# \u79c1\u306b\u3064\u3044\u3066 :wave:\n\u3053\u3093\u306b\u3061\u306f\uff01 \u79c1\u306f\u30de\u30a4\u30af\u3067\u3059\u3002 GitHub\u306ePySimpleGUI\u3067\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066PySimpleGUI\u3092\u7d99\u7d9a\u7684\u306b\u524d\u9032\u3055\u305b\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u663c\u3068\u591c\u3068\u9031\u672b\u3082\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068PySimpleGUI\u30e6\u30fc\u30b6\u30fc\u306b\u6367\u3052\u3066\u304d\u307e\u3057\u305f\u3002\u79c1\u305f\u3061\u306e\u6210\u529f\u306f\u6700\u7d42\u7684\u306b\u5171\u6709\u3055\u308c\u307e\u3059\u3002 \u3042\u306a\u305f\u304c\u6210\u529f\u3057\u305f\u3068\u304d\u306b\u79c1\u306f\u6210\u529f\u3057\u3066\u3044\u307e\u3059\u3002\n\nPython\u3067\u306f\u76f8\u5bfe\u7684\u306a\u65b0\u4eba\u3067\u3059\u304c\u300170\u5e74\u4ee3\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u3002 \u79c1\u306e\u30ad\u30e3\u30ea\u30a2\u306e\u5927\u534a\u306f\u30b7\u30ea\u30b3\u30f3\u30d0\u30ec\u30fc\u3067\u306e\u88fd\u54c1\u958b\u767a\u306b\u8cbb\u3084\u3055\u308c\u307e\u3057\u305f\u3002PySimpleGUI\u306b\u306f\u81ea\u5206\u304c\u958b\u767a\u3057\u305f\u4f01\u696d\u88fd\u54c1\u3068\u540c\u3058\u3088\u3046\u306a\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30ea\u30ba\u30e0\u3068\u732e\u8eab\u3092\u3082\u305f\u3089\u3057\u307e\u3059\u3002\u4eca\u3001\u3042\u306a\u305f\u306f\u79c1\u306e\u9867\u5ba2\u3067\u3059\u3002\n\n\n## \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u6a19 :goal_net:\n\nPySimpleGUI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u91cd\u8981\u306a\u76ee\u6a19\u306f\u4ee5\u4e0b\u306e2\u3064\u3067\u3059\u3002\n\n* \u697d\u3057\u3080\u3053\u3068\n* \u3042\u306a\u305f\u306e\u6210\u529f\n\n\u771f\u9762\u76ee\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30b4\u30fc\u30eb\u3068\u3057\u3066**\u697d\u3057\u3080**\u3068\u3044\u3046\u306e\u306f\u5909\u306b\u805e\u3053\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u306f\u771f\u9762\u76ee\u306a\u76ee\u6a19\u3067\u3059\u3002\u79c1\u306f\u3053\u308c\u3089\u306eGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u304f\u3053\u3068\u306f\u3068\u3066\u3082\u697d\u3057\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u306e\u7406\u7531\u306e1\u3064\u306f\u3001\u5b8c\u5168\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u306b\u304b\u304b\u308b\u6642\u9593\u304c\u3044\u304b\u306b\u77ed\u3044\u304b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3082\u3057\u79c1\u9054\u304c\u30d7\u30ed\u30bb\u30b9\u3092\u697d\u3057\u3093\u3067\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u8ab0\u304b\u304c\u3042\u304d\u3089\u3081\u3066\u3044\u307e\u3059\u3002\n\n\u81a8\u5927\u306a\u91cf\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3001\u30af\u30c3\u30af\u30d6\u30c3\u30af\u3001\u3059\u3050\u306b\u4f7f\u3048\u308b100\u7a2e\u985e\u4ee5\u4e0a\u306e\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3001\u8a73\u7d30\u306a\u30b3\u30fc\u30eb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3001YouTube\u306e\u30d3\u30c7\u30aa\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u306eTrinket\u306e\u30c7\u30e2\u306a\u3069\u3001\u3059\u3079\u3066\u304c\u697d\u3057\u3044\u4f53\u9a13\u3092\u751f\u307f\u51fa\u3059\u305f\u3081\u306b\u4f5c\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n**\u3042\u306a\u305f\u306e\u6210\u529f**\u306f\u5171\u901a\u306e\u76ee\u6a19\u3067\u3059\u3002 PySimpleGUI \u306f\u958b\u767a\u8005\u5411\u3051\u306b\u69cb\u7bc9\u3055\u308c\u307e\u3057\u305f\u3002\u3042\u306a\u305f\u306f\u79c1\u306e\u4ef2\u9593\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u3068PySimpleGUI\u306e\u5171\u540c\u4f5c\u696d\u306e\u7d50\u679c\u3092\u898b\u308b\u306e\u306f\u4e88\u60f3\u5916\u306e\u5831\u916c\u3067\u3057\u305f\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3084\u305d\u306e\u4ed6\u306e\u8cc7\u6599\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u69cb\u7bc9\u306b\u5f79\u7acb\u3066\u3066\u304f\u3060\u3055\u3044\u3002\u30c8\u30e9\u30d6\u30eb\u306b\u906d\u9047\u3057\u305f\u5834\u5408\u306f\u3001[PySimpleGUI GitHub \u306e\u554f\u984c](http://Issues.PySimpleGUI.org)\u3067Issue \u3092\u958b\u3044\u3066\u30d8\u30eb\u30d7\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u30b5\u30dd\u30fc\u30c8\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002\n\n<hr>\n\n# \u6559\u80b2\u30ea\u30bd\u30fc\u30b9 :books:\n\nwww.PySimpleGUI.org \u306f\u899a\u3048\u3084\u3059\u304f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5834\u6240\u3067\u3059\u3002\u4e0a\u90e8\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u7570\u306a\u308b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8868\u3059\u30bf\u30d6\u304c\u3042\u308a\u307e\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u300cRead The Docs\u300d\u306b\u8a18\u8f09\u3055\u308c\u3066\u304a\u308a\u3001\u5404\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u76ee\u6b21\u304c\u3042\u308a\u691c\u7d22\u304c\u7c21\u5358\u3067\u3059\u3002\n\n\u6570\u767e\u30da\u30fc\u30b8\u306e\u6587\u66f8\u5316\u3055\u308c\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u6570\u767e\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u3042\u308a\u3001\u3042\u306a\u305f\u304c\u975e\u5e38\u306b\u901f\u304f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\u5358\u4e00\u306e GUI \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5b66\u3076\u306e\u306b\u6570\u65e5\u307e\u305f\u306f\u6570\u9031\u9593\u6295\u8cc7\u3059\u308b\u3088\u308a\u3082\u3001PySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u5348\u5f8c\u4e00\u56de\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b8c\u6210\u3055\u305b\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\n## \u4f8b 1 - \u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u30bf\u30a4\u30d7\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c1\u56de\u8868\u793a\u3055\u308c\u3066\u53ce\u96c6\u3055\u308c\u305f\u5024\u304c\u9589\u3058\u3089\u308c\u308b\u305f\u3081\u3001\u300c\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u300d\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u547c\u3070\u308c\u307e\u3059\u3002 \u30ef\u30fc\u30c9\u30d7\u30ed\u30bb\u30c3\u30b5\u306e\u3088\u3046\u306b\u9577\u3044\u9593\u958b\u3044\u305f\u307e\u307e\u306b\u306a\u3063\u3066\u3044\u307e\u305b\u3093\u3002\n### \u5358\u7d14\u306aPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u89e3\u5256\u5b66\n\nPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f5\u3064\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\n\n\n\n```python\nimport PySimpleGUI as sg                                 # \u30d1\u30fc\u30c8 1 - \u30a4\u30f3\u30dd\u30fc\u30c8\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [  [sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],     # \u30d1\u30fc\u30c8 2 - \u30ec\u30a4\u30a2\u30a6\u30c8\n            [sg.Input()],\n            [sg.Button('\u306f\u3044')] ]\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb', layout)      # \u30d1\u30fc\u30c8 3- \u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\n                                                \n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nevent, values = window.read()                   # \u30d1\u30fc\u30c8 4- \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u307e\u305f\u306f Window.read \u547c\u3073\u51fa\u3057\n\n# \u53ce\u96c6\u3055\u308c\u305f\u60c5\u5831\u3067\u4f55\u304b\u3092\u3059\u308b\nprint('\u30cf\u30ed\u30fc ', values[0], \"! PySimpleGUI\u3092\u8a66\u3057\u3066\u304f\u308c\u3066\u3042\u308a\u304c\u3068\u3046\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()                                  #\u30d1\u30fc\u30c8 5 - \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\n```\n\n\u30b3\u30fc\u30c9\u306f\u3001\u4ee5\u4e0b\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank1.jpg\">\n</p>\n\n\n<hr>\n\n## \u4f8b 2 - \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\u304b\u3001\u307e\u305f\u306f [\u7d42\u4e86] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u307e\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u753b\u9762\u4e0a\u306b\u6b8b\u308a\u307e\u3059\u3002 \u5148\u307b\u3069\u898b\u305f\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e3b\u306a\u9055\u3044\u306f\u3001\u300c\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u300d\u306e\u8ffd\u52a0\u3067\u3059\u3002\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u304b\u3089\u30a4\u30d9\u30f3\u30c8\u3068\u5165\u529b\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002 \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4e2d\u5fc3\u306f\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],\n          [sg.Input(key='-\u5165\u529b-')],\n          [sg.Text(size=(55,1), key='-\u51fa\u529b-')],\n          [sg.Button('\u306f\u3044'), sg.Button('\u7d42\u4e86')]]\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb',\u30ec\u30a4\u30a2\u30a6\u30c8)\n\n# \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nwhile True:\n    event, values = window.read()\n# \u30e6\u30fc\u30b6\u30fc\u304c\u7d42\u4e86\u3057\u305f\u3044\u306e\u304b\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n\n    # Output a message to the window\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()\n```\n\n\u4ee5\u4e0b\u306f\u306f\u3001\u4f8b2\u304c\u4f5c\u6210\u3059\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourNameBlank.jpg\">\n</p>\n\n\n\n\u5165\u529b\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u5024\u3092\u5165\u529b\u3057\u3066 [OK] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u5f8c\u306e\u8868\u793a\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorld1.jpg\">\n</p>\n\n\n\u3053\u306e\u4f8b\u3068\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u9055\u3044\u306b\u6c17\u3065\u304f\u3067\u3057\u3087\u3046\u3002 \u7279\u306b2\u3064\u306e\u5909\u66f4\u304c\u91cd\u8981\u3067\u3059\u3002 1\u3064\u306f`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u3068`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e1\u3064\u306b`key`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u300ckey\u300d\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u540d\u524d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 \u307e\u305f\u306f\u3001Python\u306e\u8a00\u8449\u3067\u306f\u3001\u8f9e\u66f8\u30ad\u30fc\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 `Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ad\u30fc\u306f\u3001\u30b3\u30fc\u30c9\u306e\u5f8c\u534a\u3067\u8f9e\u66f8\u30ad\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\n\n\n\u3082\u30461\u3064\u306e\u9055\u3044\u306f\u3001\u3053\u306e `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u8ffd\u52a0\u3067\u3059:\n```python\n          [sg.Text(size=(40,1), key='-OUTPUT-')],\n```\n\n\u3059\u3067\u306b\u30ab\u30d0\u30fc\u3057\u3066\u3044\u308b\u300c\u30ad\u30fc\u300d\u3068\u3044\u30462\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002 `Size`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u6587\u5b57\u6570\u306e\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5e4540\u6587\u5b57\u3001\u9ad8\u30551\u6587\u5b57\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u306e\u6587\u5b57\u5217\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u7a7a\u767d\u306b\u306a\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4f5c\u6210\u3055\u308c\u305f\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306f\u7a7a\u767d\u884c\u304c\u7c21\u5358\u306b\u898b\u308c\u307e\u3059\u3002\n\n\u307e\u305f \u3001[\u7d42\u4e86]\u30dc\u30bf\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306f\u3001\u304a\u306a\u3058\u307f\u306e`window.read()`\u547c\u3073\u51fa\u3057\u3057\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u8aad\u307f\u8fbc\u3093\u3060\u5f8c\u306b\u7d9a\u304f\u306e\u306f\u3001\u3053\u306eif\u6587\u3067\u3059\u3002\n```python\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n```\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c \u300cX\uff08\u9589\u3058\u308b\uff09\u300d \u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u305f\u304b\u3001\u307e\u305f\u306f\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306e\u3044\u305a\u308c\u304b\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u30b3\u30fc\u30c9\u306f\u30a4\u30d9\u30f3\u30c8 \u30eb\u30fc\u30d7\u304b\u3089\u629c\u3051\u51fa\u3057\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305a\u3001\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u304c\u30af\u30ea\u30c3\u30af\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u52d5\u4f5c\u304c\u7d99\u7d9a\u3055\u308c\u307e\u3059\u3002 \u8d77\u3053\u308a\u3046\u308b\u552f\u4e00\u306e\u4e8b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u300cOK\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u3053\u3068\u3067\u3059\u3002 \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306e\u6700\u5f8c\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\n\n\n\n```python\n    window['-OUTPUT-'].update('\u30cf\u30ed\u30fc  ' + values['-INPUT-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n```\n\n\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u3001`-OUTPUT-`\u30ad\u30fc \u3092\u6301\u3064`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u6587\u5b57\u5217\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002`window['-OUTPUT-']`\u306f`-OUTPUT-`\u30ad\u30fc\u3092\u6301\u3064\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u691c\u7d22\u3057\u307e\u3059\u3002 \u30ad\u30fc\u306f\u3001\u7a7a\u767d\u306e`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5c5e\u3057\u307e\u3059\u3002 \u30a8\u30ec\u30e1\u30f3\u30c8\u304c\u691c\u7d22\u304b\u3089\u8fd4\u3055\u308c\u308b\u3068\u3001\u305d\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update`\u30e1\u30bd\u30c3\u30c9\u304c\u547c\u3073\u51fa\u3055\u308c\u307e\u3059\u3002 \u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306f`update`\u30e1\u30bd\u30c3\u30c9\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u5024\u3084\u69cb\u6210\u3092\u5909\u66f4\u3057\u305f\u308a\u3059\u308b\u306e\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u30c6\u30ad\u30b9\u30c8\u3092\u9ec4\u8272\u306b\u3057\u305f\u3044\u5834\u5408\u306f\u3001`update`\u30e1\u30bd\u30c3\u30c9\u306b`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u51e6\u7406\u3057\u307e\u3059\u3002\n```python\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\", text_color='yellow')\n```\n\n`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u305f\u5f8c\u3001\u3053\u308c\u304c\u65b0\u3057\u3044\u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/HelloWorldYellow.jpg\">\n</p>\n\n\n\u5404\u30a8\u30ec\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306f[call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](http://calls.PySimpleGUI.org)\u3068docstrings \u3068\u4e21\u65b9\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306b\u306f\u3001\u5229\u7528\u53ef\u80fd\u306a\u3059\u3079\u3066\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u8c4a\u5bcc\u306a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002 `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update'`\u30e1\u30bd\u30c3\u30c9\u3092\u691c\u7d22\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9a\u7fa9\u304c\u898b\u3064\u304b\u308a\u307e\u3059:\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/TextUpdate.jpg\">\n</p>\n\n\n\u3054\u89a7\u306e\u3088\u3046\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u3082\u306e\u306f\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306fPySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u7c21\u5358\u306b\u3059\u308b\u8cb4\u91cd\u306a\u30ea\u30bd\u30fc\u30b9\u3067\u3059\u3002\n\n<hr>\n\n##\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u9762\u767d\u3044\u3067\u3059 LOL! :laughing:\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u300c\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u300d(LOL)\u3067\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u300c\u884c\u300d\u306b\u5206\u5272\u3055\u308c\u307e\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5404\u884c\u306f\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u30ea\u30b9\u30c8\u306b\u306a\u308a\u307e\u3059\u3002 \u3059\u3079\u3066\u306e\u30ea\u30b9\u30c8\u3092\u9023\u7d50\u3059\u308b\u3068\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002...\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\u884c\u306e\u5b9a\u7fa9\u65b9\u6cd5\u3092\u7c21\u5358\u306b\u78ba\u8a8d\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5404\u884c\u306b\u8ffd\u52a0\u306e 'Text' \u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u305f\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u3001\u4ee5\u524d\u3068\u540c\u3058\u3067\u3059:\n\n```python\nlayout = [  [sg.Text('\u30e9\u30a4\u30f3 1'), sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\")],\n            [sg.Text('\u30e9\u30a4\u30f3 2'), sg.Input()],\n            [sg.Text('\u30e9\u30a4\u30f3 3'), sg.Button('\u306f\u3044')] ]\n```\n\n\u3053\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5404\u884c\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u306e\u884c\u306b\u8868\u793a\u3055\u308c\u308b\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/rows.jpg\">\n</p>\n\n\n\n\u30ea\u30b9\u30c8\u3092\u4f7f\u7528\u3057\u3066GUI\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u4ed6\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066GUI\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u884c\u3046\u65b9\u6cd5\u306b\u304f\u3089\u3079\u3066\u3044\u304f\u3064\u304b\u5927\u304d\u306a\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001Python \u306e\u30ea\u30b9\u30c8\u5185\u5305\u8868\u8a18\u3092\u5229\u7528\u3057\u3066\u30011 \u884c\u306e\u30b3\u30fc\u30c9\u3067\u30dc\u30bf\u30f3\u306e\u30b0\u30ea\u30c3\u30c9\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\n\n\u6b21\u306e3\u884c\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\nlayout = [[sg.Button(f'{row}, {col}') for col in range(4)] for row in range(4)]\n\nevent, values = sg.Window('List Comprehensions', layout).read(close=True)\n```\n\n\u30dc\u30bf\u30f3\u306e4 x 4\u30b0\u30ea\u30c3\u30c9\u3092\u6301\u3064\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/4x4grid.jpg\">\n</p>\n\n\u300c\u697d\u3057\u3080\u300d\u304c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u306e\uff11\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Python\u306e\u5f37\u529b\u306a\u57fa\u672c\u6a5f\u80fd\u3092GUI\u306e\u554f\u984c\u306b\u76f4\u63a5\u9069\u7528\u3059\u308b\u306e\u306f\u697d\u3057\u3044\u3067\u3059\u3002GUI\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u306e\u30da\u30fc\u30b8\u306e\u4ee3\u308f\u308a\u306b\u3001\u6570\u884c (\u307e\u305f\u306f\u591a\u304f\u306e\u5834\u54081\u884c) \u306e\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n## \u30b3\u30fc\u30c9\u306e\u6298\u308a\u305f\u305f\u307f\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30b3\u30fc\u30c9\u30921\u884c\u306e\u30b3\u30fc\u30c9\u306b\u51dd\u7e2e\u3067\u304d\u307e\u3059\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5b9a\u7fa9\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u3001\u8868\u793a\u3001\u304a\u3088\u3073\u30c7\u30fc\u30bf\u53ce\u96c6\u306f\u3059\u3079\u3066\u3001\u6b21\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u307e\u3059\u3002\n```python\nevent, values = sg.Window('Window Title', [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],[sg.Input()],[sg.Button('\u306f\u3044')]]).read(close=True)\n```\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/WhatsYourName.jpg\">\n</p>\n\n\n\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u8868\u793a\u3055\u308c\u3001PySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u793a\u3059\u4f8b\u3068\u540c\u3058\u5024\u304c\u8fd4\u3055\u308c\u307e\u3059\u3002 \u975e\u5e38\u306b\u5c11\u306a\u3044\u91cf\u3067\u591a\u304f\u306e\u3053\u3068\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001Python\u30b3\u30fc\u30c9\u306b\u3059\u3070\u3084\u304f\u7c21\u5358\u306bGUI\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u9078\u629e\u3092\u5f97\u305f\u3044\u5834\u5408\u306f\u30011\u30da\u30fc\u30b8\u306e\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u77ed\u7e2e\u30a8\u30a4\u30ea\u30a2\u30b9\u3092\u4f7f\u7528\u3057\u3066\u3088\u308a\u5c11\u306a\u3044\u6587\u5b57\u6570\u3067\u30b3\u30fc\u30c9\u306e\u30b9\u30da\u30fc\u30b9\u3092\u3055\u3089\u306b\u77ed\u304f\u3067\u304d\u307e\u3059\u3002  \u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u306f\u3001\u4f7f\u7528\u3067\u304d\u308b\u77ed\u3044\u540d\u524d\u304c\uff11\u3064\u4ee5\u4e0a\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5358\u306b`T`\u3068\u3057\u3066\u66f8\u3051\u307e\u3059\u3002`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f `I`\u3001`Button`\u306f`B`\u3068\u66f8\u3051\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306b\u306b\u306a\u308a\u307e\u3059:\n\n```python\nevent, values = sg.Window('Window Title', [[sg.T(\"\u3042\u306a\u305f\u306e\u540d\u524d\u306f\u4f55\u3067\u3059\u304b?\")],[sg.I()],[sg.B('\u306f\u3044')]]).read(close=True)\n```\n\n\n### \u30b3\u30fc\u30c9\u306e\u79fb\u690d\u6027\n\nPySimpleGUI\u306f\u73fe\u5728\u30014\u3064\u306ePython\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002 \u4f7f\u7528\u3059\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001import\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u6307\u5b9a\u3057\u307e\u3059\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3092\u5909\u66f4\u3059\u308b\u3068\u3001\u57fa\u672c\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3088\u3063\u3066\u306f\u3001\u5225\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306fimport \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u4ee5\u5916\u306e\u5909\u66f4\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002 \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u3001\u30a4\u30f3\u30dd\u30fc\u30c8\u3092`PySimpleGUI`\u304b\u3089`PySimpleGUIQt`\u3001`PySimpleGUIWx`\u3001`PySimpleGUIWeb`\u3001`PySimpleGUIWeb`\u306b\u5909\u66f4\u3059\u308b\u3068\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\n\n| \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 | \u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6 |\n|--|--|\n| PySimpleGUI |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-tkinter.jpg) |\n| PySimpleGUIQt |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Qt.jpg) |\n| PySimpleGUIWx |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-WxPython.jpg) |\n| PySimpleGUIWeb |  ![](https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ex1-Remi.jpg) |\n\n\n\nGUI\u306e\u30b3\u30fc\u30c9\u3092\u3042\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u5225\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u79fb\u690d\u3059\u308b (\u4f8b\u3048\u3070\u3001\u30b3\u30fc\u30c9\u3092tkinter\u304b\u3089Qt\u306b\u79fb\u52d5\u3059\u308b) \u306b\u306f\u3001\u901a\u5e38\u306f\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u63db\u3048\u304c\u5fc5\u8981\u3067\u3059\u3002  PySimpleGUI \u306f\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u306e\u7c21\u5358\u306a\u79fb\u52d5\u3092\u53ef\u80fd\u306b\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3044\u304f\u3064\u304b\u306e\u5909\u66f4\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u76ee\u7684\u306f\u6700\u5c0f\u9650\u306e\u5909\u66f4\u3067\u79fb\u690d\u6027\u306e\u9ad8\u3044\u30b3\u30fc\u30c9\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \n\n\u30b7\u30b9\u30c6\u30e0 \u30c8\u30ec\u30a4 \u30a2\u30a4\u30b3\u30f3\u306a\u3069\u306e\u4e00\u90e8\u306e\u6a5f\u80fd\u306f\u3001\u3059\u3079\u3066\u306e\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u306a\u3044\u3067\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u6a5f\u80fd\u306fQt\u304a\u3088\u3073WxPython\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306ftkinter\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u306f\u3001PySimpleGUIWeb\u30dd\u30fc\u30c8\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u307e\u305b\u3093\u3002\n\n##  \u30e9\u30f3\u30bf\u30a4\u30e0\u74b0\u5883\n\n|\u74b0\u5883 |\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b |\n|--|--|\n|\u30d1\u30a4\u30bd\u30f3| Python  3.4+ |\n|\u30aa\u30da\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0 \u30b7\u30b9\u30c6\u30e0 |\u30a6\u30a3\u30f3\u30c9\u30a6\u30ba, Linux, \u30de\u30c3\u30af |\n|\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2 |\u30c7\u30b9\u30af\u30c8\u30c3\u30d7 PC, \u30ce\u30fc\u30c8\u30d1\u30bd\u30b3\u30f3, \u30e9\u30ba\u30d9\u30ea\u30fc\u30d1\u30a4, PyDroid3 \u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30a2\u30f3\u30c9\u30ed\u30a4\u30c9\u30c7\u30d0\u30a4\u30b9 |\n|\u30aa\u30f3\u30e9\u30a4\u30f3 |repli.it\u3001Trinket.com (\u3069\u3061\u3089\u3082\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067tkinter\u3092\u5b9f\u884c\u3059\u308b) |\n|GUI \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af |tkinter, pyside2, WxPython, Remi |\n\n\n## \u7d71\u5408\n200 \u4ee5\u4e0a\u306e\u300c\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u300d\u306e\u4e2d\u306b\u306f\u3001\u591a\u304f\u306e\u4eba\u6c17\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u3092GUI\u306b\u7d71\u5408\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u304c\u898b\u3064\u304b\u308a\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306bMatplotlib\u306e\u63cf\u753b\u3092\u57cb\u3081\u8fbc\u307f\u305f\u3044\u3067\u3059\u304b?  \u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3001 \u30c7\u30e2\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5373\u5ea7\u306b\u3042\u306a\u305f\u306e\u5922\u306eMatplotlib\u306e\u63cf\u753b\u3092\u3042\u306a\u305f\u306eGUI\u306b\u7d44\u307f\u8fbc\u3081\u307e\u3059\u3002  \n\n\u3053\u308c\u3089\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3084\u305d\u306e\u4ed6\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3001\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30c7\u30e2\u30ec\u30dd\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001GUI\u306b\u5165\u308c\u308b\u6e96\u5099\u304c\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\n|\u30d1\u30c3\u30b1\u30fc\u30b8 |\u8aac\u660e |\n|--|--|\n Matplotlib |\u30b0\u30e9\u30d5\u3084\u30d7\u30ed\u30c3\u30c8\u306e\u591a\u304f\u306e\u7a2e\u985e |\n OpenCV |\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3 (AI\u3067\u3088\u304f\u4f7f\u7528) |\n VLC |\u30d3\u30c7\u30aa\u518d\u751f |\n pymunk |\u7269\u7406\u30a8\u30f3\u30b8\u30f3|\n psutil |\u30b7\u30b9\u30c6\u30e0\u74b0\u5883\u306e\u7d71\u8a08 |\n prawn |Reddit  API |\njson |PySimpleGUI \u306f\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u8a2d\u5b9a\u300d\u3092\u683c\u7d0d\u3059\u308b\u7279\u5225\u306aAPI\u3092\u30e9\u30c3\u30d7\u3057\u307e\u3059\u3002 |\n weather |\u304a\u5929\u6c17\u30a2\u30d7\u30ea\u3092\u4f5c\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u5929\u6c17API\u3068\u7d71\u5408 |\n mido |MIDI \u518d\u751f |\n beautiful soup |\u30a6\u30a7\u30d6\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0 (GitHub issue\u30a6\u30a9\u30c3\u30c1\u30e3\u30fc\u3067\u306e\u4f8b) |\n\n<hr>\n\n# \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb :floppy_disk:\n\n\nPySimpleGUI\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4e00\u822c\u7684\u306b2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\n\n1. PyPI\u304b\u3089pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\n2. PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u914d\u7f6e\u3057\u307e\u3059\n\n\n### Pip\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n\u73fe\u5728\u63d0\u6848\u3055\u308c\u3066\u3044\u308b`pip`\u30b3\u30de\u30f3\u30c9\u3092\u547c\u3073\u51fa\u3059\u65b9\u6cd5\u306f\u3001Python\u3092\u4f7f\u3063\u3066\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u4ee5\u524d\u306f\u3001`pip`\u307e\u305f\u306f`pip3`\u30b3\u30de\u30f3\u30c9\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3/\u30b7\u30a7\u30eb\u4e0a\u3067\n\u76f4\u63a5\u5b9f\u884c\u3055\u308c\u307e\u3057\u305f\u3002 \u63d0\u6848\u3055\u308c\u305f\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\nWindows \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python -m pip install PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python3 -m pip install PySimpleGUI`\n\n`pip`\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3059\u308b\u306b\u306f\u3001\u5358\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf`--upgrade --no-cache-dir`\u3092\u6307\u5b9a\u3059\u308b\u3060\u3051\u3067\u3059\u3002\n\nWindows \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\nLinux \u304a\u3088\u3073 MacOS \u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python3 -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\n\n### \u5358\u4e00\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nPySimpleGUI\u306fRaspberry Pi \u306e\u3088\u3046\u306a\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306b\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u30b7\u30b9\u30c6\u30e0\u306b\u3082\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u5358\u4e00\u306e .py \u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u306b\u7f6e\u304f\u3060\u3051\u3067\u3059\u3002Python \u306f\u30a4\u30f3\u30dd\u30fc\u30c8\u6642\u306b\u30ed\u30fc\u30ab\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n.py\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f\u3001PyPI\u304b\u3089\u5165\u624b\u3059\u308b\u304b\u3001\u6700\u65b0\u306e\u672a\u30ea\u30ea\u30fc\u30b9\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u306fGitHub\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\nPyPI\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306b\u306f\u3001wheel\u307e\u305f\u306f .gz \u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u89e3\u51cd\u3057\u307e\u3059\u3002 .whl\u30d5\u30a1\u30a4\u30eb\u3092.zip\u306b\u30ea\u30cd\u30fc\u30e0\u3059\u308b\u3068\u3001\u901a\u5e38\u306ezip\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u3058\u3088\u3046\u306b\u958b\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d5\u30a9\u30eb\u30c0\u306e\u4e2d\u306bPySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5b8c\u4e86\u3067\u3059\u3002\n\ntkinter \u30d0\u30fc\u30b8\u30e7\u30f3\u306e PySimpleGUI \u306e PyPI \u30ea\u30f3\u30af\u3067\u3059\nhttps://pypi.org/project/PySimpleGUI/#files\n\nGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u3001\u3053\u3061\u3089\u3067\u78ba\u8a8d\u3067\u304d\u307e\u3059\nhttps://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/PySimpleGUI.py\n\n\n\u300c\u305d\u3046\u3060\u3051\u3069\u3001\u5de8\u5927\u306a\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u3064\u3060\u3051\u6301\u3064\u306e\u306f\u306a\u3093\u3066\u3072\u3069\u3044\u8003\u3048\u3067\u3059\u300d\u3068\u4eca\u3001\u8003\u3048\u3066\u3044\u308b\u4eba\u3082\u3044\u308b\u3067\u3057\u3087\u3046\u3002 \u3053\u308c\u306f*\u6642\u306b\u306f*\u3072\u3069\u3044\u8003\u3048\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u4eca\u56de\u306f\u3001\u30e1\u30ea\u30c3\u30c8\u306f\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u5927\u5e45\u306b\u4e0a\u56de\u308a\u307e\u3057\u305f\u3002 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6982\u5ff5\u306e\u591a\u304f\u306f\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u307e\u305f\u306f\u4e3b\u89b3\u7684\u306a\u3082\u306e\u3067\u3059\u3002 \u4e00\u90e8\u306e\u4eba\u304c\u671b\u3080\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u3001\u3059\u3079\u3066\u304c\u767d\u9ed2\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u591a\u304f\u306e\u5834\u5408\u3001\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u306f\u300c\u6b21\u7b2c\u300d\u3067\u3059\u3002\n\n\n\n## \u30ae\u30e3\u30e9\u30ea\u30fc :art:\n\n\u30e6\u30fc\u30b6\u30fc\u304c\u6295\u7a3f\u3057\u305fGUI\u3068GitHub\u306b\u3042\u308bGUI\u306e\u3088\u308a\u6b63\u5f0f\u306a\u30ae\u30e3\u30e9\u30ea\u30fc\u306e\u4f5c\u6210\u306f\u9032\u884c\u4e2d\u3067\u3059\u304c\u3001readme\u3092\u4f5c\u6210\u6642\u70b9\u3067\u306f\u307e\u3060\u5b8c\u6210\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u73fe\u5728\u307e\u3068\u307e\u3063\u3066\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u898b\u308c\u308b\u5834\u6240\u306f2\u304b\u6240\u3042\u308a\u307e\u3059\u3002\u9858\u308f\u304f\u3070\u4eba\u3005\u304c\u4f5c\u3063\u3066\u3044\u308b\u7d20\u6674\u3089\u3057\u3044\u4f5c\u54c1\u3092\u6b63\u5f53\u5316\u3059\u308b\u305f\u3081\u306eWiki\u3084\u305d\u306e\u4ed6\u306e\u4ed5\u7d44\u307f\u304c\u3059\u3050\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002\n\n### \u30e6\u30fc\u30b6\u30fc\u304c\u63d0\u51fa\u3057\u305f\u30ae\u30e3\u30e9\u30ea\u30fc\n\n1\u3064\u76ee\u306f\u3001GitHub\u306b\u3042\u308b[\u30e6\u30fc\u30b6\u30fc\u304c\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u63d0\u51fa\u3057\u305fissue](https://github.com/PySimpleGUI/PySimpleGUI/issues/10)\u3067\u3059\u3002 \u3053\u308c\u306f\u3001\u4eba\u3005\u304c\u4f5c\u3063\u305f\u3082\u306e\u3092\u62ab\u9732\u3059\u308b\u305f\u3081\u306e\u975e\u516c\u5f0f\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u7406\u60f3\u7684\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u30b9\u30bf\u30fc\u30c8\u3067\u3057\u305f\u3002\n\n### \u5927\u91cf\u306b\u30b9\u30af\u30e9\u30c3\u30d7\u3055\u308c\u305fGitHub\u306e\u753b\u50cf\n\n2\u3064\u76ee\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u4f1d\u3048\u3089\u308c\u3066\u3044\u308bGitHub\u306e1,000\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u96c6\u3081\u305f[3,000\u4ee5\u4e0a\u306e\u753b\u50cf\u306e\u5927\u898f\u6a21\u306a\u30ae\u30e3\u30e9\u30ea\u30fc ](https://www.dropbox.com/sh/g67ms0darox0i2p/AAAMrkIM6C64nwHLDkboCWnaa?dl=0)\u3067\u3059\u3002 \u624b\u4f5c\u696d\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u3066\u304a\u308a\u521d\u671f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u305f\u53e4\u3044\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u3053\u306b\u3042\u306a\u305f\u306e\u60f3\u50cf\u529b\u3092\u5f15\u304d\u8d77\u3053\u3059\u4f55\u304b\u304c\u898b\u3064\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n<hr>\n\n# PySimpleGUI \u306e\u7528\u9014\u3067\u3059 :hammer:\n\n\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001PySimpleGUI\u306e\u7528\u9014\u306e\u4e00\u90e8\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 GitHub \u3060\u3051\u3067\u30821,000 \u4ee5\u4e0a\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067PySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u672c\u5f53\u306b\u3053\u308c\u3060\u3051\u306e\u591a\u304f\u306e\u4eba\u3005\u306e\u53ef\u80fd\u6027\u304c\u5e83\u304c\u3063\u305f\u3053\u3068\u306f\u306f\u672c\u5f53\u306b\u9a5a\u304f\u3079\u304d\u3053\u3068\u3067\u3059\u3002 \u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u4ee5\u524d\u306bPython\u3067GUI\u3092\u4f5c\u6210\u3057\u3088\u3046\u3068\u3057\u3066\u5931\u6557\u3057\u3057\u305f\u3068\u8a71\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5f7c\u3089\u304cPySimpleGUI\u3092\u8a66\u3057\u3066\u307f\u305f\u3068\u304d\u306b\u6700\u7d42\u7684\u306b\u81ea\u5206\u306e\u5922\u3092\u9054\u6210\u3057\u305f\u3068\u8a71\u3092\u3057\u307e\u3057\u305f\u3002\n\n## \u6700\u521d\u306eGUI\n\n\u3082\u3061\u308d\u3093\u3001PySimpleGUI\u306e\u6700\u3082\u512a\u308c\u305f\u4f7f\u3044\u65b9\u306e\u4e00\u3064\u306fPython\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u3060\u3051\u306e\u5c0f\u3055\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u3089\u958b\u59cb\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u305f\u3081\u306b\u306f\u3001`popup`\u3068\u547c\u3070\u308c\u308b\u300c\u30cf\u30a4\u30ec\u30d9\u30eb\u95a2\u6570\u300d\u306e1\u3064\u30921\u56de\u547c\u3073\u51fa\u3059\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002 \u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u306b\u306f\u3042\u3089\u3086\u308b\u7a2e\u985e\u304c\u3042\u308a\u3001\u4e00\u90e8\u306f\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\n\n`popup`\u81ea\u4f53\u3067\u60c5\u5831\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002print\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6e21\u305b\u307e\u3059\u3002\u60c5\u5831\u3092\u53d6\u5f97\u3057\u305f\u3044\u5834\u5408\u306f\u3001`popup_get_filename`\u306e\u3088\u3046\u306b`popup_get_\u3067`\u59cb\u307e\u308b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3057\u307e\u3059\u3002\n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e1\u884c\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u300c\u666e\u901a\u306e\u4eba\u300d\u304c\u5feb\u9069\u306b\u4f7f\u7528\u3067\u304d\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5909\u8eab\u3057\u307e\u3059\u3002\n\n\n```python\nimport PySimpleGUI as sg\n\nfilename = sg.popup_get_file('\u51e6\u7406\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044')\nsg.popup('\u5165\u529b\u3057\u305f', filename)\n```\n\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u30012\u3064\u306e\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u307e\u3059\u3002 1\u3064\u306f\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u3067\u3001\u5165\u529b\u30dc\u30c3\u30af\u30b9\u306e\u95b2\u89a7\u3084\u30da\u30fc\u30b9\u30c8\u304c\u3067\u304d\u307e\u3059\u3002  \n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupgetfilename.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\u3082\u3046\u4e00\u65b9\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u53ce\u96c6\u3055\u308c\u305f\u5185\u5bb9\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/popupyouentered.jpg\"  alt=\"img\" width=\"175px\"></a>\n\n</p>\n\n\n<br>\n\n## Rainmeter\u98a8\u30b9\u30bf\u30a4\u30eb\u30a6\u30a3\u30f3\u30c9\u30a6\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/RainmeterStyleWidgets.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u898b\u6804\u3048\u306e\u826f\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u4f5c\u6210\u3067\u304d\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u7d30\u90e8\u306b\u6ce8\u610f\u3059\u308b\u3053\u3068\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9b45\u529b\u7684\u306b\u898b\u305b\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u3053\u3068\u3092\u304a\u3053\u306a\u3048\u307e\u3059\u3002 PySimpleGUI\u306f\u3001\u8272\u3084\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u306a\u3069\u306e\u6a5f\u80fd\u3092\u3088\u308a\u7c21\u5358\u306b\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002 \u305d\u306e\u7d50\u679c\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u3092Windows\u3067\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u3053\u306e\u4f8b\u3067\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30bf\u30a4\u30c8\u30eb \u30d0\u30fc\u304c\u524a\u9664\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u7d50\u679c\u3068\u3057\u3066\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u306eRainmeter\u3088\u3046\u306b\u898b\u3048\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002\n\n<br><br>\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u900f\u660e\u5ea6\u3082\u7c21\u5358\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002 \u540c\u3058Rainmeter\u30b9\u30bf\u30a4\u30eb\u306e\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u4ed6\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u534a\u900f\u660e\u306a\u306e\u3067\u3001\u8584\u6697\u304f\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/semi-transparent.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\n\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u3068\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u534a\u900f\u660e\u5316\u306e\u4e21\u65b9\u306e\u52b9\u679c\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306fPySimpleGUI\u304c\u3044\u304b\u306b\u6a5f\u80fd\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u304b\u3092\u793a\u3059\u4f8b\u3067\u3059\u3002 \u307e\u305f\u3001PySimpleGUI \u306e\u30b3\u30fc\u30c9\u306fGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u3067\u79fb\u690d\u53ef\u80fd\u306a\u306e\u3067\u3001Qt\u306e\u3088\u3046\u306a\u4ed6\u306e\u30dd\u30fc\u30c8\u3067\u3082\u540c\u3058\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002\n\n\n\u4f8b\uff11\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u306e\u547c\u3073\u51fa\u3057\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u5909\u66f4\u3059\u308b\u3068\u540c\u69d8\u306e\u534a\u900f\u660e\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\n```python\nwindow = sg.Window('My window', layout, no_titlebar=True, alpha_channel=0.5)\n```\n\n## \u30b2\u30fc\u30e0\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306e SDK \u3068\u3057\u3066\u306f\u7279\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Chess.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u3053\u306e\u30c1\u30a7\u30b9\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30c1\u30a7\u30b9\u3092\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u30c1\u30a7\u30b9AI\u300cStockfish\u300d\u3092\u7d71\u5408\u3057\u307e\u3059\u3002\n<br><br><br><br><br><br><br><br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Minesweeper.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u306e\u3044\u304f\u3064\u304b\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\n\n<br><br><br><br>\n<br><br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/minesweeper_israel_dryer.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br><br><br><br><br><br><br><br><br>\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Solitaire.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n<br><br>\n\nPySimpleGUI\u306e`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u3068\u753b\u50cf\u306e\u64cd\u4f5c\u304c\u7c21\u5358\u306a\u306e\u3067\u3001\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306eSDK\u3068\u3057\u3066\u66f8\u304b\u308c\u305f\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002<br><br>\n<br><br>\n<br><br><br>\n\n\n## \u30e1\u30c7\u30a3\u30a2\u306e\u30ad\u30e3\u30d7\u30c1\u30e3\u3068\u518d\u751f\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/OpenCV.jpg\"  alt=\"img\" align=\"right\" width=\"400px\"></a>\n\n\nWEB\u30ab\u30e1\u30e9\u304b\u3089\u30d3\u30c7\u30aa\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3057\u3066GUI\u3067\u8868\u793a\u3059\u308b\u306e\u306b\u306f\u3001PySimpleGUI\u306e\u30b3\u30fc\u30c9\u3067\u306f4\u884c\u3067\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u5370\u8c61\u7684\u306a\u306e\u306f\u3053\u3089\u306e4\u884c\u306e\u30b3\u30fc\u30c9\u304c tkinter\u3001Qt\u3001\u304a\u3088\u3073 Web \u30dd\u30fc\u30c8\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002  tkinter\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u8868\u793a\u3059\u308b\u306e\u3068\u540c\u3058\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30d6\u30e9\u30a6\u30b6\u3067Web\u30ab\u30e1\u30e9\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u304c\u8868\u793a\u3067\u304d\u307e\u3059\u3002\n\n\u307e\u305f\u3001VLC\u30d7\u30ec\u30fc\u30e4\u30fc\u3092\u4f7f\u3063\u3066\u3001\u30aa\u30fc\u30c7\u30a3\u30aa\u3084\u30d3\u30c7\u30aa\u306a\u3069\u306e\u30e1\u30c7\u30a3\u30a2\u518d\u751f\u3082\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30e2\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5b9f\u969b\u306e\u4f5c\u696d\u4f8b\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u306ereadme\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u306f\u5168\u3066\u3001\u3042\u306a\u305f\u81ea\u8eab\u306e\u5275\u4f5c\u306e\u51fa\u767a\u70b9\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u307e\u3059\u3002\n<br><br><br><br><br>\n<br><br><br><br><br>\n<br><br>\n## \u4eba\u5de5\u77e5\u80fd\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO_GIF.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\nAI\u3068Python\u306f\u9577\u3044\u9593\u3001\u3053\u306e2\u3064\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u305f\u3068\u304d\u306e\u30b9\u30fc\u30d1\u30fc\u30d1\u30ef\u30fc\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u591a\u304f\u306e\u5834\u5408\u3001\u30e6\u30fc\u30b6\u30fc\u304cGUI\u3092\u4f7f\u7528\u3057\u3066\u3053\u308c\u3089\u306eAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8eab\u8fd1\u306b\u64cd\u4f5c\u3059\u308b\u65b9\u6cd5\u304c\u6b20\u3051\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u308c\u3089\u306eYOLO\u306e\u30c7\u30e2\u306f\u3001GUI\u304cAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u306e\u5bfe\u8a71\u306b\u304a\u3044\u3066\u3044\u304b\u306b\u5927\u304d\u306a\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u304b\u306e\u7d20\u6674\u3089\u3057\u3044\u4f8b\u3067\u3059\u3002 \u3053\u308c\u3089\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e0b\u90e8\u306b\u3042\u308b2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306f\u3001YOLO\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u4f7f\u7528\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066YOLO\u30c7\u30e2\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5834\u5408\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u8d77\u52d5\u3059\u308b\u3068\u304d\u306b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8a2d\u5b9a\u3057\u3001\u305d\u306e\u5b9f\u884c\u65b9\u6cd5\u3092\u78ba\u8a8d\u3057\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u505c\u6b62\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5909\u66f4\u3057\u3001\u6700\u5f8c\u306b\u65b0\u3057\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n<br><br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/YOLO%20Object%20Detection.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u3053\u308c\u3089\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3001GUI\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u884c\u3067\u304d\u308b\u64cd\u4f5c\u3068\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\u3002 GUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u3001\u3059\u3050\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\n\n<br><br><br><br><br>\n<br><br>\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Colorizer.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u516c\u958b\u3055\u308c\u3066\u3044\u308bAI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u52d5\u304b\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u975e\u5e38\u306b\u591a\u304f\u5b58\u5728\u3057\u307e\u3059\u3002 \u3053\u308c\u81ea\u4f53\u306f\u5927\u304d\u306a\u30cf\u30fc\u30c9\u30eb\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5165\u529b/\u8cbc\u308a\u4ed8\u3051\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u3066\u3001\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30d3\u30e5\u30fc\u30a2\u3067\u958b\u304f\u306b\u306f\u5341\u5206\u300c\u9762\u5012\u304f\u3055\u3044\u300d\u3067\u3059\u3002\n\n\nGUI \u306b\u306f\u3001**\u30e6\u30fc\u30b6\u30fc\u30a8\u30af\u30b9\u30da\u30ea\u30a8\u30f3\u30b9\u3092\u5909\u66f4\u3059\u308b**\u3092\u300cGUI\u30ae\u30e3\u30c3\u30d7\u300d\u306b\u5909\u5316\u3055\u305b\u308b\u529b\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306f\u753b\u50cf\u304c\u683c\u7d0d\u3055\u308c\u3066\u305f\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3057\u3066\u3001\u753b\u50cf\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3068\u7d50\u679c\u8868\u793a\u306e\u4e21\u65b9\u3092\u884c\u3048\u307e\u3059\u3002  \n\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0/\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u81ea\u7531\u306b\u5229\u7528\u53ef\u80fd\u3067\u3001\u4f7f\u7528\u53ef\u80fd\u3067\u3057\u305f\u3002 \u4e0d\u8db3\u3057\u3066\u3044\u305f\u306e\u306fGUI\u304c\u3082\u305f\u3089\u3059\u4f7f\u3044\u3084\u3059\u3055\u3067\u3059\u3002\n\n\n<hr>\n\n## \u30b0\u30e9\u30d5\u5316\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/CPU%20Cores%20Dashboard%202.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\nGUI\u3067\u306e\u30c7\u30fc\u30bf\u306e\u8868\u793a\u3068\u64cd\u4f5c\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\u3044\u304f\u3064\u304b\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002\n\u7d44\u307f\u8fbc\u307f\u306e\u63cf\u753b/\u30b0\u30e9\u30d5\u4f5c\u6210\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30ab\u30b9\u30bf\u30e0\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u3053\u306eCPU\u4f7f\u7528\u7387\u30e2\u30cb\u30bf\u306f`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nMatplotlib\u306fPython\u30e6\u30fc\u30b6\u30fc\u306b\u4eba\u6c17\u304c\u3042\u308a\u307e\u3059\u3002 PySimpleGUI\u306f\u3001Matplotlib\u306e\u30b0\u30e9\u30d5\u3092GUI\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u57cb\u3081\u8fbc\u3081\u307e\u3059\u3002 Matplotlib\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u6a5f\u80fd\u3092\u4fdd\u6301\u3057\u305f\u3044\u5834\u5408\u306f\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u57cb\u3081\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Matplotlib2.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\nPySimpleGUI\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u306e\u4eba\u304cMatplotlib\u3067\u4f5c\u6210\u3059\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u3088\u308a\u3082\u4e00\u6bb5\u4e0a\u306e\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<hr>\n\n## \u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\n\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/JumpCutter.png\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u524d\u8ff0\u306e\u300cGUI \u30ae\u30e3\u30c3\u30d7\u300d\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002 GUI\u3092\u8ffd\u52a0\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u7528\u610f\u3059\u308b\u5fc5\u8981\u3082\u3042\u308a\u307e\u305b\u3093\u3002 \u300c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u300dGUI \u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u6e21\u3059\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308bGUI\u3067\u3059\u3002\n\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9GUI \u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3068\u3063\u3066\u30e6\u30fc\u30b6\u30fc\u304c\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30fb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u4f7f\u3044\u5fc3\u5730\u3088\u304f\u611f\u3058\u306a\u304b\u3063\u305f\u305f\u3081\u306b\u3001\u4ee5\u524d\u306f\u4f7f\u3044\u305f\u304c\u3089\u306a\u304b\u3063\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u914d\u5e03\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u3089\u306eGUI\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u305f\u3081\u306e\u552f\u4e00\u306e\u9078\u629e\u80a2\u3067\u3059\u3002\n\u3053\u306e\u4f8b\u306f\u3001\u300cJump Cutter\u300d\u3068\u3044\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u3067\u3059\u3002 \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306fGUI\u3092\u3068\u304a\u3057\u3066\u53ce\u96c6\u3055\u308c\u3066\u3001\u305d\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304c\u69cb\u7bc9\u3055\u308c\u3066\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u304cGUI\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3055\u308c\u3066\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001\u5b9f\u884c\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u304c\u9ec4\u8272\u3067\u8868\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n<br><br>\n<hr>\n\n## Raspberry Pi\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Raspberry%20Pi.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\nPySimpleGUI\u306fPython 3.4\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u305f\u3081\u3001Raspberry Pi\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u30bf\u30c3\u30c1\u30b9\u30af\u30ea\u30fc\u30f3\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u7279\u306b\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u3002 \u30e2\u30cb\u30bf\u30fc\u304c\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001PySimpleGUIWeb\u3092\u4f7f\u7528\u3057\u3066Pi\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br><br>\n<hr>\n\n\n## \u9ad8\u5ea6\u306a\u6a5f\u80fd\u3078\u306e\u7c21\u5358\u306a\u30a2\u30af\u30bb\u30b9\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Customized%20Titlebar.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\n\u57fa\u790e\u3068\u306a\u308b GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u591a\u304f\u306b\u975e\u5e38\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u305f\u3081\u3001GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u76f4\u63a5\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u307e\u3059\u3002\n\n\u305f\u3068\u3048\u3070\u3001tkinter\u3084\u305d\u306e\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3057\u3066\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u8272\u3084\u5916\u898b\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u304c\u3001PySimpleGUI \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30ab\u30b9\u30bf\u30e0\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u3092\u6301\u3063\u3066\u3044\u308b\u304b\u306e\u3088\u3046\u306b\u8868\u793a\u3055\u308c\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n<br><br><br>\n\n<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/Desktop%20Bouncing%20Balls.gif\"  alt=\"img\" align=\"right\" width=\"500px\"></a>\n\n\u4fe1\u3058\u3089\u308c\u306a\u3044\u3053\u3068\u306b\u3001\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u30b9\u30af\u30ea\u30fc\u30f3\u30bb\u30fc\u30d0\u30fc\u306e\u3088\u3046\u306b\u898b\u3048\u308b\u3082\u306e\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306btkinter\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306ftkinter \u306f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u80cc\u666f\u3092\u5b8c\u5168\u306b\u53d6\u308a\u9664\u3051\u307e\u3059\u3002 \u7e70\u308a\u8fd4\u3057\u307e\u3059\u304cPySimpleGUI\u306f\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3092\u7c21\u5358\u306b\u3057\u307e\u3059\u3002 \u900f\u660e\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001`Window`\u3092\u4f5c\u6210\u3059\u308b\u547c\u3073\u51fa\u3057\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u30921\u3064\u8ffd\u52a0\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 1\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u66f4\u3060\u3051\u3067\u3001\u6b21\u306e\u52b9\u679c\u3092\u6301\u3064\u5358\u7d14\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u4e0a\u306e\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u30d5\u30eb\u30b9\u30af\u30ea\u30fc\u30f3\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002\n<hr>\n\n# \u30c6\u30fc\u30de\n\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30ec\u30fc\u306eGUI\u306b\u3046\u3093\u3056\u308a\u3057\u307e\u3057\u305f\u304b?  PySimpleGUI \u306f`theme`\u95a2\u6570\u306e\u547c\u3073\u51fa\u3057\u3092\u884c\u3046\u3053\u3060\u3051\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u898b\u305f\u76ee\u3092\u7d20\u6575\u306b\u3057\u307e\u3059\u3002 150\u7a2e\u985e\u4ee5\u4e0a\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u9078\u629e\u3067\u304d\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/ThemePreview.jpg\"  alt=\"img\" width=\"900px\"></a>\n</p>\n\n\n\u307b\u3068\u3093\u3069\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u306f\u3001\u4f5c\u6210\u3059\u308b\u3059\u3079\u3066\u306e\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002  PySimpleGUI\u306f\u3001\u3053\u306e\u96d1\u7528\u3092\u4ee3\u308f\u308a\u306b\u884c\u3044\u81ea\u52d5\u7684\u306b\u9078\u629e\u3057\u305f\u30c6\u30fc\u30de\u306b\u5408\u308f\u305b\u3066\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8272\u4ed8\u3051\u3057\u307e\u3059\u3002\n\n\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u524d\u306b\u30c6\u30fc\u30de\u540d\u3092\u6307\u5b9a\u3057\u3066`theme`\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u307e\u3059\u3002\u8aad\u307f\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c6\u30fc\u30de\u3092\u300cdark grey 9\u300d\u306b\u8a2d\u5b9a\u3059\u308b\u306b\u306f\n```python\nimport PySimpleGUI as sg\n\nsg.theme('dark grey 9')\n```\n\n\u3053\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5916\u89b3\u3092\u5b8c\u5168\u306b\u5909\u66f4\u3057\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/DarkGreyJapanese.jpg\"  alt=\"img\" width=\"400px\"></a>\n</p>\n\n\n\u30c6\u30fc\u30de\u306f\u3001\u80cc\u666f\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u5165\u529b\u80cc\u666f\u3001\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3001\u304a\u3088\u3073\u30dc\u30bf\u30f3\u306e\u8272\u3092\u5909\u66f4\u3057\u307e\u3057\u305f\u3002 \u3053\u306e\u3088\u3046\u306a\u914d\u8272\u3092\u5909\u66f4\u3059\u308b\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u306f\u3001\u5404\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u500b\u5225\u306b\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u30b3\u30fc\u30c9\u3092\u4f55\u5ea6\u3082\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n<hr>\n\n# \u30b5\u30dd\u30fc\u30c8:muscle:\n\n\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3](http://www.PySimpleGUI.org)\u3068[\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0](http://Demos.PySimpleGUI.org)\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u3067\u3059\u3002\u3082\u3057\u307e\u3060\u8cea\u554f\u304c\u3042\u3063\u305f\u308a\u3001\u30d8\u30eb\u30d7\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f...\u554f\u984c\u3042\u308a\u307e\u305b\u3093...\u30d8\u30eb\u30d7\u306f\u7121\u6599\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3067[Issue\u3092\u63d0\u51fa](http://Issues.PySimpleGUI.org)\u3059\u308b\u3060\u3051\u3067\u3001\u52a9\u3051\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\u307b\u3068\u3093\u3069\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4f1a\u793e\u306f\u3001\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306b\u4ed8\u968f\u3059\u308b\u30d5\u30a9\u30fc\u30e0\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u306f\u60aa\u3044\u53d6\u5f15\u3067\u306f\u3042\u308a\u307e\u305b\u3093.\u30d5\u30a9\u30fc\u30e0\u306b\u5fc5\u8981\u4e8b\u9805\u8a18\u5165\u3059\u308c\u3070\u3001\u7121\u6599\u3067\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u60c5\u5831\u306f\u52b9\u7387\u7684\u306b\u56de\u7b54\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\nPySimpleGUI\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u756a\u53f7\u3084\u57fa\u306b\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3069\u306e\u60c5\u5831\u3092\u8981\u6c42\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u554f\u984c\u306e\u89e3\u6c7a\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u30ea\u30b9\u30c8\u3082\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\n***\u30d5\u30a9\u30fc\u30e0\u306b\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044 \u3002*** \u3000\u3042\u306a\u305f\u306b\u306f\u7121\u610f\u5473\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u307b\u3093\u306e\u4e00\u77ac\u3067\u3059\u304c\u82e6\u75db\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u8a18\u5165\u306f\u3042\u306a\u305f\u304c\u3088\u308a\u65e9\u304f\u89e3\u6c7a\u7b56\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u3082\u3057\u3042\u306a\u305f\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u30fc\u306a\u56de\u7b54\u3068\u89e3\u6c7a\u3092\u5f97\u308b\u305f\u3081\u306b\u5f79\u7acb\u3064\u5fc5\u8981\u306a\u60c5\u5831\u3067\u306a\u3051\u308c\u3070\u3001\u8a18\u5165\u306f\u5fc5\u8981\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u79c1\u306f\u3042\u306a\u305f\u3092\u52a9\u3051\u308b\u305f\u3081\u306b\u79c1\u3092\u52a9\u3051\u308b\u300d\u3002\n\n\n# \u30b5\u30dd\u30fc\u30c8\t<a href=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"><img src=\"https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/images/for_readme/PSGSuperHero.png\"  alt=\"img\"  width=\"90px\"></a>\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8ca1\u653f\u7684\u652f\u63f4\u306f\u975e\u5e38\u306b\u9ad8\u304f\u8a55\u4fa1\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u6b63\u76f4\u306b\u8a00\u3046\u3068\u3001\u7d4c\u6e08\u7684\u306a\u63f4\u52a9\u304c\u5fc5\u8981\u3067\u3059\u3002 \u30e9\u30a4\u30c8\u3092\u3064\u3051\u7d9a\u3051\u308b\u3060\u3051\u3067\u9ad8\u4fa1\u3067\u3059\u3002 \u30c9\u30e1\u30a4\u30f3\u540d\u767b\u9332\u3001\u30c8\u30ea\u30f3\u30b1\u30c3\u30c8\u3001\u30b3\u30f3\u30b5\u30eb\u30c6\u30a3\u30f3\u30b0\u30d8\u30eb\u30d7\u306a\u3069\u306e\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u9577\u3044\u30ea\u30b9\u30c8\u306f\u3001\u3059\u3050\u306b\u304b\u306a\u308a\u306e\u7e70\u308a\u8fd4\u3057\u30b3\u30b9\u30c8\u306b\u52a0\u7b97\u3055\u308c\u307e\u3059\u3002\n\nPySimpleGUI \u306f\u4f5c\u6210\u3059\u308b\u306e\u306b\u5b89\u4fa1\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u611b\u306e\u52b4\u50cd\u306f\u3001\u975e\u5e38\u306b\u9762\u5012\u3067\u3057\u305f\u3002\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3068\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306b\u5230\u9054\u3059\u308b\u306b\u306f\u3001\u90317\u65e5\u306e\u4f5c\u696d\u306b2\u5e74\u4ee5\u4e0a\u5fc5\u8981\u3067\u3059\u3002\n\nPySimpleGUI\u306b\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u3001\u305d\u306e\u307e\u307e\u6b8b\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\u7d20\u6674\u3089\u3057\u3044\u3053\u3068\u3067\u3059\u3002 \u304a\u5ba2\u69d8\u307e\u305f\u306f\u304a\u5ba2\u69d8\u306e\u4f1a\u793e (\u7279\u306b\u4f01\u696d\u3067PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u5834\u5408) \u304c\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7d4c\u6e08\u7684\u306b\u5229\u76ca\u3092\u5f97\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5bff\u547d\u3092\u5ef6\u9577\u3059\u308b\u6a5f\u80fd\u3092\u6301\u3061\u307e\u3059\u3002\n\n###\u3000Buy Me a Coffee\n\n\u300cBuy Me a Coffee\u300d\u306f\u3001\u958b\u767a\u8005\u3092\u516c\u7684\u306b\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002 \u7d20\u65e9\u304f\u3001\u7c21\u5358\u306b\u3001\u8ca2\u732e\u306f\u8a18\u9332\u3055\u308c\u308b\u306e\u3067\u3001\u3042\u306a\u305f\u304cPySimpleGUI \u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308b\u3053\u3068\u3092\u4ed6\u306e\u4eba\u306b\u898b\u305b\u3089\u308c\u307e\u3059\u3002\u5bc4\u4ed8\u3092\u975e\u516c\u958b\u306b\u3082\u3067\u304d\u307e\u3059\u3002\n\n<a href=\"https://www.buymeacoffee.com/PySimpleGUI\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" width=\"217px\" ></a>\n\n\n\n### GitHub\u30b9\u30dd\u30f3\u30b5\u30fc\n\n<a href=\"https://github.com/sponsors/PySimpleGUI\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=%3Curl%3E&color=f88379\"></a>\n\n[GitHub\u5b9a\u671f\u7684\u306a\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7](https://github.com/sponsors\u30fc/PySimpleGUI)\u306f\u3001\u7d99\u7d9a\u7684\u306b\u3055\u307e\u3056\u307e\u306a\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u30b9\u30dd\u30f3\u30b5\u30fc\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u591a\u304f\u306e\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u4f01\u696d\u30ec\u30d9\u30eb\u306e\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u91d1\u92ad\u7684\u306b\u8ca2\u732e\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3001\u975e\u5e38\u306b\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u958b\u767a\u8005\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u7d4c\u6e08\u7684\u306b\u56f0\u96e3\u3067\u3059\u3002YouTube\u52d5\u753b\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306f\u3001\u52d5\u753b\u4f5c\u6210\u3067\u751f\u8a08\u3092\u7acb\u3066\u3066\u3044\u307e\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u306f\u307e\u3060\u305d\u308c\u307b\u3069\u7c21\u5358\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n# \u8ca2\u732e:construction_worker:\n\nPySimpleGUI \u306f\u73fe\u5728\u3001\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u81ea\u4f53\u306f\u88fd\u54c1\u306e\u3088\u3046\u306b\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u306f\u53d7\u3051\u4ed8\u3051\u3089\u308c\u307e\u305b\u3093\u3002\n\n\u8ca2\u732e\u3059\u308b\u6700\u3082\u826f\u3044\u65b9\u6cd5\u306e1\u3064\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u3068\u516c\u958b\u3067\u3059\u3002 \u30e6\u30fc\u30b6\u30fc\u306f\u3001\u4ed6\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u69cb\u7bc9\u3059\u308b\u3082\u306e\u3092\u898b\u3066\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3044\u307e\u3059\u3002 GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u30b3\u30fc\u30c9\u3092\u6295\u7a3f\u3057\u3066\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u306eReadme\u30d5\u30a1\u30a4\u30eb\u306b\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\u3002  \n\n\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a5f\u80fd\u304c\u3042\u3063\u305f\u308a\u3001\u6a5f\u80fd\u5f37\u5316\u3092\u63d0\u6848\u3057\u305f\u3044\u5834\u5408\u306f\u3001[issue\u3092\u958b\u3044\u3066\u304f\u3060\u3055\u3044](https://github.com/PySimpleGUI/PySimpleGUI/issues/new?assignees=&labels=&template=issue-form---must-fill-in-this-form-with-every-new-issue-submitted.md&title=%5B+Enhancement%2FBug%2FQuestion%5D+My+problem+is.)) \u3002\n\n\n# \u7279\u5225\u306a\u611f\u8b1d :pray:\n\n\nPySimpleGUI\u306e\u3053\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306ereadme\u306f[@M4cs](https://github.com/M4cs)\u306e\u52a9\u3051\u306a\u3057\u3067\u306f\u5b9f\u73fe\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5f7c\u306f\u7d20\u6674\u3089\u3057\u3044\u958b\u767a\u8005\u3067\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u7acb\u3061\u4e0a\u3052\u304b\u3089\u305a\u3063\u3068PySimpleGUI\u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3059\u3002 [@Israel Dryer](https://github.com/israel-dryer)\u3082\u307e\u305f\u9577\u671f\u7684\u306a\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308a\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u6a5f\u80fd\u306e\u9650\u754c\u3092\u62bc\u3057\u5e83\u3052\u305f\u3044\u304f\u3064\u304b\u306ePySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u3044\u3066\u3044\u307e\u3059\u3002 \u30dc\u30fc\u30c9\u306e\u753b\u50cf\u3092\u4f7f\u7528\u3057\u305f\u30e6\u30cb\u30fc\u30af\u306a\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30fc\u306fIsrael\u306b\u304c\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 [@jason990420](https://github.com/jason990420)\u306f\u4e0a\u306e\u5199\u771f\u306e\u3088\u3046\u306a\u3001PySimpleGUI \u3092\u4f7f\u3063\u305f\u6700\u521d\u306e\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u3068\u3001PySimpleGUI \u3067\u4f5c\u3089\u308c\u305f\u6700\u521d\u306e\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30b2\u30fc\u30e0\u3092\u516c\u958b\u3057\u3066\u3001\u591a\u304f\u306e\u4eba\u3092\u9a5a\u304b\u305b\u307e\u3057\u305f\u3002\n\u65e5\u672c\u8a9e\u7248\u306e readme \u306f[@okajun35](https://github.com/okajun35) \u3055\u3093\u306e\u5354\u529b\u3067\u5927\u5e45\u306b\u6539\u5584\u3055\u308c\u307e\u3057\u305f\u3002\n\nPySimpleGUI \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b 1,200 \u4ee5\u4e0a\u306e GitHub \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3082\u300c\u3042\u308a\u304c\u3068\u3046\u300d\u306e\u8a00\u8449\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30a8\u30f3\u30b8\u30f3\u3092\u52d5\u304b\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u3042\u306a\u305f\u306e\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u304a\u304b\u3052\u3067\u3059\u3002\n\n\u4e00\u6669\u4e2dTwitter \u306b\u6295\u7a3f\u3057\u3066\u304f\u308c\u308b\u6d77\u5916\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001PySimpleGUI\u306e\u4e00\u65e5\u306e\u4f5c\u696d\u3092\u59cb\u3081\u308b\u304d\u3063\u304b\u3051\u3068\u306a\u308a\u307e\u3059\u3002\u5f7c\u3089\u306f\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u6e90\u3067\u3042\u308a\u3001\u958b\u767a\u30a8\u30f3\u30b8\u30f3\u3092\u59cb\u52d5\u3055\u305b\u3001\u6bce\u65e5\u7a3c\u50cd\u3055\u305b\u308b\u6e96\u5099\u3092\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\u611f\u8b1d\u306e\u610f\u3092\u8fbc\u3081\u3066\u3001\u3053\u306ereadme\u30d5\u30a1\u30a4\u30eb\u3092(\u65e5\u672c\u8a9e)[https://github.com/PySimpleGUI/PySimpleGUI/blob/master/readme.ja.md]\u306b\u7ffb\u8a33\u3057\u307e\u3057\u305f\u3002\n\n\u7686\u3055\u3093\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u671b\u3080\u6700\u9ad8\u306e\u30e6\u30fc\u30b6\u30fc\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3067\u3059\u3002\n\n\n\n&copy; Copyright 2020 PySimpleGUI.org \n"
 },
 {
  "repo": "bytedeco/javacv",
  "language": "Java",
  "readme_contents": "JavaCV\r\n======\r\n\r\n[![Gitter](https://badges.gitter.im/bytedeco/javacv.svg)](https://gitter.im/bytedeco/javacv) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/javacv-platform) [![Sonatype Nexus (Snapshots)](https://img.shields.io/nexus/s/https/oss.sonatype.org/org.bytedeco/javacv.svg)](http://bytedeco.org/builds/) [![Build Status](https://travis-ci.org/bytedeco/javacv.svg?branch=master)](https://travis-ci.org/bytedeco/javacv) <sup>Commercial support:</sup> [![xscode](https://img.shields.io/badge/Available%20on-xs%3Acode-blue?style=?style=plastic&logo=appveyor&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////////VXz1bAAAAAJ0Uk5T/wDltzBKAAAAlUlEQVR42uzXSwqAMAwE0Mn9L+3Ggtgkk35QwcnSJo9S+yGwM9DCooCbgn4YrJ4CIPUcQF7/XSBbx2TEz4sAZ2q1RAECBAiYBlCtvwN+KiYAlG7UDGj59MViT9hOwEqAhYCtAsUZvL6I6W8c2wcbd+LIWSCHSTeSAAECngN4xxIDSK9f4B9t377Wd7H5Nt7/Xz8eAgwAvesLRjYYPuUAAAAASUVORK5CYII=)](https://xscode.com/bytedeco/javacv)\r\n\r\n\r\nIntroduction\r\n------------\r\nJavaCV uses wrappers from the [JavaCPP Presets](https://github.com/bytedeco/javacpp-presets) of commonly used libraries by researchers in the field of computer vision ([OpenCV](http://opencv.org/), [FFmpeg](http://ffmpeg.org/), [libdc1394](http://damien.douxchamps.net/ieee1394/libdc1394/), [PGR FlyCapture](https://www.ptgrey.com/flycapture-sdk), [OpenKinect](http://openkinect.org/), [librealsense](https://github.com/IntelRealSense/librealsense), [CL PS3 Eye Driver](https://codelaboratories.com/downloads/), [videoInput](http://muonics.net/school/spring05/videoInput/), [ARToolKitPlus](https://launchpad.net/artoolkitplus), [flandmark](http://cmp.felk.cvut.cz/~uricamic/flandmark/), [Leptonica](http://www.leptonica.org/), and [Tesseract](https://github.com/tesseract-ocr/tesseract)) and provides utility classes to make their functionality easier to use on the Java platform, including Android.\r\n\r\nJavaCV also comes with hardware accelerated full-screen image display (`CanvasFrame` and `GLCanvasFrame`), easy-to-use methods to execute code in parallel on multiple cores (`Parallel`), user-friendly geometric and color calibration of cameras and projectors (`GeometricCalibrator`, `ProCamGeometricCalibrator`, `ProCamColorCalibrator`), detection and matching of feature points (`ObjectFinder`), a set of classes that implement direct image alignment of projector-camera systems (mainly `GNImageAligner`, `ProjectiveTransformer`, `ProjectiveColorTransformer`, `ProCamTransformer`, and `ReflectanceInitializer`), a blob analysis package (`Blobs`), as well as miscellaneous functionality in the `JavaCV` class. Some of these classes also have an OpenCL and OpenGL counterpart, their names ending with `CL` or starting with `GL`, i.e.: `JavaCVCL`, `GLCanvasFrame`, etc.\r\n\r\nTo learn how to use the API, since documentation currently lacks, please refer to the [Sample Usage](#sample-usage) section below as well as the [sample programs](https://github.com/bytedeco/javacv/tree/master/samples/), including two for Android (`FacePreview.java` and `RecordActivity.java`), also found in the `samples` directory. You may also find it useful to refer to the source code of [ProCamCalib](https://github.com/bytedeco/procamcalib) and [ProCamTracker](https://github.com/bytedeco/procamtracker) as well as [examples ported from OpenCV2 Cookbook](https://github.com/bytedeco/javacv-examples/) and the associated [wiki pages](https://github.com/bytedeco/javacv-examples/tree/master/OpenCV_Cookbook).\r\n\r\nPlease keep me informed of any updates or fixes you make to the code so that I may integrate them into the next release. Thank you! And feel free to ask questions on [the mailing list](http://groups.google.com/group/javacv) if you encounter any problems with the software! I am sure it is far from perfect...\r\n\r\n\r\nDownloads\r\n---------\r\nArchives containing JAR files are available as [releases](https://github.com/bytedeco/javacv/releases). The binary archive contains builds for Android, iOS, Linux, Mac OS X, and Windows. The JAR files for specific child modules or platforms can also be obtained individually from the [Maven Central Repository](http://search.maven.org/#search|ga|1|bytedeco).\r\n\r\nTo install manually the JAR files, follow the instructions in the [Manual Installation](#manual-installation) section below.\r\n\r\nWe can also have everything downloaded and installed automatically with:\r\n\r\n * Maven (inside the `pom.xml` file)\r\n```xml\r\n  <dependency>\r\n    <groupId>org.bytedeco</groupId>\r\n    <artifactId>javacv-platform</artifactId>\r\n    <version>1.5.4</version>\r\n  </dependency>\r\n```\r\n\r\n * Gradle (inside the `build.gradle` file)\r\n```groovy\r\n  dependencies {\r\n    implementation group: 'org.bytedeco', name: 'javacv-platform', version: '1.5.4'\r\n  }\r\n```\r\n\r\n * Leiningen (inside the `project.clj` file)\r\n```clojure\r\n  :dependencies [\r\n    [org.bytedeco/javacv-platform \"1.5.4\"]\r\n  ]\r\n```\r\n\r\n * sbt (inside the `build.sbt` file)\r\n```scala\r\n  libraryDependencies += \"org.bytedeco\" % \"javacv-platform\" % \"1.5.4\"\r\n```\r\n\r\nThis downloads binaries for all platforms, but to get binaries for only one platform we can set the `javacpp.platform` system property (via the `-D` command line option) to something like `android-arm`, `linux-x86_64`, `macosx-x86_64`, `windows-x86_64`, etc. Please refer to the [README.md file of the JavaCPP Presets](https://github.com/bytedeco/javacpp-presets#downloads) for details. Another option available to Gradle users is [Gradle JavaCPP](https://github.com/bytedeco/gradle-javacpp), and similarly for Scala users there is [SBT-JavaCV](https://github.com/bytedeco/sbt-javacv).\r\n\r\n\r\nRequired Software\r\n-----------------\r\nTo use JavaCV, you will first need to download and install the following software:\r\n\r\n * An implementation of Java SE 7 or newer:\r\n   * OpenJDK  http://openjdk.java.net/install/  or\r\n   * Oracle JDK  http://www.oracle.com/technetwork/java/javase/downloads/  or\r\n   * IBM JDK  http://www.ibm.com/developerworks/java/jdk/\r\n\r\nFurther, although not always required, some functionality of JavaCV also relies on:\r\n\r\n * CL Eye Platform SDK (Windows only)  http://codelaboratories.com/downloads/\r\n * Android SDK API 21 or newer  http://developer.android.com/sdk/\r\n * JOCL and JOGL from JogAmp  http://jogamp.org/\r\n\r\nFinally, please make sure everything has the same bitness: **32-bit and 64-bit modules do not mix under any circumstances**.\r\n\r\n\r\nManual Installation\r\n-------------------\r\nSimply put all the desired JAR files (`opencv*.jar`, `ffmpeg*.jar`, etc.), in addition to `javacpp.jar` and `javacv.jar`, somewhere in your class path. Here are some more specific instructions for common cases:\r\n\r\nNetBeans (Java SE 7 or newer):\r\n\r\n 1. In the Projects window, right-click the Libraries node of your project, and select \"Add JAR/Folder...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nEclipse (Java SE 7 or newer):\r\n\r\n 1. Navigate to Project > Properties > Java Build Path > Libraries and click \"Add External JARs...\".\r\n 2. Locate the JAR files, select them, and click OK.\r\n\r\nIntelliJ IDEA (Android 5.0 or newer):\r\n\r\n 1. Follow the instructions on this page: http://developer.android.com/training/basics/firstapp/\r\n 2. Copy all the JAR files into the `app/libs` subdirectory.\r\n 3. Navigate to File > Project Structure > app > Dependencies, click `+`, and select \"2 File dependency\".\r\n 4. Select all the JAR files from the `libs` subdirectory.\r\n\r\nAfter that, the wrapper classes for OpenCV and FFmpeg, for example, can automatically access all of their C/C++ APIs:\r\n\r\n * [OpenCV documentation](http://docs.opencv.org/master/)\r\n * [FFmpeg documentation](http://ffmpeg.org/doxygen/trunk/)\r\n\r\n\r\nSample Usage\r\n------------\r\nThe class definitions are basically ports to Java of the original header files in C/C++, and I deliberately decided to keep as much of the original syntax as possible. For example, here is a method that tries to load an image file, smooth it, and save it back to disk:\r\n\r\n```java\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgcodecs.*;\r\n\r\npublic class Smoother {\r\n    public static void smooth(String filename) {\r\n        Mat image = imread(filename);\r\n        if (image != null) {\r\n            GaussianBlur(image, image, new Size(3, 3), 0);\r\n            imwrite(filename, image);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nJavaCV also comes with helper classes and methods on top of OpenCV and FFmpeg to facilitate their integration to the Java platform. Here is a small demo program demonstrating the most frequently useful parts:\r\n\r\n```java\r\nimport java.io.File;\r\nimport java.net.URL;\r\nimport org.bytedeco.javacv.*;\r\nimport org.bytedeco.javacpp.*;\r\nimport org.bytedeco.javacpp.indexer.*;\r\nimport org.bytedeco.opencv.opencv_core.*;\r\nimport org.bytedeco.opencv.opencv_imgproc.*;\r\nimport org.bytedeco.opencv.opencv_calib3d.*;\r\nimport org.bytedeco.opencv.opencv_objdetect.*;\r\nimport static org.bytedeco.opencv.global.opencv_core.*;\r\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\r\nimport static org.bytedeco.opencv.global.opencv_calib3d.*;\r\nimport static org.bytedeco.opencv.global.opencv_objdetect.*;\r\n\r\npublic class Demo {\r\n    public static void main(String[] args) throws Exception {\r\n        String classifierName = null;\r\n        if (args.length > 0) {\r\n            classifierName = args[0];\r\n        } else {\r\n            URL url = new URL(\"https://raw.github.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\");\r\n            File file = Loader.cacheResource(url);\r\n            classifierName = file.getAbsolutePath();\r\n        }\r\n\r\n        // We can \"cast\" Pointer objects by instantiating a new object of the desired class.\r\n        CascadeClassifier classifier = new CascadeClassifier(classifierName);\r\n        if (classifier == null) {\r\n            System.err.println(\"Error loading classifier file \\\"\" + classifierName + \"\\\".\");\r\n            System.exit(1);\r\n        }\r\n\r\n        // The available FrameGrabber classes include OpenCVFrameGrabber (opencv_videoio),\r\n        // DC1394FrameGrabber, FlyCapture2FrameGrabber, OpenKinectFrameGrabber, OpenKinect2FrameGrabber,\r\n        // RealSenseFrameGrabber, RealSense2FrameGrabber, PS3EyeFrameGrabber, VideoInputFrameGrabber, and FFmpegFrameGrabber.\r\n        FrameGrabber grabber = FrameGrabber.createDefault(0);\r\n        grabber.start();\r\n\r\n        // CanvasFrame, FrameGrabber, and FrameRecorder use Frame objects to communicate image data.\r\n        // We need a FrameConverter to interface with other APIs (Android, Java 2D, JavaFX, Tesseract, OpenCV, etc).\r\n        OpenCVFrameConverter.ToMat converter = new OpenCVFrameConverter.ToMat();\r\n\r\n        // FAQ about IplImage and Mat objects from OpenCV:\r\n        // - For custom raw processing of data, createBuffer() returns an NIO direct\r\n        //   buffer wrapped around the memory pointed by imageData, and under Android we can\r\n        //   also use that Buffer with Bitmap.copyPixelsFromBuffer() and copyPixelsToBuffer().\r\n        // - To get a BufferedImage from an IplImage, or vice versa, we can chain calls to\r\n        //   Java2DFrameConverter and OpenCVFrameConverter, one after the other.\r\n        // - Java2DFrameConverter also has static copy() methods that we can use to transfer\r\n        //   data more directly between BufferedImage and IplImage or Mat via Frame objects.\r\n        Mat grabbedImage = converter.convert(grabber.grab());\r\n        int height = grabbedImage.rows();\r\n        int width = grabbedImage.cols();\r\n\r\n        // Objects allocated with `new`, clone(), or a create*() factory method are automatically released\r\n        // by the garbage collector, but may still be explicitly released by calling deallocate().\r\n        // You shall NOT call cvReleaseImage(), cvReleaseMemStorage(), etc. on objects allocated this way.\r\n        Mat grayImage = new Mat(height, width, CV_8UC1);\r\n        Mat rotatedImage = grabbedImage.clone();\r\n\r\n        // The OpenCVFrameRecorder class simply uses the VideoWriter of opencv_videoio,\r\n        // but FFmpegFrameRecorder also exists as a more versatile alternative.\r\n        FrameRecorder recorder = FrameRecorder.createDefault(\"output.avi\", width, height);\r\n        recorder.start();\r\n\r\n        // CanvasFrame is a JFrame containing a Canvas component, which is hardware accelerated.\r\n        // It can also switch into full-screen mode when called with a screenNumber.\r\n        // We should also specify the relative monitor/camera response for proper gamma correction.\r\n        CanvasFrame frame = new CanvasFrame(\"Some Title\", CanvasFrame.getDefaultGamma()/grabber.getGamma());\r\n\r\n        // Let's create some random 3D rotation...\r\n        Mat randomR    = new Mat(3, 3, CV_64FC1),\r\n            randomAxis = new Mat(3, 1, CV_64FC1);\r\n        // We can easily and efficiently access the elements of matrices and images\r\n        // through an Indexer object with the set of get() and put() methods.\r\n        DoubleIndexer Ridx = randomR.createIndexer(),\r\n                   axisIdx = randomAxis.createIndexer();\r\n        axisIdx.put(0, (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4,\r\n                       (Math.random() - 0.5) / 4);\r\n        Rodrigues(randomAxis, randomR);\r\n        double f = (width + height) / 2.0;  Ridx.put(0, 2, Ridx.get(0, 2) * f);\r\n                                            Ridx.put(1, 2, Ridx.get(1, 2) * f);\r\n        Ridx.put(2, 0, Ridx.get(2, 0) / f); Ridx.put(2, 1, Ridx.get(2, 1) / f);\r\n        System.out.println(Ridx);\r\n\r\n        // We can allocate native arrays using constructors taking an integer as argument.\r\n        Point hatPoints = new Point(3);\r\n\r\n        while (frame.isVisible() && (grabbedImage = converter.convert(grabber.grab())) != null) {\r\n            // Let's try to detect some faces! but we need a grayscale image...\r\n            cvtColor(grabbedImage, grayImage, CV_BGR2GRAY);\r\n            RectVector faces = new RectVector();\r\n            classifier.detectMultiScale(grayImage, faces);\r\n            long total = faces.size();\r\n            for (long i = 0; i < total; i++) {\r\n                Rect r = faces.get(i);\r\n                int x = r.x(), y = r.y(), w = r.width(), h = r.height();\r\n                rectangle(grabbedImage, new Point(x, y), new Point(x + w, y + h), Scalar.RED, 1, CV_AA, 0);\r\n\r\n                // To access or pass as argument the elements of a native array, call position() before.\r\n                hatPoints.position(0).x(x - w / 10     ).y(y - h / 10);\r\n                hatPoints.position(1).x(x + w * 11 / 10).y(y - h / 10);\r\n                hatPoints.position(2).x(x + w / 2      ).y(y - h / 2 );\r\n                fillConvexPoly(grabbedImage, hatPoints.position(0), 3, Scalar.GREEN, CV_AA, 0);\r\n            }\r\n\r\n            // Let's find some contours! but first some thresholding...\r\n            threshold(grayImage, grayImage, 64, 255, CV_THRESH_BINARY);\r\n\r\n            // To check if an output argument is null we may call either isNull() or equals(null).\r\n            MatVector contours = new MatVector();\r\n            findContours(grayImage, contours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);\r\n            long n = contours.size();\r\n            for (long i = 0; i < n; i++) {\r\n                Mat contour = contours.get(i);\r\n                Mat points = new Mat();\r\n                approxPolyDP(contour, points, arcLength(contour, true) * 0.02, true);\r\n                drawContours(grabbedImage, new MatVector(points), -1, Scalar.BLUE);\r\n            }\r\n\r\n            warpPerspective(grabbedImage, rotatedImage, randomR, rotatedImage.size());\r\n\r\n            Frame rotatedFrame = converter.convert(rotatedImage);\r\n            frame.showImage(rotatedFrame);\r\n            recorder.record(rotatedFrame);\r\n        }\r\n        frame.dispose();\r\n        recorder.stop();\r\n        grabber.stop();\r\n    }\r\n}\r\n```\r\n\r\nFurthermore, after creating a `pom.xml` file with the following content:\r\n```xml\r\n<project>\r\n    <modelVersion>4.0.0</modelVersion>\r\n    <groupId>org.bytedeco.javacv</groupId>\r\n    <artifactId>demo</artifactId>\r\n    <version>1.5.4</version>\r\n    <properties>\r\n        <maven.compiler.source>1.7</maven.compiler.source>\r\n        <maven.compiler.target>1.7</maven.compiler.target>\r\n    </properties>\r\n    <dependencies>\r\n        <dependency>\r\n            <groupId>org.bytedeco</groupId>\r\n            <artifactId>javacv-platform</artifactId>\r\n            <version>1.5.4</version>\r\n        </dependency>\r\n    </dependencies>\r\n    <build>\r\n        <sourceDirectory>.</sourceDirectory>\r\n    </build>\r\n</project>\r\n```\r\n\r\nAnd by placing the source code above in `Demo.java`, or similarly for other classes found in the [`samples`](samples), we can use the following command to have everything first installed automatically and then executed by Maven:\r\n```bash\r\n $ mvn compile exec:java -Dexec.mainClass=Demo\r\n```\r\n\r\n**Note**: In case of errors, please make sure that the `artifactId` in the `pom.xml` file reads `javacv-platform`, not `javacv` only, for example. The artifact `javacv-platform` adds all the necessary binary dependencies.\r\n\r\n\r\nBuild Instructions\r\n------------------\r\nIf the binary files available above are not enough for your needs, you might need to rebuild them from the source code. To this end, the project files were created for:\r\n\r\n * Maven 3.x  http://maven.apache.org/download.html\r\n * JavaCPP 1.5.4  https://github.com/bytedeco/javacpp\r\n * JavaCPP Presets 1.5.4  https://github.com/bytedeco/javacpp-presets\r\n\r\nOnce installed, simply call the usual `mvn install` command for JavaCPP, its Presets, and JavaCV. By default, no other dependencies than a C++ compiler for JavaCPP are required. Please refer to the comments inside the `pom.xml` files for further details.\r\n\r\nInstead of building the native libraries manually, we can run `mvn install` for JavaCV only and rely on the snapshot artifacts from the CI builds:\r\n\r\n * http://bytedeco.org/builds/\r\n\r\n\r\n----\r\nProject lead: Samuel Audet [samuel.audet `at` gmail.com](mailto:samuel.audet&nbsp;at&nbsp;gmail.com)  \r\nDeveloper site: https://github.com/bytedeco/javacv  \r\nDiscussion group: http://groups.google.com/group/javacv\r\n"
 },
 {
  "repo": "nuno-faria/tiler",
  "language": "Python",
  "readme_contents": "![title](images/title_stripes.png)\n\n\ud83d\udc77 Build images with images.\n\n\n## About\n\nTiler is a tool to create an image using all kinds of other smaller images (tiles). It is different from other mosaic tools since it can adapt to tiles with multiple shapes and sizes (i.e. not limited to squares).\n\nAn image can be built out of circles, lines, waves, cross stitches, legos, minecraft blocks, paper clips, letters, ... The possibilities are endless!\n\n\n## Installation\n\n- Clone the repo: `git clone https://github.com/nuno-faria/tiler.git`;\n- Install Python 3;\n- Install pip (optional, to install the dependencies);\n- Install dependencies: `pip install -r requirements.txt`\n\n## Usage\n\n- Make a folder with the tiles (and only the tiles) to build the image;\n    - The script `gen_tiles.py` can help in this task; it builds tiles with multiple colors based on the source tile (note: its recommended for the source file to have an RGB color of (240,240,240)). It is used as `python gen_tiles.py path/to/image` and creates a folder with a 'gen_' prefix in the same path as the base image.\n- Run `python tiler.py path/to/image path/to/tiles_folder/`.\n\n## Configuration\n\nAll configurations can be changed in the `conf.py` file.\n\n#### `gen_tiles.py`\n\n- `DEPTH` - number of divisions in each color channel (ex: DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `ROTATIONS` - list of rotations, in degrees, to apply over the original image (ex: [0, 90]).\n\n#### `tiler.py`\n\n- `COLOR_DEPTH` - number of divisions in each color channel (ex: COLOR_DEPTH = 4 -> 4 * 4 * 4 = 64 colors);\n- `RESIZING_SCALES` - scale to apply to each tile (ex: [1, 0.75, 0.5, 0.25]);\n- `PIXEL_SHIFT` - number of pixels shifted to create each box (ex: (5,5)); if None, shift will be the same as the tile dimension);\n    <img src=\"images/pixel_shift.png\" width=\"100%\">\n- `OVERLAP_TILES` - if tiles can overlap;\n- `RENDER` - render image as its being built;\n- `POOL_SIZE` - multiprocessing pool size;\n- `IMAGE_TO_TILE` - image to tile (ignored if passed as the 1st arg);\n- `TILES_FOLDER` - folder with tiles (ignored if passed as the 2nd arg);\n- `OUT` - result image filename.\n\n\n## Examples\n\n### Circles\n\n#### Various sizes\n\n<img src=\"images/cake_circles.png\" width=\"40%\">\n\n[Original](https://www.flaticon.com/free-icon/cake_1102780) cake image by [pongsakornred](https://www.flaticon.com/authors/pongsakornred) from [FLATICON](https://www.flaticon.com).\n\n#### Fixed\n\n- 10x10\n<img src=\"images/cake_circles_simple.png\" width=\"40%\">\n<img src=\"images/starry_night_circles_10x10.png\" width=\"100%\">\n- 25x25\n<img src=\"images/starry_night_circles_25x25.png\" width=\"100%\">\n- 50x50\n<img src=\"images/starry_night_circles_50x50.png\" width=\"100%\">\n\n\n### Paper clips\n\n<img src=\"images/cake_clips.png\" width=\"40%\">\n\n\n### Cross stitch (times)\n\n<img src=\"images/cake_times.png\" width=\"40%\">\n\n<img src=\"images/starry_night_times.png\" width=\"100%\">\n\n\n### Hearts\n\n<img src=\"images/heart_hearts.png\" width=\"40%\">\n\n\n### Legos\n\n<img src=\"images/cake_lego.png\" width=\"40%\">\n<img src=\"images/starry_night_lego.png\" width=\"100%\">\n\n\n### Minecraft blocks\n\n<img src=\"images/cake_minecraft.png\" width=\"40%\">\n<img src=\"images/starry_night_minecraft.png\" width=\"100%\">\n\n\n### Stripes (lines)\n\n<img src=\"images/cake_stripes.png\" width=\"50%\">\n\n\n### At\n\n<img src=\"images/github_logo_at.png\" width=\"50%\">\n"
 },
 {
  "repo": "kelaberetiv/TagUI",
  "language": "JavaScript",
  "readme_contents": "<img src=\"https://raw.githubusercontent.com/kelaberetiv/TagUI/master/src/media/tagui_logo.png\" height=\"111\" align=\"right\">\n\n# TagUI\n\n### TagUI is a command-line tool for digital process automation (RPA)\n\n### [Download TagUI v6](https://tagui.readthedocs.io/en/latest/setup.html)&ensp;|&ensp;[Visit documentation](https://tagui.readthedocs.io/en/latest/index.html)&ensp;|&ensp;[User feedback](https://forms.gle/mieY66xTN4NNm5Gq5)&ensp;|&ensp;[We're hiring](https://nuscareers.taleo.net/careersection/2/jobdetail.ftl?job=00CMO)\n\nWrite flows in simple TagUI language and automate your web, mouse and keyboard interactions on the screen.\n\nTagUI is free to use and open-source. It's easy to setup and use, and works on Windows, macOS and Linux.\n\nIn TagUI language, you use steps like `click` and `type` to interact with identifiers, which include web identifiers, image snapshots, screen coordinates, or even text using OCR. Below is a sample flow to download a report:\n\n```\nhttps://www.typeform.com\n\nclick login\ntype username as user@gmail.com\ntype password as 12345678\nclick btnlogin\n\ndownload https://admin.typeform.com/xxx to report.csv\n```\n```\n// besides web identifiers, images of UI elements can be used\nclick login_button.png\ntype username_box.png as user@gmail.com\n```\n```\n// (x,y) coordinates of user-interface elements can also be used\nclick (1200,200)\ntype (800,400) as user@gmail.com\n```\n\n# v6 Features\n\n### TagUI live mode\nYou can run live mode directly for faster development by running `tagui live` on the command line.\n\n### Click text using OCR\nTagUI can now click on the screen with visual automation just using text input, by using OCR technology.\n\n```\nclick v6 Features using ocr\n```\n\n### Deploy flows to run when double clicked\nYou can now create a shortcut for a flow, which can be moved to your desktop and double-clicked to run the flow. The flow will be run with all the options used when creating the shortcut.\n\n```\n$ tagui my_flow.tag -deploy\nOR\n$ tagui my_flow.tag -d\n```\n\n### Running flows with options can be done with abbreviations\nFor example, you can now do ``tagui my_flow.tag -h`` instead of ``tagui my_flow.tag -headless``.\n\n# Migrating to v6\n\n### Mandatory .tag file extension\nAll flow files must have a .tag extension.\n\n### Options must be used with a leading hyphen (-)\nWhen running a flow with options, prefix a - to the options.\n\nBefore v6:\n```\n$ tagui my_flow.tag headless\n```\n\nAfter v6:\n```\n$ tagui my_flow.tag -headless\nOR\n$ tagui my_flow.tag -h\n```\n\n### Change in syntax for echo, dump, write steps\nThe echo, dump and write steps are now consistent with the other steps. They no longer require quotes surrounding the string input. Instead, variables now need to be surrounded by backticks.\n\nBefore v6:\n```\necho 'This works!' some_text_variable\n```\n\nAfter v6:\n```\necho This works! `some_text_variable`\n```\n\n### if and loop code blocks can use indentation instead of curly braces {}\nThis increases readability and ease of use. Just indent your code within the if and loop code blocks. \n\nBefore v6:\n```\nif some_condition\n{\ndo_some_step_A\ndo_some_step_B\n}\n```\n\nAfter v6:\n```\nif some_condition\n  do_some_step_A\n  do_some_step_B\n```\n\n# TagUI v5.11\n\n### [Visit TagUI v5.11 homepage & documentation](https://github.com/kelaberetiv/TagUI/tree/pre_v6)\n\n# Credits\n- [TagUI v3](https://github.com/kensoh/TagUI/tree/before_aisg) - Ken Soh from Singapore\n- [SikuliX](http://sikulix.com) - Raimund Hocke from Germany\n- [CasperJS](http://casperjs.org) - Nicolas Perriault from France\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia\n- [SlimerJS](https://slimerjs.org) - Laurent Jouanneau from France\n\nThis project  is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG-RP-2019-050). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n"
 },
 {
  "repo": "bijection/sistine",
  "language": "Python",
  "readme_contents": "# Project Sistine\n\n![Sistine * 3/2](splash.png)\n\nWe turned a MacBook into a touchscreen using only $1 of hardware and a little bit of computer vision. The proof-of-concept, dubbed \u201cProject Sistine\u201d after our [recreation](https://www.anishathalye.com/media/2018/04/03/thumbnail.jpg) of the famous [painting](https://en.wikipedia.org/wiki/The_Creation_of_Adam) in the Sistine Chapel, was prototyped by [Anish Athalye](https://www.anishathalye.com/), [Kevin Kwok](https://twitter.com/antimatter15), [Guillermo Webster](https://twitter.com/biject), and [Logan Engstrom](https://github.com/lengstrom) in about 16 hours.\n\n## Basic Principle\n\nThe basic principle behind Sistine is simple. Surfaces viewed from an angle tend to look shiny, and you can tell if a finger is touching the surface by checking if it\u2019s touching its own reflection.\n\n![Hover versus touch](https://www.anishathalye.com/media/2018/04/03/explanation.png)\n\nKevin, back in middle school, noticed this phenomenon and built [ShinyTouch](https://antimatter15.com/project/shinytouch/), utilizing an external webcam to build a touch input system requiring virtually no setup. We wanted to see if we could miniaturize the idea and make it work without an external webcam. Our idea was to retrofit a small mirror in front of a MacBook\u2019s built-in webcam, so that the webcam would be looking down at the computer screen at a sharp angle. The camera would be able to see fingers hovering over or touching the screen, and we\u2019d be able to translate the video feed into touch events using computer vision.\n\n(Read the rest of our blog post, including a video demo and a high-level explanation of the algorithm, [here](https://www.anishathalye.com/2018/04/03/macbook-touchscreen/))\n\n## Installation (with Homebrew Python)\n\n* First, make sure you have [Mac Homebrew](https://brew.sh/) installed on your computer. If not, you can install it by running `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\n* Install Python 2 via Homebrew with `brew install python2`\n\n* Install OpenCV 3 via Homebrew with `brew install opencv3`\n\n* Install PyObjC via Pip with `pip2 install pyobjc`\n\n## Running\n\nRun `python2 sistine.py`\n\n## License\n\nCopyright (c) 2016-2018 Anish Athalye, Kevin Kwok, Guillermo Webster, and Logan\nEngstrom. Released under the MIT License. See [LICENSE.md][license] for\ndetails.\n\n[license]: LICENSE.md\n"
 },
 {
  "repo": "justadudewhohacks/opencv4nodejs",
  "language": "C++",
  "readme_contents": "opencv4nodejs\n=============\n\n![opencv4nodejs](https://user-images.githubusercontent.com/31125521/37272906-67187fdc-25d8-11e8-9704-40e9e94c1e80.jpg)\n\n[![Build Status](https://travis-ci.org/justadudewhohacks/opencv4nodejs.svg?branch=master)](http://travis-ci.org/justadudewhohacks/opencv4nodejs)\n[![Build status](https://ci.appveyor.com/api/projects/status/cv3o65nrosh1udbb/branch/master?svg=true)](https://ci.appveyor.com/project/justadudewhohacks/opencv4nodejs/branch/master)\n[![Coverage](https://codecov.io/github/justadudewhohacks/opencv4nodejs/coverage.svg?branch=master)](https://codecov.io/gh/justadudewhohacks/opencv4nodejs)\n[![npm download](https://img.shields.io/npm/dm/opencv4nodejs.svg?style=flat)](https://www.npmjs.com/package/opencv4nodejs)\n[![node version](https://img.shields.io/badge/node.js-%3E=_6-green.svg?style=flat)](http://nodejs.org/download/)\n[![Slack](https://slack.bri.im/badge.svg)](https://slack.bri.im/)\n\n**opencv4nodejs allows you to use the native OpenCV library in nodejs. Besides a synchronous API the package provides an asynchronous API, which allows you to build non-blocking and multithreaded computer vision tasks. opencv4nodejs supports OpenCV 3 and OpenCV 4.**\n\n**The ultimate goal of this project is to provide a comprehensive collection of nodejs bindings to the API of OpenCV and the OpenCV-contrib modules. To get an overview of the currently implemented bindings, have a look at the [type declarations](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/lib/typings) of this package. Furthermore, contribution is highly appreciated. If you want to add missing bindings check out the <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/CONTRIBUTING.md\"><b>contribution guide</b>.**\n\n* **[Examples](#examples)**\n* **[How to install](#how-to-install)**\n* **[Usage with Docker](#usage-with-docker)**\n* **[Usage with Electron](#usage-with-electron)**\n* **[Usage with NW.js](#usage-with-nwjs)**\n* **[Quick Start](#quick-start)**\n* **[Async API](#async-api)**\n* **[With TypeScript](#with-typescript)**\n* **[External Memory Tracking (v4.0.0)](#external-mem-tracking)**\n<a name=\"examples\"></a>\n\n# Examples\n\nSee <a href=\"https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples\"><b>examples</b></a> for implementation.\n\n### Face Detection\n\n![face0](https://user-images.githubusercontent.com/31125521/29702727-c796acc4-8972-11e7-8043-117dd2761833.jpg)\n![face1](https://user-images.githubusercontent.com/31125521/29702730-c79d3904-8972-11e7-8ccb-e8c467244ad8.jpg)\n\n### Face Recognition with the OpenCV face module\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-opencv-for-face-recognition-37fa7cb860e8\"><b>Node.js + OpenCV for Face Recognition</b></a>.\n\n![facerec](https://user-images.githubusercontent.com/31125521/35453007-eac9d516-02c8-11e8-9c4d-a77c01ae1f77.jpg)\n\n### Face Landmarks with the OpenCV face module\n\n![facelandmarks](https://user-images.githubusercontent.com/31125521/39297394-af14ae26-4943-11e8-845a-a06cbfa28d5a.jpg)\n\n### Face Recognition with <a href=\"https://github.com/justadudewhohacks/face-recognition.js\"><b>face-recognition.js</b></a>\n\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-face-recognition-js-simple-and-robust-face-recognition-using-deep-learning-ea5ba8e852\"><b>Node.js + face-recognition.js : Simple and Robust Face Recognition using Deep Learning</b></a>.\n\n[![IMAGE ALT TEXT](https://user-images.githubusercontent.com/31125521/35453884-055f3bde-02cc-11e8-8fa6-945f320652c3.jpg)](https://www.youtube.com/watch?v=ArcFHpX-usQ \"Nodejs Face Recognition using face-recognition.js and opencv4nodejs\")\n\n### Hand Gesture Recognition\nCheck out <a href=\"https://medium.com/@muehler.v/simple-hand-gesture-recognition-using-opencv-and-javascript-eb3d6ced28a0\"><b>Simple Hand Gesture Recognition using OpenCV and JavaScript</b></a>.\n\n![gesture-rec_sm](https://user-images.githubusercontent.com/31125521/30052864-41bd5680-9227-11e7-8a62-6205f3d99d5c.gif)\n\n### Object Recognition with Deep Neural Networks\nCheck out <a href=\"https://medium.com/@muehler.v/node-js-meets-opencvs-deep-neural-networks-fun-with-tensorflow-and-caffe-ff8d52a0f072\"><b>Node.js meets OpenCV\u2019s Deep Neural Networks\u200a\u2014\u200aFun with Tensorflow and Caffe</b></a>.\n\n#### Tensorflow Inception\n\n![husky](https://user-images.githubusercontent.com/31125521/32703295-f6b0e7ee-c7f3-11e7-8039-b3ada21810a0.jpg)\n![car](https://user-images.githubusercontent.com/31125521/32703296-f6cea892-c7f3-11e7-8aaa-9fe48b88fe05.jpeg)\n![banana](https://user-images.githubusercontent.com/31125521/32703297-f6e932ca-c7f3-11e7-9a66-bbc826ebf007.jpg)\n\n\n#### Single Shot Multibox Detector with COCO\n\n![dishes-detection](https://user-images.githubusercontent.com/31125521/32703228-eae787d4-c7f2-11e7-8323-ea0265deccb3.jpg)\n![car-detection](https://user-images.githubusercontent.com/31125521/32703229-eb081e36-c7f2-11e7-8b26-4d253b4702b4.jpg)\n\n### Machine Learning\nCheck out <a href=\"https://medium.com/@muehler.v/machine-learning-with-opencv-and-javascript-part-1-recognizing-handwritten-letters-using-hog-and-88719b70efaa\"><b>Machine Learning with OpenCV and JavaScript: Recognizing Handwritten Letters using HOG and SVM</b></a>.\n\n![resulttable](https://user-images.githubusercontent.com/31125521/30635645-5a466ea8-9df3-11e7-8498-527e1293c4fa.png)\n\n### Object Tracking\n\n![trackbgsubtract](https://user-images.githubusercontent.com/31125521/29702733-c7b59864-8972-11e7-996b-d28cb508f3b8.gif)\n![trackbycolor](https://user-images.githubusercontent.com/31125521/29702735-c8057686-8972-11e7-9c8d-13e30ab74628.gif)\n\n### Feature Matching\n\n![matchsift](https://user-images.githubusercontent.com/31125521/29702731-c79e3142-8972-11e7-947e-db109d415469.jpg)\n\n### Image Histogram\n\n![plotbgr](https://user-images.githubusercontent.com/31125521/29995016-1b847970-8fdf-11e7-9316-4eb0fd550adc.jpg)\n![plotgray](https://user-images.githubusercontent.com/31125521/29995015-1b83e06e-8fdf-11e7-8fa8-5d18326b9cd3.jpg)\n\n### Boiler plate for combination of opencv4nodejs, express and websockets.\n\n[opencv4nodejs-express-websockets](https://github.com/Mudassir-23/opencv4nodejs-express-websockets) - Boilerplate express app for getting started on opencv with nodejs and to live stream the video through websockets.\n\n### Automating lights by people detection through classifier\n\nCheck out <a href=\"https://medium.com/softway-blog/automating-lights-with-computer-vision-nodejs-fb9b614b75b2\"><b>Automating lights with Computer Vision & NodeJS</b></a>.\n\n![user-presence](https://user-images.githubusercontent.com/34403479/70385871-8d62e680-19b7-11ea-855c-3b2febfdbd72.png)\n\n<a name=\"how-to-install\"></a>\n\n# How to install\n\n``` bash\nnpm install --save opencv4nodejs\n```\n\nNative node modules are built via node-gyp, which already comes with npm by default. However, node-gyp requires you to have python installed. If you are running into node-gyp specific issues have a look at known issues with [node-gyp](https://github.com/nodejs/node-gyp) first.\n\n**Important note:** node-gyp won't handle whitespaces properly, thus make sure, that the path to your project directory does **not contain any whitespaces**. Installing opencv4nodejs under \"C:\\Program Files\\some_dir\" or similar will not work and will fail with: \"fatal error C1083: Cannot open include file: 'opencv2/core.hpp'\"!**\n\nOn Windows you will furthermore need Windows Build Tools to compile OpenCV and opencv4nodejs. If you don't have Visual Studio or Windows Build Tools installed, you can easily install the VS2015 build tools:\n\n``` bash\nnpm install --global windows-build-tools\n```\n\n## Installing OpenCV Manually\n\nSetting up OpenCV on your own will require you to set an environment variable to prevent the auto build script to run:\n\n``` bash\n# linux and osx:\nexport OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n# on windows:\nset OPENCV4NODEJS_DISABLE_AUTOBUILD=1\n```\n\n### Windows\n\nYou can install any of the OpenCV 3 or OpenCV 4 <a href=\"https://github.com/opencv/opencv/releases/\"><b>releases</b></a> manually or via the [Chocolatey](https://chocolatey.org/) package manager:\n\n``` bash\n# to install OpenCV 4.1.0\nchoco install OpenCV -y -version 4.1.0\n```\n\nNote, this will come without contrib modules. To install OpenCV under windows with contrib modules you have to build the library from source or you can use the auto build script.\n\nBefore installing opencv4nodejs with an own installation of OpenCV you need to expose the following environment variables:\n- *OPENCV_INCLUDE_DIR* pointing to the directory with the subfolder *opencv2* containing the header files\n- *OPENCV_LIB_DIR* pointing to the lib directory containing the OpenCV .lib files\n\nAlso you will need to add the OpenCV binaries to your system path:\n- add an environment variable *OPENCV_BIN_DIR* pointing to the binary directory containing the OpenCV .dll files\n- append `;%OPENCV_BIN_DIR%;` to your system path variable\n\nNote: Restart your current console session after making changes to your environment.\n\n### MacOSX\n\nUnder OSX we can simply install OpenCV via brew:\n\n``` bash\nbrew update\nbrew install opencv@4\nbrew link --force opencv@4\n```\n\n### Linux\n\nUnder Linux we have to build OpenCV from source manually or using the auto build script.\n\n## Installing OpenCV via Auto Build Script\n\nThe auto build script comes in form of the [opencv-build](https://github.com/justadudewhohacks/npm-opencv-build) npm package, which will run by default when installing opencv4nodejs. The script requires you to have git and a recent version of cmake installed.\n\n### Auto Build Flags\n\nYou can customize the autobuild flags using *OPENCV4NODEJS_AUTOBUILD_FLAGS=<flags>*.\nFlags must be space-separated.\n\nThis is an advanced customization and you should have knowledge regarding the OpenCV compilation flags. Flags added by default are listed [here](https://github.com/justadudewhohacks/npm-opencv-build/blob/master/src/constants.ts#L44-L82).\n\n### Installing a Specific Version of OpenCV\n\nYou can specify the Version of OpenCV you want to install via the script by setting an environment variable:\n`export OPENCV4NODEJS_AUTOBUILD_OPENCV_VERSION=4.1.0`\n\n### Installing only a Subset of OpenCV modules\n\nIf you only want to build a subset of the OpenCV modules you can pass the *-DBUILD_LIST* cmake flag via the *OPENCV4NODEJS_AUTOBUILD_FLAGS* environment variable. For example `export OPENCV4NODEJS_AUTOBUILD_FLAGS=-DBUILD_LIST=dnn` will build only modules required for `dnn` and reduces the size and compilation time of the OpenCV package.\n\n## Configuring Environments via package.json\n\nIt's possible to specify build environment variables by inserting them into the `package.json` as follows:\n\n```json\n{\n  \"name\": \"my-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"opencv4nodejs\": \"^X.X.X\"\n  },\n  \"opencv4nodejs\": {\n    \"disableAutoBuild\": 1,\n    \"opencvIncludeDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\include\",\n    \"opencvLibDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\lib\",\n    \"opencvBinDir\": \"C:\\\\tools\\\\opencv\\\\build\\\\x64\\\\vc14\\\\bin\"\n  }\n}\n```\n\nThe following environment variables can be passed:\n\n- autoBuildBuildCuda\n- autoBuildFlags\n- autoBuildOpencvVersion\n- autoBuildWithoutContrib\n- disableAutoBuild\n- opencvIncludeDir\n- opencvLibDir\n- opencvBinDir\n\n<a name=\"usage-with-docker\"></a>\n\n# Usage with Docker\n\n### [opencv-express](https://github.com/justadudewhohacks/opencv-express) - example for opencv4nodejs with express.js and docker\n\nOr simply pull from [justadudewhohacks/opencv-nodejs](https://hub.docker.com/r/justadudewhohacks/opencv-nodejs/) for opencv-3.2 + contrib-3.2 with opencv4nodejs globally installed:\n\n``` docker\nFROM justadudewhohacks/opencv-nodejs\n```\n\n**Note**: The aforementioned Docker image already has ```opencv4nodejs``` installed globally. In order to prevent build errors during an ```npm install```, your ```package.json``` should not include ```opencv4nodejs```, and instead should include/require the global package either by requiring it by absolute path or setting the ```NODE_PATH``` environment variable to ```/usr/lib/node_modules``` in your Dockerfile and requiring the package as you normally would.\n\nDifferent OpenCV 3.x base images can be found here: https://hub.docker.com/r/justadudewhohacks/.\n\n<a name=\"usage-with-electron\"></a>\n\n# Usage with Electron\n\n### [opencv-electron](https://github.com/justadudewhohacks/opencv-electron) - example for opencv4nodejs with electron\n\nAdd the following script to your package.json:\n``` python\n\"electron-rebuild\": \"electron-rebuild -w opencv4nodejs\"\n```\n\nRun the script:\n``` bash\n$ npm run electron-rebuild\n```\n\nRequire it in the application:\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"usage-with-nwjs\"></a>\n\n# Usage with NW.js\n\nAny native modules, including opencv4nodejs, must be recompiled to be used with [NW.js](https://nwjs.io/). Instructions on how to do this are available in the **[Use Native Modules](http://docs.nwjs.io/en/latest/For%20Users/Advanced/Use%20Native%20Node%20Modules/)** section of the the NW.js documentation.\n\nOnce recompiled, the module can be installed and required as usual:\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n<a name=\"quick-start\"></a>\n\n# Quick Start\n\n``` javascript\nconst cv = require('opencv4nodejs');\n```\n\n### Initializing Mat (image matrix), Vec, Point\n\n``` javascript\nconst rows = 100; // height\nconst cols = 100; // width\n\n// empty Mat\nconst emptyMat = new cv.Mat(rows, cols, cv.CV_8UC3);\n\n// fill the Mat with default value\nconst whiteMat = new cv.Mat(rows, cols, cv.CV_8UC1, 255);\nconst blueMat = new cv.Mat(rows, cols, cv.CV_8UC3, [255, 0, 0]);\n\n// from array (3x3 Matrix, 3 channels)\nconst matData = [\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]],\n  [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n  [[255, 0, 0], [255, 0, 0], [255, 0, 0]]\n];\nconst matFromArray = new cv.Mat(matData, cv.CV_8UC3);\n\n// from node buffer\nconst charData = [255, 0, ...];\nconst matFromArray = new cv.Mat(Buffer.from(charData), rows, cols, cv.CV_8UC3);\n\n// Point\nconst pt2 = new cv.Point(100, 100);\nconst pt3 = new cv.Point(100, 100, 0.5);\n\n// Vector\nconst vec2 = new cv.Vec(100, 100);\nconst vec3 = new cv.Vec(100, 100, 0.5);\nconst vec4 = new cv.Vec(100, 100, 0.5, 0.5);\n```\n\n### Mat and Vec operations\n\n``` javascript\nconst mat0 = new cv.Mat(...);\nconst mat1 = new cv.Mat(...);\n\n// arithmetic operations for Mats and Vecs\nconst matMultipliedByScalar = mat0.mul(0.5);  // scalar multiplication\nconst matDividedByScalar = mat0.div(2);       // scalar division\nconst mat0PlusMat1 = mat0.add(mat1);          // addition\nconst mat0MinusMat1 = mat0.sub(mat1);         // subtraction\nconst mat0MulMat1 = mat0.hMul(mat1);          // elementwise multiplication\nconst mat0DivMat1 = mat0.hDiv(mat1);          // elementwise division\n\n// logical operations Mat only\nconst mat0AndMat1 = mat0.and(mat1);\nconst mat0OrMat1 = mat0.or(mat1);\nconst mat0bwAndMat1 = mat0.bitwiseAnd(mat1);\nconst mat0bwOrMat1 = mat0.bitwiseOr(mat1);\nconst mat0bwXorMat1 = mat0.bitwiseXor(mat1);\nconst mat0bwNot = mat0.bitwiseNot();\n```\n\n### Accessing Mat data\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\nconst matGray = new cv.Mat(..., cv.CV_8UC1);\n\n// get pixel value as vector or number value\nconst vec3 = matBGR.at(200, 100);\nconst grayVal = matGray.at(200, 100);\n\n// get raw pixel value as array\nconst [b, g, r] = matBGR.atRaw(200, 100);\n\n// set single pixel values\nmatBGR.set(50, 50, [255, 0, 0]);\nmatBGR.set(50, 50, new Vec(255, 0, 0));\nmatGray.set(50, 50, 255);\n\n// get a 25x25 sub region of the Mat at offset (50, 50)\nconst width = 25;\nconst height = 25;\nconst region = matBGR.getRegion(new cv.Rect(50, 50, width, height));\n\n// get a node buffer with raw Mat data\nconst matAsBuffer = matBGR.getData();\n\n// get entire Mat data as JS array\nconst matAsArray = matBGR.getDataAsArray();\n```\n\n### IO\n\n``` javascript\n// load image from file\nconst mat = cv.imread('./path/img.jpg');\ncv.imreadAsync('./path/img.jpg', (err, mat) => {\n  ...\n})\n\n// save image\ncv.imwrite('./path/img.png', mat);\ncv.imwriteAsync('./path/img.jpg', mat,(err) => {\n  ...\n})\n\n// show image\ncv.imshow('a window name', mat);\ncv.waitKey();\n\n// load base64 encoded image\nconst base64text='data:image/png;base64,R0lGO..';//Base64 encoded string\nconst base64data =base64text.replace('data:image/jpeg;base64','')\n                            .replace('data:image/png;base64','');//Strip image type prefix\nconst buffer = Buffer.from(base64data,'base64');\nconst image = cv.imdecode(buffer); //Image is now represented as Mat\n\n// convert Mat to base64 encoded jpg image\nconst outBase64 =  cv.imencode('.jpg', croppedImage).toString('base64'); // Perform base64 encoding\nconst htmlImg='<img src=data:image/jpeg;base64,'+outBase64 + '>'; //Create insert into HTML compatible <img> tag\n\n// open capture from webcam\nconst devicePort = 0;\nconst wCap = new cv.VideoCapture(devicePort);\n\n// open video capture\nconst vCap = new cv.VideoCapture('./path/video.mp4');\n\n// read frames from capture\nconst frame = vCap.read();\nvCap.readAsync((err, frame) => {\n  ...\n});\n\n// loop through the capture\nconst delay = 10;\nlet done = false;\nwhile (!done) {\n  let frame = vCap.read();\n  // loop back to start on end of stream reached\n  if (frame.empty) {\n    vCap.reset();\n    frame = vCap.read();\n  }\n\n  // ...\n\n  const key = cv.waitKey(delay);\n  done = key !== 255;\n}\n```\n\n### Useful Mat methods\n\n``` javascript\nconst matBGR = new cv.Mat(..., cv.CV_8UC3);\n\n// convert types\nconst matSignedInt = matBGR.convertTo(cv.CV_32SC3);\nconst matDoublePrecision = matBGR.convertTo(cv.CV_64FC3);\n\n// convert color space\nconst matGray = matBGR.bgrToGray();\nconst matHSV = matBGR.cvtColor(cv.COLOR_BGR2HSV);\nconst matLab = matBGR.cvtColor(cv.COLOR_BGR2Lab);\n\n// resize\nconst matHalfSize = matBGR.rescale(0.5);\nconst mat100x100 = matBGR.resize(100, 100);\nconst matMaxDimIs100 = matBGR.resizeToMax(100);\n\n// extract channels and create Mat from channels\nconst [matB, matG, matR] = matBGR.splitChannels();\nconst matRGB = new cv.Mat([matR, matB, matG]);\n```\n\n### Drawing a Mat into HTML Canvas\n\n``` javascript\nconst img = ...\n\n// convert your image to rgba color space\nconst matRGBA = img.channels === 1\n  ? img.cvtColor(cv.COLOR_GRAY2RGBA)\n  : img.cvtColor(cv.COLOR_BGR2RGBA);\n\n// create new ImageData from raw mat data\nconst imgData = new ImageData(\n  new Uint8ClampedArray(matRGBA.getData()),\n  img.cols,\n  img.rows\n);\n\n// set canvas dimensions\nconst canvas = document.getElementById('myCanvas');\ncanvas.height = img.rows;\ncanvas.width = img.cols;\n\n// set image data\nconst ctx = canvas.getContext('2d');\nctx.putImageData(imgData, 0, 0);\n```\n\n### Method Interface\n\nOpenCV method interface from official docs or src:\n``` c++\nvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT);\n```\n\ntranslates to:\n\n``` javascript\nconst src = new cv.Mat(...);\n// invoke with required arguments\nconst dst0 = src.gaussianBlur(new cv.Size(5, 5), 1.2);\n// with optional paramaters\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, 0.8, cv.BORDER_REFLECT);\n// or pass specific optional parameters\nconst optionalArgs = {\n  borderType: cv.BORDER_CONSTANT\n};\nconst dst2 = src.gaussianBlur(new cv.Size(5, 5), 1.2, optionalArgs);\n```\n\n<a name=\"async-api\"></a>\n\n# Async API\n\nThe async API can be consumed by passing a callback as the last argument of the function call. By default, if an async method is called without passing a callback, the function call will yield a Promise.\n\n### Async Face Detection\n\n``` javascript\nconst classifier = new cv.CascadeClassifier(cv.HAAR_FRONTALFACE_ALT2);\n\n// by nesting callbacks\ncv.imreadAsync('./faceimg.jpg', (err, img) => {\n  if (err) { return console.error(err); }\n\n  const grayImg = img.bgrToGray();\n  classifier.detectMultiScaleAsync(grayImg, (err, res) => {\n    if (err) { return console.error(err); }\n\n    const { objects, numDetections } = res;\n    ...\n  });\n});\n\n// via Promise\ncv.imreadAsync('./faceimg.jpg')\n  .then(img =>\n    img.bgrToGrayAsync()\n      .then(grayImg => classifier.detectMultiScaleAsync(grayImg))\n      .then((res) => {\n        const { objects, numDetections } = res;\n        ...\n      })\n  )\n  .catch(err => console.error(err));\n\n// using async await\ntry {\n  const img = await cv.imreadAsync('./faceimg.jpg');\n  const grayImg = await img.bgrToGrayAsync();\n  const { objects, numDetections } = await classifier.detectMultiScaleAsync(grayImg);\n  ...\n} catch (err) {\n  console.error(err);\n}\n```\n\n<a name=\"with-typescript\"></a>\n\n# With TypeScript\n\n``` javascript\nimport * as cv from 'opencv4nodejs'\n```\n\nCheck out the TypeScript [examples](https://github.com/justadudewhohacks/opencv4nodejs/tree/master/examples/typed).\n\n<a name=\"external-mem-tracking\"></a>\n\n# External Memory Tracking (v4.0.0)\n\nSince version 4.0.0 was released, external memory tracking has been enabled by default. Simply put, the memory allocated for Matrices (cv.Mat) will be manually reported to the node process. This solves the issue of inconsistent Garbage Collection, which could have resulted in spiking memory usage of the node process eventually leading to overflowing the RAM of your system, prior to version 4.0.0.\n\nNote, that in doubt this feature can be **disabled** by setting an environment variable `OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING` before requiring the module:\n\n``` bash\nexport OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // linux\nset OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING=1 // windows\n```\n\nOr directly in your code:\n``` javascript\nprocess.env.OPENCV4NODEJS_DISABLE_EXTERNAL_MEM_TRACKING = 1\nconst cv = require('opencv4nodejs')\n```\n\n\n"
 },
 {
  "repo": "peterbraden/node-opencv",
  "language": "C++",
  "readme_contents": "# node-opencv\n\n[![Build Status](https://secure.travis-ci.org/peterbraden/node-opencv.svg)](http://travis-ci.org/peterbraden/node-opencv)\n\n[OpenCV](http://opencv.org) bindings for Node.js. OpenCV is\nthe defacto computer vision library - by interfacing with it natively in node,\nwe get powerful real time vision in js.\n\nPeople are using node-opencv to fly control quadrocoptors, detect faces from\nwebcam images and annotate video streams. If you're using it for something\ncool, I'd love to hear about it!\n\n## Install\n\nYou'll need OpenCV 2.3.1 or newer installed before installing node-opencv.\n\n## Specific for macOS\nInstall OpenCV using brew\n```bash\nbrew install pkg-config\nbrew install opencv@2\nbrew link --force opencv@2\n```\n\n\n## Specific for Windows\n1. Download and install OpenCV (Be sure to use a 2.4 version) @\nhttp://opencv.org/releases.html\nFor these instructions we will assume OpenCV is put at C:\\OpenCV, but you can\nadjust accordingly.\n\n2. If you haven't already, create a system variable called OPENCV_DIR and set it\n   to C:\\OpenCV\\build\\x64\\vc12\n\n   Make sure the \"x64\" part matches the version of NodeJS you are using.\n\n   Also add the following to your system PATH\n        ;%OPENCV_DIR%\\bin\n\n3. Install Visual Studio 2013. Make sure to get the C++ components.\n   You can use a different edition, just make sure OpenCV supports it, and you\n   set the \"vcxx\" part of the variables above to match.\n\n4. Download peterbraden/node-opencv fork\ngit clone https://github.com/peterbraden/node-opencv\n\n5. run npm install\n\n```bash\n$ npm install opencv\n```\n\n## Examples\nRun the examples from the parent directory.\n\n### Face Detection\n\n```javascript\ncv.readImage(\"./examples/files/mona.png\", function(err, im){\n  im.detectObject(cv.FACE_CASCADE, {}, function(err, faces){\n    for (var i=0;i<faces.length; i++){\n      var x = faces[i]\n      im.ellipse(x.x + x.width/2, x.y + x.height/2, x.width/2, x.height/2);\n    }\n    im.save('./out.jpg');\n  });\n})\n```\n\n\n## API Documentation\n\n### Matrix\n\nThe [matrix](http://opencv.jp/opencv-2svn_org/cpp/core_basic_structures.html#mat) is the most useful\nbase data structure in OpenCV. Things like images are just matrices of pixels.\n\n#### Creation\n\n```javascript\nnew Matrix(rows, cols)\n```\n\nOr if you're thinking of a Matrix as an image:\n\n```javascript\nnew Matrix(height, width)\n```\n\nOr you can use opencv to read in image files. Supported formats are in the OpenCV docs, but jpgs etc are supported.\n\n```javascript\ncv.readImage(filename, function(err, mat){\n  ...\n})\n\ncv.readImage(buffer, function(err, mat){\n  ...\n})\n```\n\nIf you need to pipe data into an image, you can use an ImageDataStream:\n\n```javascript\nvar s = new cv.ImageDataStream()\n\ns.on('load', function(matrix){\n  ...\n})\n\nfs.createReadStream('./examples/files/mona.png').pipe(s);\n```\n\nIf however, you have a series of images, and you wish to stream them into a\nstream of Matrices, you can use an ImageStream. Thus:\n\n```javascript\nvar s = new cv.ImageStream()\n\ns.on('data', function(matrix){\n   ...\n})\n\nardrone.createPngStream().pipe(s);\n```\n\nNote: Each 'data' event into the ImageStream should be a complete image buffer.\n\n\n\n#### Accessing Data\n\n```javascript\nvar mat = new cv.Matrix.Eye(4,4); // Create identity matrix\n\nmat.get(0,0) // 1\n\nmat.row(0)  // [1,0,0,0]\nmat.col(3)  // [0,0,0,1]\n```\n\n##### Save\n\n```javascript\nmat.save('./pic.jpg')\n```\n\nor:\n\n```javascript\nvar buff = mat.toBuffer()\n```\n\n#### Image Processing\n\n```javascript\nim.convertGrayscale()\nim.canny(5, 300)\nim.houghLinesP()\n```\n\n\n#### Simple Drawing\n\n```javascript\nim.ellipse(x, y)\nim.line([x1,y1], [x2, y2])\n```\n\n#### Object Detection\n\nThere is a shortcut method for\n[Viola-Jones Haar Cascade](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) object\ndetection. This can be used for face detection etc.\n\n```javascript\nmat.detectObject(haar_cascade_xml, opts, function(err, matches){})\n```\n\nFor convenience in face detection, cv.FACE_CASCADE is a cascade that can be used for frontal face detection.\n\nAlso:\n\n```javascript\nmat.goodFeaturesToTrack\n```\n\n#### Contours\n\n```javascript\nmat.findCountours\nmat.drawContour\nmat.drawAllContours\n```\n\n### Using Contours\n\n`findContours` returns a `Contours` collection object, not a native array. This object provides\nfunctions for accessing, computing with, and altering the contours contained in it.\nSee [relevant source code](src/Contours.cc) and [examples](examples/)\n\n```javascript\nvar contours = im.findContours();\n\n// Count of contours in the Contours object\ncontours.size();\n\n// Count of corners(verticies) of contour `index`\ncontours.cornerCount(index);\n\n// Access vertex data of contours\nfor(var c = 0; c < contours.size(); ++c) {\n  console.log(\"Contour \" + c);\n  for(var i = 0; i < contours.cornerCount(c); ++i) {\n    var point = contours.point(c, i);\n    console.log(\"(\" + point.x + \",\" + point.y + \")\");\n  }\n}\n\n// Computations of contour `index`\ncontours.area(index);\ncontours.arcLength(index, isClosed);\ncontours.boundingRect(index);\ncontours.minAreaRect(index);\ncontours.isConvex(index);\ncontours.fitEllipse(index);\n\n// Destructively alter contour `index`\ncontours.approxPolyDP(index, epsilon, isClosed);\ncontours.convexHull(index, clockwise);\n```\n\n#### Face Recognization\n\nIt requires to `train` then `predict`. For acceptable result, the face should be cropped, grayscaled and aligned, I ignore this part so that we may focus on the api usage.\n\n** Please ensure your OpenCV 3.2+ is configured with contrib. MacPorts user may `port install opencv +contrib` **\n\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst cv = require('opencv');\n\nfunction forEachFileInDir(dir, cb) {\n  let f = fs.readdirSync(dir);\n  f.forEach(function (fpath, index, array) {\n    if (fpath != '.DS_Store')\n     cb(path.join(dir, fpath));\n  });\n}\n\nlet dataDir = \"./_training\";\nfunction trainIt (fr) {\n  // if model existe, load it\n  if ( fs.existsSync('./trained.xml') ) {\n    fr.loadSync('./trained.xml');\n    return;\n  }\n\n  // else train a model\n  let samples = [];\n  forEachFileInDir(dataDir, (f)=>{\n      cv.readImage(f, function (err, im) {\n          // Assume all training photo are named as id_xxx.jpg\n          let labelNumber = parseInt(path.basename(f).substring(3));\n          samples.push([labelNumber, im]);\n      })\n  })\n\n  if ( samples.length > 3 ) {\n    // There are async and sync version of training method:\n    // .train(info, cb)\n    //     cb : standard Nan::Callback\n    //     info : [[intLabel,matrixImage],...])\n    // .trainSync(info)\n    fr.trainSync(samples);\n    fr.saveSync('./trained.xml');\n  }else {\n    console.log('Not enough images uploaded yet', cvImages)\n  }\n}\n\nfunction predictIt(fr, f){\n  cv.readImage(f, function (err, im) {\n    let result = fr.predictSync(im);\n    console.log(`recognize result:(${f}) id=${result.id} conf=${100.0-result.confidence}`);\n  });\n}\n\n//using defaults: .createLBPHFaceRecognizer(radius=1, neighbors=8, grid_x=8, grid_y=8, threshold=80)\nconst fr = new cv.FaceRecognizer();\ntrainIt(fr);\nforEachFileInDir('./_bench', (f) => predictIt(fr, f));\n```\n\n## Test\n\nUsing [tape](https://github.com/substack/tape). Run with command:\n\n`npm test`.\n\n## Contributing\n\nI (@peterbraden) don't spend much time maintaining this library, it runs\nprimarily on contributor support. I'm happy to accept most PR's if the tests run\ngreen, all new functionality is tested, and there are no objections in the PR.\n\nBecause I haven't got much time for maintenance, I'd prefer to keep an absolute\nminimum of dependencies.\n\n\n## MIT License\nThe library is distributed under the MIT License - if for some reason that\ndoesn't work for you please get in touch.\n"
 },
 {
  "repo": "hybridgroup/gocv",
  "language": "Go",
  "readme_contents": "# GoCV\n\n[![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/gocvlogo.jpg)](http://gocv.io/)\n\n[![GoDoc](https://godoc.org/gocv.io/x/gocv?status.svg)](https://godoc.org/github.com/hybridgroup/gocv)\n[![CircleCI Build status](https://circleci.com/gh/hybridgroup/gocv/tree/dev.svg?style=svg)](https://circleci.com/gh/hybridgroup/gocv/tree/dev)\n[![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/9asd5foet54ru69q/branch/dev?svg=true)](https://ci.appveyor.com/project/deadprogram/gocv/branch/dev)\n[![codecov](https://codecov.io/gh/hybridgroup/gocv/branch/dev/graph/badge.svg)](https://codecov.io/gh/hybridgroup/gocv)\n[![Go Report Card](https://goreportcard.com/badge/github.com/hybridgroup/gocv)](https://goreportcard.com/report/github.com/hybridgroup/gocv)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/hybridgroup/gocv/blob/release/LICENSE.txt)\n\nThe GoCV package provides Go language bindings for the [OpenCV 4](http://opencv.org/) computer vision library.\n\nThe GoCV package supports the latest releases of Go and OpenCV (v4.5.0) on Linux, macOS, and Windows. We intend to make the Go language a \"first-class\" client compatible with the latest developments in the OpenCV ecosystem.\n\nGoCV supports [CUDA](https://en.wikipedia.org/wiki/CUDA) for hardware acceleration using Nvidia GPUs. Check out the [CUDA README](./cuda/README.md) for more info on how to use GoCV with OpenCV/CUDA.\n\nGoCV also supports [Intel OpenVINO](https://software.intel.com/en-us/openvino-toolkit). Check out the [OpenVINO README](./openvino/README.md) for more info on how to use GoCV with the Intel OpenVINO toolkit.\n\n## How to use\n\n### Hello, video\n\nThis example opens a video capture device using device \"0\", reads frames, and shows the video in a GUI window:\n\n```go\npackage main\n\nimport (\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n\twebcam, _ := gocv.OpenVideoCapture(0)\n\twindow := gocv.NewWindow(\"Hello\")\n\timg := gocv.NewMat()\n\n\tfor {\n\t\twebcam.Read(&img)\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### Face detect\n\n![GoCV](https://raw.githubusercontent.com/hybridgroup/gocv/release/images/face-detect.jpg)\n\nThis is a more complete example that opens a video capture device using device \"0\". It also uses the CascadeClassifier class to load an external data file containing the classifier data. The program grabs each frame from the video, then uses the classifier to detect faces. If any faces are found, it draws a green rectangle around each one, then displays the video in an output window:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"image/color\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc main() {\n    // set to use a video capture device 0\n    deviceID := 0\n\n\t// open webcam\n\twebcam, err := gocv.OpenVideoCapture(deviceID)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tdefer webcam.Close()\n\n\t// open display window\n\twindow := gocv.NewWindow(\"Face Detect\")\n\tdefer window.Close()\n\n\t// prepare image matrix\n\timg := gocv.NewMat()\n\tdefer img.Close()\n\n\t// color for the rect when faces detected\n\tblue := color.RGBA{0, 0, 255, 0}\n\n\t// load classifier to recognize faces\n\tclassifier := gocv.NewCascadeClassifier()\n\tdefer classifier.Close()\n\n\tif !classifier.Load(\"data/haarcascade_frontalface_default.xml\") {\n\t\tfmt.Println(\"Error reading cascade file: data/haarcascade_frontalface_default.xml\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"start reading camera device: %v\\n\", deviceID)\n\tfor {\n\t\tif ok := webcam.Read(&img); !ok {\n\t\t\tfmt.Printf(\"cannot read device %v\\n\", deviceID)\n\t\t\treturn\n\t\t}\n\t\tif img.Empty() {\n\t\t\tcontinue\n\t\t}\n\n\t\t// detect faces\n\t\trects := classifier.DetectMultiScale(img)\n\t\tfmt.Printf(\"found %d faces\\n\", len(rects))\n\n\t\t// draw a rectangle around each face on the original image\n\t\tfor _, r := range rects {\n\t\t\tgocv.Rectangle(&img, r, blue, 3)\n\t\t}\n\n\t\t// show the image in the window, and wait 1 millisecond\n\t\twindow.IMShow(img)\n\t\twindow.WaitKey(1)\n\t}\n}\n```\n\n### More examples\n\nThere are examples in the [cmd directory](./cmd) of this repo in the form of various useful command line utilities, such as [capturing an image file](./cmd/saveimage), [streaming mjpeg video](./cmd/mjpeg-streamer), [counting objects that cross a line](./cmd/counter), and [using OpenCV with Tensorflow for object classification](./cmd/tf-classifier).\n\n## How to install\n\nTo install GoCV, run the following command:\n\n```\ngo get -u -d gocv.io/x/gocv\n```\n\nTo run code that uses the GoCV package, you must also install OpenCV 4.5.0 on your system. Here are instructions for Ubuntu, Raspian, macOS, and Windows.\n\n## Ubuntu/Linux\n\n### Installation\n\nYou can use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install\n\nIf you need static opencv libraries\n\n\tmake install BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n#### Install Cuda\n\n\t[cuda directory](./cuda)\n\n#### Install OpenVINO\n\n\t[openvino directory](./openvino)\n\t\n#### Install OpenVINO and Cuda\n\n\tThe following commands should do everything to download and install OpenCV 4.5.0 with Cuda and OpenVINO on Linux:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_all\n\nIf you need static opencv libraries\n\n\tmake install_all BUILD_SHARED_LIBS=OFF\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0-openvino\n    cuda information:\n      Device 0:  \"GeForce MX150\"  2003Mb, sm_61, Driver/Runtime ver.10.0/10.0\n\n#### Complete Install\n\nIf you have already done the \"Quick Install\" as described above, you do not need to run any further commands. For the curious, or for custom installations, here are the details for each of the steps that are performed when you run `make install`.\n\n##### Install required packages\n\nFirst, you need to change the current directory to the location of the GoCV repo, so you can access the `Makefile`:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNext, you need to update the system, and install any required packages:\n\n\tmake deps\n\n#### Download source\n\nNow, download the OpenCV 4.5.0 and OpenCV Contrib source code:\n\n\tmake download\n\n#### Build\n\nBuild everything. This will take quite a while:\n\n\tmake build\n\nIf you need static opencv libraries\n\n\tmake build BUILD_SHARED_LIBS=OFF\n\n#### Install\n\nOnce the code is built, you are ready to install:\n\n\tmake sudo_install\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n#### Cleanup extra files\n\nAfter the installation is complete, you can remove the extra files and folders:\n\n\tmake clean\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CPPFLAGS=\"-I/usr/local/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/lib -lopencv_core -lopencv_face -lopencv_videoio -lopencv_imgproc -lopencv_highgui -lopencv_imgcodecs -lopencv_objdetect -lopencv_features2d -lopencv_video -lopencv_dnn -lopencv_xfeatures2d\"\n\nPlease note that you will need to run these 2 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n### Docker\n\nThe project now provides `Dockerfile` which lets you build [GoCV](https://gocv.io/) Docker image which you can then use to build and run `GoCV` applications in Docker containers. The `Makefile` contains `docker` target which lets you build Docker image with a single command:\n\n```\nmake docker\n```\n\nBy default Docker image built by running the command above ships [Go](https://golang.org/) version `1.13.5`, but if you would like to build an image which uses different version of `Go` you can override the default value when running the target command:\n\n```\nmake docker GOVERSION='1.13.5'\n```\n\n#### Running GUI programs in Docker on macOS\n\nSometimes your `GoCV` programs create graphical interfaces like windows eg. when you use `gocv.Window` type when you display an image or video stream. Running the programs which create graphical interfaces in Docker container on macOS is unfortunately a bit elaborate, but not impossible. First you need to satisfy the following prerequisites:\n* install [xquartz](https://www.xquartz.org/). You can also install xquartz using [homebrew](https://brew.sh/) by running `brew cask install xquartz`\n* install [socat](https://linux.die.net/man/1/socat) `brew install socat`\n\nNote, you will have to log out and log back in to your machine once you have installed `xquartz`. This is so the X window system is reloaded.\n\nOnce you have installed all the prerequisites you need to allow connections from network clients to `xquartz`. Here is how you do that. First run the following command to open `xquart` so you can configure it:\n\n```shell\nopen -a xquartz\n```\nClick on *Security* tab in preferences and check the \"Allow connections\" box:\n\n![app image](./images/xquartz.png)\n\nNext, you need to create a TCP proxy using `socat` which will stream [X Window](https://en.wikipedia.org/wiki/X_Window_System) data into `xquart`. Before you start the proxy you need to make sure that there is no process listening in port `6000`. The following command should **not** return any results:\n\n```shell\nlsof -i TCP:6000\n```\nNow you can start a local proxy which will proxy the X Window traffic into xquartz which acts a your local X server:\n\n```shell\nsocat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\\"$DISPLAY\\\"\n```\n\nYou are now finally ready to run your `GoCV` GUI programs in Docker containers. In order to make everything work you must set `DISPLAY` environment variables as shown in a sample command below:\n\n```shell\ndocker run -it --rm -e DISPLAY=docker.for.mac.host.internal:0 your-gocv-app\n```\n\n**Note, since Docker for MacOS does not provide any video device support, you won't be able run GoCV apps which require camera.**\n\n### Alpine 3.7 Docker image\n\nThere is a Docker image with Alpine 3.7 that has been created by project contributor [@denismakogon](https://github.com/denismakogon). You can find it located at [https://github.com/denismakogon/gocv-alpine](https://github.com/denismakogon/gocv-alpine).\n\n## Raspbian\n\n### Installation\n\nWe have a special installation for the Raspberry Pi that includes some hardware optimizations. You use `make` to install OpenCV 4.5.0 with the handy `Makefile` included with this repo. If you already have installed OpenCV, you do not need to do so again. The installation performed by the `Makefile` is minimal, so it may remove OpenCV options such as Python or Java wrappers if you have already installed OpenCV some other way.\n\n#### Quick Install\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Raspbian:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\tmake install_raspi\n\nIf it works correctly, at the end of the entire process, the following message should be displayed:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n## macOS\n\n### Installation\n\nYou can install OpenCV 4.5.0 using Homebrew.\n\nIf you already have an earlier version of OpenCV (3.4.x) installed, you should probably remove it before installing the new version:\n\n\tbrew uninstall opencv\n\nYou can then install OpenCV 4.5.0:\n\n\tbrew install opencv\n\n### pkgconfig Installation\npkg-config is used to determine the correct flags for compiling and linking OpenCV.\nYou can install it by using Homebrew:\n\n    brew install pkgconfig\n\n### Verifying the installation\n\nTo verify your installation you can run one of the included examples.\n\nFirst, change the current directory to the location of the GoCV repo:\n\n\tcd $GOPATH/src/gocv.io/x/gocv\n\nNow you should be able to build or run any of the examples:\n\n\tgo run ./cmd/version/main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, pkg-config is used to determine the correct flags for compiling and linking OpenCV. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nFor example:\n\n\texport CGO_CXXFLAGS=\"--std=c++11\"\n\texport CGO_CPPFLAGS=\"-I/usr/local/Cellar/opencv/4.5.0/include\"\n\texport CGO_LDFLAGS=\"-L/usr/local/Cellar/opencv/4.5.0/lib -lopencv_stitching -lopencv_superres -lopencv_videostab -lopencv_aruco -lopencv_bgsegm -lopencv_bioinspired -lopencv_ccalib -lopencv_dnn_objdetect -lopencv_dpm -lopencv_face -lopencv_photo -lopencv_fuzzy -lopencv_hfs -lopencv_img_hash -lopencv_line_descriptor -lopencv_optflow -lopencv_reg -lopencv_rgbd -lopencv_saliency -lopencv_stereo -lopencv_structured_light -lopencv_phase_unwrapping -lopencv_surface_matching -lopencv_tracking -lopencv_datasets -lopencv_dnn -lopencv_plot -lopencv_xfeatures2d -lopencv_shape -lopencv_video -lopencv_ml -lopencv_ximgproc -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_flann -lopencv_xobjdetect -lopencv_imgcodecs -lopencv_objdetect -lopencv_xphoto -lopencv_imgproc -lopencv_core\"\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv ./cmd/version/main.go\n\n## Windows\n\n### Installation\n\nThe following assumes that you are running a 64-bit version of Windows 10.\n\nIn order to build and install OpenCV 4.5.0 on Windows, you must first download and install MinGW-W64 and CMake, as follows.\n\n#### MinGW-W64\n\nDownload and run the MinGW-W64 compiler installer from [https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/).\n\nThe latest version of the MinGW-W64 toolchain is `7.3.0`, but any version from `7.X` on should work.\n\nChoose the options for \"posix\" threads, and for \"seh\" exceptions handling, then install to the default location `c:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2`.\n\nAdd the `C:\\Program Files\\mingw-w64\\x86_64-7.3.0-posix-seh-rt_v5-rev2\\mingw64\\bin` path to your System Path.\n\n#### CMake\n\nDownload and install CMake [https://cmake.org/download/](https://cmake.org/download/) to the default location. CMake installer will add CMake to your system path.\n\n#### OpenCV 4.5.0 and OpenCV Contrib Modules\n\nThe following commands should do everything to download and install OpenCV 4.5.0 on Windows:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\twin_build_opencv.cmd\n\nIt might take up to one hour.\n\nLast, add `C:\\opencv\\build\\install\\x64\\mingw\\bin` to your System Path.\n\n### Verifying the installation\n\nChange the current directory to the location of the GoCV repo:\n\n\tchdir %GOPATH%\\src\\gocv.io\\x\\gocv\n\nNow you should be able to build or run any of the command examples:\n\n\tgo run cmd\\version\\main.go\n\nThe version program should output the following:\n\n\tgocv version: 0.22.0\n\topencv lib version: 4.5.0\n\nThat's it, now you are ready to use GoCV.\n\n### Cache builds\n\nIf you are running a version of Go older than v1.10 and not modifying GoCV source, precompile the GoCV package to significantly decrease your build times:\n\n\tgo install gocv.io/x/gocv\n\n### Custom Environment\n\nBy default, OpenCV is expected to be in `C:\\opencv\\build\\install\\include`. This behavior can be disabled by supplying `-tags customenv` when building/running your application. When building with this tag you will need to supply the CGO environment variables yourself.\n\nDue to the way OpenCV produces DLLs, including the version in the name, using this method is required if you're using a different version of OpenCV.\n\nFor example:\n\n\tset CGO_CXXFLAGS=\"--std=c++11\"\n\tset CGO_CPPFLAGS=-IC:\\opencv\\build\\install\\include\n\tset CGO_LDFLAGS=-LC:\\opencv\\build\\install\\x64\\mingw\\lib -lopencv_core412 -lopencv_face412 -lopencv_videoio412 -lopencv_imgproc412 -lopencv_highgui412 -lopencv_imgcodecs412 -lopencv_objdetect412 -lopencv_features2d412 -lopencv_video412 -lopencv_dnn412 -lopencv_xfeatures2d412 -lopencv_plot412 -lopencv_tracking412 -lopencv_img_hash412\n\nPlease note that you will need to run these 3 lines of code one time in your current session in order to build or run the code, in order to setup the needed ENV variables. Once you have done so, you can execute code that uses GoCV with your custom environment like this:\n\n\tgo run -tags customenv cmd\\version\\main.go\n\n## Android\n\nThere is some work in progress for running GoCV on Android using Gomobile. For information on how to install OpenCV/GoCV for Android, please see:\nhttps://gist.github.com/ogero/c19458cf64bd3e91faae85c3ac887481\n\nSee original discussion here:\nhttps://github.com/hybridgroup/gocv/issues/235\n\n## Profiling\n\nSince memory allocations for images in GoCV are done through C based code, the go garbage collector will not clean all resources associated with a `Mat`.  As a result, any `Mat` created *must* be closed to avoid memory leaks.\n\nTo ease the detection and repair of the resource leaks, GoCV provides a `Mat` profiler that records when each `Mat` is created and closed.  Each time a `Mat` is allocated, the stack trace is added to the profile.  When it is closed, the stack trace is removed. See the [runtime/pprof documentation](https://golang.org/pkg/runtime/pprof/#Profile).\n\nIn order to include the MatProfile custom profiler, you MUST build or run your application or tests using the `-tags matprofile` build tag. For example:\n\n\tgo run -tags matprofile cmd/version/main.go\n\nYou can get the profile's count at any time using:\n\n```go\ngocv.MatProfile.Count()\n```\n\nYou can display the current entries (the stack traces) with:\n\n```go\nvar b bytes.Buffer\ngocv.MatProfile.WriteTo(&b, 1)\nfmt.Print(b.String())\n```\n\nThis can be very helpful to track down a leak.  For example, suppose you have\nthe following nonsense program:\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tfmt.Printf(\"initial MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tleak()\n\n\tfmt.Printf(\"final MatProfile count: %v\\n\", gocv.MatProfile.Count())\n\tvar b bytes.Buffer\n\tgocv.MatProfile.WriteTo(&b, 1)\n\tfmt.Print(b.String())\n}\n```\n\nRunning this program produces the following output:\n\n```\ninitial MatProfile count: 0\nfinal MatProfile count: 1\ngocv.io/x/gocv.Mat profile: total 1\n1 @ 0x40b936c 0x40b93b7 0x40b94e2 0x40b95af 0x402cd87 0x40558e1\n#\t0x40b936b\tgocv.io/x/gocv.newMat+0x4b\t/go/src/gocv.io/x/gocv/core.go:153\n#\t0x40b93b6\tgocv.io/x/gocv.NewMat+0x26\t/go/src/gocv.io/x/gocv/core.go:159\n#\t0x40b94e1\tmain.leak+0x21\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:11\n#\t0x40b95ae\tmain.main+0xae\t\t\t/go/src/github.com/dougnd/gocvprofexample/main.go:16\n#\t0x402cd86\truntime.main+0x206\t\t/usr/local/Cellar/go/1.11.1/libexec/src/runtime/proc.go:201\n```\n\nWe can see that this program would leak memory.  As it exited, it had one `Mat` that was never closed.  The stack trace points to exactly which line the allocation happened on (line 11, the `gocv.NewMat()`).\n\nFurthermore, if the program is a long running process or if GoCV is being used on a web server, it may be helpful to install the HTTP interface )). For example:\n\n```go\npackage main\n\nimport (\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"time\"\n\n\t\"gocv.io/x/gocv\"\n)\n\nfunc leak() {\n\tgocv.NewMat()\n}\n\nfunc main() {\n\tgo func() {\n\t\tticker := time.NewTicker(time.Second)\n\t\tfor {\n\t\t\t<-ticker.C\n\t\t\tleak()\n\t\t}\n\t}()\n\n\thttp.ListenAndServe(\"localhost:6060\", nil)\n}\n\n```\n\nThis will leak a `Mat` once per second.  You can see the current profile count and stack traces by going to the installed HTTP debug interface: [http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat](http://localhost:6060/debug/pprof/gocv.io/x/gocv.Mat?debug=1).\n\n\n## How to contribute\n\nPlease take a look at our [CONTRIBUTING.md](./CONTRIBUTING.md) document to understand our contribution guidelines.\n\nThen check out our [ROADMAP.md](./ROADMAP.md) document to know what to work on next.\n\n## Why this project exists\n\nThe [https://github.com/go-opencv/go-opencv](https://github.com/go-opencv/go-opencv) package for Go and OpenCV does not support any version above OpenCV 2.x, and work on adding support for OpenCV 3 had stalled for over a year, mostly due to the complexity of [SWIG](http://swig.org/). That is why we started this project.\n\nThe GoCV package uses a C-style wrapper around the OpenCV 4 C++ classes to avoid having to deal with applying SWIG to a huge existing codebase. The mappings are intended to match as closely as possible to the original OpenCV project structure, to make it easier to find things, and to be able to figure out where to add support to GoCV for additional OpenCV image filters, algorithms, and other features.\n\nFor example, the [OpenCV `videoio` module](https://github.com/opencv/opencv/tree/master/modules/videoio) wrappers can be found in the GoCV package in the `videoio.*` files.\n\nThis package was inspired by the original https://github.com/go-opencv/go-opencv project, the blog post https://medium.com/@peterleyssens/using-opencv-3-from-golang-5510c312a3c and the repo at https://github.com/sensorbee/opencv thank you all!\n\n## License\n\nLicensed under the Apache 2.0 license. Copyright (c) 2017-2020 The Hybrid Group.\n\nLogo generated by GopherizeMe - https://gopherize.me\n"
 },
 {
  "repo": "hamuchiwa/AutoRCCar",
  "language": "Python",
  "readme_contents": "## AutoRCCar\n### Python3 + OpenCV3\n\nSee self-driving in action  \n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=BBwEF6WBUQs\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/BBwEF6WBUQs/0.jpg\" width=\"360\" height=\"240\" border=\"10\" /></a>\n\nThis project builds a self-driving RC car using Raspberry Pi, Arduino and open source software. Raspberry Pi collects inputs from a camera module and an ultrasonic sensor, and sends data to a computer wirelessly. The computer processes input images and sensor data for object detection (stop sign and traffic light) and collision avoidance respectively. A neural network model runs on computer and makes predictions for steering based on input images. Predictions are then sent to the Arduino for RC car control. \n  \n### Setting up environment with Anaconda\n  1. Install [`miniconda(Python3)`](https://conda.io/miniconda.html) on your computer\n  2. Create `auto-rccar` environment with all necessary libraries for this project  \n     ```conda env create -f environment.yml```\n     \n  3. Activate `auto-rccar` environment  \n     ```source activate auto-rccar```\n  \n  &ensp; To exit, simply close the terminal window. More info about managing Anaconda environment, please see [here](https://conda.io/docs/user-guide/tasks/manage-environments.html).\n  \n### About the files\n**test/**  \n  &emsp; &emsp; `rc_control_test.py`: RC car control with keyboard  \n  &emsp; &emsp;  `stream_server_test.py`: video streaming from Pi to computer  \n  &emsp; &emsp;  `ultrasonic_server_test.py`: sensor data streaming from Pi to computer  \n  &emsp; &emsp;  **model_train_test/**  \n      &emsp; &emsp;  &emsp; &emsp; `data_test.npz`: sample data  \n      &emsp; &emsp;  &emsp; &emsp; `train_predict_test.ipynb`: a jupyter notebook that goes through neural network model in OpenCV3  \n  \n**raspberryPi/**    \n  &emsp; &emsp;  `stream_client.py`:        stream video frames in jpeg format to the host computer  \n  &emsp; &emsp;  `ultrasonic_client.py`:    send distance data measured by sensor to the host computer  \n  \n**arduino/**  \n  &emsp; &emsp;  `rc_keyboard_control.ino`: control RC car controller  \n  \n**computer/**    \n  &emsp; &emsp;  **cascade_xml/**  \n      &emsp; &emsp;  &emsp; &emsp;  trained cascade classifiers  \n  &emsp; &emsp;  **chess_board/**   \n      &emsp; &emsp;  &emsp; &emsp;  images for calibration, captured by pi camera  \n      \n  &emsp; &emsp;  `picam_calibration.py`:     pi camera calibration  \n  &emsp; &emsp;  `collect_training_data.py`: collect images in grayscale, data saved as `*.npz`  \n  &emsp; &emsp;  `model.py`:                 neural network model  \n  &emsp; &emsp;  `model_training.py`:        model training and validation  \n  &emsp; &emsp;  `rc_driver_helper.py`:      helper classes/functions for `rc_driver.py`  \n  &emsp; &emsp;  `rc_driver.py`:             receive data from raspberry pi and drive the RC car based on model prediction  \n  &emsp; &emsp;  `rc_driver_nn_only.py`:     simplified `rc_driver.py` without object detection  \n  \n  \n**Traffic_signal**  \n  &emsp; &emsp;  trafic signal sketch contributed by [@geek111](https://github.com/geek1111)\n\n\n### How to drive\n1. **Testing:** Flash `rc_keyboard_control.ino` to Arduino and run `rc_control_test.py` to drive the RC car with keyboard. Run `stream_server_test.py` on computer and then run `stream_client.py` on raspberry pi to test video streaming. Similarly, `ultrasonic_server_test.py` and `ultrasonic_client.py` can be used for sensor data streaming testing.   \n\n2. **Pi Camera calibration (optional):** Take multiple chess board images using pi camera module at various angles and put them into **`chess_board`** folder, run `picam_calibration.py` and returned parameters from the camera matrix will be used in `rc_driver.py`.\n\n3. **Collect training/validation data:** First run `collect_training_data.py` and then run `stream_client.py` on raspberry pi. Press arrow keys to drive the RC car, press `q` to exit. Frames are saved only when there is a key press action. Once exit, data will be saved into newly created **`training_data`** folder.\n\n4. **Neural network training:** Run `model_training.py` to train a neural network model. Please feel free to tune the model architecture/parameters to achieve a better result. After training, model will be saved into newly created **`saved_model`** folder.\n\n5. **Cascade classifiers training (optional):** Trained stop sign and traffic light classifiers are included in the **`cascade_xml`** folder, if you are interested in training your own classifiers, please refer to [OpenCV doc](http://docs.opencv.org/doc/user_guide/ug_traincascade.html) and this great [tutorial](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n6. **Self-driving in action**: First run `rc_driver.py` to start the server on the computer (for simplified no object detection version, run `rc_driver_nn_only.py` instead), and then run `stream_client.py` and `ultrasonic_client.py` on raspberry pi. \n\n[\u4e2d\u6587\u6587\u6863](https://github.com/zhaoying9105/AutoRCCar) (\u611f\u8c22[zhaoying9105](https://github.com/zhaoying9105))\n"
 },
 {
  "repo": "esimov/pigo",
  "language": "Go",
  "readme_contents": "<h1 align=\"center\"><img alt=\"pigo-logo\" src=\"https://user-images.githubusercontent.com/883386/55795932-8787cf00-5ad1-11e9-8c3e-8211ba9427d8.png\" height=240/></h1>\r\n\r\n[![Build Status](https://travis-ci.org/esimov/pigo.svg?branch=master)](https://travis-ci.org/esimov/pigo)\r\n[![GoDoc](https://godoc.org/github.com/golang/gddo?status.svg)](https://godoc.org/github.com/esimov/pigo/core)\r\n[![license](https://img.shields.io/github/license/esimov/pigo)](./LICENSE)\r\n[![release](https://img.shields.io/badge/release-v1.4.2-blue.svg)](https://github.com/esimov/pigo/releases/tag/v1.4.2)\r\n[![snapcraft](https://img.shields.io/badge/snapcraft-v1.3.0-green.svg)](https://snapcraft.io/pigo)\r\n\r\nPigo is a pure Go face detection, pupil/eyes localization and facial landmark points detection library based on ***Pixel Intensity Comparison-based Object detection*** paper (https://arxiv.org/pdf/1305.4537.pdf).\r\n\r\n| Rectangle face marker | Circle face marker\r\n|:--:|:--:\r\n| ![rectangle](https://user-images.githubusercontent.com/883386/40916662-2fbbae1a-6809-11e8-8afd-d4ed40c7d4e9.png) | ![circle](https://user-images.githubusercontent.com/883386/40916683-447088a8-6809-11e8-942f-3112c10bede3.png) |\r\n\r\n### Motivation\r\nI've intended to implement this face detection method because all of the existing solutions for face detection in the Go ecosystem are only bindings to some C/C++ libraries like OpenCV, but installing OpenCV on various platforms is cumbersome.\r\n\r\nThe library does not require any third party modules or applications to be installed. However in case you wish to try the real time, webcam based face detection you might need to have Python2 and OpenCV installed, but **the core API does not require any third party module or external dependency**.\r\n\r\n### Key features\r\n- [x] Does not require OpenCV or any 3rd party modules to be installed\r\n- [x] High processing speed\r\n- [x] There is no need for image preprocessing prior detection\r\n- [x] There is no need for the computation of integral images, image pyramid, HOG pyramid or any other similar data structure\r\n- [x] The face detection is based on pixel intensity comparison encoded in the binary file tree structure\r\n- [x] Fast detection of in-plane rotated faces\r\n- [x] The library can detect even faces with eyeglasses\r\n- [x] [Pupils/eyes localization](#pupils--eyes-localization)\r\n- [x] [Facial landmark points detection](#facial-landmark-points-detection)\r\n- [x] **[Webassembly support \ud83c\udf89](#wasm-webassembly-support)**\r\n\r\n### Todo\r\n- [ ] Features detection and description\r\n\r\n**The library can also detect in plane rotated faces.** For this reason a new `-angle` parameter have been included into the command line utility. The command below will generate the following result (see the table below for all the supported options).\r\n\r\n```bash\r\n$ pigo -in input.jpg -out output.jpg -cf cascade/facefinder -angle=0.8 -iou=0.01\r\n```\r\n\r\n| Input file | Output file\r\n|:--:|:--:\r\n| ![input](https://user-images.githubusercontent.com/883386/50761018-015db180-1272-11e9-93d9-d3693cae9d66.jpg) | ![output](https://user-images.githubusercontent.com/883386/50761024-03277500-1272-11e9-9c20-2568b87a2344.png) |\r\n\r\n\r\nNote: In case of in plane rotated faces the angle value should be adapted to the provided image.\r\n\r\n### Pupils / eyes localization\r\n\r\nStarting from **v1.2.0** Pigo offer pupils/eyes localization capabilites. The implementation is based on [Eye pupil localization with an ensemble of randomized trees](https://www.sciencedirect.com/science/article/abs/pii/S0031320313003294).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/puploc\r\n\r\n![puploc](https://user-images.githubusercontent.com/883386/62784340-f5b3c100-bac6-11e9-865e-a2b4b9520b08.png)\r\n\r\n### Facial landmark points detection\r\n\r\n**v1.3.0** marks a new milestone in the library evolution, Pigo being able for facial landmark points detection. The implementation is based on [Fast Localization of Facial Landmark Points](https://arxiv.org/pdf/1403.6888.pdf).\r\n\r\nCheck out this example for a realtime demo: https://github.com/esimov/pigo/tree/master/examples/facial_landmark\r\n\r\n![flp_example](https://user-images.githubusercontent.com/883386/66802771-3b0cc880-ef26-11e9-9ee3-7e9e981ef3f7.png)\r\n\r\n## Install\r\n\r\n**Important note: for the Webassembly demo at least Go 1.13 is required!**\r\n\r\nInstall Go, set your `GOPATH`, and make sure `$GOPATH/bin` is on your `PATH`.\r\n\r\n```bash\r\n$ export GOPATH=\"$HOME/go\"\r\n$ export PATH=\"$PATH:$GOPATH/bin\"\r\n```\r\nNext download the project and build the binary file.\r\n\r\n```bash\r\n$ go get -u -f github.com/esimov/pigo/cmd/pigo\r\n$ go install\r\n```\r\n\r\n### Binary releases\r\nIn case you do not have installed or do not wish to install Go, you can obtain the binary file from the [releases](https://github.com/esimov/pigo/releases) folder.\r\n\r\nThe library can be accessed as a snapcraft function too.\r\n\r\n<a href=\"https://snapcraft.io/pigo\"><img src=\"https://raw.githubusercontent.com/snapcore/snap-store-badges/master/EN/%5BEN%5D-snap-store-white-uneditable.png\" alt=\"snapcraft pigo\"></a>\r\n\r\n## API\r\nBelow is a minimal example of using the face detection API.\r\n\r\nFirst you need to load and parse the binary classifier, then convert the image to grayscale mode,\r\nand finally to run the cascade function which returns a slice containing the row, column, scale and the detection score.\r\n\r\n```Go\r\ncascadeFile, err := ioutil.ReadFile(\"/path/to/cascade/file\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %v\", err)\r\n}\r\n\r\nsrc, err := pigo.GetImage(\"/path/to/image\")\r\nif err != nil {\r\n\tlog.Fatalf(\"Cannot open the image file: %v\", err)\r\n}\r\n\r\npixels := pigo.RgbToGrayscale(src)\r\ncols, rows := src.Bounds().Max.X, src.Bounds().Max.Y\r\n\r\ncParams := pigo.CascadeParams{\r\n\tMinSize:     20,\r\n\tMaxSize:     1000,\r\n\tShiftFactor: 0.1,\r\n\tScaleFactor: 1.1,\r\n\r\n\tImageParams: pigo.ImageParams{\r\n\t\tPixels: pixels,\r\n\t\tRows:   rows,\r\n\t\tCols:   cols,\r\n\t\tDim:    cols,\r\n\t},\r\n}\r\n\r\npigo := pigo.NewPigo()\r\n// Unpack the binary file. This will return the number of cascade trees,\r\n// the tree depth, the threshold and the prediction from tree's leaf nodes.\r\nclassifier, err := pigo.Unpack(cascadeFile)\r\nif err != nil {\r\n\tlog.Fatalf(\"Error reading the cascade file: %s\", err)\r\n}\r\n\r\nangle := 0.0 // cascade rotation angle. 0.0 is 0 radians and 1.0 is 2*pi radians\r\n\r\n// Run the classifier over the obtained leaf nodes and return the detection results.\r\n// The result contains quadruplets representing the row, column, scale and detection score.\r\ndets := classifier.RunCascade(cParams, angle)\r\n\r\n// Calculate the intersection over union (IoU) of two clusters.\r\ndets = classifier.ClusterDetections(dets, 0.2)\r\n```\r\n\r\n**A note about imports**:  in order to decode the image you will need to import `image/jpeg` or `image/png` (depending on the provided image type) and the Pigo library as well, otherwise you will get a `\"Image: Unkown format\"` error. See the following example:\r\n```Go\r\nimport (\r\n    _ \"image/jpeg\"\r\n    pigo \"github.com/esimov/pigo/core\"\r\n)\r\n```\r\n\r\n## Usage\r\nA command line utility is bundled into the library to detect faces in static images.\r\n\r\n```bash\r\n$ pigo -in input.jpg -out out.jpg -cf cascade/facefinder\r\n```\r\n\r\n### Supported flags:\r\n\r\n```bash\r\n$ pigo --help\r\n\r\n\u250c\u2500\u2510\u252c\u250c\u2500\u2510\u250c\u2500\u2510\r\n\u251c\u2500\u2518\u2502\u2502 \u252c\u2502 \u2502\r\n\u2534  \u2534\u2514\u2500\u2518\u2514\u2500\u2518\r\n\r\nGo (Golang) Face detection library.\r\n    Version: 1.4.2\r\n\r\n  -angle float\r\n    \t0.0 is 0 radians and 1.0 is 2*pi radians\r\n  -cf string\r\n    \tCascade binary file\r\n  -flpc string\r\n    \tFacial landmark points cascade directory\r\n  -in string\r\n    \tSource image (default \"-\")\r\n  -iou float\r\n    \tIntersection over union (IoU) threshold (default 0.2)\r\n  -json string\r\n    \tOutput the detection points into a json file\r\n  -mark\r\n    \tMark detected eyes (default true)\r\n  -marker string\r\n    \tDetection marker: rect|circle|ellipse (default \"rect\")\r\n  -max int\r\n    \tMaximum size of face (default 1000)\r\n  -min int\r\n    \tMinimum size of face (default 20)\r\n  -out string\r\n    \tDestination image (default \"-\")\r\n  -plc string\r\n    \tPupils/eyes localization cascade file\r\n  -scale float\r\n    \tScale detection window by percentage (default 1.1)\r\n  -shift float\r\n    \tShift detection window by percentage (default 0.1)\r\n```\r\n\r\n**Important notice:** In case the `plc` flag is not empty and the provided path is a valid file it will run the pupil/eyes detection method. The same is true for the `flpc` flag, only that in this case you need to provide the directory to the landmark point cascades found under `cascades/lps`.\r\n\r\n### CLI command examples\r\nYou can also use the `stdin` and `stdout` pipe commands:\r\n\r\n```bash\r\n$ cat input/source.jpg | pigo > -in - -out - >out.jpg -cf=/path/to/cascade\r\n```\r\n\r\n`in` and `out` default to `-` so you can also use:\r\n```bash\r\n$ cat input/source.jpg | pigo >out.jpg -cf=/path/to/cascade\r\n$ pigo -out out.jpg < input/source.jpg -cf=/path/to/cascade\r\n```\r\nUsing the `empty` string as value for the `-out` flag will skip the image generation part. This, combined with the `-json` flag will encode the detection results into the specified json file. You can also use the pipe `-` value for the `-json` flag to output the detection coordinates to the standard (`stdout`) output.\r\n\r\n## Real time face detection (running as a shared object)\r\n\r\nIn case you wish to test the library real time face detection capabilities using a webcam, the `examples` folder contains a  web and a few Python examples. Prior running it you need to have Python2 and OpenCV2 installed.\r\n\r\nSelect one of the few Python files provided in the `examples` folder and simply run them. Each of them will execute the exported Go binary file as a shared library. This is also a proof of concept how Pigo can be integrated into different programming languages. I have provided examples only for Python, since this was the only viable way to access the webcam, the Go ecosystem suffering badly from a comprehensive, cross platform and widely available library for accessing the webcam.\r\n\r\n## WASM (Webassembly) support \ud83c\udf89\r\n\r\nStarting from version **v1.4.0** the library has been ported to [**WASM**](http://webassembly.org/). This gives the library a huge performance gain in terms of real time face detection capabilities. \r\n\r\n### WASM demo\r\n\r\nTo run the `wasm` demo select the `wasm` folder and type `make`.\r\n\r\nFor more details check the subpage description: https://github.com/esimov/pigo/tree/master/wasm.\r\n\r\n## Benchmark results\r\n\r\nBelow are the benchmark results obtained running Pigo against [GoCV](https://github.com/hybridgroup/gocv) using the same conditions.\r\n\r\n```\r\n    BenchmarkGoCV-4   \t       3\t 414122553 ns/op\t     704 B/op\t       1 allocs/op\r\n    BenchmarkPIGO-4   \t      10\t 173664832 ns/op\t       0 B/op\t       0 allocs/op\r\n    PASS\r\n    ok  \tgithub.com/esimov/gocv-test\t4.530s\r\n```\r\nThe code used for the above test can be found under the following link: https://github.com/esimov/pigo-gocv-benchmark\r\n\r\n## Author\r\n\r\n* Endre Simo ([@simo_endre](https://twitter.com/simo_endre))\r\n\r\n## License\r\n\r\nCopyright \u00a9 2019 Endre Simo\r\n\r\nThis software is distributed under the MIT license. See the [LICENSE](https://github.com/esimov/pigo/blob/master/LICENSE) file for the full license text.\r\n"
 },
 {
  "repo": "jrosebr1/imutils",
  "language": "Python",
  "readme_contents": "# imutils\nA series of convenience functions to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and ***both*** Python 2.7 and Python 3.\n\nFor more information, along with a detailed code review check out the following posts on the [PyImageSearch.com](http://www.pyimagesearch.com) blog:\n\n- [http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/](http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/)\n- [http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/)\n- [http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/](http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/)\n\n## Installation\nProvided you already have NumPy, SciPy, Matplotlib, and OpenCV already installed, the `imutils` package is completely `pip`-installable:\n\n<pre>$ pip install imutils</pre>\n\n## Finding function OpenCV functions by name\nOpenCV can be a big, hard to navigate library, especially if you are just getting started learning computer vision and image processing. The `find_function` method allows you to quickly search function names across modules (and optionally sub-modules) to find the function you are looking for.\n\n#### Example:\nLet's find all function names that contain the text `contour`:\n\n<pre>import imutils\nimutils.find_function(\"contour\")</pre>\n\n#### Output:\n<pre>1. contourArea\n2. drawContours\n3. findContours\n4. isContourConvex</pre>\n\nThe `contourArea` function could therefore be accessed via: `cv2.contourArea`\n\n\n## Translation\nTranslation is the shifting of an image in either the *x* or *y* direction. To translate an image in OpenCV you would need to supply the *(x, y)*-shift, denoted as *(t<sub>x</sub>, t<sub>y</sub>)* to construct the translation matrix *M*:\n\n![Translation equation](docs/images/translation_eq.png?raw=true)\n\nAnd from there, you would need to apply the `cv2.warpAffine` function.\n\nInstead of manually constructing the translation matrix *M* and calling `cv2.warpAffine`, you can simply make a call to the `translate` function of `imutils`.\n\n#### Example:\n<pre># translate the image x=25 pixels to the right and y=75 pixels up\ntranslated = imutils.translate(workspace, 25, -75)</pre>\n\n#### Output:\n\n<img src=\"docs/images/translation.png?raw=true\" alt=\"Translation example\" style=\"max-width: 500px;\">\n\n## Rotation\nRotating an image in OpenCV is accomplished by making a call to `cv2.getRotationMatrix2D` and `cv2.warpAffine`. Further care has to be taken to supply the *(x, y)*-coordinate of the point the image is to be rotated about. These calculation calls can quickly add up and make your code bulky and less readable. The `rotate` function in `imutils` helps resolve this problem.\n\n#### Example:\n<pre># loop over the angles to rotate the image\nfor angle in xrange(0, 360, 90):\n\t# rotate the image and display it\n\trotated = imutils.rotate(bridge, angle=angle)\n\tcv2.imshow(\"Angle=%d\" % (angle), rotated)</pre>\n\n#### Output:\n\n<img src=\"docs/images/rotation.png?raw=true\" alt=\"Rotation example\" style=\"max-width: 500px;\">\n\n## Resizing\nResizing an image in OpenCV is accomplished by calling the `cv2.resize` function. However, special care needs to be taken to ensure that the aspect ratio is maintained.  This `resize` function of `imutils` maintains the aspect ratio and provides the keyword arguments `width` and `height` so the image can be resized to the intended width/height while (1) maintaining aspect ratio and (2) ensuring the dimensions of the image do not have to be explicitly computed by the developer.\n\nAnother optional keyword argument, `inter`, can be used to specify interpolation method as well.\n\n#### Example:\n<pre># loop over varying widths to resize the image to\nfor width in (400, 300, 200, 100):\n\t# resize the image and display it\n\tresized = imutils.resize(workspace, width=width)\n\tcv2.imshow(\"Width=%dpx\" % (width), resized)</pre>\n\n#### Output:\n\n<img src=\"docs/images/resizing.png?raw=true\" alt=\"Resizing example\" style=\"max-width: 500px;\">\n\n## Skeletonization\nSkeletonization is the process of constructing the \"topological skeleton\" of an object in an image, where the object is presumed to be white on a black background. OpenCV does not provide a function to explicitly construct the skeleton, but does provide the morphological and binary functions to do so.\n\nFor convenience, the `skeletonize` function of `imutils` can be used to construct the topological skeleton of the image.\n\nThe first argument, `size` is the size of the structuring element kernel. An optional argument, `structuring`, can be used to control the structuring element -- it defaults to `cv2.MORPH_RECT`\t, but can be any valid structuring element.\n\n#### Example:\n<pre># skeletonize the image\ngray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nskeleton = imutils.skeletonize(gray, size=(3, 3))\ncv2.imshow(\"Skeleton\", skeleton)</pre>\n\n#### Output:\n\n<img src=\"docs/images/skeletonization.png?raw=true\" alt=\"Skeletonization example\" style=\"max-width: 500px;\">\n\n## Displaying with Matplotlib\nIn the Python bindings of OpenCV, images are represented as NumPy arrays in BGR order. This works fine when using the `cv2.imshow` function. However, if you intend on using Matplotlib, the `plt.imshow` function assumes the image is in RGB order. A simple call to `cv2.cvtColor` will resolve this problem, or you can use the `opencv2matplotlib` convenience function.\n\n#### Example:\n<pre># INCORRECT: show the image without converting color spaces\nplt.figure(\"Incorrect\")\nplt.imshow(cactus)\n\n# CORRECT: convert color spaces before using plt.imshow\nplt.figure(\"Correct\")\nplt.imshow(imutils.opencv2matplotlib(cactus))\nplt.show()</pre>\n\n#### Output:\n\n<img src=\"docs/images/matplotlib.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## URL to Image\nThis the `url_to_image` function accepts a single parameter: the `url` of the image we want to download and convert to a NumPy array in OpenCV format. This function performs the download in-memory. The `url_to_image` function has been detailed [here](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/) on the PyImageSearch blog.\n\n#### Example:\n<pre>url = \"http://pyimagesearch.com/static/pyimagesearch_logo_github.png\"\nlogo = imutils.url_to_image(url)\ncv2.imshow(\"URL to Image\", logo)\ncv2.waitKey(0)</pre>\n\n#### Output:\n\n<img src=\"docs/images/url_to_image.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Checking OpenCV Versions\nOpenCV 3 has finally been released! But with the major release becomes backward compatibility issues (such as with the `cv2.findContours` and `cv2.normalize` functions). If you want your OpenCV 3 code to be backwards compatible with OpenCV 2.4.X, you'll need to take special care to check which version of OpenCV is currently being used and then take appropriate action. The `is_cv2()` and `is_cv3()` are simple functions that can be used to automatically determine the OpenCV version of the current environment.\n\n#### Example:\n<pre>print(\"Your OpenCV version: {}\".format(cv2.__version__))\nprint(\"Are you using OpenCV 2.X? {}\".format(imutils.is_cv2()))\nprint(\"Are you using OpenCV 3.X? {}\".format(imutils.is_cv3()))</pre>\n\n#### Output:\n<pre>Your OpenCV version: 3.0.0\nAre you using OpenCV 2.X? False\nAre you using OpenCV 3.X? True</pre>\n\n## Automatic Canny Edge Detection\nThe Canny edge detector requires two parameters when performing hysteresis. However, tuning these two parameters to obtain an optimal edge map is non-trivial, especially when working with a dataset of images. Instead, we can use the `auto_canny` function which uses the median of the grayscale pixel intensities to derive the upper and lower thresholds. You can read more about the `auto_canny` function [here](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/).\n\n#### Example:\n<pre>gray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\nedgeMap = imutils.auto_canny(gray)\ncv2.imshow(\"Original\", logo)\ncv2.imshow(\"Automatic Edge Map\", edgeMap)</pre>\n\n#### Output:\n\n<img src=\"docs/images/auto_canny.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## 4-point Perspective Transform\nA common task in computer vision and image processing is to perform a 4-point perspective transform of a ROI in an image and obtain a top-down, \"birds eye view\" of the ROI. The `perspective` module takes care of this for you. A real-world example of applying a 4-point perspective transform can be bound in this blog on on [building a kick-ass mobile document scanner](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/).\n\n#### Example\nSee the contents of `demos/perspective_transform.py`\n\n#### Output:\n\n<img src=\"docs/images/perspective_transform.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## Sorting Contours\nThe contours returned from `cv2.findContours` are unsorted. By using the `contours` module the the `sort_contours` function we can sort a list of contours from left-to-right, right-to-left, top-to-bottom, and bottom-to-top, respectively.\n\n#### Example:\nSee the contents of `demos/sorting_contours.py`\n\n#### Output:\n\n<img src=\"docs/images/sorting_contours.png?raw=true\" alt=\"Matplotlib example\" style=\"max-width: 500px;\">\n\n## (Recursively) Listing Paths to Images\nThe `paths` sub-module of `imutils` includes a function to recursively find images based on a root directory.\n\n#### Example:\nAssuming we are in the `demos` directory, let's list the contents of the `../demo_images`:\n\n<pre>from imutils import paths\nfor imagePath in paths.list_images(\"../demo_images\"):\n\tprint imagePath</pre>\n\n#### Output:\n<pre>../demo_images/bridge.jpg\n../demo_images/cactus.jpg\n../demo_images/notecard.png\n../demo_images/pyimagesearch_logo.jpg\n../demo_images/shapes.png\n../demo_images/workspace.jpg</pre>\n"
 },
 {
  "repo": "mapillary/OpenSfM",
  "language": "JavaScript",
  "readme_contents": "OpenSfM ![Docker workflow](https://github.com/mapillary/opensfm/workflows/Docker%20CI/badge.svg)\n=======\n\n## Overview\nOpenSfM is a Structure from Motion library written in Python. The library serves as a processing pipeline for reconstructing camera poses and 3D scenes from multiple images. It consists of basic modules for Structure from Motion (feature detection/matching, minimal solvers) with a focus on building a robust and scalable reconstruction pipeline. It also integrates external sensor (e.g. GPS, accelerometer) measurements for geographical alignment and robustness. A JavaScript viewer is provided to preview the models and debug the pipeline.\n\n<p align=\"center\">\n  <img src=\"https://docs.opensfm.org/_images/berlin_viewer.jpg\" />\n</p>\n\nCheckout this [blog post with more demos](http://blog.mapillary.com/update/2014/12/15/sfm-preview.html)\n\n\n## Getting Started\n\n* [Building the library][]\n* [Running a reconstruction][]\n* [Documentation][]\n\n\n[Building the library]: https://docs.opensfm.org/building.html (OpenSfM building instructions)\n[Running a reconstruction]: https://docs.opensfm.org/using.html (OpenSfM usage)\n[Documentation]: https://docs.opensfm.org  (OpenSfM documentation)\n\n## License\nOpenSfM is BSD-style licensed, as found in the LICENSE file.  See also the Facebook Open Source [Terms of Use][] and [Privacy Policy][]\n\n[Terms of Use]: https://opensource.facebook.com/legal/terms (Facebook Open Source - Terms of Use)\n[Privacy Policy]: https://opensource.facebook.com/legal/privacy (Facebook Open Source - Privacy Policy)\n"
 },
 {
  "repo": "HuTianQi/SmartOpenCV",
  "language": "C++",
  "readme_contents": "# SmartOpenCV\n\n![SmartOpenCV](art/logo.png)  \n### \u524d\u8a00\n\n:fire: :fire: :fire: \u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u4ee5\u53ca\u7ec8\u7aef\u8bbe\u5907\u786c\u4ef6\u6c34\u5e73\u7684\u4e0d\u65ad\u63d0\u5347\uff0c\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u76f4\u63a5\u8fd0\u884c\u667a\u80fd\u7cfb\u7edf\u6210\u4e3a\u53ef\u80fd\uff0c\u7aef\u4fa7\u667a\u80fd\u5177\u5907\u4f4e\u5ef6\u65f6\uff0c\u9690\u79c1\u5b89\u5168\u7b49\u7279\u70b9\u3002\u540c\u65f6\u964d\u4f4e\u4e86\u4e91\u7aef\u667a\u80fd\u5b58\u5728\u7684\u7f51\u7edc\u4f20\u8f93\u4e0d\u53ef\u9760\u98ce\u9669\uff0c\u4f7f\u5f97\u7aef\u4fa7\u667a\u80fd\u8d8a\u6765\u8d8a\u5f97\u5230\u91cd\u89c6\u3002\u7aef\u4fa7\u667a\u80fd\u6bd4\u8f83\u6210\u719f\u7684\u9886\u57df\u5c31\u662fNLP\u4ee5\u53caCV\u3002\u5728CV\u9886\u57dfOpenCV\u4f5c\u4e3a\u5f00\u6e90\u4e14\u5f3a\u5927\u7684\u8de8\u5e73\u53f0\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u5728\u56fe\u50cf\u5904\u7406\u4ee5\u53ca\u56fe\u50cf\u8bc6\u522b\u65b9\u5411\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u4f46\u662f\u5728Android\u5e73\u53f0OpenCV\u5b98\u65b9SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u8bf8\u591a\u7f3a\u9677\u3002\n\n### SmartOpenCV\u662f\u4ec0\u4e48\nSmartOpenCV\u662f\u4e00\u4e2aOpenCV\u5728Android\u7aef\u7684\u589e\u5f3a\u5e93\uff0c\u89e3\u51b3\u4e86OpenCV Android SDK\u5728\u56fe\u50cf\u9884\u89c8\u65b9\u9762\u5b58\u5728\u7684\u8bf8\u591a\u95ee\u9898\uff0c\u800c\u4e14\u65e0\u9700\u4fee\u6539OpenCV SDK\u6e90\u7801\uff0c\u4e0eOpenCV\u7684SDK\u89e3\u8026\uff0c\u53ea\u9700\u66ff\u6362xml\u4e2d\u539fOpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\u8fbe\u5230\u5177\u5907OpenCV\u5b98\u65b9SDK\u7684\u539f\u529f\u80fd\u4ee5\u53caSmartOpenCV\u7684\u589e\u5f3a\u529f\u80fd\u3002\n\n### OpenCV\u5b98\u65b9SDK\u5b58\u5728\u7684\u95ee\u9898\n\nOpenCV Android\u7aefSDK\u867d\u7136\u5f88\u5bb9\u6613\u4e0a\u624b\u548c\u4f7f\u7528\uff0c\u4f46\u662f\u9884\u89c8\u5b58\u5728\u5f88\u591a\u95ee\u9898\uff0c\u5e38\u89c1\u95ee\u9898\u5982\u4e0b\uff1a\n\n- **\u9ed8\u8ba4\u6a2a\u5c4f\u663e\u793a\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u63a5\u53e3\u4fee\u6539\u9884\u89c8\u65b9\u5411**\n\n- **\u9884\u89c8\u7ed8\u5236\u5b58\u5728\u9ed1\u8fb9**\uff1aOpenCV\u9ed8\u8ba4\u7ed8\u5236\u7b97\u6cd5\u5728\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\u5230Canvas\u65f6\u5b58\u5728\u4e00\u5b9a\u7684\u504f\u79fb\uff0c\u5728\u89c6\u89c9\u4e0a\u8868\u73b0\u5c31\u662f\u9884\u89c8\u5e27\u53ea\u4f1a\u5360SurfaceView\u63a7\u4ef6\u7684\u4e00\u90e8\u5206\u533a\u57df\uff0c\u504f\u79fb\u90e8\u5206\u533a\u57df\u4f1a\u663e\u793a\u4e3a\u9ed1\u8272\n\n  ```java\n  if (mScale != 0) {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2),\n                      (int) ((canvas.getWidth() - mScale * mCacheBitmap.getWidth()) / 2 + mScale * mCacheBitmap.getWidth()),\n                      (int) ((canvas.getHeight() - mScale * mCacheBitmap.getHeight()) / 2 + mScale * mCacheBitmap.getHeight())), null);\n  } else {\n      canvas.drawBitmap(mCacheBitmap, new Rect(0, 0, mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),\n              new Rect((canvas.getWidth() - mCacheBitmap.getWidth()) / 2,\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2,\n                      (canvas.getWidth() - mCacheBitmap.getWidth()) / 2 + mCacheBitmap.getWidth(),\n                      (canvas.getHeight() - mCacheBitmap.getHeight()) / 2 + mCacheBitmap.getHeight()), null);\n  }\n  ```\n\n  \n\n- **\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\u4e0d\u7b26\u5408\u5b9e\u9645\u573a\u666f\u8981\u6c42**\uff1a\u5bf9\u4e8e\u9884\u89c8\u5e27\u5927\u5c0f\u7684\u9009\u62e9\uff0cOpenCV\u9ed8\u8ba4\u7b97\u6cd5\u662f\u9009\u62e9**\u5c0f\u4e8e**\u9884\u89c8\u63a7\u4ef6(\u6216\u8bbe\u7f6e\u7684\u6700\u5927\u5e27\u5927\u5c0f)\u7684\u6700\u5927\u9884\u89c8\uff0c\u8fd9\u5c06\u5bfc\u81f4\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u9884\u89c8\u56fe\u50cf\u7684\u663e\u793a\u4e0d\u80fd\u94fa\u6ee1\u6574\u4e2a\u63a7\u4ef6\u751a\u81f3\u8fdc\u5c0f\u4e8e\u63a7\u4ef6\u5927\u5c0f\uff0c \u5728\u7edd\u5927\u90e8\u5206\u4e1a\u52a1\u573a\u666f\u4e0b\uff0c\u8fd9\u79cd\u7b97\u6cd5\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\n\n  ```java\n  protected Size calculateCameraFrameSize(List<?> supportedSizes, ListItemAccessor accessor, int surfaceWidth, int surfaceHeight) {\n          int calcWidth = 0;\n          int calcHeight = 0;\n  \n          int maxAllowedWidth = (mMaxWidth != MAX_UNSPECIFIED && mMaxWidth < surfaceWidth)? mMaxWidth : surfaceWidth;\n          int maxAllowedHeight = (mMaxHeight != MAX_UNSPECIFIED && mMaxHeight < surfaceHeight)? mMaxHeight : surfaceHeight;\n  \n          for (Object size : supportedSizes) {\n              int width = accessor.getWidth(size);\n              int height = accessor.getHeight(size);\n              Log.d(TAG, \"trying size: \" + width + \"x\" + height);\n  \n              if (width <= maxAllowedWidth && height <= maxAllowedHeight) {\n                  if (width >= calcWidth && height >= calcHeight) {\n                      calcWidth = (int) width;\n                      calcHeight = (int) height;\n                  }\n              }\n          }\n          if ((calcWidth == 0 || calcHeight == 0) && supportedSizes.size() > 0)\n          {\n              Log.i(TAG, \"fallback to the first frame size\");\n              Object size = supportedSizes.get(0);\n              calcWidth = accessor.getWidth(size);\n              calcHeight = accessor.getHeight(size);\n          }\n  \n          return new Size(calcWidth, calcHeight);\n      }\n  ```\n\n  \n\n\n### SmartOpenCV\u7684\u7279\u70b9\n\n- **\u6613\u4f7f\u7528**\uff1a\u5982\u679c\u4f60\u9879\u76ee\u4e2d\u4e4b\u524d\u4f7f\u7528\u7684\u662fOpenCV\u7684\u5b98\u65b9SDK\uff0c\u90a3\u4e48\u5f15\u5165SmartOpenCV\u540e\u53ea\u9700\u5c06xml\u6587\u4ef6\u4e2d\u7684`JavaCameraView`/`JavaCamera2View`\u66ff\u6362\u4e3aSmartOpenCV\u7684`CamerPreview`/`Camera2Preview`\u5373\u53ef\u8fbe\u5230\u4e0e\u4f7f\u7528\u5b98\u65b9SDK\u76f8\u540c\u7684\u6548\u679c\n\n- **\u529f\u80fd\u589e\u5f3a**\uff1a\n  1. \u9884\u89c8\u81ea\u9002\u5e94\uff1a\u81ea\u52a8\u6839\u636e\u524d\u540e\u6444\u50cf\u5934\uff0c\u6a2a\u7ad6\u5c4f\u4ee5\u53ca\u4e0d\u540c\u6444\u50cf\u5934\u53c2\u6570\u6765\u8c03\u6574\u4e0e\u9002\u914d\u9884\u89c8\u65b9\u5411\u4ee5\u53ca\u5927\u5c0f\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5199\u4efb\u4f55\u989d\u5916\u4ee3\u7801\n  2. \u53ef\u6269\u5c55\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u7ed8\u5236\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u573a\u666f\u81ea\u5b9a\u4e49\u9884\u89c8\u7ed8\u5236\u7b97\u6cd5\n  3. \u53ef\u6269\u5c55\u9884\u89c8\u5e27\u5927\u5c0f\u9009\u62e9\u7b97\u6cd5\uff1aSmartOpenCV\u5185\u7f6e\u4e86\u4e00\u79cd\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7b56\u7565\u63a5\u53e3\u8ba9\u5f00\u53d1\u8005\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b97\u6cd5\n  4. \u652f\u6301**USB\u6444\u50cf\u5934**\uff1aUSB\u6444\u50cf\u5934\u4f5c\u4e3a\u5916\u8bbe\u63a5\u5165\u8bbe\u5907\uff0c\u548c\u624b\u673a/\u5e73\u677f\u7b49\u79fb\u52a8\u8bbe\u5907\u5185\u7f6e\u6444\u50cf\u5934\u5b58\u5728\u5dee\u5f02\uff0cSDK\u5185\u90e8\u5728\u5904\u7406\u79fb\u52a8\u8bbe\u5907\u6444\u50cf\u5934\u7684\u903b\u8f91\u65f6\u4e5f\u517c\u5bb9\u4e86\u5bf9\u95f8\u673a\u7b49\u7684USB\u6444\u50cf\u5934\u7684\u5904\u7406\n  \n- **\u63d0\u4f9b\u66f4\u53cb\u597d\u7684API\u63a5\u53e3**\uff1a\u5728\u7ee7\u627fOpenCV\u5b98\u65b9\u63a5\u53e3\u7684\u540c\u65f6\uff0cSmartOpenCV\u5c06\u4f17\u591a\u7e41\u6742\u64cd\u4f5c\u7edf\u4e00\u901a\u8fc7CameraConfiguration\u6765\u914d\u7f6e\uff0c\u63d0\u4f9b\u66f4\u53cb\u597d\u7684Fluent API\u63a5\u53e3\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\n\n- **\u4e0d\u76f4\u63a5\u4f9d\u8d56\u5b98\u65b9SDK\uff0c\u65b9\u4fbf\u5347\u7ea7\u5b98\u65b9SDK**\uff1a\u4e0eOpenCV\u5b98\u65b9SDK\u89e3\u8026\uff0c\u53ea\u8981\u5b98\u65b9SDK\u5185\u90e8\u6838\u5fc3\u903b\u8f91\u672a\u505a\u4fee\u6539\uff0c\u90a3\u4e48SmartOpenCV\u53ef\u4ee5\u517c\u5bb9\u6240\u6709\u7248\u672c\u7684\u5b98\u65b9SDK\uff0c\u4f7f\u7528SmartOpenCV\u540e\u5982\u679c\u4ee5\u540e\u6253\u7b97\u5347\u7ea7\u4f9d\u8d56\u7684OpenCV\u4e3a\u66f4\u65b0\u7248\u672c\uff0c\u53ea\u9700\u5c06OpenCV\u7684\u4f9d\u8d56\u66f4\u65b0\u4e3a\u65b0\u7248\u672c\u5373\u53ef\uff0c\u4ee3\u7801\u65e0\u9700\u505a\u4efb\u4f55\u6539\u52a8\n\n### \u6548\u679c\u5bf9\u6bd4\n#### \u4ee5\u4eba\u8138\u8bc6\u522b\u4e3a\u4f8b\n\n|            | \u6a2a\u5c4f                                                         | \u7ad6\u5c4f   |\n| ---------- | ------------------------------------------------------------ | ------ |\n| OpenCV | <div align=center>**\u5373\u4f7f\u5bbd\u4e0e\u9ad8\u90fd\u8bbe\u7f6e\u4e3amatch_parent\u4e5f\u65e0\u6cd5\u5168\u5c4f\uff0c\u5b58\u5728\u9ed1\u8fb9**  <img src=\"./art/screenshort/opencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\"/></div>  |<div align=center> **\u5b58\u5728\u9ed1\u8fb9\uff0c\u4e14\u9ed8\u8ba4\u4e0d\u652f\u6301\u7ad6\u5c4f**  <img src=\"./art/screenshort/opencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\" /></div>  |\n| SmartOpenCV | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_landscape.jpg\" width = \"60%\" height = \"60%\" /></div> | <div align=center><img src=\"./art/screenshort/smartopencv_back_camera_portrait.jpg\" width = \"60%\" height = \"50%\"/></div> |\n\n### Demo\u5bf9\u6bd4\u4f53\u9a8c\n[smartopencv-app-debug.apk](demo/smartopencv-app-debug.apk)  \n[opencv-app-debug.apk](demo/opencv-app-debug.apk)  \n\n### Integration\n\nStep1\uff1a\u5728\u9879\u76ee\u6839\u76ee\u5f55\u7684build.gradle\u4e2d\u6dfb\u52a0\u5bf9jitpack\u4ed3\u5e93\u7684\u914d\u7f6e\n\n```\nallprojects {\n    repositories {\n        ...\n        maven { url 'https://jitpack.io' }\n    }\n}\n```\n\nStep2\uff1a\u5728\u9700\u8981\u4f7f\u7528`SmartOpenCV`\u5e93\u7684\u6a21\u5757\u4e2d\u6dfb\u52a0\u4f9d\u8d56\n\n```\ndependencies {\n\timplementation('com.github.HuTianQi:SmartOpenCV:1.0.1') { // \u7248\u672c\u53f7\u5efa\u8bae\u4f7f\u7528\u5df2release\u7684\u6700\u65b0\u7248\u672c\n        exclude module: 'openCVLibrary411' // \u7531\u4e8e\u76ee\u524d\u591a\u6a21\u5757\u4f9d\u8d56\u65f6jitpack\u6253\u5305\u5b58\u5728bug\uff0c\u6392\u9664\u6253\u5305\u65f6\u4f9d\u8d56\u7684\u8be5\u6a21\u5757\n    }\n}\n```\n\n\n### Usage\n\n#### \u57fa\u7840\u7528\u6cd5\n\n\u5728\u9879\u76ee\u4e2d\u9700\u8981\u4f7f\u7528\u9884\u89c8\u7684xml\u4e2d\u7528SmartOpenCV\u7684`CameraPreview`/`Camera2Preview`\u66ff\u6362OpenCV\u7684`JavaCameraView`/`JavaCamera2View`\u5373\u53ef\uff0c\u5c31\u8fd9\u4e48\u7b80\u5355\uff0c\u5176\u4f59\u7684\u4ec0\u4e48\u90fd\u4e0d\u7528\u505a\n\n```xml\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <!--<org.opencv.android.JavaCameraView-->\n    <!--android:id=\"@+id/fd_activity_surface_view\"-->\n    <!--android:layout_width=\"match_parent\"-->\n    <!--android:layout_height=\"match_parent\" />-->\n\n    <tech.huqi.smartopencv.core.preview.CameraPreview\n        android:id=\"@+id/fd_activity_surface_view\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</LinearLayout>\n```\n\n#### \u9ad8\u7ea7\u7528\u6cd5\n\n\u5982\u679c\u6253\u7b97\u901a\u8fc7SmartOpenCV\u63d0\u4f9b\u7684\u63a5\u53e3\u6765\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u9884\u89c8\u663e\u793a\u76f8\u5173\u53c2\u6570\u4e0e\u914d\u7f6e\uff0c\u90a3\u4e48\u8c03\u7528`SmartOpenCV.getInstance().init()`\u4f20\u5165\u524d\u9762\u83b7\u53d6\u7684\u9884\u89c8\u63a7\u4ef6\u5bf9\u8c61\u5373\u53ef\uff0c\u7528\u6cd5\u5982\u4e0b\uff1a\n\n```java\nSmartOpenCV.getInstance().init(mOpenCvCameraView, new CameraConfiguration.Builder()\n    .debug(true)\n    .cameraIndex(0)      // \u8bbe\u7f6e\u6444\u50cf\u5934\u7d22\u5f15,\u4e3b\u8981\u7528\u4e8e\u591a\u6444\u50cf\u5934\u8bbe\u5907\uff0c\u4f18\u5148\u7ea7\u4f4e\u4e8efrontCamera\n    .keepScreenOn(false) // \u662f\u5426\u4fdd\u6301\u5c4f\u5e55\u5e38\u4eae\n    .frontCamera(true)   // \u662f\u5426\u4f7f\u7528\u524d\u7f6e\u6444\u50cf\u5934\n    .openCvDefaultDrawStrategy(false)      // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u56fe\u50cf\u7ed8\u5236\u7b56\u7565\n    .openCvDefaultPreviewCalculator(false) // \u662f\u5426\u4f7f\u7528OpenCV\u9ed8\u8ba4\u7684\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n    .landscape(false)     // \u662f\u5426\u6a2a\u5c4f\u663e\u793a\n    .enableFpsMeter(true) // \u5f00\u542f\u9884\u89c8\u5e27\u7387\u7684\u663e\u793a\n    .usbCamera(false)     // \u662f\u5426\u4f7f\u7528USB\u6444\u50cf\u5934\uff0c\u5f53\u8bbe\u5907\u63a5\u5165\u7684\u662fUSB\u6444\u50cf\u5934\u65f6\u5c06\u5176\u8bbe\u7f6e\u4e3atrue\n    .maxFrameSize(400, 300)     // \u8bbe\u7f6e\u9884\u89c8\u5e27\u7684\u6700\u5927\u5927\u5c0f\n    .cvCameraViewListener(this) // \u8bbe\u7f6eOpenCV\u56de\u8c03\u76d1\u542c\u5668\n    .previewSizeCalculator(new IPreviewSizeCalculator() { // \u81ea\u5b9a\u4e49\u9884\u89c8\u5e27\u5927\u5c0f\u8ba1\u7b97\u7b56\u7565\n        @Override\n        public Size calculateCameraFrameSize(List<Size> supportedSizes, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u6539\u5199\u89c8\u5e27\u5927\u5c0f\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n            return new Size(1080,1920); \n        }\n    })\n    .drawStrategy(new IDrawStrategy() { // \u81ea\u5b9a\u4e49\u7ed8\u5236\u7b56\u7565\n        @Override\n        public void drawBitmap(Canvas canvas, Bitmap frameBitmap, int surfaceWidth, int surfaceHeight) {\n            // \u82e5\u9700\u6839\u636e\u81ea\u5df1\u7684\u5177\u4f53\u4e1a\u52a1\u573a\u666f\u7ed8\u5236\u9884\u89c8\u5e27\u56fe\u50cf\uff0c\u8986\u5199\u8be5\u65b9\u6cd5\u903b\u8f91\n        }\n    })\n    .build());\n```\n\n\n\n### LICENSE\n[LICENSE](LICENSE)  \n\n### \u516c\u4f17\u53f7\n![\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\u4ea4\u6d41\u53cd\u9988](art/wx_gzh.jpg)  \n"
 },
 {
  "repo": "soruly/trace.moe",
  "language": "PHP",
  "readme_contents": "# trace.moe\n\n[![License](https://img.shields.io/github/license/soruly/trace.moe.svg)](https://github.com/soruly/trace.moe/blob/master/LICENSE)\n[![Discord](https://img.shields.io/discord/437578425767559188.svg)](https://discord.gg/K9jn6Kj)\n[![Donate](https://img.shields.io/badge/donate-patreon-orange.svg)](https://www.patreon.com/soruly)\n\nThe website of trace.moe (whatanime.ga)\n\nImage Reverse Search for Anime Scenes\n\nUse anime screenshots to search where this scene is taken from.\n\nIt tells you which anime, which episode, and exactly which moment this scene appears in Japanese Anime.\n\n## Demo\n\nDemo image\n\n![](https://images.plurk.com/2FKxneXP64qiKwjlUA7sKj.jpg)\n\nSearch result tells you which moment it appears.\n\n![](https://addons.cdn.mozilla.net/user-media/previews/full/209/209947.png)\n\n## How does it work\n\ntrace.moe uses [sola](https://github.com/soruly/sola) to index video and work with liresolr. This repo only include the webapp for trace.moe, which demonstrate how to integrate anilist info, and how thumbnail/video previews are generated. If you want to make your own video scene search engine, please refer to sola instead.\n\nTo learn more, read the presentation slides below\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga.slide) May 2016\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2017.slide) Jun 2017\n\n[Presentation slides](https://go-talks.appspot.com/github.com/soruly/slides/whatanime.ga-2018.slide) Jun 2018\n\n[Presentation slides](https://github.com/soruly/slides/blob/master/2019-COSCUP-trace.moe.md) Aug 2019\n\nSystem Overview\n\n![](https://pbs.twimg.com/media/CstZmrxUIAAi8La.jpg)\n\nYou may find some other related repo here\n\n- [sola](https://github.com/soruly/sola)\n- [LireSolr](https://github.com/soruly/liresolr)\n- [anilist-crawler](https://github.com/soruly/anilist-crawler)\n- [trace.moe-WebExtension](https://github.com/soruly/trace.moe-WebExtension)\n- [trace.moe-telegram-bot](https://github.com/soruly/trace.moe-telegram-bot)\n\n## Official API Docs (Beta)\n\nhttps://soruly.github.io/trace.moe/\n\n## Mobile Apps (3rd party)\n\nWhatAnime by Andr\u00e9e Torres\nhttps://play.google.com/store/apps/details?id=com.maddog05.whatanime\nSource: https://github.com/maddog05/whatanime-android\n\nWhatAnime - \u4ee5\u56fe\u641c\u756a by Mystery0 (Simplified Chinese)\nhttps://play.google.com/store/apps/details?id=pw.janyo.whatanime\nSource: https://github.com/JanYoStudio/WhatAnime\n\n## Integrating search with trace.moe\n\nTo add trace.moe as a search option for your site, pass the image URL via query string like this\n\n```\nhttps://trace.moe/?url=http://searchimageurl\n```\n\nYou can also specify playback options like this\n\n```\nhttps://trace.moe/?autoplay=0&loop&mute=1&url=http://searchimageurl\n```\n\nPlayback URL params:\n\n| param    | value  | default (not set in URL param) | set with empty or other value |\n| -------- | ------ | ------------------------------ | ----------------------------- |\n| autoplay | 0 or 1 | 1                              | 1                             |\n| mute     | 0 or 1 | 0                              | 1                             |\n| loop     | 0 or 1 | 0                              | 1                             |\n\nThe `auto` URL parameter is no longer used, it would always search automatically when there is `?url=` param.\n\nNote that the server cannot access private image URLs.\nIn that case, users has to copy and paste (Ctrl+V/Cmd+V) the image directly, or save and upload the file.\n"
 },
 {
  "repo": "CodecWang/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "# \u9762\u5411\u521d\u5b66\u8005\u7684OpenCV-Python\u6559\u7a0b\n\n- \u6559\u7a0b\u5730\u5740: [http://codec.wang/#/opencv/](http://codec.wang/#/opencv/)\n- \u672c\u4ed3\u5e93\u4e3a\u6559\u7a0b\u4e2d\u6240\u7528\u5230\u7684\u6e90\u7801\u3001\u56fe\u7247\u548c\u97f3\u89c6\u9891\u7d20\u6750\u7b49\n\n![](http://cos.codec.wang/opencv-python-tutorial-amend-new-cover.png)\n\n## \u76ee\u5f55\n\n### \u5165\u95e8\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u7b80\u4ecb\u4e0e\u5b89\u88c5](http://codec.wang/#/opencv/start/01-introduction-and-installation) | \u4e86\u89e3\u548c\u5b89\u88c5OpenCV-Python |\n| [\u756a\u5916\u7bc7: \u4ee3\u7801\u6027\u80fd\u4f18\u5316](http://codec.wang/#/opencv/start/extra-01-code-optimization) | \u5ea6\u91cf\u8fd0\u884c\u65f6\u95f4/\u63d0\u5347\u6548\u7387\u7684\u51e0\u79cd\u65b9\u5f0f |\n| [\u57fa\u672c\u5143\u7d20: \u56fe\u7247](http://codec.wang/#/opencv/start/02-basic-element-image) | \u56fe\u7247\u7684\u8f7d\u5165/\u663e\u793a\u548c\u4fdd\u5b58 |\n| [\u756a\u5916\u7bc7: \u65e0\u635f\u4fdd\u5b58\u548cMatplotlib\u4f7f\u7528](http://codec.wang/#/opencv/start/extra-02-high-quality-save-and-matplotlib) | \u9ad8\u4fdd\u771f\u4fdd\u5b58\u56fe\u7247\u3001Matplotlib\u5e93\u7684\u7b80\u5355\u4f7f\u7528 |\n| [\u6253\u5f00\u6444\u50cf\u5934](http://codec.wang/#/opencv/start/03-open-camera) | \u6253\u5f00\u6444\u50cf\u5934\u6355\u83b7\u56fe\u7247/\u5f55\u5236\u89c6\u9891/\u64ad\u653e\u672c\u5730\u89c6\u9891 |\n| [\u756a\u5916\u7bc7: \u6ed1\u52a8\u6761](http://codec.wang/#/opencv/start/extra-03-trackbar) | \u6ed1\u52a8\u6761\u7684\u4f7f\u7528 |\n| [\u56fe\u50cf\u57fa\u672c\u64cd\u4f5c](http://codec.wang/#/opencv/start/04-basic-operations) | \u8bbf\u95ee\u50cf\u7d20\u70b9/ROI/\u901a\u9053\u5206\u79bb\u5408\u5e76/\u56fe\u7247\u5c5e\u6027 |\n| [\u989c\u8272\u7a7a\u95f4\u8f6c\u6362](http://codec.wang/#/opencv/start/05-changing-colorspaces) | \u989c\u8272\u7a7a\u95f4\u8f6c\u6362/\u8ffd\u8e2a\u7279\u5b9a\u989c\u8272\u7269\u4f53 |\n| [\u9608\u503c\u5206\u5272](http://codec.wang/#/opencv/start/06-image-thresholding) | \u9608\u503c\u5206\u5272/\u4e8c\u503c\u5316 |\n| [\u756a\u5916\u7bc7: Otsu\u9608\u503c\u6cd5](http://codec.wang/#/opencv/start/extra-04-otsu-thresholding) | \u53cc\u5cf0\u56fe\u7247/Otsu\u81ea\u52a8\u9608\u503c\u6cd5 |\n| [\u56fe\u50cf\u51e0\u4f55\u53d8\u6362](http://codec.wang/#/opencv/start/07-image-geometric-transformation) | \u65cb\u8f6c/\u5e73\u79fb/\u7f29\u653e/\u7ffb\u8f6c |\n| [\u756a\u5916\u7bc7: \u4eff\u5c04\u53d8\u6362\u4e0e\u900f\u89c6\u53d8\u6362](http://codec.wang/#/opencv/start/extra-05-warpaffine-warpperspective) | \u57fa\u4e8e2\u00d73\u7684\u4eff\u5c04\u53d8\u6362/\u57fa\u4e8e3\u00d73\u7684\u900f\u89c6\u53d8\u6362 |\n| [\u7ed8\u56fe\u529f\u80fd](http://codec.wang/#/opencv/start/08-drawing-function) | \u753b\u7ebf/\u753b\u5706/\u753b\u77e9\u5f62/\u6dfb\u52a0\u6587\u5b57 |\n| [\u756a\u5916\u7bc7: \u9f20\u6807\u7ed8\u56fe](http://codec.wang/#/opencv/start/extra-06-drawing-with-mouse) | \u7528\u9f20\u6807\u5b9e\u65f6\u7ed8\u56fe |\n| [\u6311\u6218\u7bc7: \u753b\u52a8\u6001\u65f6\u949f](http://codec.wang/#/opencv/start/challenge-01-draw-dynamic-clock) | / |\n| [\u6311\u6218\u7bc7: PyQt5\u7f16\u5199GUI\u754c\u9762](http://codec.wang/#/opencv/start/challenge-02-create-gui-with-pyqt5) | / |\n\n### \u57fa\u7840\u7bc7\n\n| \u6807\u9898 | \u7b80\u4ecb |\n| :--- | :--- |\n| [\u56fe\u50cf\u6df7\u5408](http://codec.wang/#/opencv/basic/09-image-blending) | \u7b97\u6570\u8fd0\u7b97/\u6df7\u5408/\u6309\u4f4d\u8fd0\u7b97 |\n| [\u756a\u5916\u7bc7: \u4eae\u5ea6\u4e0e\u5bf9\u6bd4\u5ea6](http://codec.wang/#/opencv/basic/extra-07-contrast-and-brightness) | \u8c03\u6574\u56fe\u7247\u7684\u4eae\u5ea6\u548c\u5bf9\u6bd4\u5ea6 |\n| [\u5e73\u6ed1\u56fe\u50cf](http://codec.wang/#/opencv/basic/10-smoothing-images) | \u5377\u79ef/\u6ee4\u6ce2/\u6a21\u7cca/\u964d\u566a |\n| [\u756a\u5916\u7bc7: \u5377\u79ef\u57fa\u7840-\u56fe\u7247\u8fb9\u6846](http://codec.wang/#/opencv/basic/extra-08-padding-and-convolution) | \u4e86\u89e3\u5377\u79ef/\u6ee4\u6ce2\u7684\u57fa\u7840\u77e5\u8bc6/\u7ed9\u56fe\u7247\u6dfb\u52a0\u8fb9\u6846 |\n| [\u8fb9\u7f18\u68c0\u6d4b](http://codec.wang/#/opencv/basic/11-edge-detection) | Canny/Sobel\u7b97\u5b50 |\n| [\u756a\u5916\u7bc7: \u56fe\u50cf\u68af\u5ea6](http://codec.wang/#/opencv/basic/extra-09-image-gradients) | \u4e86\u89e3\u56fe\u50cf\u68af\u5ea6\u548c\u8fb9\u7f18\u68c0\u6d4b\u7684\u76f8\u5173\u6982\u5ff5 |\n| [\u8150\u8680\u4e0e\u81a8\u80c0](http://codec.wang/#/opencv/basic/12-erode-and-dilate) | \u5f62\u6001\u5b66\u64cd\u4f5c/\u8150\u8680/\u81a8\u80c0/\u5f00\u8fd0\u7b97/\u95ed\u8fd0\u7b97 |\n| [\u8f6e\u5ed3](http://codec.wang/#/opencv/basic/13-contours) | \u5bfb\u627e/\u7ed8\u5236\u8f6e\u5ed3 |\n| [\u756a\u5916\u7bc7: \u8f6e\u5ed3\u5c42\u7ea7](http://codec.wang/#/opencv/basic/extra-10-contours-hierarchy) | \u4e86\u89e3\u8f6e\u5ed3\u95f4\u7684\u5c42\u7ea7\u5173\u7cfb |\n| [\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/14-contour-features) | \u9762\u79ef/\u5468\u957f/\u6700\u5c0f\u5916\u63a5\u77e9\\(\u5706\\)/\u5f62\u72b6\u5339\u914d |\n| [\u756a\u5916\u7bc7: \u51f8\u5305\u53ca\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81](http://codec.wang/#/opencv/basic/extra-11-convex-hull) | \u8ba1\u7b97\u51f8\u5305/\u4e86\u89e3\u66f4\u591a\u8f6e\u5ed3\u7279\u5f81 |\n| [\u76f4\u65b9\u56fe](http://codec.wang/#/opencv/basic/15-histograms) | \u8ba1\u7b97\u7ed8\u5236\u76f4\u65b9\u56fe/\u5747\u8861\u5316 |\n| [\u6a21\u677f\u5339\u914d](http://codec.wang/#/opencv/basic/16-template-matching) | \u56fe\u4e2d\u627e\u5c0f\u56fe |\n| [\u970d\u592b\u53d8\u6362](http://codec.wang/#/opencv/basic/17-hough-transform) | \u63d0\u53d6\u76f4\u7ebf/\u5706 |\n| [\u6311\u6218\u4efb\u52a1: \u8f66\u9053\u68c0\u6d4b](http://codec.wang/#/opencv/basic/challenge-03-lane-road-detection) | / |\n\n> \u5982\u679c\u60a8\u89c9\u5f97\u5199\u7684\u4e0d\u9519\u7684\u8bdd\uff0c\u6b22\u8fce\u6253\u8d4f\uff0c\u6211\u4f1a\u52aa\u529b\u5199\u51fa\u66f4\u597d\u7684\u5185\u5bb9\uff01\u270a\ud83e\udd1f\n\n![](http://cos.codec.wang/wechat_alipay_pay_pic.png)\n\n"
 },
 {
  "repo": "Roujack/mathAI",
  "language": "Python",
  "readme_contents": "# mathAI\n\n\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c\u3002\n**\u8bf7\u67e5\u770b\u7cfb\u7edf\u6587\u6863\u8bf4\u660e\u6765\u8fd0\u884c\u7a0b\u5e8f\u3002\u6ce8\u610f\uff0c\u8fd9\u662f\u4e00\u4e2a\u534a\u5f00\u6e90\u7684\u9879\u76ee\uff0c\u76ee\u524d\u4e0a\u4f20\u7684\u7248\u672c\u53ea\u80fd\u5904\u7406\u7b80\u5355\u7684\u4e00\u7ef4\u52a0\u51cf\u4e58\u9664\u7b97\u672f\u8868\u8fbe\u5f0f\uff08\u5982\u679c\u60f3\u8981\u8bc6\u522b\u66f4\u52a0\u590d\u6742\u7684\u8868\u8fbe\u5f0f\uff0c\u53ef\u4ee5\u53c2\u8003\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u7684\u8bba\u6587\uff09\u3002\u53ef\u4ee5\u53c2\u8003\u7684\u4ee3\u7801\u662f\u524d\u9762\u5b57\u7b26\u8bc6\u522b\u90e8\u5206\u4ee5\u53ca\u6574\u4e2a\u7b97\u6cd5\u5904\u7406\u6846\u67b6\u3002**\n![image](https://github.com/Roujack/mathAI/blob/master/test.png)\n\n\u6574\u4e2a\u7a0b\u5e8f\u4f7f\u7528python\u5b9e\u73b0\uff0c\u5177\u4f53\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u4e86\u56fe\u50cf\u9884\u5904\u7406\u3001\u5b57\u7b26\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bc6\u522b\u3001\u6570\u5b66\u516c\u5f0f\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u679c\u8f93\u51fa\u3002\n\n\u672c\u7a0b\u5e8f\u4f7f\u7528opencv\u5bf9\u8f93\u5165\u7684\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5e76\u5c06\u5b57\u7b26\u88c1\u526a\u51fa\u6765\u518d\u5f52\u4e00\u5316\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u77e9\u9635\u3002\u6211\u5728TensorFlow\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2alenet5\n\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u8bc6\u522b\u6570\u5b66\u5b57\u7b26\uff0c\u8bad\u7ec3\u4f7f\u7528CHROME\u6570\u636e\u96c6\u3002\u5bf9\u4e8e\u6570\u5b66\u516c\u5f0f\u7684\u8bc6\u522b\uff0c\u4e3b\u8981\u662f\u5c06\u8bc6\u522b\u51fa\u7684\u72ec\u7acb\u7684\u5b57\u7b26\u7ec4\u7ec7\u6210\u8ba1\u7b97\u673a\u80fd\u591f\n\u7406\u89e3\u7684\u6570\u5b66\u516c\u5f0f\uff08\u8fd9\u91cc\u7684\u6570\u5b66\u516c\u5f0f\u5c31\u662f\u7eaf\u5b57\u7b26\u7684\u53ef\u6c42\u89e3\u7684\u6570\u5b66\u8ba1\u7b97\u9898\uff09\u3002\u5927\u6982\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u7f16\u8bd1\u539f\u7406\u7684\u7b97\u7b26\u4f18\u5148\u6cd5\u548c\u9012\u5f52\u4e0b\u964d\u6cd5\u8fdb\u884c\u5b9e\u73b0\u3002\n\u7136\u540e\u6839\u636e\u5c5e\u6027\u6587\u6cd5\u7684\u503c\u4f20\u9012\u601d\u60f3\uff0c\u5c06\u6570\u5b66\u516c\u5f0f\u7684\u503c\u8ba1\u7b97\u51fa\u6765\u3002\u6700\u540e\u4f7f\u7528python\u7684matlibplot\u5e93\u628a\u8ba1\u7b97\u8fc7\u7a0b\u548c\u7b54\u6848\u6253\u5370\u51fa\u6765\u3002\n\n\u4f18\u70b9\uff1a\u8fd9\u662f\u4e00\u6574\u5957\u62cd\u7167\u505a\u9898\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u540c\u65f6\u80fd\u591f\u5904\u7406\u591a\u79cd\u591a\u6837\u7684\u8ba1\u7b97\u9898\uff0c\u76ee\u524d\u5e02\u9762\u4e0a\u8fd8\u6ca1\u6709\u770b\u5230\u5b9e\u73b0\u3002OCR\u6280\u672f\u5982\u6b64\u6210\u719f\u7684\u4eca\u5929\u5b57\u7b26\u8bc6\u522b\n\u5df2\u7ecf\u4e0d\u7b97\u6709\u6311\u6218\u7684\u4e1c\u897f\u4e86\u3002\n\u7f3a\u70b9\uff1a\u5b57\u7b26\u7a7a\u95f4\u5173\u7cfb\u5224\u65ad\u53ea\u7528\u4e86\u4eba\u7c7b\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u56fe\u50cf\u9884\u5904\u7406\u4e0d\u591f\u9c81\u68d2\uff0c\u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784\u8bc6\u522b\u7b97\u6cd5\u4e0d\u591f\u5b8c\u7f8e\uff08\u53ef\u4ee5\u8003\u8651\u4f7f\u7528\u4e8c\u7ef4\u6587\u6cd5\u6765\u505a\uff09\u3002\n\u7cfb\u7edf\u8fd8\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002\n"
 },
 {
  "repo": "makelove/OpenCV-Python-Tutorial",
  "language": "Python",
  "readme_contents": "- \u6211\u5728B\u7ad9\u505a\u89c6\u9891\u535a\u5ba2VLoger\uff0c\u6b22\u8fce\u5927\u5bb6\u6765\u6367\u573a\u3002\u4e0d\u53ea\u662fOpenCV\n    - \u7a0b\u5e8f\u5458\u8d5a\u94b1\u6307\u5357 https://space.bilibili.com/180948619\n\n# [OpenCV-Python-Tutorial](https://github.com/makelove/OpenCV-Python-Tutorial)\n- \u6709\u670b\u53cb\u53cd\u6620\u8bf4\u4e0b\u8f7drepo\u6162\uff0c\u56e0\u4e3adata\u6709\u4e00\u4e9b\u89c6\u9891sample\n    - \u73b0\u57282020-8-15\u628arepo\u538b\u7f29\uff0c\u4e0a\u4f20\u5230\u767e\u5ea6\u4e91\u76d8\n        - \u94fe\u63a5: https://pan.baidu.com/s/1jpjpfum5EMpNrZoEHGvn1g \u63d0\u53d6\u7801: 8cab\n\n## [OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf](OpenCV-Python-Tutorial-\u4e2d\u6587\u7248.pdf)\n- \u8fd9\u4e2arepo\u662f\u8fd9\u672c\u4e66PDF\u7684\u6240\u6709\u6e90\u4ee3\u7801\uff0c\u51e0\u4e4e\u90fd\u88ab\u6d4b\u8bd5\u8fc7\uff0c\u80fd\u6b63\u5e38\u8fd0\u884c\u3002\u7a0b\u5e8f\u4f7f\u7528\u7684\u56fe\u7247\u548c\u89c6\u9891\uff0c\u90fd\u5728data\u6587\u4ef6\u5185\u3002\n\n### \u5e73\u65f6\u4f1a\u6dfb\u52a0\u4e00\u4e9b\u6709\u8da3\u7684\u4ee3\u7801\uff0c\u5b9e\u73b0\u67d0\u79cd\u529f\u80fd\u3002\n- \u5b98\u7f51 https://opencv.org/\n- \u5b98\u65b9\u6587\u6863api https://docs.opencv.org/4.0.0/\n- \u5b98\u65b9\u82f1\u6587\u6559\u7a0b http://docs.opencv.org/3.2.0/d6/d00/tutorial_py_root.html\n\n## \u8fd0\u884c:\u5b98\u65b9samples/demo.py \u4f1a\u6709\u5f88\u591a\u6709\u8da3\u7684\u4f8b\u5b50\uff0c\u4ecb\u7ecd\u4f60\u53bb\u4e86\u89e3OpenCV\u7684\u529f\u80fd\u3002\n\n\n~~python 2.7 \u5206\u652f\u88ab\u5e9f\u5f03\u4e86\uff0c\u4e0d\u518d\u66f4\u65b0~~\n\n~~# \u6dfb\u52a0\u4e86 Python3.6\u5206\u652f,\n\u8be5\u5206\u652f\u662f\u4f7f\u7528 opencv3.2+Python3.6~~\n\n## \u628a\u539f\u6765\u7684master\u5206\u652f\u6539\u4e3apython2.7\u5206\u652f\uff0cpython3.6\u5206\u652f\u6539\u4e3amaster\u5206\u652f\n* git clone https://github.com/makelove/OpenCV-Python-Tutorial.git\n* ~~git checkout python3.6~~\n\n##### \u5efa\u8bae\u4f7f\u7528PyCharm\u6765\u7f16\u5199/\u8c03\u8bd5Python\u4ee3\u7801\n\n## \u5f00\u53d1\u73af\u5883\n* macOS Mojave 10.14\n* Python 3.6.1\n* OpenCV 3.2.0\n* PyCharm 2018.3\n\n\n### VMware \u865a\u62df\u673a\n\u5982\u679c\u5b89\u88c5OpenCV\u6709\u95ee\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528VMware \u865a\u62df\u673a\u5b89\u88c5Ubuntu\u7cfb\u7edf\uff0c\u672c\u4eba\u53ef\u4ee5\u5e2e\u4f60\u4eec\u5b89\u88c5\u4e00\u4e2a\uff0c\u518d\u5171\u4eab\u5230\u767e\u5ea6\u4e91\n\n### \u6811\u8393\u6d3e3b\n\u672c\u4eba\u6709\u4e00\u5757\u3010\u6811\u8393\u6d3e3b\u3011\u5f00\u53d1\u677f\uff0c\u4e5f\u5b89\u88c5\u4e86OpenCV3\uff0c\u5f88\u597d\u7528\uff0c\u5efa\u8bae\u4f60\u4eec\u4e5f\u4e70\u4e00\u5757\u6765\u73a9\u4e00\u73a9\u3002\n\n### \u6444\u50cf\u5934\n* MacBook pro\u81ea\u5e26\n* \u6dd8\u5b9d\uff0c[130W\u50cf\u7d20\u9ad8\u6e05\u6444\u50cf\u5934\u6a21\u7ec4 720P 1280x720 USB2.0\u514d\u9a71 \u5fae\u8ddd\u6a21\u5757](https://s.click.taobao.com/gOB3ACw)\n* \u6dd8\u5b9d\uff0c[\u6811\u8393\u6d3e3\u4ee3B Raspberry Pi USB\u6444\u50cf\u5934\uff0c\u514d\u9a71\u52a8](https://s.click.taobao.com/kTu2ACw) \u4e0d\u597d\u7528\uff0c\u53ef\u89c6\u89d2\u5ea6\u592a\u5c0f\uff01\n* Kinect for Xbox360 Slim\uff0c AUX\u63a5\u53e3\u4e0d\u80fd\u76f4\u63a5\u63d2\u5165\u7535\u8111\uff0c\u9700\u8981\u8d2d\u4e70\u7535\u6e90\u9002\u914d\u5668 [\u6dd8\u5b9d](https://s.click.taobao.com/t?e=m%3D2%26s%3DuOhQTZaHKEQcQipKwQzePOeEDrYVVa64LKpWJ%2Bin0XLjf2vlNIV67rEUhWAGPPKrYFMBzHxYoCOlldgrEKAMDfvtTsPa%2Bvw8FDXjhIkoffd7RTQd3LKg2nJi6DFpZGNc%2Bht3wBcxEogkdIkZMKiRbrUG0ypJDuSgXlTpbZcV4j5YC7K2OdchcA%3D%3D&scm=null&pvid=null&app_pvid=59590_11.9.33.73_524_1585572680125&ptl=floorId%3A17741&originalFloorId%3A17741&app_pvid%3A59590_11.9.33.73_524_1585572680125&union_lens=lensId%3APUB%401585572666%400b1a25a5_48ac_1712b7ede03_179a%40023mXY9mmpUNuNySUoJofoOt)\n\n## \u6559\u7a0b\u8d44\u6e90\n- http://www.learnopencv.com/\n- http://www.pyimagesearch.com/\n- [YouTube\u4e0asentex\u7684OpenCV\u89c6\u9891\u6559\u7a0b](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq)\n- B\u7ad9 [OpenCV YouTube](https://search.bilibili.com/all?keyword=OpenCV%20YouTube)\n- [\u5b98\u65b9\u6559\u7a0b](https://opencv.org/courses/)\n\n## \u65b0\u95fbNews https://opencv.org/news.html\n- \u4e2d\u6587\u8bba\u575b http://www.opencv.org.cn/\n- [OpenCV 3.3\u53d1\u5e03\u4e86](http://opencv.org/opencv-3-3.html) \n    1. \u4e3b\u8981\u6d88\u606f\u662f\u6211\u4eec\u5c06DNN\u6a21\u5757\u4eceopencv_contrib\u63a8\u5e7f\u5230\u4e3b\u5b58\u50a8\u5e93\uff0c\u6539\u8fdb\u548c\u52a0\u901f\u4e86\u5f88\u591a\u3002\u4e0d\u518d\u9700\u8981\u5916\u90e8BLAS\u5b9e\u73b0\u3002\u5bf9\u4e8eGPU\uff0c\u4f7f\u7528Halide\uff08http://halide-lang.org\uff09\u8fdb\u884c\u5b9e\u9a8c\u6027DNN\u52a0\u901f\u3002\u6709\u5173\u8be5\u6a21\u5757\u7684\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728\u6211\u4eec\u7684wiki\u4e2d\u627e\u5230\uff1a[OpenCV\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV)\u3002\n    2. OpenCV\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u6807\u5fd7ENABLE_CXX11\u6784\u5efa\u4e3aC ++ 11\u5e93\u3002\u6dfb\u52a0\u4e86C ++ 11\u7a0b\u5e8f\u5458\u7684\u4e00\u4e9b\u5f88\u9177\u7684\u529f\u80fd\u3002\n    3. \u7531\u4e8e\u201c\u52a8\u6001\u8c03\u5ea6\u201d\u529f\u80fd\uff0c\u6211\u4eec\u8fd8\u5728OpenCV\u7684\u9ed8\u8ba4\u7248\u672c\u4e2d\u542f\u7528\u4e86\u4e0d\u5c11AVX / AVX2\u548cSSE4.x\u4f18\u5316\u3002DNN\u6a21\u5757\u8fd8\u5177\u6709\u4e00\u4e9bAVX / AVX2\u4f18\u5316\u3002\nIntel Media SDK\u73b0\u5728\u53ef\u4ee5\u88ab\u6211\u4eec\u7684videoio\u6a21\u5757\u7528\u6765\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u7684\u89c6\u9891\u7f16\u7801/\u89e3\u7801\u3002\u652f\u6301MPEG1 / 2\uff0c\u4ee5\u53caH.264\u3002\n    4. \u5d4c\u5165OpenCV Intel IPP\u5b50\u96c6\u5df2\u4ece2015.12\u5347\u7ea7\u52302017.2\u7248\u672c\uff0c\u4ece\u800c\u5728\u6211\u4eec\u7684\u6838\u5fc3\u548cimgproc perf\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e8615\uff05\u7684\u901f\u5ea6\u3002\n    5. 716\u62c9\u8bf7\u6c42\u5df2\u7ecf\u5408\u5e76\uff0c588\u6211\u4eec\u7684\u9519\u8bef\u8ddf\u8e2a\u5668\u4e2d\u7684\u95ee\u9898\u5df2\u7ecf\u5173\u95ed\uff0c\u56e0\u4e3aOpenCV 3.2\u3002\u53e6\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u4e25\u683c\u7684\u9759\u6001\u5206\u6790\u4eea\u5de5\u5177\u8fd0\u884cOpenCV\uff0c\u5e76\u4fee\u590d\u4e86\u68c0\u6d4b\u5230\u7684\u95ee\u9898\u3002\u6240\u4ee5OpenCV 3.3\u5e94\u8be5\u662f\u975e\u5e38\u7a33\u5b9a\u548c\u53ef\u9760\u7684\u91ca\u653e\u3002\n    6. \u6709\u5173OpenCV 3.3\u7684\u66f4\u6539\u548c\u65b0\u529f\u80fd\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95eehttps://github.com/opencv/opencv/wiki/ChangeLog\u3002\n    7. [\u4e0b\u8f7dOpenCV 3.3](https://github.com/opencv/opencv/releases/tag/3.3.0)\n    8. [\u5b89\u88c5OpenCV 3.3](http://www.linuxfromscratch.org/blfs/view/cvs/general/opencv.html)\n- OpenCV 4.0\u53d1\u5e03\u4e86 https://opencv.org/opencv-4-0-0.html\n\n## \u600e\u6837\u7ffb\u5899\uff1f\u4f7f\u7528Google\u641c\u7d22\u5f15\u64ce\uff0c\u89c2\u770bYouTube\u89c6\u9891\u6559\u7a0b\n- shadowsocks\n    - \u65b9\u4fbf\uff0c\u968f\u5730\u968f\u65f6\u7ffb\u5899\n    - \u624b\u673a\u4f7f\u75284G\u4fe1\u53f7\u4e0a\u7f51\uff0c\u4e5f\u53ef\u4ee5\u3002\n    - \u5f3a\u70c8\u63a8\u8350\uff01\n    - \u8d2d\u7269\u56fd\u5916\u670d\u52a1\u5668\uff0c\u642d\u5efa\u4e5f\u5f88\u5bb9\u6613\n        - \u53c2\u8003 https://isweic.com/build-shadowsocks-python-server/\n        - pip install shadowsocks\n        - \u8fd0\u884c\n            - shell\u7a97\u53e3\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb\n                - 8388\u662f\u7aef\u53e3\u53f7\uff0cpassword\u662f\u5bc6\u7801\uff0caes-256-cfb\u662f\u52a0\u5bc6\u7c7b\u578b\uff0c\u901a\u8fc7Ctrl+C\u7ed3\u675f\n            - \u540e\u53f0\u8fd0\u884c\n                - ssserver -p 8388 -k password -m aes-256-cfb --user nobody -d start\n                - \u7ed3\u675f\u540e\u53f0\u8fd0\u884c\n                    - ssserver -d stop\n            - \u68c0\u67e5\u8fd0\u884c\u65e5\u5fd7\n                - less /var/log/shadowsocks.log\n- [Lantern\u84dd\u706f](https://github.com/getlantern/lantern/releases/tag/latest)\n    - \u672c\u4eba\u4e0d\u4f7f\u7528\u84dd\u706f\u4e86\u3002\n    1. \u53ef\u4ee5\u514d\u8d39\u4f7f\u7528\uff0c\u4f46\u7528\u5b8c800m\u6d41\u91cf\u540e\u4f1a\u9650\u901f\uff0c\u8fd8\u80fd\u6b63\u5e38\u4f7f\u7528\uff0c\u5c31\u662f\u6709\u70b9\u6162\n    2. \u4e13\u4e1a\u7248\u4e0d\u8d35\uff0c2\u5e74336\u5143\uff0c\u6bcf\u59290.46\u5143\u3002[Lantern\u84dd\u706f\u4e13\u4e1a\u7248\u8d2d\u4e70\u6d41\u7a0b](https://github.com/getlantern/forum/issues/3863)\n    3. \u9080\u8bf7\u597d\u53cb\u6765\u83b7\u5f97\u66f4\u591a\u7684\u4e13\u4e1a\u7248\u4f7f\u7528\u65f6\u95f4\u3002\u6211\u7684\u9080\u8bf7\u7801\uff1aGW2362\n    \n## \u66f4\u65b0\n- [\u7834\u89e3\u9a8c\u8bc1\u7801](my06-\u9a8c\u8bc1\u7801\u8bc6\u522b/solving_captchas_code_examples/README.md)\n    \n## \u6350\u8d60\u6253\u8d4f  \n- OpenCV\u95ee\u7b54\u7fa41,QQ\u7fa4\u53f7:187436093\n- \u5fae\u4fe1  \n    - <img src=\"data/wechat_donate.jpg\" width = \"200\" height = \"200\" alt=\"wechat_donate\"  />\n\n\n- \u652f\u4ed8\u5b9d\n    - <img src=\"data/alipay_donate.jpg\" width = \"200\" height = \"200\" alt=\"alipay_donate\"  />\n \n- \u798f\u5229\n    - \u514d\u8d39\u56fd\u5185\u670d\u52a1\u5668\uff0c\u4f46\u9700\u8981\u4ea4\u62bc\u91d1\uff0c\u968f\u65f6\u5168\u989d\u539f\u8def\u9000\u8fd8\n        - \u6709\u9700\u8981\u7684\u670b\u53cb\u8bf7\u52a0\u5165QQ\u7fa4\uff0c\u53d1\u3010\u624b\u673a\u53f7\u3011\u7ed9\u7fa4\u4e3b\n        - ![free_server](data/free_server.jpeg)"
 },
 {
  "repo": "MasteringOpenCV/code",
  "language": "C++",
  "readme_contents": "==============================================================================\r\nMastering OpenCV with Practical Computer Vision Projects\r\n==============================================================================\r\nFull source-code for the book.\r\n--------------------------------------------------------------------------------\r\n\r\n    Source-Code:    https://github.com/MasteringOpenCV/code\r\n    Book:           http://www.packtpub.com/cool-projects-with-opencv/book\r\n    Copyright:      Packt Publishing 2012.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nTo build & run the projects for the book:\r\n--------------------------------------------------------------------------------\r\n- Install OpenCV (versions between 2.4.2 to 2.4.11 are supported, whereas OpenCV 3.0 is not yet supported). eg: go to \"http://opencv.org/\", click on\r\n  Downloads, download the latest OpenCV 2.4 version (including prebuilt library), and extract\r\n  it to \"C:\\OpenCV\" for Windows or \"~/OpenCV\" for Linux. In OpenCV v2.4.3, the\r\n  prebuilt OpenCV library is in \"C:\\OpenCV\\build\" or \"~/OpenCV/build\", such as\r\n  \"C:\\OpenCV\\build\\x64\\vc9\" for MS Visual Studio 2008 (or \"vs10\" folder for MS \r\n  Visual Studio 2010, or the \"x86\" parent folder for 32-bit Windows).\r\n\r\n- Install all the source code of the book. eg: extract the code to\r\n  \"C:\\MasteringOpenCV\" for Windows or \"~/MasteringOpenCV\" for Linux.\r\n  \r\n- Install CMake v2.8 or later from \"http://www.cmake.org/\".\r\n\r\nEach chapter of the book is for a separate project. Therefore there are 9\r\nprojects for the 9 chapters (remember that Chapter 9 is an online chapter that\r\ncan be downloaded from \"http://www.packtpub.com/cool-projects-with-opencv/book\").\r\nYou can run each project separately, they each contain a README.md text file\r\ndescribing how to build that project, using CMake in most cases, because CMake\r\ncan be used with many compilers and many operating systems.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nChapters:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android, by Shervin Emami.\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad, by Khvedchenia Ievgen.\r\n- Ch3) Marker-less Augmented Reality, by Khvedchenia Ievgen.\r\n- Ch4) Exploring Structure from Motion using OpenCV, by Roy Shilkrot.\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks, by David Escriv\u00e1.\r\n- Ch6) Non-rigid Face Tracking, by Jason Saragih.\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT, by Daniel L\u00e9lis Baggio.\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces, by Shervin Emami.\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect, by Naureen Mahmood.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nWhat you need for this book:\r\n--------------------------------------------------------------------------------\r\nYou don't need to have special knowledge in computer vision to read this book,\r\nbut you should have good C/C++ programming skills and basic experience with\r\nOpenCV before reading this book. Readers without experience in OpenCV may wish to\r\nread the book Learning OpenCV for an introduction to the OpenCV features, or read\r\n\"OpenCV 2 Cookbook\" for examples on how to use OpenCV with recommended C/C++\r\npatterns, because \"Mastering OpenCV with Practical Computer Vision Projects\" will\r\nshow you how to solve real problems, assuming you are already familiar with the\r\nbasics of OpenCV and C/C++ development.\r\n\r\nIn addition to C/C++ and OpenCV experience, you will also need a computer, and an\r\nIDE of your choice (such as Visual Studio, XCode, Eclipse, or QtCreator, running\r\non Windows, Mac or Linux). Some chapters have further requirements, particularly:\r\n\r\n- To develop the Android app, you will need an Android device, Android\r\n  development tools, and basic Android development experience.\r\n- To develop the iOS app, you will need an iPhone, iPad, or iPod Touch device,\r\n  iOS development tools (including an Apple computer, XCode IDE, and an Apple\r\n  Developer Certificate), and basic iOS and Objective-C development experience.\r\n- Several desktop projects require a webcam connected to your computer. Any\r\n  common USB webcam should suffice, but a webcam of at least 1 megapixel may be\r\n  desirable.\r\n- CMake is used in most projects, including OpenCV itself, to build across\r\n  operating systems and compilers. A basic understanding of build systems is\r\n  required, and knowledge of cross-platform building is recommended.\r\n- An understanding of linear algebra is expected, such as basic vector and matrix\r\n  operations and eigen decomposition.\r\n\r\nPer-chapter Requirements:\r\n- Ch1: webcam (for desktop app), or Android development system (for Android app).\r\n- Ch2: iOS development system (to build an iOS app).\r\n- Ch3: OpenGL built into OpenCV.\r\n- Ch4: PCL (http://pointclouds.org/) and SSBA (http://www.inf.ethz.ch/personal/chzach/opensource.html).\r\n- Ch5: nothing.\r\n- Ch6: nothing, but requires training data for execution.\r\n- Ch7: nothing.\r\n- Ch8: webcam.\r\n- Ch9: Kinect depth sensor.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nScreenshots:\r\n--------------------------------------------------------------------------------\r\n- Ch1) Cartoonifier and Skin Changer for Android:\r\n![Ch1) Cartoonifier and Skin Changer for Android](https://raw.github.com/MasteringOpenCV/code/master/Chapter1_AndroidCartoonifier/screenshot.png)\r\n- Ch2) Marker-based Augmented Reality on iPhone or iPad:\r\n![Ch2) Marker-based Augmented Reality on iPhone or iPad](https://raw.github.com/MasteringOpenCV/code/master/Chapter2_iPhoneAR/screenshot.png)\r\n- Ch3) Marker-less Augmented Reality:\r\n![Ch3) Marker-less Augmented Reality](https://raw.github.com/MasteringOpenCV/code/master/Chapter3_MarkerlessAR/screenshot.png)\r\n- Ch4) Exploring Structure from Motion using OpenCV:\r\n![Ch4) Exploring Structure from Motion using OpenCV](https://raw.github.com/MasteringOpenCV/code/master/Chapter4_StructureFromMotion/screenshot.png)\r\n- Ch5) Number Plate Recognition using SVM and Neural Networks:\r\n![Ch5) Number Plate Recognition using SVM and Neural Networks](https://raw.github.com/MasteringOpenCV/code/master/Chapter5_NumberPlateRecognition/screenshot.png)\r\n- Ch6) Non-rigid Face Tracking:\r\n![Ch6) Non-rigid Face Tracking](https://raw.github.com/MasteringOpenCV/code/master/Chapter6_NonRigidFaceTracking/screenshot.png)\r\n- Ch7) 3D Head Pose Estimation using AAM and POSIT:\r\n![Ch7) 3D Head Pose Estimation using AAM and POSIT](https://raw.github.com/MasteringOpenCV/code/master/Chapter7_HeadPoseEstimation/screenshot.png)\r\n- Ch8) Face Recognition using Eigenfaces or Fisherfaces:\r\n![Ch8) Face Recognition using Eigenfaces or Fisherfaces](https://raw.github.com/MasteringOpenCV/code/master/Chapter8_FaceRecognition/screenshot.png)\r\n- Ch9) Developing Fluid Wall using the Microsoft Kinect:\r\n![Ch9) Developing Fluid Wall using the Microsoft Kinect](https://raw.github.com/MasteringOpenCV/code/master/Chapter9_FluidInteractionUsingKinect/screenshot.png)\r\n\r\n\r\n"
 },
 {
  "repo": "amusi/AI-Job-Notes",
  "language": null,
  "readme_contents": "# AI-Job-Notes\nAI\u7b97\u6cd5\u5c97\u6c42\u804c\u653b\u7565\uff1a\u6db5\u76d6\u6821\u62db\u65f6\u95f4\u8868\u3001\u51c6\u5907\u653b\u7565\u3001\u5237\u9898\u6307\u5357\u3001\u5185\u63a8\u3001AI\u516c\u53f8\u6e05\u5355\u548c\u7b54\u7591\u7b49\u8d44\u6599\n\nAI\u7b97\u6cd5\u5c97\u65b9\u5411\uff1a\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u50cf\u5904\u7406\u548cSLAM\u7b49\n\n>  \u6ce8\uff1a\u5982\u679c\u4f60\u770b\u5230\u8fd9\u7bc7\u6587\u7ae0\uff0c\u4e14\u6709\u4e00\u4e9b\u7591\u95ee\u6216\u8005\u60f3\u63d0\u4f9b\u4e00\u4e9b\u8d44\u6e90\uff0c\u6b22\u8fce\u63d0\u4ea4issues\uff01\n\n# \u76ee\u5f55\n\n<!-- MarkdownTOC depth=4 -->\n\n- [1 \u6821\u62db\u65f6\u95f4\u8868](#Scheduled)\n- [2 \u51c6\u5907\u653b\u7565](#Strategy)\n- [3 AI \u9762\u7ecf\u548c\u5237\u9898\u6307\u5357](#Coding)\n- [4 \u5185\u63a8](#Recommend)\n- [5 \u7b80\u5386\u6a21\u677f](#Resume)\n- [6 AI \u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09](#Company)\n- [7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5](#Salary)\n- [8 \u7b54\u7591\uff08\u542b130\u4e2a\u95ee\u7b54\uff09](#Q&A)\n\n<a name=\"Scheduled\"></a>\n\n## 1 \u6821\u62db\u65f6\u95f4\u8868\n\n![](imgs/\u6821\u62db\u65f6\u95f4\u8868.png)\n\n\u4ee5\u4eca\u5e74(2020)\u4e3a\u4f8b\uff0c\u9ed8\u8ba4\u4e3a2021\u5c4a\u5b66\u751f\uff082020\u5c4a\u5b66\u751f\u79f0\u4e3a\u4e0a\u5c4a\uff09\n\n| \u65f6\u95f4           | \u4efb\u52a1                                |\n| -------------- | ----------------------------------- |\n| 2020\u5e743\u6708~6\u6708  | \u627e\u6691\u671f\u5b9e\u4e60/\u4e0a\u5c4a\u6625\u62db\uff08\u8865\u62db\uff09         |\n| 2020\u5e746\u6708~8\u6708  | \u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09              |\n| 2020\u5e749\u6708~11\u6708 | \u79cb\u62db\u6b63\u5f0f\u6279\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09 |\n\n### 1.1 \u6691\u671f\u5b9e\u4e60\n\n2020\u5e743\u6708~6\u6708\uff1a\u6691\u671f\u5b9e\u4e60\u3002\n\n\u5b9e\u4e60\u4e00\u822c\u5206\u6210\u4e24\u79cd\uff1a\n\n- \u65e5\u5e38\u5b9e\u4e60\n- \u6691\u671f\u5b9e\u4e60\n\n![](imgs/\u5b9e\u4e60.png)\n\n**\u65e5\u5e38\u5b9e\u4e60**\uff1a\u65e5\u5e38\u5b9e\u4e60\u662f\u4efb\u4f55\u65f6\u5019\u90fd\u53ef\u4ee5\u627e\u7684\uff0c\u901a\u5e38\u662f\u6839\u636e\u5177\u4f53\u90e8\u95e8\u7684\u9700\u6c42\uff0c\u7531\u516c\u53f8HR\u3001\u90e8\u95e8\u4e3b\u7ba1\u6216\u8005\u90e8\u95e8\u5458\u5de5\u53d1\u5e03\u62db\u8058\u6d88\u606f\uff0c\u76f8\u5bf9\u8f83\u4e3a\u96f6\u6563\u4e5f\u6bd4\u8f83\u7075\u6d3b\u3002\n\n**\u6691\u671f\u5b9e\u4e60**\uff1a\u5f88\u591a\u516c\u53f8\uff0c\u7279\u522b\u662f\u5927\u516c\u53f8\uff08\u5982BAT\u7b49\u5927\u5382\uff09\uff0c\u90fd\u4f1a\u7ec4\u7ec7\u4e13\u9879\u7684**\u6691\u671f\u5b9e\u4e60\u751f**\u62db\u8058\u6d3b\u52a8\u3002\u4e00\u65b9\u9762\u662f\u9488\u5bf9\u5728\u6821\u5b66\u751f\u7684\u60c5\u51b5\uff08\u5f88\u591a\u5b66\u751f\u53ea\u6709\u6691\u671f\u624d\u6709\u5047\u671f\uff0c\u6216\u8005\u5bfc\u5e08\u6691\u5047\u624d\u653e\u4eba\uff09\uff0c\u53e6\u4e00\u65b9\u9762\u5c31\u662f\u4e3a\u4e86\u79cb\u5b63\u6821\u62db\uff08\u5927\u89c4\u6a21\u62db\u8058\uff09\u5438\u5f15\u4eba\u624d\u3002\u6691\u671f\u5b9e\u4e60\u5177\u6709\u5f88\u5927\u7684\u610f\u4e49\uff0c\u5bf9\u5b66\u751f\u6765\u8bf4\uff0c\u6700\u76f4\u63a5\u7684\u597d\u5904\u5c31\u662f\u8f6c\u6b63\u673a\u4f1a\u3002\u6691\u671f\u5b9e\u4e60\uff0c\u4e00\u822c6\u6708\u5e95\u5de6\u53f3\u5b9e\u4e60\u5165\u804c\uff08\u4e5f\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u7684\u65f6\u95f4\uff0c\u63d0\u524d\u5165\u804c\uff09\uff0c\u4e00\u822c8\u6708\u5e95\u62169\u6708\u4efd\u4f1a\u6709\u4e13\u9879\u6691\u671f\u5b9e\u4e60\u7b54\u8fa9\uff0c\u6839\u636e\u7efc\u5408\u8868\u73b0\uff0c\u7b54\u8fa9\u901a\u8fc7\u540e\u5c31\u53ef\u4ee5\u57fa\u672c\u7ed3\u675f\u79cb\u62db\u4e86\u3002\n\n\u6ce8\uff1a\u8fd9\u91cc\u5efa\u8bae\u5728\u8fdb\u5165\u516c\u53f8\u53c2\u52a0\u6691\u671f\u5b9e\u4e60\u7684\u671f\u95f4\uff0c\u4e5f\u8981\u53c2\u52a0\u79cb\u62db\u63d0\u524d\u6279\u548c\u79cb\u62db\u6b63\u5f0f\u6279\uff0c\u5e76\u591a\u6295\u9012\u4e00\u4e9b\u516c\u53f8\uff0c\u5373\u4f7f\u5728\u5b9e\u4e60\uff0c\u6240\u8c13\u7684\u5f88\u5fd9\uff0c\u6ca1\u65f6\u95f4\u51c6\u5907\u79cb\u62db\u4e86\uff0c\u90a3\u4e5f\u8981\u591a\u6295\u3002\u6691\u671f\u5b9e\u4e60\u7684\u53e6\u4e00\u4e2a\u597d\u5904\u662f\u589e\u52a0\u53ef\u8d35\u7684\u5b9e\u4e60\u7ecf\u9a8c\uff0c\u7b80\u5386\u4f1a\u597d\u770b\u5f88\u591a\u3002\n\n### 1.2 \u79cb\u62db\u63d0\u524d\u6279\n\n**2020\u5e746\u6708~8\u6708\uff1a\u79cb\u62db\u63d0\u524d\u6279\uff08\u795e\u4ed9\u6253\u67b6\uff09**\n\n\u636e\u6211\u4e86\u89e3\u4e0a\u5c4a\u6253\u54cd\u79cb\u62db\u7b2c\u4e00\u67aa\u7684\u662f\u5927\u7586(DJI)\u79d1\u6280\uff0c\u5176\u57286\u6708\u5e95\u5c31\u5df2\u7ecf\u7ed3\u675f\u7b80\u5386\u6295\u9012\u4e86\uff0c\u7136\u540eBAT\u7b49\u5927\u5382\u90fd\u662f7\u6708\u4efd\u5f00\u59cb\u3002\u8fd9\u65f6\u5019\u7684\u6821\u62db\uff0c\u7edd\u5927\u90e8\u5206\u90fd\u662f\u5185\u63a8/\u63d0\u524d\u6279\uff0c\u800c\u4e0d\u662f\u6b63\u5f0f\u6279\uff0c\u5927\u5bb6\u4e00\u5b9a\u8981\u73cd\u60dc\u8fd9\u4e2a\u65f6\u95f4\u70b9\uff1a6\u6708~8\u6708\u3002\u867d\u7136\u6211\u8c03\u4f83\u7740\u8bf4\u795e\u4ed9\u6253\u67b6\uff0c\u4f46\u8fd8\u662f\u8981\u6ce8\u610f\u8fd9\u65f6\u5019\u6027\u4ef7\u6bd4\u7279\u522b\u9ad8\u3002\u4e00\u65b9\u9762\u662f\u85aa\u8d44\u666e\u904d\u9ad8\uff0c\u901a\u5e38\u4e00\u4e9bSP/SSP Offer\u90fd\u662f\u8fd9\u4e2a\u8282\u70b9\u53d1\u51fa\u6765\u7684\uff0c\u53e6\u4e00\u65b9\u9762\u662f\u6295\u9012\u7684\u4eba\u6570\u8fd8\u4e0d\u662f\u5f88\u591a\uff0c\u56e0\u4e3a\u6709\u4e9b\u4eba\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e2a\u63d0\u524d\u6279\u7684\u91cd\u8981\u6027\uff0c\u8001\u60f3\u7740\u591a\u51c6\u5907\u4e00\u70b9\uff0c\u5230\u79cb\u62db\u6b63\u5f0f\u6279\u518d\u5927\u5e72\u4e00\u573a\u3002\n\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a\u53c2\u4e0e\u79cb\u62db\u63d0\u524d\u6279\u7684\u5927\u4f6c\u7279\u522b\u591a\uff0c\u540c\u65f6\u5c97\u4f4dhc\u5e76\u4e0d\u591a\uff08\u56e0\u4e3a\u4f01\u4e1a\u8981\u8003\u8651\u6b63\u5f0f\u6279\u7684\u60c5\u51b5\uff0c\u4f1a\u63a7\u5236\u62db\u8058\u4eba\u6570\uff09\uff0c\u6240\u4ee5\u6211\u628a\u79cb\u62db\u63d0\u524d\u6279\u6bd4\u4f5c\uff1a\u795e\u4ed9\u6253\u67b6\u3002\u53e6\u5916\uff0c\u79cb\u62db\u63d0\u524d\u6279\u5927\u591a\u4ee5\u5185\u63a8\u4e3a\u4e3b\uff0c\u540e\u9762\u7ae0\u8282\u4e2d\u6211\u4f1a\u8bf4\u5230\u5982\u4f55\u83b7\u53d6\u62db\u8058\u4fe1\u606f\u4ee5\u53ca\u5982\u4f55\u5185\u63a8\u3002\n\n\u6ce8\uff1a\u63d0\u53d6\u6279\u6302\u4e86\uff0c\u6b63\u5f0f\u6279\u53ef\u4ee5\u518d\u7ee7\u7eed\u6295\uff08\u5177\u4f53\u770b\u4e0d\u540c\u516c\u53f8\u7684\u62db\u8058\u4ecb\u7ecd\uff09\u3002\n\n### 1.3 \u79cb\u62db\u6b63\u5f0f\u6279\n\n**2020\u5e749\u6708~11\u6708\uff1a\u79cb\u62db\u6b63\u5f0f\uff08\u795e\u4ed9\u7ee7\u7eed\u6253\u67b6+\u83dc\u9e21\u4e92\u5544\uff09**\n\n\u6709\u53e5\u8bdd\u53eb\u505a\u91d1\u4e5d\u94f6\u5341\uff0c\u4e5f\u5c31\u662f9\u6708\u4efd\u7684 Offer \u6bd410\u6708\u4efd\u7684 Offer \u66f4\u53ef\u8d35\uff0c\u8fd9\u8bdd\u5176\u5b9e\u5f88\u6709\u9053\u7406\uff0c\u6240\u4ee5\u5927\u5bb6\u53ef\u4ee5\u8111\u8865\u52307\u30018\u6708\u4efd\u7684 Offer \u5c5e\u4e8e\u4ec0\u4e48 level \u4e86\u3002\u8fd9\u65f6\u5019\u4e5f\u5f88\u8003\u9a8c\u5927\u5bb6\u7684\u5fc3\u6001\uff0c\u6bd4\u59829\u6708\u4efd\u621610\u6708\u4efd\u4e86\uff0c\u5982\u679c\u4f60\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u518d\u770b\u770b\u8eab\u8fb9\u5df2\u7ecf\u62ff\u5230Offer\u7684\u540c\u5b66\uff0c\u4e00\u5b9a\u53d8\u6210\u67e0\u6aac\u7cbe\u3002\n\n\u6240\u4ee5 Amusi \u8fd9\u91cc\u5f3a\u70c8\u5efa\u8bae\u4e00\u5b9a\u8981\u628a\u63e1\u4f4f 1.2\u8282\u4e2d\u7684**\u79cb\u62db\u63d0\u524d\u6279 **\u3002\u5f53\u7136\u4e86\uff0c\u5982\u679c9\u6708\u4efd\u624b\u91cc\u8fd8\u6ca1\u6709Offer\uff0c\u5fc3\u6001\u5343\u4e07\u522b\u5d29\uff0c\u7ee7\u7eed\u6295\u7ee7\u7eed\u5e72\uff0c\u8bb0\u4f4f\u4e00\u53e5\u8bdd\uff1a\u591a\u6295\u51c6\u6ca1\u9519\uff01\u5176\u5b9e\u5927\u90e8\u5206\u540c\u5b66\u90fd\u662f9\u6708\u300110\u6708\u624d\u9646\u7eed\u6536\u5230Offer\u7684\uff0c\u6240\u4ee5\u4f60\u591a\u6295\u7ee7\u7eed\u52aa\u529b\uff0c\u6536\u83b7\u80af\u5b9a\u4f1a\u6709\u7684\u3002\n\n<a name=\"Strategy\"></a>\n\n## 2 \u51c6\u5907\u653b\u7565\n\n\u51c6\u5907\u653b\u7565\uff0c\u6211\u6ca1\u6709\u5177\u4f53\u7684\u65b9\u6848\uff0c\u56e0\u4e3a\u8fd9\u5c31\u597d\u50cf\u662f\u5b66\u4e60\u8ba1\u5212\u4e00\u6837\uff0c\u6bcf\u4e2a\u4eba\u90fd\u8981\u81ea\u5df1\u7684\u4e60\u60ef\uff0c\u6211\u7684\u4f60\u5e76\u4e0d\u4e00\u5b9a\u9002\u7528\u3002\u6240\u4ee5\u6211\u5c31\u7528\u4e00\u4e2a\u7cbe\u7b80\u7684\u516c\u53f8\u6765\u4ecb\u7ecd\u3002\n~~\u516c\u5f0f1.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60(\u53ef\u9009)+\u7ade\u8d5b(\u53ef\u9009)+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)~~\n\n\u516c\u5f0f2.0\uff1a\u5237\u9898+\u80cc\u9898+\u9879\u76ee+\u5b9e\u4e60+\u7ade\u8d5b+\u9876\u4f1a/\u9876\u520a(\u53ef\u9009)\n\n<a name=\"Coding\"></a>\n\n## 3 AI\u9762\u7ecf\u548c\u5237\u9898\u6307\u5357\n\n### 3.1 \u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\n\n\u8be6\u89c1\uff1a[\u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\uff08\u542b\u6570\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cSLAM\u7b49\u65b9\u5411\uff09](<https://github.com/amusi/Deep-Learning-Interview-Book>)\n\n**Deep Learning Interview Book** \u90e8\u5206\u5185\u5bb9\u5982\u4e0b\uff1a\n\n- \ud83d\ude03 [\u81ea\u6211\u4ecb\u7ecd](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D.md)\n- \ud83d\udd22 [\u6570\u5b66](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E5%AD%A6.md)\n- \ud83c\udf93 [\u673a\u5668\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd5 [\u6df1\u5ea6\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udcd7 [\u5f3a\u5316\u5b66\u4e60](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.md)\n- \ud83d\udc40 [\u8ba1\u7b97\u673a\u89c6\u89c9](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.md)\n- \ud83d\udcf7 [\u4f20\u7edf\u56fe\u50cf\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.md)\n- \ud83c\udc04\ufe0f [\u81ea\u7136\u8bed\u8a00\u5904\u7406](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.md)\n- \ud83c\udfc4 [SLAM](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/SLAM.md)\n- \ud83d\udc65 [\u63a8\u8350\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.md)\n- \ud83d\udcca [\u6570\u636e\u7ed3\u6784\u4e0e\u7b97\u6cd5](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.md)\n- \ud83d\udc0d [\u7f16\u7a0b\u8bed\u8a00\uff1aC/C++/Python](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.md)\n- \ud83c\udf86 [\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6.md)\n- \u270f\ufe0f [\u9762\u8bd5\u7ecf\u9a8c](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C.md)\n- \ud83d\udca1 [\u9762\u8bd5\u6280\u5de7](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E9%9D%A2%E8%AF%95%E6%8A%80%E5%B7%A7.md)\n- \ud83d\udce3 [\u5176\u5b83\uff08\u8ba1\u7b97\u673a\u7f51\u7edc/Linux\u7b49\uff09](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E5%85%B6%E5%AE%83.md)\n\n### 3.2 \u5237\u9898\u6307\u5357\n\n\u5237\u9898\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b66\u4e60\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\uff0c\u953b\u70bc\u7f16\u7a0b\u80fd\u529b\u548c\u719f\u6089\u5237\u9898\u6280\u5de7\n\n\u5237\u9898\u5efa\u8bae\uff1a\u5148\u5237[\u300a\u5251\u6307Offer\u300b](https://www.nowcoder.com/ta/coding-interviews)\uff0866\u9898\uff09\uff0c\u518d\u5237 [LeetCode](https://leetcode.com/)\uff08\u76ee\u524dLeetCode\u5df2\u7ecf\u6709900+\u9898\uff0c\u53ef\u4ee5\u6839\u636e\u7c7b\u522b\u6765\u5237\uff0c\u4f46\u5f3a\u70c8\u5efa\u8bae\u5148\u5237\u5b8c [LeetCode \u9762\u8bd5\u9ad8\u9891\u9898](https://leetcode.com/problemset/top-interview-questions/)\uff09\n\n> \u6ce8\uff1a\u6839\u636e 2020 \u5e74\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0cLeetCode \u5efa\u8bae\u81f3\u5c11\u5237200-300\u9898\uff0c\u6240\u4ee52020\u5e74\uff082021\u5c4a\uff09\u627e\u5de5\u4f5c\u7684\u540c\u5b66\u4e00\u5b9a\u8981\u52aa\u529b\u5237\u8d77\u6765\u4e86\uff01\n\n#### 3.2.1 \u5237\u9898\u7f16\u7a0b\u8bed\u8a00\n\n- C/C++\n- Python\n- JAVA\uff08\u4e0d\u63a8\u8350\uff09\n\n> \u6ce8\uff1a\u5982\u679c\u65f6\u95f4\u5145\u88d5\uff0c\u800c\u4e14\u6709 C++ \u57fa\u7840\uff0c\u90a3\u4e48\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 C++\u548c Python \u540c\u65f6\u5237\u9898\u3002\n>\n> \u6839\u636e 2019 \u5e74(2020\u5c4a)\u6821\u62db\u63d0\u524d\u6279\u7684\u60c5\u51b5\u6765\u770b\uff0c\u4f1a C++ \u7684\u540c\u5b66\u5177\u6709\u6709\u4e00\u5b9a\u4f18\u52bf\u3002\n\n#### 3.2.2 \u4e66\u7c4d\u63a8\u8350\n\n| \u4e66\u7c4d                                                         | \u8c46\u74e3\u8bc4\u5206 | \u63a8\u8350\u6307\u6570 |\n| ------------------------------------------------------------ | -------- | -------- |\n| [\u300a\u5251\u6307Offer\u300b](https://book.douban.com/subject/25910559/)   | 9.1      | \u2606\u2606\u2606\u2606\u2606    |\n| [\u300a\u6570\u636e\u7ed3\u6784(C++\u8bed\u8a00\u7248)\u300b](https://book.douban.com/subject/25859528/) | 9.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u7b97\u6cd5\u56fe\u89e3\u300b](https://book.douban.com/subject/26979890/)    | 8.4      | \u2606\u2606\u2606\u2606     |\n| [\u300a\u5927\u8bdd\u6570\u636e\u7ed3\u6784\u300b](https://book.douban.com/subject/6424904/) | 7.9      | \u2606\u2606\u2606      |\n| [\u300a\u7b97\u6cd5\u300b(\u7b2c\u56db\u7248)](https://book.douban.com/subject/19952400/) | 9.4      | \u2606\u2606\u2606      |\n\n>  \u6ce8\uff1a\u5176\u5b9e\u8fd8\u6709\u5f88\u591a\u65b9\u5411\u6ca1\u6709\u6d89\u53ca\uff0c\u5982linux\u3001\u6570\u636e\u5e93\uff0c\u4f46\u6682\u65f6\u5148\u63a8\u8350\u8fd9\u4e9b\uff0c\u540e\u9762\u518d\u8865\u5145\n\n#### 3.2.3 \u5728\u7ebf\u5237\u9898\u7f51\u7ad9\n\n- [LeetCode(\u82f1\u6587)](https://leetcode.com/)\n- [LeetCode(\u4e2d\u6587)](https://leetcode-cn.com/)\n- [\u725b\u5ba2\u7f51](https://www.nowcoder.com/)\uff1a\u63a8\u8350\u5251\u6307Offer\u548c\u5404\u5927\u516c\u53f8\u5f80\u5e74\u9898\u5e93\uff0c\u725b\u5ba2\u7f51\u7684\u4f18\u52bf\u5728\u4e8e\u5f88\u591a\u516c\u53f8\u90fd\u4f1a\u4f7f\u7528\u5176\u4f5c\u4e3a\u5728\u7ebf\u5237\u9898\u5e73\u53f0\uff0c\u6240\u4ee5\u5728\u8fd9\u4e0a\u9762\u5237\u9898\uff0c\u6709\u5229\u4e8e\u61c2\u5f97\u8f93\u5165\u8f93\u51fa\u7b49\"\u5957\u8def\"\n\n#### 3.2.4 \u5237\u9898\u65b9\u6cd5\n\n- \u300a\u5251\u6307Offer\u300b\u5168\u5237\u5b8c\n- LeetCode\u9009\u62e9\u6027\u5237\uff1a\u53ef\u4ee5\u7c7b\u522b\u6765\u5237\u9898\uff0c\u5982\u6570\u7ec4\u7c7b\u3001\u94fe\u8868\u7c7b\uff0c\u6216\u8005\u9762\u8bd5\u9ad8\u9891\u7c7b\n\n#### 3.2.5 \u5237\u9898\u65f6\u95f4\n\n\u73b0\u5728\u8d77~2020-10-15\n\n#### 3.2.6 \u5237\u9898\u91cd\u8981\u6027\n\n\u6b63\u5e38\u6821\u62db\u6d41\u7a0b\u90fd\u8981\u8fdb\u884c\u5728\u7ebf\u7b14\u8bd5\uff0c\u9762\u8bd5\u4e2d\u4e5f\u53ef\u80fd\u4f1a\u624b\u6495\u4ee3\u7801\uff0c\u6240\u4ee5\u5237\u9898\u5341\u5206\u5f71\u54cd\u9762\u8bd5\u7ed3\u679c\u3002\n\n<a name=\"Recommend\"></a>\n\n## 4 \u5185\u63a8\n\n\u56fd\u5185\u516c\u53f8\u4eba\u5de5\u667a\u80fd\u65b9\u5411\u5c97\u4f4d\u7684\u5185\u63a8\u673a\u4f1a\uff0c\u542b\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u65b9\u5411\u3002\n\n[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4e3b\u8981\u4ee5\u5168\u804c\u3001\u5b9e\u4e60\u548c\u6821\u62db\u4e3a\u4e3b\uff0c\u5e76\u4e14\u5168\u90fd\u662f\u5185\u63a8\u65b9\u5f0f\uff01\n\n- [\u5b9e\u4e60\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%AE%9E%E4%B9%A0/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u3001\u4eac\u4e1c\u548c\u5546\u6c64\u7b49\u516c\u53f8\n\n- [\u5168\u804c\u5185\u63a8](https://github.com/amusi/AI-Job-Recommend/blob/master/%E5%85%A8%E8%81%8C/README.md)\uff1a\u5df2\u542b\u817e\u8baf\u7b49\u516c\u53f8\n\n\u6ce8\uff1a2020\u5e746\u6708\u5f00\u59cb\uff0c[AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend) \u4f1a\u63a8\u51fa\u5927\u91cf\u6821\u62db\u5185\u63a8\u8d44\u6e90\uff0c\u6b22\u8fcestar/fork/watching\u3002\n\n### 4.1 \u5185\u63a8\u7684\u91cd\u8981\u6027\n\n\u5185\u63a8\uff0c\u771f\u7684\u592a\u91cd\u8981\u4e86\u3002\u5176\u5b9e\u73b0\u5728\u627e\u5b9e\u4e60\u4e5f\u4e00\u6837\uff0c\u5185\u63a8\u7684\u91cd\u8981\u6027\u5c31\u63d0\u9192\u51fa\u6765\u4e86\uff0c\u6bd4\u5982\u6211\u8fd9\u8fb9\u7684\u8d44\u6e90\u5c31\u53ef\u4ee5\u5185\u63a8\u5230BAT\u3001\u5546\u6c64\u3001\u65f7\u89c6\u7b49\u516c\u53f8\uff0c\u4e00\u822c\u5e38\u89c4\u64cd\u4f5c\u662f\u7f51\u4e0a\u6295\u9012\u7b80\u5386\uff0c\u800c\u5feb\u901f\u76f4\u63a5\u7684\u5c31\u662f\u5c06\u7b80\u5386\u9001\u5230leader/\u4e3b\u7ba1\u90a3\u91cc\u3002\u800c\u4e14\u5185\u63a8\u662f\u5efa\u7acb\u5728\u4e00\u79cd\u4e92\u4fe1\u7684\u57fa\u7840\u4e0a(\u867d\u7136\u4e0d\u5927)\uff0c\u8be5\u8d70\u7684\u6d41\u7a0b\u8fd8\u662f\u8981\u8d70\uff0c\u4f46\u65e0\u5f62\u4e2d\u589e\u5927\u4e86\u9762\u8bd5\u901a\u8fc7\u6982\u7387\u3002\u4f60\u8981\u77e5\u9053\uff0c\u5f88\u591a\u4eba\u7684\u7b80\u5386\u5728\u5b98\u7f51\u6216\u8005\u5176\u4ed6\u7b2c\u4e09\u65b9\u62db\u8058\u7f51\u7ad9\u4e0a\u5c31\u76f4\u63a5\u5361\u6b7b\u4e86\u3002\n\n### 4.2 \u5982\u4f55\u5185\u63a8\uff1f\n\n\u5185\u63a8\u7684\u65b9\u5f0f\u5f88\u591a\uff0c\u6bd4\u5982\uff1a\n\n1. \u5f3a\u5173\u8054\uff1a\u76f4\u63a5\u627e\u5df2\u7ecf\u6bd5\u4e1a\u7684\u5e08\u5144\u5e08\u59d0\u6216\u670b\u53cb\u5185\u63a8\uff08\u7f3a\u70b9\u662f\u8eab\u8fb9\u670b\u53cb\u53bb\u7684\u4f01\u4e1a\u6709\u9650\uff0c\u5f88\u591a\u4eba\u662f\u7b2c\u4e00\u6279\u4ece\u4e8b\u7b97\u6cd5\u5c97\u7684\uff0c\u53ef\u80fd\u90fd\u6ca1\u6709\u5e08\u5144\u5e08\u59d0\u641e\u8fd9\u4e2a\uff09\n2. \u5e38\u89c4\u64cd\u4f5c\uff1a\u4e0a\u725b\u5ba2\u7f51\u8bba\u575b\u770b\u4f01\u4e1a\u4eba\u5458\u53d1\u5185\u63a8\u5e16\u5b50\u3001\u5173\u6ce8\u4e00\u4e9b\u62db\u8058\u516c\u4f17\u53f7\uff08\u8fd9\u91cc\u6211\u5c31\u4e0d\u63a8\u8350\uff0c\u56e0\u4e3a\u5f88\u591a\u516c\u4f17\u53f7\u90fd\u5f88\u6709\u5957\u8def\uff0c\u5185\u63a8\u4e00\u4e2a\u4f01\u4e1a\uff0c\u8fd8\u8981\u8f6c\u53d1\u6587\u7ae0\u5230\u5176\u5b83\u7fa4\u91cc\uff0c\u7136\u540e\u622a\u56fe\u7ed9\u4ed6\u4eec\uff0c\u53ef\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u4eba\uff0c\u4e3a\u4e86\u5185\u63a8\uff0c\u53ea\u80fd\u8fd9\u4e48\u5e72\uff09\n3. Amusi \u5185\u63a8\u3002\u8fd9\u91cc\u611f\u89c9\u50cf\u4f3c\u6253\u5e7f\u544a\u4e00\u6837\uff0c\u4f46\u786e\u5b9e\u662f\u4e00\u4e2a\u65b9\u5f0f\uff0c\u56e0\u4e3a\u6211\u624b\u91cc\u8d44\u6e90\u633a\u591a\u7684\uff0c\u5f88\u591a\u516c\u53f8\u7684\u4eba\u90fd\u8ba4\u8bc6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5185\u63a8\u3002\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u5173\u6ce8\u4e00\u4e0b\u8fd9\u4e2a\u6c42\u804c\u7fa4[\u300c2020AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4\u300d](https://t.zsxq.com/VFUZR3n) \u6216\u8005 [AI-Job-Recommend](https://github.com/amusi/AI-Job-Recommend)\n\n![](imgs/2020\u5e74AI\u7b97\u6cd5\u5c97\u6c42\u804c\u7fa4.png)\n\n<a name=\"Resume\"></a>\n\n## 5 \u7b80\u5386\u6a21\u677f\n\n\u63d0\u4f9b\u4e86\u4e09\u4efd\u7b80\u5386\u6a21\u677f\uff0c\u8be6\u89c1\uff1a[AI \u7b97\u6cd5\u5c97\u7b80\u5386\u6a21\u677f](https://github.com/amusi/AI-Job-Resume)\n\n![](imgs/Resume-Demo.png)\n\n<a name=\"Company\"></a>\n\n## 6 AI\u7c7b\u516c\u53f8\u6e05\u5355\uff08\u4ee5CV\u5c97\u4e3a\u4e3b\uff09\n\n\u9996\u5148 AI > CV\uff0c\u6240\u4ee5\u63d0\u4f9bCV\u5c97\u7684\u516c\u53f8\u80af\u5b9a\u5c31\u63d0\u4f9b AI\u5c97\u3002\u4f46\u81f3\u4e8e\u8fd9\u4e9b\u516c\u53f8\u662f\u5426\u8fd8\u6709 NLP\u3001\u673a\u5668\u5b66\u4e60\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u63a8\u8350\u7b97\u6cd5\u548c SLAM\u7b49\u5c97\u4f4d\uff0c\u8fd9\u4e2a\u9700\u8981\u5927\u5bb6\u81ea\u884c\u53bb\u5b98\u7f51\u8fdb\u884c\u4e86\u89e3\u3002\n\n\u8350\u8bfb\uff1a[\u56fd\u5185\u63d0\u4f9b\u8ba1\u7b97\u673a\u89c6\u89c9(CV)\u7b97\u6cd5\u5c97\u4f4d\u7684\u516c\u53f8\u540d\u5355(\u542b\u5916\u4f01\u548c\u56fd\u5185\u516c\u53f8)](https://github.com/amusi/CV-Company-List )\n\n**\u5317\u4eac\u63d0\u4f9bCV\u7b97\u6cd5\u5c97\u7684\u516c\u53f8\u540d\u5355**\n\n![](/imgs/Beijing.png)\n\n\u66f4\u591a\u57ce\u5e02\u4fe1\u606f\uff08\u4e0a\u6d77\uff0c\u6df1\u5733\uff0c\u676d\u5dde\uff0c\u5357\u4eac\uff0c\u5e7f\u5dde\u548c\u6210\u90fd\u7b49\uff09\u8be6\u89c1\uff1ahttps://github.com/amusi/CV-Jobs\n\n<a name=\"Salary\"></a>\n\n## 7 2019\u5c4aAI\u7b97\u6cd5\u5c97\u85aa\u8d44\u60c5\u51b5\n\n\u4eca\u5e74\u662f19\u5e74\uff0c\u6240\u4ee5\u8fd9\u91cc\u4ee52020\u5c4a\u4e3a\u4f8b\u3002\u6211\u662f18\u5e74\u627e\u7684\u5de5\u4f5c\uff0c\u4f46\u5e94\u8be5\u662f2019\u5c4a\u7684\uff0c\u518d\u6b21\u5f3a\u8c03\u4e00\u4e0b\u65f6\u95f4\u4e0d\u8981\u641e\u6df7\u4e86\uff0c\u6240\u4ee5\u6211\u8fd9\u91cc\u8bf4\u8bf42019\u5c4aAI\u7b97\u6cd5\u5c97\u7684\u85aa\u8d44\u60c5\u51b5\u3002\n\n\u6211\u53ea\u4ee5**\u7855\u58eb\u53ca\u4e00\u7ebf\u5de6\u53f3\u57ce\u5e02**\u4e3a\u4f8b\uff08\u5317\u4e0a\u5e7f\u6df1\u3001\u5357\u4eac\u3001\u676d\u5dde\u7b49\uff09\uff0c\u56e0\u4e3a\u50cf\u6b66\u6c49\u3001\u6210\u90fd\uff0c\u4f60\u5373\u4f7f\u627e\u7684AI\u7b97\u6cd5\u5c97\uff0c\u4f46\u57ce\u5e02\u4e0d\u4e00\u6837\uff0c\u85aa\u8d44\u8fd8\u662f\u591a\u5c11\u6709\u533a\u522b\uff0c\u660e\u663e\u4e0d\u80fd\u53ea\u770bMoney\uff0c\u4e0d\u8003\u8651\u57ce\u5e02\u5927\u73af\u5883\u3002\n\n- **\u767d\u83dc\u4ef7\uff1a25w~30w**\n\n- **SP\uff1a30w~40w**\n\n- **SSP\uff1a40w+**\n\n\u8bf4\u5e74\u85aa\u6709\u70b9\u7b3c\u7edf\uff0c\u6211\u518d\u8bf4\u7ec6\u4e00\u70b9\uff0c\u5927\u5bb6\u4e5f\u53ef\u4ee5\u63d0\u53d6\u719f\u6089\u4e00\u4e0b\u3002\n\n\u4e00\u822c\u4f01\u4e1a\u85aa\u8d44\u6784\u6210\u662f\uff1a\n\n- \u5e74\u85aa = \u6708\u85aa*12 + \u5e74\u7ec8\u5956\n\n\u5e74\u7ec8\u5956\u4e00\u822c\u662f2~5\u4e2a\u6708\u7684\u85aa\u8d44\uff0c\u5927\u6982\u662f3\u4e2a\u6708\n\n\u6240\u4ee5\uff0c\u5e74\u85aa=\u6708\u85aa*15\n\n\u5982\u679c\u4f60\u6708\u85aa2w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f30w=2*15\uff08\u767d\u83dc\u7684Top\uff0cSP\u7684Down\uff09\n\n\u5982\u679c\u4f60\u6708\u85aa2.7w\uff0c\u90a3\u4e48\u5e74\u85aa\u5c31\u662f40.5w=2.7*15\uff08SP\u7684Top\uff0cSSP\u7684Down\uff09\n\n\u8fd9\u91ccpo\u4e00\u5f20\u5f88\u5168\u5f88\u5168\u7684\u9ad8\u85aa\u56fe\uff0c\u6765\u81eaOfferShow\n\n\u6ce8\uff1a\u8ddfhr\u8c08\u85aa\u8d44\u7684\u65f6\u5019\uff0c\u5982\u679c\u5979/\u4ed6\u95ee\u4f60\uff1a\u4f60\u7684\u5e0c\u671b\u85aa\u8d44\u662f\u591a\u5c11\uff1f\uff01\u8fd9\u65f6\u5019\u4f60\u4e00\u5b9a\u8981\u5f80\u9ad8\u4e86\u8981\uff0c\u81f3\u5c11\u6bd4\u4f60\u60f3\u8981\u7684\u9ad830%\u3002\u542c\u6211\u7684\uff0c\u6ca1\u6709\u9519\uff0c\u4e0d\u7136...\n\n![](imgs/salary.png)\n\n<a name=\"Q&A\"></a>\n\n## 8 \u7b54\u7591\n\n130\u4e2a\u95ee\u7b54\u8bf7\u6233\u2014> [Q&A](Q&A.md)"
 },
 {
  "repo": "anandpawara/Real_Time_Image_Animation",
  "language": "Python",
  "readme_contents": "# Real time Image Animation\nThe Project is real time application in opencv using first order model\n\n# Steps to setup\n\n## Step 1: Create virtual environment\n\n**Python version** : python v3.7.3 or higher\n\n**create virual environment** : ```pip install virtualenv```\n\n**activate virtual environment** : ```virtualenv env```\n\n## Step 2: Activate virtual environment\n\n**For windows** : ```env/Script/activate```\n\n**For Linux** : ```source env/bin/activate```\n\n## Step 3 : Install required modules\n\n**Install modules** : ``` pip install -r requirements.txt ```\n\n**Install pytorch and torchvision** : ```pip install torch===1.0.0 torchvision===0.2.1 -f https://download.pytorch.org/whl/cu100/torch_stable.html ```\n\n## Step 4 : Download cascade file ,weights and model and save in folder named extract\n\n```gdown --id 1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK```\nThe file is also availible via direct link on Google's Drive:\nhttps://drive.google.com/uc?id=1wCzJP1XJNB04vEORZvPjNz6drkXm5AUK\n\n**On Linux machine** : ```unzip checkpoints.zip```\n\nIf on windows platfrom unzip checkpoints.zip using unzipping software like 7zip.\n\n**Delete zip file** : ```rm checkpoints.zip```\n\n## Step 5 : Run the project\n\n**Run application from live camera** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar```\n\n**Run application from video file** : ```python image_animation.py -i path_to_input_file -c path_to_checkpoint -v path_to_video_file```\n\n**Example** : ```python .\\image_animation.py -i .\\Inputs\\Monalisa.png -c .\\checkpoints\\vox-cpk.pth.tar -v .\\video_input\\test1.mp4 ```\n\n![test demo](animate.gif)\n\n### TODO:\nTkinter version\n\nNeed work on face alignments\n\nFuture plans adding deepfake voice and merging with video\n\nCredits\n=======\n```\n@InProceedings{Siarohin_2019_NeurIPS,\n  author={Siarohin, Aliaksandr and Lathuili\u00e8re, St\u00e9phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},\n  title={First Order Motion Model for Image Animation},\n  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},\n  month = {December},\n  year = {2019},\n  url = {https://github.com/AliaksandrSiarohin/first-order-model}\n}\n```\n- Original Project\n    * [AliaksandrSiarohin](https://github.com/AliaksandrSiarohin/first-order-model)\n\n    If you like this project give your support to original author of this project by giving github star to author's project\n\n- video explanation to the project <br/>\n    * [Video explanation by original author](https://www.youtube.com/watch?v=u-0cQ-grXBQ)\n    * [Two min papers](https://www.youtube.com/watch?v=mUfJOQKdtAk)    \n\n- try project on google colab\n    * [youtube link](https://www.youtube.com/watch?v=RsOJJd1q6Bg&feature=youtu.be)\n    * [link to colab version](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)\n\n    For any valueable feedback feel free to contact me on [linkedin](https://www.linkedin.com/in/anand-pawara-8045/)\n\n"
 },
 {
  "repo": "shimat/opencvsharp",
  "language": "C#",
  "readme_contents": "# OpenCvSharp [![CircleCI Status](https://circleci.com/gh/shimat/opencvsharp/tree/master.svg?style=svg)](https://circleci.com/gh/shimat/opencvsharp/tree/master) [![Appveyor Build status](https://ci.appveyor.com/api/projects/status/dvjexft02s6b3re6/branch/master?svg=true)](https://ci.appveyor.com/project/shimat/opencvsharp/branch/master) [![Github Actions Ubuntu Status](https://github.com/shimat/opencvsharp/workflows/Ubuntu%2018.04/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![Github Actions MacOS Status](https://github.com/shimat/opencvsharp/workflows/macOS%2010.15/badge.svg)](https://github.com/shimat/opencvsharp/actions) [![GitHub license](https://img.shields.io/github/license/shimat/opencvsharp.svg)](https://github.com/shimat/opencvsharp/blob/master/LICENSE) \n\nWrapper of OpenCV for .NET\n\nOld versions of OpenCvSharp are stored in [opencvsharp_2410](https://github.com/shimat/opencvsharp_2410).\n\n## NuGet\n\n| Package | Description | Link |\n|---------|-------------|------|\n|**OpenCvSharp4**| OpenCvSharp core libraries | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.svg)](https://badge.fury.io/nu/OpenCvSharp4) |\n|**OpenCvSharp4.WpfExtensions**| WPF Extensions | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions.svg)](https://badge.fury.io/nu/OpenCvSharp4.WpfExtensions) |\n|**OpenCvSharp4.Windows**| All-in-one package for Windows (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.Windows.svg)](https://badge.fury.io/nu/OpenCvSharp4.Windows) |\n|**OpenCvSharp4.runtime.win**| Native bindings for Windows x64/x86 (except UWP) | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.win.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.win) |\n|**OpenCvSharp4.runtime.uwp**| Native bindings for UWP (Universal Windows Platform) x64/x86/ARM | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.uwp) |\n|**OpenCvSharp4.runtime.ubuntu.18.04-x64**| Native bindings for Ubuntu 18.04 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64.svg)](https://badge.fury.io/nu/OpenCvSharp4.runtime.ubuntu.18.04-x64) |\n|**OpenCvSharp4.runtime.osx.10.15-x64**| Native bindings for macOS 10.15 x64 | [![NuGet version](https://badge.fury.io/nu/OpenCvSharp4.runtime.osx.10.15-x64.svg)](https://www.nuget.org/packages/OpenCvSharp4.runtime.osx.10.15-x64/) |\n|(beta packages)| Development Build Package    | https://ci.appveyor.com/nuget/opencvsharp |\n\nNative binding (OpenCvSharpExtern.dll / libOpenCvSharpExtern.so) is required to work OpenCvSharp. To use OpenCvSharp, you should add both `OpenCvSharp4` and `OpenCvSharp4.runtime.*` packages to your project. Currently, native bindings for Windows, UWP, Ubuntu 18.04 and macOS are released.\n\nPackages named OpenCvSharp3-* and OpenCvSharp-* are deprecated.\n> [OpenCvSharp3-AnyCPU](https://www.nuget.org/packages/OpenCvSharp3-AnyCPU/) / [OpenCvSharp3-WithoutDll](https://www.nuget.org/packages/OpenCvSharp3-WithoutDll/) / [OpenCvSharp-AnyCPU](https://www.nuget.org/packages/OpenCvSharp-AnyCPU/) /  [OpenCvSharp-WithoutDll](https://www.nuget.org/packages/OpenCvSharp-WithoutDll/)\n\n## Docker images\nhttps://hub.docker.com/u/shimat\n- [shimat/ubuntu18-dotnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/ubuntu18-dotnetcore3.1-opencv4.5.0)\n- [shimat/appengine-aspnetcore3.1-opencv4.5.0](https://hub.docker.com/r/shimat/appengine-aspnetcore3.1-opencv4.5.0)\n\n\n## Installation\n\n### Windows (except UWP)\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.win` NuGet packages to your project. You can use `OpenCvSharp4.Windows` instead.\n\n### UWP\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.uwp` NuGet packages to your project. Note that `OpenCvSharp4.runtime.win` and `OpenCvSharp4.Windows` don't work for UWP. \n\n### Ubuntu 18.04\nAdd `OpenCvSharp4` and `OpenCvSharp4.runtime.ubuntu.18.04.x64` NuGet packages to your project.\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\ndotnet add package OpenCvSharp4.runtime.ubuntu.18.04-x64\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Google AppEngine Flexible (Ubuntu 16.04)\nSome Docker images are provided to use OpenCvSharp with AppEngine Flexible. The native binding (libOpenCvSharpExtern) is already built in the docker image and you don't need to worry about it.\n```\nFROM shimat/appengine-aspnetcore3.1-opencv4.5.0:20201030\n\nADD ./ /app \nENV ASPNETCORE_URLS=http://*:${PORT} \n\nWORKDIR /app \nENTRYPOINT [ \"dotnet\", \"YourAspNetCoreProject.dll\" ]\n```\n\n### Ubuntu 18.04 Docker image\nYou can use the `shimat/ubuntu18-dotnetcore3.1-opencv4.5.0` docker image.\nThis issue may be helpful: https://github.com/shimat/opencvsharp/issues/920\n\n### Downloads\nIf you do not use NuGet, get DLL files from the [release page](https://github.com/shimat/opencvsharp/releases).\n\n## Target OpenCV\n* [OpenCV 4.5.0](http://opencv.org/) with [opencv_contrib](https://github.com/opencv/opencv_contrib)\n\n## Requirements\n* [.NET Framework 4.6.1](http://www.microsoft.com/ja-jp/download/details.aspx?id=1639) / [.NET Core 2.0](https://www.microsoft.com/net/download) / [Mono](http://www.mono-project.com/Main_Page)\n* (Windows) [Visual C++ 2019 Redistributable Package](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\n* (Windows Server) Media Foundation\n```\nPS1> Install-WindowsFeature Server-Media-Foundation\n```\n* (Ubuntu, Mac) You must pre-install all the dependency packages needed to build OpenCV. Many packages such as libjpeg must be installed in order to work OpenCV. \nhttps://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n\n\n**OpenCvSharp won't work on Unity and Xamarin platform.** For Unity, please consider using [OpenCV for Unity](https://www.assetstore.unity3d.com/en/#!/content/21088) or some other solutions.\n\n**OpenCvSharp does not support CUDA.** If you want to use the CUDA features, you need to customize the native bindings yourself.\n\n## Usage\nFor more details, see **[samples](https://github.com/shimat/opencvsharp_samples/)** and **[Wiki](https://github.com/shimat/opencvsharp/wiki)** pages.\n\n**Always remember to release Mat instances! The `using` syntax is useful.**\n```C#\n// C# 8\n// Edge detection by Canny algorithm\nusing OpenCvSharp;\n\nclass Program \n{\n    static void Main() \n    {\n        using var src = new Mat(\"lenna.png\", ImreadModes.Grayscale);\n \u00a0 \u00a0 \u00a0 \u00a0using var dst = new Mat();\n        \n        Cv2.Canny(src, dst, 50, 200);\n        using (new Window(\"src image\", src)) \n        using (new Window(\"dst image\", dst)) \n        {\n            Cv2.WaitKey();\n        }\n    }\n}\n```\n\n## Features\n* OpenCvSharp is modeled on the native OpenCV C/C++ API style as much as possible.\n* Many classes of OpenCvSharp implement IDisposable. There is no need to manage unsafe resources. \n* OpenCvSharp does not force object-oriented programming style on you. You can also call native-style OpenCV functions.\n* OpenCvSharp provides functions for converting from `Mat` into `Bitmap`(GDI+) or `WriteableBitmap`(WPF).\n\n## Code samples\nhttps://github.com/shimat/opencvsharp_samples/\n\n## Documents\nhttps://shimat.github.io/opencvsharp_docs/index.html\n\n## OpenCvSharp Build Instructions\n### Windows\n- Install Visual Studio 2019 or later\n  - VC++ features are required.\n- Run `download_opencv_windows.ps1` to download OpenCV libs and headers from https://github.com/shimat/opencv_files. Those lib files are precompiled by the owner of OpenCvSharp using AppVeyor CI.\n```\n.\\download_opencv_windows.ps1\n```\n- Build OpenCvSharp\n  - Open `OpenCvSharp.sln` and build\n  \n#### How to customize OpenCV binaries yourself\nIf you want to use some OpenCV features that are not provided by default in OpenCvSharp (e.g. GPU), you will have to build OpenCV yourself. The binary files of OpenCV for OpenCvSharp for Windows are created in the [opencv_files](https://github.com/shimat/opencv_files) repository. See the README.\n\n- `git clone --recursive https://github.com/shimat/opencv_files`\n- Edit `build_windows.ps1` or `build_uwp.ps1` to customize the CMake parameters .\n- Run the PowerShell script.\n\n### Ubuntu 18.04\n- Build OpenCV with opencv_contrib. \n  - https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/\n- Install .NET Core SDK. https://docs.microsoft.com/ja-jp/dotnet/core/install/linux-package-manager-ubuntu-1804\n- Get OpenCvSharp source files\n```\ngit clone https://github.com/shimat/opencvsharp.git\ncd opencvsharp\ngit fetch --all --tags --prune && git checkout ${OPENCVSHARP_VERSION}\n```\n\n- Build native wrapper `OpenCvSharpExtern`\n```\ncd opencvsharp/src\nmkdir build\ncd build\ncmake -D CMAKE_INSTALL_PREFIX=${YOUR_OPENCV_INSTALL_PATH} ..\nmake -j \nmake install\n```\nYou should add reference to `opencvsharp/src/build/OpenCvSharpExtern/libOpenCvSharpExtern.so`\n```\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/home/shimat/opencvsharp/src/build/OpenCvSharpExtern\"\n```\n\n- Add `OpenCvSharp4` NuGet package to your project\n```\ndotnet new console -n ConsoleApp01\ncd ConsoleApp01\ndotnet add package OpenCvSharp4\n# -- edit Program.cs --- # \ndotnet run\n```\n\n### Older Ubuntu\nRefer to the [Dockerfile](https://github.com/shimat/opencvsharp/blob/master/docker/google-appengine-ubuntu.16.04-x64/Dockerfile) and [Wiki pages](https://github.com/shimat/opencvsharp/wiki).\n\n## Donations\nIf you find the OpenCvSharp library useful and would like to show your gratitude by donating, here are some donation options. Thank you.\n\nhttps://github.com/sponsors/shimat\n"
 },
 {
  "repo": "abhiTronix/vidgear",
  "language": "Python",
  "readme_contents": "<!--\r\n===============================================\r\nvidgear library source-code is deployed under the Apache 2.0 License:\r\n\r\nCopyright (c) 2019-2020 Abhishek Thakur(@abhiTronix) <abhi.una12@gmail.com>\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n===============================================\r\n-->\r\n\r\n<h1 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/vidgear.webp\" alt=\"VidGear\" title=\"Logo designed by Abhishek Thakur(@abhiTronix), under CC-BY-NC-SA 4.0 License\" width=\"80%\"/>\r\n</h1>\r\n<h2 align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/tagline.svg\" alt=\"VidGear tagline\" width=\"40%\"/>\r\n</h2>\r\n\r\n<div align=\"center\">\r\n\r\n[Releases][release]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Gears][gears]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentation][docs]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Installation][installation]&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[License](#license)\r\n\r\n[![Build Status][travis-cli]][travis] [![Codecov branch][codecov]][code] [![Build Status][appveyor]][app]\r\n\r\n[![Glitter chat][gitter-bagde]][gitter] [![PyPi version][pypi-badge]][pypi] [![Twitter][twitter-badge]][twitter-intent]\r\n\r\n[![Code Style][black-badge]][black]\r\n\r\n</div>\r\n\r\n&nbsp;\r\n\r\nVidGear is a high-performance Video Processing python library that provides an easy-to-use, highly extensible, **Multi-Threaded + Asyncio wrapper** around many state-of-the-art specialized libraries like *[OpenCV][opencv], [FFmpeg][ffmpeg], [ZeroMQ][zmq], [picamera][picamera], [starlette][starlette], [pafy][pafy] and [python-mss][mss]* at its backend, and enable us to flexibly exploit their internal parameters and methods, while silently delivering robust error-handling and unparalleled real-time performance.\r\n\r\nVidGear primarily focuses on simplicity, and thereby lets programmers and software developers to easily integrate and perform Complex Video Processing Tasks, in just a few lines of code.\r\n\r\n&nbsp;\r\n\r\nThe following **functional block diagram** clearly depicts the generalized functioning of VidGear APIs:\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/gears_fbd.webp\" alt=\"@Vidgear Functional Block Diagram\" />\r\n</p>\r\n\r\n&nbsp;\r\n\r\n# Table of Contents\r\n\r\n* [**TL;DR**](#tldr)\r\n* [**Getting Started**](#getting-started)\r\n* [**Gears: What are these?**](#gears)\r\n  * [**CamGear**](#camgear)\r\n  * [**PiGear**](#pigear)\r\n  * [**VideoGear**](#videogear)\r\n  * [**ScreenGear**](#screengear)\r\n  * [**WriteGear**](#writegear)\r\n  * [**StreamGear**](#streamgear)\r\n  * [**NetGear**](#netgear)\r\n  * [**WebGear**](#webgear)\r\n  * [**NetGear_Async**](#netgear_async)\r\n* [**Documentation**](#documentation)\r\n* [**Community Channel**](#community-channel)\r\n* [**Contributions & Support**](#contributions--support)\r\n  * [**Support**](#support)\r\n  * [**Contributors**](#contributors)\r\n* [**Citation**](#citation)\r\n* [**Copyright**](#copyright)\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# TL;DR\r\n  \r\n#### What is vidgear?\r\n\r\n> *\"VidGear is a High-Performance Framework that provides an all-in-one complete Video Processing solution for building real-time applications in python.\"*\r\n\r\n#### What does it do?\r\n\r\n> *\"VidGear can read, write, process, send & receive video frames from/to various devices in real-time.\"*\r\n\r\n#### What is its purpose?\r\n\r\n> *\"Built with simplicity in mind, VidGear lets programmers and software developers to easily integrate and perform complex Video Processing tasks in their existing or new applications, in just a [few lines of code][switch_from_cv]. Beneficial for both, if you're new to programming with Python language or already a pro at it.\"*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## Getting Started\r\n\r\nIf this is your first time using VidGear, head straight to the [Installation \u27b6][installation] to install VidGear.\r\n\r\nOnce you have VidGear installed, **checkout its well-documented [Gears \u27b6][gears]**\r\n\r\nAlso, if you're already familar with [OpenCV][opencv] library, then see [Switching from OpenCV \u27b6][switch_from_cv]\r\n\r\nOr, if you're just getting started with OpenCV with Python, then see [here \u27b6](https://abhitronix.github.io/vidgear/help/general_faqs/#im-new-to-python-programming-or-its-usage-in-computer-vision-how-to-use-vidgear-in-my-projects)\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## Gears: What are these?\r\n\r\n> **VidGear is built with multiple [Gears][gears] (APIs), each with some unique functionality.**\r\n\r\nEach of these APIs is exclusively designed to handle/control different device-specific video streams, network streams, and media encoders. These APIs provide an easy-to-use, highly extensible, multi-threaded and asyncio layer above state-of-the-art libraries under the hood to exploit their internal parameters and methods flexibly while providing robust error-handling and unparalleled performance. \r\n\r\n**These Gears can be classified as follows:**\r\n\r\n**A. VideoCapture Gears:**\r\n\r\n  * [**CamGear:**](#camgear) Multi-threaded API targeting various IP-USB-Cameras/Network-Streams/YouTube-Video-URLs.\r\n  * [**PiGear:**](#pigear) Multi-threaded API targeting  various Raspberry Pi Camera Modules.\r\n  * [**ScreenGear:**](#screengear) Multi-threaded ultra-fast Screencasting.    \r\n  * [**VideoGear:**](#videogear) Common API with internal [Video Stabilizer](/gears/stabilizer/overview/) wrapper.\r\n\r\n**B. VideoWriter Gears:**\r\n\r\n  * [**WriteGear:**](#writegear) Handles Flexible Lossless Video Encoding and Compression.\r\n\r\n**C. Streaming Gears:**\r\n\r\n  * [**StreamGear**](#streamgear): Handles Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats.\r\n\r\n\r\n**D. Network Gears:**\r\n\r\n  * [**NetGear:**](#netgear) Handles high-performance video-frames & data transfer between interconnecting systems over the network.\r\n\r\n  * **Asynchronous I/O Network Gears:**\r\n\r\n    * [**WebGear:**](#webgear) ASGI Video Server that can send live video-frames to any web browser on the network.\r\n    * [**NetGear_Async:**](#netgear_async) Immensely Memory-efficient Asyncio video-frames network messaging framework.\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## CamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/camgear.webp\" alt=\"CamGear Functional Block Diagram\" width=\"45%\"/>\r\n</p>\r\n\r\n> *CamGear can grab ultra-fast frames from diverse range of devices/streams, which includes almost any IP/USB Cameras, multimedia video file format ([_upto 4k tested_][test-4k]), various network stream protocols such as `http(s), rtp, rstp, rtmp, mms, etc.`, plus support for live Gstreamer's stream pipeline and YouTube video/live-streams URLs.*\r\n\r\nCamGear provides a flexible, high-level multi-threaded wrapper around `OpenCV's` [VideoCapture class][opencv-vc] with access almost all of its available parameters and also employs [`pafy`][pafy] python APIs for live [YouTube streaming][youtube-doc]. Furthermore, CamGear relies exclusively on [**Threaded Queue mode**][TQM-doc] for ultra-fast, error-free and synchronized frame handling.\r\n\r\n### CamGear API Guide:\r\n\r\n[**>>> Usage Guide**][camgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## VideoGear\r\n\r\n> *VideoGear API provides a special internal wrapper around VidGear's exclusive [**Video Stabilizer**][stablizer-doc] class.*\r\n\r\nVideoGear also act as a Common API, that provides an internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs and their parameters, with a special `enablePiCamera` boolean flag.\r\n\r\nVideoGear is basically ideal when you need to switch to different video sources without changing your code much. Also, it enables easy stabilization for various video-streams _(real-time or not)_  with minimum effort and using way fewer lines of code.\r\n\r\n\r\n**Below is a snapshot of a VideoGear Stabilizer in action  (_See its detailed usage [here][stablizer-doc-ex]_):**\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://github.com/abhiTronix/Imbakup/raw/master/Images/stabilizer.gif\" alt=\"VideoGear Stabilizer in action!\"/>\r\n  <br>\r\n  <sub><i>Original Video Courtesy <a href=\"http://liushuaicheng.org/SIGGRAPH2013/database.html\" title=\"opensourced video samples database\">@SIGGRAPH2013</a></i></sub>\r\n</p>\r\n\r\n**Code to generate above result:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import VideoGear\r\nimport numpy as np\r\nimport cv2\r\n\r\n# open any valid video stream with stabilization enabled(`stabilize = True`)\r\nstream_stab = VideoGear(source = \"test.mp4\", stabilize = True).start()\r\n\r\n# open same stream without stabilization for comparison\r\nstream_org = VideoGear(source = \"test.mp4\").start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read stabilized frames\r\n    frame_stab = stream_stab.read()\r\n\r\n    # check for stabilized frame if Nonetype\r\n    if frame_stab is None:\r\n        break\r\n\r\n    # read un-stabilized frame\r\n    frame_org = stream_org.read()\r\n\r\n    # concatenate both frames\r\n    output_frame = np.concatenate((frame_org, frame_stab), axis=1)\r\n\r\n    # put text over concatenated frame\r\n    cv2.putText(\r\n        output_frame, \"Before\", (10, output_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n    cv2.putText(\r\n        output_frame, \"After\", (output_frame.shape[1] // 2 + 10, output_frame.shape[0] - 10),\r\n        cv2.FONT_HERSHEY_SIMPLEX,\r\n        0.6, (0, 255, 0), 2,\r\n    )\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Stabilized Frame\", output_frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close both video streams\r\nstream_org.stop()\r\nstream_stab.stop()\r\n```\r\n\r\n### VideoGear API Guide:\r\n\r\n[**>>> Usage Guide**][videogear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## PiGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/picam2.webp\" alt=\"PiGear\" width=\"50%\" />\r\n</p>\r\n\r\n> *PiGear is similar to CamGear but made to support various Raspberry Pi Camera Modules *(such as [OmniVision OV5647 Camera Module][OV5647-picam] and [Sony IMX219 Camera Module][IMX219-picam])*.*\r\n\r\nPiGear provides a flexible multi-threaded wrapper around complete [picamera](https://picamera.readthedocs.io/en/release-1.13/index.html) python library, and also provides us the ability to exploit almost all of its parameters like `brightness, saturation, sensor_mode, iso, exposure, etc.` effortlessly. Furthermore, PiGear supports multiple camera modules, such as in case of Raspberry Pi Compute module IO boards.\r\n\r\nBest of all, PiGear provides excellent error-handling with features like a **Threaded Internal Timer** - that keeps active track of any frozen-threads/hardware-failures robustly, and exit safely if it does occurs, i.e. If you're running PiGear API in your script, and someone accidentally pulls Camera module cable out, instead of going into possible kernel panic, PiGear will exit safely to save resources. \r\n\r\n\r\n**Code to open picamera stream with variable parameters in PiGear API:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import PiGear\r\nimport cv2\r\n\r\n# add various Picamera tweak parameters to dictionary\r\noptions = {\"hflip\": True, \"exposure_mode\": \"auto\", \"iso\": 800, \"exposure_compensation\": 15, \"awb_mode\": \"horizon\", \"sensor_mode\": 0}\r\n\r\n# open pi video stream with defined parameters\r\nstream = PiGear(resolution = (640, 480), framerate = 60, logging = True, **options).start() \r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n### PiGear API Guide:\r\n\r\n[**>>> Usage Guide**][pigear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## ScreenGear\r\n\r\n> *ScreenGear is designed exclusively for ultra-fast Screencasting, that means it can grab frames from your monitor in real-time, either by define an area on the computer screen, or full-screen, at the expense of inconsiderable latency. ScreenGear also seamlessly support frame capturing from multiple monitors.*\r\n\r\nScreenGear implements a multi-threaded wrapper around [**python-mss**][mss] python library API and also supports a easy and flexible direct internal parameter manipulation. \r\n\r\n**Below is a snapshot of a ScreenGear API in action:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/screengear.gif\" alt=\"ScreenGear in action!\"/>\r\n</p>\r\n\r\n**Code to generate the above results:**\r\n\r\n```python\r\n# import required libraries\r\nfrom vidgear.gears import ScreenGear\r\nimport cv2\r\n\r\n# open video stream with default parameters\r\nstream = ScreenGear().start()\r\n\r\n# loop over\r\nwhile True:\r\n\r\n    # read frames from stream\r\n    frame = stream.read()\r\n\r\n    # check for frame if Nonetype\r\n    if frame is None:\r\n        break\r\n\r\n\r\n    # {do something with the frame here}\r\n\r\n\r\n    # Show output window\r\n    cv2.imshow(\"Output Frame\", frame)\r\n\r\n    # check for 'q' key if pressed\r\n    key = cv2.waitKey(1) & 0xFF\r\n    if key == ord(\"q\"):\r\n        break\r\n\r\n# close output window\r\ncv2.destroyAllWindows()\r\n\r\n# safely close video stream\r\nstream.stop()\r\n```\r\n\r\n### ScreenGear API Guide:\r\n\r\n[**>>> Usage Guide**][screengear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n## WriteGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/writegear.webp\" alt=\"WriteGear Functional Block Diagram\" width=\"70%\" />\r\n</p>\r\n\r\n> *WriteGear handles various powerful Writer Tools that provide us the freedom to do almost anything imagine with multimedia files.*\r\n\r\nWriteGear API provides a complete, flexible and robust wrapper around [**FFmpeg**][ffmpeg], a leading multimedia framework. With WriteGear, we can process real-time frames into a lossless compressed video-file with any suitable specification in just few easy lines of codes. These specifications include setting video/audio properties such as `bitrate, codec, framerate, resolution, subtitles,  etc.`, and also performing complex tasks such as multiplexing video with audio in real-time _(see this [doc][live-audio-doc])_, while handling all errors robustly. \r\n\r\nBest of all, WriteGear grants the complete freedom to play with any FFmpeg parameter with its exclusive **Custom Commands function** _(see this [doc][custom-command-doc])_, without relying on any Third-party library.\r\n\r\nIn addition to this, WriteGear also provides flexible access to [**OpenCV's VideoWriter API**][opencv-writer] which provides some basic tools for video frames encoding but without compression.\r\n\r\n**WriteGear primarily operates in the following two modes:**\r\n\r\n  * **Compression Mode:** In this mode, WriteGear utilizes powerful [**FFmpeg**][ffmpeg] inbuilt encoders to encode lossless multimedia files. This mode provides us the ability to exploit almost any parameter available within FFmpeg, effortlessly and flexibly, and while doing that it robustly handles all errors/warnings quietly. *You can find more about this mode [here \u27b6][cm-writegear-doc]*\r\n\r\n  * **Non-Compression Mode:**  In this mode, WriteGear utilizes basic [**OpenCV's inbuilt VideoWriter API**][opencv-vw] tools. This mode also supports all parameters manipulation available within VideoWriter API, but it lacks the ability to manipulate encoding parameters and other important features like video compression, audio encoding, etc. *You can learn about this mode [here \u27b6][ncm-writegear-doc]*\r\n\r\n### WriteGear API Guide:\r\n\r\n[**>>> Usage Guide**][writegear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## StreamGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/streamgear_flow.webp\" alt=\"NetGear API\" width=80%/>\r\n</p>\r\n\r\n\r\n> *StreamGear automates transcoding workflow for generating Ultra-Low Latency, High-Quality, Dynamic & Adaptive Streaming Formats (such as MPEG-DASH) in just few lines of python code.*\r\n\r\nStreamGear provides a standalone, highly extensible and flexible wrapper around [**FFmpeg**][ffmpeg] - a leading multimedia framework, for generating chunked-encoded media segments of the content.\r\n\r\nSteamGear API automatically transcodes source videos/audio files & real-time frames, and breaks them into a sequence of multiple smaller chunks/segments (typically 2-4 seconds in length) at different quality levels _(i.e. different bitrates or spatial resolutions)_. It also creates a Manifest file _(such as MPD in-case of DASH)_ that describes these segment information _(timing, URL, media characteristics like video resolution and bit rates)_, and is provided to the client prior to the streaming session. Thereby, segments are served on a web server and can be downloaded through HTTP standard compliant GET requests. This makes it possible to stream videos at different quality levels, and to switch in the middle of a video from one quality level to another one \u2013 if bandwidth permits \u2013 on a per segment basis.\r\n\r\n\r\nSteamGear currently only supports [**MPEG-DASH**](https://www.encoding.com/mpeg-dash/) _(Dynamic Adaptive Streaming over HTTP, ISO/IEC 23009-1)_ , but other adaptive streaming technologies such as Apple HLS, Microsoft Smooth Streaming, will be added soon.\r\n\r\n**StreamGear primarily works in two independent modes for transcoding which serves different purposes:**\r\n\r\n  * **Single-Source Mode:** In this mode, StreamGear transcodes entire video/audio file _(as opposed to frames by frame)_ into a sequence of multiple smaller chunks/segments for streaming. This mode works exceptionally well, when you're transcoding lossless long-duration videos(with audio) for streaming and required no extra efforts or interruptions. But on the downside, the provided source cannot be changed or manipulated before sending onto FFmpeg Pipeline for processing.  This mode can be easily activated by assigning suitable video path as input to `-video_source` attribute, during StreamGear initialization. ***Learn more about this mode [here \u27b6][ss-mode-doc]***\r\n\r\n  * **Real-time Frames Mode:** When no valid input is received on `-video_source` attribute, StreamGear API activates this mode where it directly transcodes video-frames _(as opposed to a entire file)_, into a sequence of multiple smaller chunks/segments for streaming. In this mode, StreamGear supports real-time [`numpy.ndarray`](https://numpy.org/doc/1.18/reference/generated/numpy.ndarray.html#numpy-ndarray) frames, and process them over FFmpeg pipeline. But on the downside, audio has to added manually _(as separate source)_ for streams. ***Learn more about this mode [here \u27b6][rtf-mode-doc]***\r\n\r\n\r\n### StreamGear API Guide:\r\n\r\n[**>>> Usage Guide**][streamgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/netgear.webp\" alt=\"NetGear API\" width=65%/>\r\n</p>\r\n\r\n> *NetGear is exclusively designed to transfer video frames synchronously and asynchronously between interconnecting systems over the network in real-time.*\r\n\r\nNetGear implements a high-level wrapper around [**PyZmQ**][pyzmq] python library that contains python bindings for [ZeroMQ][zmq] - a high-performance asynchronous distributed messaging library that provides a message queue, but unlike message-oriented middleware, its system can run without a dedicated message broker. \r\n\r\nNetGear provides seamless support for [*Bidirectional data transmission*][netgear_bidata_doc] between receiver(client) and sender(server) through bi-directional synchronous messaging patterns such as zmq.PAIR _(ZMQ Pair Pattern)_ & zmq.REQ/zmq.REP _(ZMQ Request/Reply Pattern)_. \r\n\r\nNetGear also supports real-time [*Frame Compression capabilities*][netgear_compression_doc] for optimizing performance while sending the frames directly over the network, by encoding the frame before sending it and decoding it on the client's end automatically in real-time. \r\n\r\nFor security, NetGear implements easy access to ZeroMQ's powerful, smart & secure Security Layers, that enables [*Strong encryption on data*][netgear_security_doc], and unbreakable authentication between the Server and the Client with the help of custom certificates/keys and brings easy, standardized privacy and authentication for distributed systems over the network. \r\n\r\nBest of all, NetGear can robustly handle [*Multiple Server-Systems*][netgear_multi_server_doc] and [*Multiple Client-Systems*][netgear_multi_client_doc] and at once, thereby providing access to seamless Live Streaming of the multiple device in a network at the same time.\r\n\r\n\r\n**NetGear as of now seamlessly supports three ZeroMQ messaging patterns:**\r\n\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear API Guide:\r\n\r\n[**>>> Usage Guide**][netgear-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n## WebGear\r\n\r\n> *WebGear is a powerful [ASGI](https://asgi.readthedocs.io/en/latest/) Video-streamer API, that is built upon [Starlette](https://www.starlette.io/) - a lightweight ASGI framework/toolkit, which is ideal for building high-performance asyncio services.*\r\n\r\nWebGear API provides a highly extensible and flexible asyncio wrapper around [Starlette][starlette] ASGI application, and provides easy access to its complete framework. Thereby, WebGear API can flexibly interact with the Starlette's ecosystem of shared middleware and mountable applications, and its various [Response classes](https://www.starlette.io/responses/), [Routing tables](https://www.starlette.io/routing/), [Static Files](https://www.starlette.io/staticfiles/), [Templating engine(with Jinja2)](https://www.starlette.io/templates/), etc. \r\n\r\nIn layman's terms, WebGear can acts as powerful **Video Streaming Server** that transfers live video-frames to any web browser on a network. It addition to this, WebGear API also provides a special internal wrapper around [VideoGear](#videogear), which itself provides internal access to both [CamGear](#camgear) and [PiGear](#pigear) APIs thereby granting it exclusive power for streaming frames incoming from any device/source, such as streaming [Stabilization enabled Video][stabilize_webgear_doc] in real-time.\r\n\r\n**Below is a snapshot of a WebGear Video Server in action on the Mozilla Firefox browser:**\r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/gifs/webgear.gif\" alt=\"WebGear in action!\" width=\"70%\" />\r\n  <br>\r\n  <sub><i>WebGear Video Server at <a href=\"http://localhost:8000/\" title=\"default address\">http://localhost:8000/</a> address.</i></sub>\r\n</p>\r\n\r\n**Code to generate the above result:**\r\n\r\n```python\r\n# import required libraries\r\nimport uvicorn\r\nfrom vidgear.gears.asyncio import WebGear\r\n\r\n#various performance tweaks\r\noptions = {\"frame_size_reduction\": 40, \"frame_jpeg_quality\": 80, \"frame_jpeg_optimize\": True, \"frame_jpeg_progressive\": False}\r\n\r\n#initialize WebGear app  \r\nweb = WebGear(source = \"foo.mp4\", logging = True, **options)\r\n\r\n#run this app on Uvicorn server at address http://localhost:8000/\r\nuvicorn.run(web(), host='localhost', port=8000)\r\n\r\n#close app safely\r\nweb.shutdown()\r\n```\r\n\r\n### WebGear API Guide:\r\n\r\n[**>>> Usage Guide**][webgear-doc]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## NetGear_Async \r\n\r\n<p align=\"center\">\r\n  <img src=\"docs/overrides/assets/images/zmq_asyncio.webp\" alt=\"WebGear in action!\" width=\"70%\"/>\r\n</p>\r\n\r\n> _NetGear_Async can generate double performance as compared to [NetGear API](#netgear) at about 1/3rd of memory consumption, and also provide complete server-client handling with various options to use variable protocols/patterns similar to NetGear, but it doesn't support any [NetGear's Exclusive Modes][netgear-exm] yet._\r\n\r\nNetGear_Async is an asyncio videoframe messaging framework, built on [`zmq.asyncio`][asyncio-zmq], and powered by high-performance asyncio event loop called [**`uvloop`**][uvloop] to achieve unmatchable high-speed and lag-free video streaming over the network with minimal resource constraints. Basically, this API is able to transfer thousands of frames in just a few seconds without causing any significant load on your system. \r\n\r\nNetGear_Async provides complete server-client handling and options to use variable protocols/patterns similar to [NetGear API](#netgear) but doesn't support any [NetGear Exclusive modes][netgear-exm] yet. Furthermore, NetGear_Async allows us to  define our own custom Server Source to manipulate frames easily before sending them across the network(see this [doc][netgear_Async-cs] example).\r\n\r\nNetGear_Async as of now supports [all four ZeroMQ messaging patterns](#attributes-and-parameters-wrench):\r\n* [**`zmq.PAIR`**][zmq-pair] _(ZMQ Pair Pattern)_ \r\n* [**`zmq.REQ/zmq.REP`**][zmq-req-rep] _(ZMQ Request/Reply Pattern)_\r\n* [**`zmq.PUB/zmq.SUB`**][zmq-pub-sub] _(ZMQ Publish/Subscribe Pattern)_ \r\n* [**`zmq.PUSH/zmq.PULL`**][zmq-pull-push] _(ZMQ Push/Pull Pattern)_\r\n\r\nWhereas supported protocol are: `tcp` and `ipc`.\r\n\r\n### NetGear_Async API Guide:\r\n\r\n[**>>> Usage Guide**][netgear_async-doc]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Documentation\r\n\r\nThe complete documentation for all VidGear APIs can be found in the link below:\r\n\r\n* [**Documentation - English**][docs]\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n# Contributions & Support\r\n\r\nContributions are welcome, We'd love your contribution to VidGear in order to fix bugs or to implement new features!  \r\n\r\nPlease see our **[Contribution Guidelines](contributing.md)** for more details.\r\n\r\n### Support\r\n\r\n**VidGear is free, but rely on your support.** \r\n\r\nSending a donation using link below is **extremely** helpful in keeping VidGear development alive:\r\n\r\n[![ko-fi][kofi-badge]][kofi]\r\n\r\n### Contributors\r\n\r\n<a href=\"https://github.com/abhiTronix/vidgear/graphs/contributors\">\r\n  <img src=\"https://contributors-img.web.app/image?repo=abhiTronix/vidgear\" />\r\n</a>\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Community Channel\r\n\r\nIf you've come up with some new idea, or looking for the fastest way troubleshoot your problems, then *join our [Gitter community channel \u27b6][gitter]*\r\n\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n# Citation\r\n\r\nHere is a Bibtex entry you can use to cite this project in a publication:\r\n\r\n\r\n```BibTeX\r\n@misc{vidgear,\r\n    author = {Abhishek Thakur},\r\n    title = {vidgear},\r\n    howpublished = {\\url{https://github.com/abhiTronix/vidgear}},\r\n    year = {2019-2020}\r\n  }\r\n```\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n\r\n# Copyright\r\n\r\n**Copyright \u00a9 abhiTronix 2019-2020**\r\n\r\nThis library is released under the **[Apache 2.0 License][license]**.\r\n\r\n\r\n\r\n\r\n<!--\r\nBadges\r\n-->\r\n\r\n[appveyor]:https://img.shields.io/appveyor/ci/abhitronix/vidgear.svg?style=for-the-badge&logo=appveyor\r\n[codecov]:https://img.shields.io/codecov/c/github/abhiTronix/vidgear/testing?style=for-the-badge&logo=codecov\r\n[travis-cli]:https://img.shields.io/travis/com/abhiTronix/vidgear/testing?logo=travis&style=for-the-badge\r\n[prs-badge]:https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAABC0lEQVRYhdWVPQoCMRCFX6HY2ghaiZUXsLW0EDyBrbWtN/EUHsHTWFnYyCL4gxibVZZlZzKTnWz0QZpk5r0vIdkF/kBPAMOKeddE+CQPKoc5Yt5cTjBMdQSwDQToWgBJAn3jmhqgltapAV6E6b5U17MGGAUaUj07TficMfIBZDV6vxowBm1BP9WbSQE4o5h9IjPJmy73TEPDDxVmoZdQrQ5jRhly9Q8tgMUXkIIWn0oG4GYQfAXQzz1PGoCiQndM7b4RgJay/h7zBLT3hASgoKjamQJMreKf0gfuAGyYtXEIAKcL/Dss15iq6ohXghozLYiAMxPuACwtIT4yeQUxAaLrZwAoqGRKGk7qDSYTfYQ8LuYnAAAAAElFTkSuQmCC\r\n[twitter-badge]:https://img.shields.io/badge/Tweet-Now-blue.svg?style=for-the-badge&logo=twitter\r\n[pypi-badge]:https://img.shields.io/pypi/v/vidgear.svg?style=for-the-badge&logo=pypi\r\n[gitter-bagde]:https://img.shields.io/badge/Chat-Gitter-blue.svg?style=for-the-badge&logo=gitter\r\n[Coffee-badge]:https://abhitronix.github.io/img/vidgear/orange_img.png\r\n[kofi-badge]:https://www.ko-fi.com/img/githubbutton_sm.svg\r\n[black-badge]:https://img.shields.io/badge/code%20style-black-000000.svg?style=for-the-badge&logo=github\r\n\r\n\r\n<!--\r\nInternal URLs\r\n-->\r\n\r\n[release]:https://github.com/abhiTronix/vidgear/releases/latest\r\n[pypi]:https://pypi.org/project/vidgear/\r\n[gitter]:https://gitter.im/vidgear/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\r\n[twitter-intent]:https://twitter.com/intent/tweet?url=https%3A%2F%2Fabhitronix.github.io%2Fvidgear&via%20%40abhi_una12&text=Checkout%20VidGear%20-%20A%20High-Performance%20Video-Processing%20Python%20Framework.&hashtags=vidgear%20%23videoprocessing%20%23python%20%23threaded%20%23asyncio\r\n[coffee]:https://www.buymeacoffee.com/2twOXFvlA\r\n[kofi]: https://ko-fi.com/W7W8WTYO\r\n[license]:https://github.com/abhiTronix/vidgear/blob/master/LICENSE\r\n[travis]:https://travis-ci.com/github/abhiTronix/vidgear\r\n[app]:https://ci.appveyor.com/project/abhiTronix/vidgear\r\n[code]:https://codecov.io/gh/abhiTronix/vidgear\r\n\r\n[test-4k]:https://github.com/abhiTronix/vidgear/blob/e0843720202b0921d1c26e2ce5b11fadefbec892/vidgear/tests/benchmark_tests/test_benchmark_playback.py#L65\r\n[bs_script_dataset]:https://github.com/abhiTronix/vidgear/blob/testing/scripts/bash/prepare_dataset.sh\r\n\r\n[faq]:https://abhitronix.github.io/vidgear/help/get_help/#frequently-asked-questions\r\n[doc-vidgear-purpose]:https://abhitronix.github.io/vidgear/help/motivation/#why-is-vidgear-a-thing\r\n[live-audio-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/usage/#using-compression-mode-with-live-audio-input\r\n[ffmpeg-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/ffmpeg_install/\r\n[youtube-doc]:https://abhitronix.github.io/vidgear/gears/camgear/usage/#using-camgear-with-youtube-videos\r\n[TQM-doc]:https://abhitronix.github.io/vidgear/bonus/TQM/#threaded-queue-mode\r\n[camgear-doc]:https://abhitronix.github.io/vidgear/gears/camgear/overview/\r\n[stablizer-doc]:https://abhitronix.github.io/vidgear/gears/stabilizer/overview/\r\n[stablizer-doc-ex]:https://abhitronix.github.io/vidgear/gears/videogear/usage/#using-videogear-with-video-stabilizer-backend\r\n[videogear-doc]:https://abhitronix.github.io/vidgear/gears/videogear/overview/\r\n[pigear-doc]:https://abhitronix.github.io/vidgear/gears/pigear/overview/\r\n[cm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/overview/\r\n[ncm-writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/non_compression/overview/\r\n[screengear-doc]:https://abhitronix.github.io/vidgear/gears/screengear/overview/\r\n[streamgear-doc]:https://abhitronix.github.io/vidgear/gears/streamgear/overview/\r\n[writegear-doc]:https://abhitronix.github.io/vidgear/gears/writegear/introduction/\r\n[netgear-doc]:https://abhitronix.github.io/vidgear/gears/netgear/overview/\r\n[webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/overview/\r\n[netgear_async-doc]:https://abhitronix.github.io/vidgear/gears/netgear_async/overview/\r\n[drop35]:https://github.com/abhiTronix/vidgear/issues/99\r\n[custom-command-doc]:https://abhitronix.github.io/vidgear/gears/writegear/compression/advanced/cciw/\r\n[advanced-webgear-doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/\r\n[netgear_bidata_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/bidirectional_mode/\r\n[netgear_compression_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/compression/\r\n[netgear_security_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/secure_mode/\r\n[netgear_multi_server_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_server/\r\n[netgear_multi_client_doc]:https://abhitronix.github.io/vidgear/gears/netgear/advanced/multi_client/\r\n[netgear-exm]: https://abhitronix.github.io/vidgear/gears/netgear/overview/#modes-of-operation\r\n[stabilize_webgear_doc]:https://abhitronix.github.io/vidgear/gears/webgear/advanced/#using-webgear-with-real-time-video-stabilization-enabled\r\n[netgear_Async-cs]: https://abhitronix.github.io/vidgear/gears/netgear_async/usage/#using-netgear_async-with-a-custom-sourceopencv\r\n[installation]:https://abhitronix.github.io/vidgear/installation/\r\n[gears]:https://abhitronix.github.io/vidgear/gears\r\n[switch_from_cv]:https://abhitronix.github.io/vidgear/switch_from_cv/\r\n[ss-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#a-single-source-mode\r\n[rtf-mode-doc]: https://abhitronix.github.io/vidgear/gears/streamgear/usage/#b-real-time-frames-mode\r\n[docs]: https://abhitronix.github.io/vidgear\r\n\r\n<!--\r\nExternal URLs\r\n-->\r\n[asyncio-zmq]:https://pyzmq.readthedocs.io/en/latest/api/zmq.asyncio.html\r\n[uvloop]: https://github.com/MagicStack/uvloop\r\n[uvloop-ns]: https://github.com/MagicStack/uvloop/issues/14\r\n[ffmpeg]:https://www.ffmpeg.org/\r\n[flake8]: https://flake8.pycqa.org/en/latest/\r\n[black]: https://github.com/psf/black\r\n[pytest]:https://docs.pytest.org/en/latest/\r\n[opencv-writer]:https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5\r\n[OpenCV-windows]:https://www.learnopencv.com/install-opencv3-on-windows/\r\n[OpenCV-linux]:https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/\r\n[OpenCV-pi]:https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/\r\n[starlette]:https://www.starlette.io/\r\n[uvicorn]:http://www.uvicorn.org/\r\n[daphne]:https://github.com/django/daphne/\r\n[hypercorn]:https://pgjones.gitlab.io/hypercorn/\r\n[prs]:http://makeapullrequest.com\r\n[opencv]:https://github.com/opencv/opencv\r\n[picamera]:https://github.com/waveform80/picamera\r\n[pafy]:https://github.com/mps-youtube/pafy\r\n[pyzmq]:https://github.com/zeromq/pyzmq\r\n[zmq]:https://zeromq.org/\r\n[mss]:https://github.com/BoboTiG/python-mss\r\n[pip]:https://pip.pypa.io/en/stable/installing/\r\n[opencv-vc]:https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#a57c0e81e83e60f36c83027dc2a188e80\r\n[OV5647-picam]:https://github.com/techyian/MMALSharp/doc/OmniVision-OV5647-Camera-Module\r\n[IMX219-picam]:https://github.com/techyian/MMALSharp/doc/Sony-IMX219-Camera-Module\r\n[opencv-vw]:https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html\r\n[yt-dl]:https://github.com/ytdl-org/youtube-dl/\r\n[numpy]:https://github.com/numpy/numpy\r\n[zmq-pair]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pair.html\r\n[zmq-req-rep]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/client_server.html\r\n[zmq-pub-sub]:https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pubsub.html\r\n[zmq-pull-push]: https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pushpull.html#push-pull\r\n[picamera-setting]:https://picamera.readthedocs.io/en/release-1.13/quickstart.html"
 },
 {
  "repo": "oreillymedia/Learning-OpenCV-3_examples",
  "language": "C++",
  "readme_contents": "# Learning OpenCV 3\n\n## INTRO\n\nThis is the example code that accompanies Learning OpenCV 3 by Adrian Kaehler and Gary Bradski ([9781491937990](http:*shop.oreilly.com/product/0636920044765.do)).\n\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n\n\n## NOTES\n\nFor default suggestions of how the run the code, it assumes you put your build directory under `Learning-OpenCV-3_examples` directory. \n\nThus, from the `Learning-OpenCV-3_examples` directory:\n\n```\t\n  mkdir build\n  cd build\n  cmake ..\n  make -j\n```\n\n#### Docker\nFor your interest, included here is an Ubuntu _Docker_ file that\n* Shares a directory with the host operating system\n* Shares the first camera between both systems\n* Loads Ubuntu 16.04 \n* Loads all dependencies for OpenCV 3.2 and opencv_contrib\n* Loads and builds OpenCV 3.2 and opencv_contrib into a build directory \n  * executable files end up in `opencv-3.2.0/build/bin`\n* Next, it `git clones` the code (and Docker file) for Learning OpenCV 3 and builds it\n  * executable files end up in `Learning_OpenCV-3_examples/build`\n* To get to the top level directory, just type: `cd`\n\n\n## CONTENTS:\n\n### SPECIAL FILES:\n\n* README.md       -- this readme file\n* Dockerfile      -- complete self contained opencv environment using Ubuntu 16-04\n* CMakeLists.txt  -- how to buld everything here \n\n### EXERCISES:\n\n* Exercises at end of Chapter 5\n* Exercises at end of Chapter 7\n* Exercises_8_1.cpp Exercises at end of Chapter 8\n* Exercises_9_1-2-10-11-12-15-16.cpp Exercises at end of Chapter 8\n* Exercises_9_4.cpp Exercises at end of Chapter 9\n* Exercises_9_5.cpp Exercises at end of Chapter 9\n* Exercises at end of Chapter 11\n* Exercises_13_1-2-11.cpp\tExercises for Chapter 13\n* Exercises_13_9.cpp\t\n\n### EXAMPLES:\n\n* Example 2-1. A simple OpenCV program that loads an image from disk and displays it\n* Example 2-2. Same as Example 2-1 but employing the \u201cusing namespace\u201d directive\n* Example 2-3. A simple OpenCV program for playing a video file from disk\n* Example 2-4. Adding a trackbar slider to the basic viewer window for moving around\n* Example 2-5. Loading and then smoothing an image before it is displayed on the screen\n* Example 2-6. Using cv::pyrDown() to create a new image that is half the width and\n* Example 2-7. The Canny edge detector writes its output to a single-channel (grayscale) image\n* Example 2-8. Combining the pyramid down operator (twice) and the Canny\n* Example 2-9. Getting and setting pixels in Example 2-8\n* Example 2-10. The same object can load videos from a camera or a file\n* Example 2-11. A complete program to read in a color video and write out the log-polar-\n* Example 4-1. Summation of a multidimensional array, done plane by plane\n* Example 4-2. Summation of two arrays using the N-ary operator\n* Example 4-3. Printing all of the nonzero elements of a sparse array\n* Example 4-4. A better way to print a matrix\n* Example 5-1. Complete program to alpha-blend the ROI starting at (0,0) in src2 with the ROI starting at (x,y) in src1\n* Example 7-1. Using the default random number generator to generate a pair of integers\n* Example 8-1. Unpacking a four-character code to identify a video codec\n* Example 8-2. Using cv::FileStorage to create a .yml data file\n* Example 8-3. Using cv::FileStorage to read a .yml file\n* Example 9-1. Creating a window and displaying an image in that window\n* Example 9-2. Toy program for using a mouse to draw boxes on the screen\n* Example 9-3. Using a trackbar to create a \u201cswitch\u201d tha t the user can turn on and off;\n* Example 9-4. Slightly modified code from the OpenCV documentation that draws a\n* Example 9-5. An example program ch4_qt.cpp, which takes a single argument\n* Example 9-6. The QMoviePlayer object header file QMoviePlayer.hpp\n* Example 9-7. The QMoviePlayer object source file: QMoviePlayer.cpp\n* Example 9-8. An example program which takes a single argument\n* Example 9-9. The WxMoviePlayer object header file WxMoviePlayer.hpp\n* Example 9-10. The WxMoviePlayer object source file WxMoviePlayer.cpp\n* Example 9-11. An example header file for our custom View class\n* Example 10-1. Using cv::threshold() to sum three channels of an image\n* Example 10-2. Alternative method to combine and threshold image planes\n* Example 10-3. Threshold versus adaptive threshold\n* Example 11-1. An affine transformation.\n* Example 11-2. Code for perspective transformation\n* Example 11-3. Log-polar transform example\n* Example 12-1. Using cv::dft() and cv::idft() to accelerate the computation of\n* Example 12-2. Using cv::HoughCircles() to return a sequence of circles found in a\n* **EXTRA** Example 12-3. Using GrabCut for background removal\n* **EXTRA** Example 12-4. Using GrabCut for background removal\n* Example 13-1. Histogram computation and display\n* Example 13-2. Creating signatures from histograms for EMD; note that this code is the\n* Example 13-3. Template matching\n* Example 14-1. Finding contours based on a trackbar\u2019s location; the contours are\n* Example 14-2. Finding and drawing contours on an input image\n* Example 14-3. Drawing labeled connected components\n* Example 14-4. Using the shape context distance extractor\n* Example 15-1. Reading out the RGB values of all pixels in one row of a video and\n* Example 15-2. Learning a background model to identify foreground pixels\n* Example 15-3. Computing the on and off-diagonal elements of a variance/covariance model\n* Example 15-4. Codebook algorithm implementation\n* Example 15-5. Cleanup using connected components\n* **EXTRA** Example 15-6, using OpenCV's background subtractor class.  Modified by Gary Bradski, 6/4/2017\n* Example 16-1. Pyramid L-K optical flow\n* **EXTRA** Example 16-2. 2D Feature detectors and 2D Extra Features framework\n* Example 17-1. Kalman filter example code\n* Example 17-2. Farneback optical flow example code\n* Example 18-1. Reading a chessboard\u2019s width and height, reading them and calibrating \n* **EXTRA** Example 18-1. From disk. Reading a chessboard\u2019s width and height, reading them and calibrating \n* Example 19-1. Bird\u2019s - eye view\n* Example 19-2. Computing the fundamental matrix using RANSAC\n* Example 19-3. Stereo calibration, rectification, and correspondence\n* Example 19-4. Two-dimensional line fitting\n* Example 20-01. Using K-means\n* Example 20-02. Using the Mahalanobis distance for classification\n* Example 21-1. Creating and training a decision tree\n* Example 22-1. Detecting and drawing faces\n\n### IMAGES:\n\n* box.png\n* box_in_scene.png\n* checkerboard9x6.png\n* example_16-01-imgA.png\n* example_16-01-imgB.png\n* faces.png\n* BlueCup.jpg\n* HandIndoorColor.jpg\n* HandOutdoorColor.jpg\n* HandOutdoorSunColor.jpg\n* adrian.jpg\n* faceScene.jpg\n* faceTemplate.jpg\n* fruits.jpg\n* stuff.jpg\n\n### MOVIES:\n\n* test.avi\n* tree.avi\n\n### CLASSIFIERS:\n\n* haarcascade_frontalcatface.xml           #Cat faces!\n* haarcascade_frontalcatface_extended.xml\n* haarcascade_frontalface_alt.xml\n\n### DIRECTORIES:\n\n* birdseye     -- where the images are of checkerboards on the floor\n* build        -- you will make and build things in this directory\n* calibration  -- checkerboard images to calibrate on\n* muchroom     -- machine learning database\n* shape_sample -- silhoette shapes to recognize\n* stereoData   -- left, right image pairs of checkboards to calibrate and view on\n\n\n## LINKS:\nClick the Download Zip button to the right to download example code.\n\nVisit the catalog page [here](http:*shop.oreilly.com/product/0636920044765.do).\n\nSee an error? Report it [here](http:*oreilly.com/catalog/errata.csp?isbn=0636920044765), or simply fork and send us a pull request\n"
 },
 {
  "repo": "skvark/opencv-python",
  "language": "Shell",
  "readme_contents": "[![Downloads](http://pepy.tech/badge/opencv-python)](http://pepy.tech/project/opencv-python)\n\n## OpenCV on Wheels\n\n**Unofficial** pre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA. \n\n### Installation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n    There are four different packages (see options 1, 2, 3 and 4 below) and you should **SELECT ONLY ONE OF THEM**. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (`cv2`). If you installed multiple different packages in the same environment, uninstall them all with ``pip uninstall`` and reinstall only one package.\n\n    **a.** Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)\n\n    - Option 1 - Main modules package: ``pip install opencv-python``\n    - Option 2 - Full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n    **b.** Packages for server (headless) environments (such as Docker, cloud environments etc.), no GUI library dependencies\n\n    These packages are smaller than the two other packages above because they do not contain any GUI functionality (not compiled with Qt / other GUI components). This means that the packages avoid a heavy dependency chain to X11 libraries and you will have for example smaller Docker images as a result. You should always use these packages if you do not use `cv2.imshow` et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI.\n\n    - Option 3 - Headless main modules package: ``pip install opencv-python-headless``\n    - Option 4 - Headless full package (contains both main modules and contrib/extra modules): ``pip install opencv-contrib-python-headless`` (check contrib/extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n4. Import the package:\n\n    ``import cv2``\n\n    All packages contain haarcascade files. ``cv2.data.haarcascades`` can be used as a shortcut to the data folder. For example:\n\n    ``cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")``\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Pip install fails with ``Could not find a version that satisfies the requirement ...``?**\n\nA: Most likely the issue is related to too old pip and can be fixed by running ``pip install --upgrade pip``. Note that the wheel (especially manylinux) format does not currently support properly ARM architecture so there are no packages for ARM based platforms in PyPI. However, ``opencv-python`` packages for Raspberry Pi can be found from https://www.piwheels.org/.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions).\n\nIf you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/skvark/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\n## Documentation for opencv-python\n\n[![AppVeyor CI test status (Windows)](https://img.shields.io/appveyor/ci/skvark/opencv-python.svg?maxAge=3600&label=Windows)](https://ci.appveyor.com/project/skvark/opencv-python)\n[![Travis CI test status (Linux and macOS)](https://img.shields.io/travis/com/skvark/opencv-python/master?label=Linux%20%26%20macOS)](https://travis-ci.com/github/skvark/opencv-python/)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\n### CI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example ``appveyor.yml`` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n   -  OpenCV is included as submodule and the version is updated\n      manually by maintainers when a new OpenCV release has been made\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV\n\n   -  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix \n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build. \n\nSee the next section for more info about manual builds outside the CI environment.\n\n### Manual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/skvark/opencv-python.git`\n2. ``cd opencv-python``\n    - you can use `git` to checkout some other version of OpenCV in the `opencv` and `opencv_contrib` submodules if needed\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n    - this might take anything from 5 minutes to over 2 hours depending on your hardware\n6. You'll have the wheel file in the `dist` folder and you can do with that whatever you wish\n    - Optional: on Linux use some of the `manylinux` images as a build hosts if maximum portability is needed and run `auditwheel` for the wheel after build\n    - Optional: on macOS use ``delocate`` (same as ``auditwheel`` but for macOS) for better portability\n\n#### Source distributions\n\nSince OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples: \n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies. \n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\n### Licensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [3-clause BSD License](https://github.com/opencv/opencv/blob/master/LICENSE).\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux and MacOS wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\n### Versioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\n### Releases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\n### Development builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\n### Manylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\n### Supported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.6\n- 3.7\n- 3.8\n- 3.9\n\n### Backward compatibility\n\nStarting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\n"
 },
 {
  "repo": "ivanseidel/Is-Now-Illegal",
  "language": "JavaScript",
  "readme_contents": "# Is Now Illegal!\nA NERD protest against Trump's Immigration ban \ud83d\udeab\nGo to [IsNowIllegal.com](http://isnowillegal.com) and type what you want to make illegal!\n\n## What's this?\nA webapp that gives you the Donald J. Trump power. Pretend you are Trump for a few seconds, make something illegal, share with friends and have fun!\n\n## Donate\n\nThe server costs are too high and we will shutdown very soon if we don't get enough donations. For real. \ud83d\ude14\nPlease click to [Donate via Patreon](https://www.patreon.com/isnowillegal) or contact us below.\n\n## Who made this?\n![](https://github.com/ivanseidel.png?size=100)\nIvan Seidel ([github](https://github.com/ivanseidel))\n\n![](https://github.com/brunolemos.png?size=100)\nBruno Lemos ([github](https://github.com/brunolemos), [twitter](https://twitter.com/brunolemos))\n\n![](https://github.com/joaopedrovbs.png?size=100)\nJo\u00e3o Pedro ([github](https://github.com/joaopedrovbs))\n\nSee full list of [contributors](https://github.com/ivanseidel/Is-Now-Illegal/graphs/contributors)."
 },
 {
  "repo": "andrewssobral/bgslibrary",
  "language": "C++",
  "readme_contents": "## BGSLibrary\nA Background Subtraction Library\n\n[![Release](https://img.shields.io/badge/Release-3.0.0-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![License: GPL v3](https://img.shields.io/badge/License-MIT-blue.svg)](http://www.gnu.org/licenses/gpl-3.0) [![Platform: Windows, Linux, OS X](https://img.shields.io/badge/Platform-Windows%2C%20Linux%2C%20OS%20X-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![OpenCV](https://img.shields.io/badge/OpenCV-2.4.x%2C%203.x%2C%204.x-blue.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Wrapper: Python, MATLAB](https://img.shields.io/badge/Wrapper-Java%2C%20Python%2C%20MATLAB-orange.svg)](https://github.com/andrewssobral/bgslibrary/wiki/Build-status) [![Algorithms](https://img.shields.io/badge/Algorithms-43-red.svg)](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n\n<p align=\"center\">\n<a href=\"https://youtu.be/_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgs_giphy2.gif\" border=\"0\" />\n</a>\n</p>\n\nLast page update: **06/08/2019**\n\nLibrary Version: **3.0.0** (see **[Build Status](https://github.com/andrewssobral/bgslibrary/wiki/Build-status)** and **[Release Notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)** for more info)\n\nThe **BGSLibrary** was developed early 2012 by [Andrews Sobral](http://andrewssobral.wixsite.com/home) to provide an easy-to-use C++ framework (wrappers for Python, Java and MATLAB are also available) for foreground-background separation in videos based on [OpenCV](http://www.opencv.org/). The bgslibrary is compatible with OpenCV 2.4.x, 3.x and 4.x, and compiles under Windows, Linux, and Mac OS X. Currently the library contains **43** algorithms. The source code is available under the [MIT license](https://opensource.org/licenses/MIT), the library is available free of charge to all users, academic and commercial.\n\n* [List of available algorithms](https://github.com/andrewssobral/bgslibrary/wiki/List-of-available-algorithms)\n* [Algorithms benchmark](https://github.com/andrewssobral/bgslibrary/wiki/Algorithms-benchmark)\n* [Which algorithms really matter?](https://github.com/andrewssobral/bgslibrary/wiki/Which-algorithms-really-matter%3F)\n* [Library architecture](https://github.com/andrewssobral/bgslibrary/wiki/Library-architecture)\n\n* Installation instructions\n\nYou can either install BGSLibrary via [pre-built binary package](https://github.com/andrewssobral/bgslibrary/releases) or build it from source via:\n\n`git clone --recursive https://github.com/andrewssobral/bgslibrary.git`\n\n* * [Windows installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions---Windows)\n\n* * [Ubuntu / OS X installation](https://github.com/andrewssobral/bgslibrary/wiki/Installation-instructions-Ubuntu-or-OSX)\n\nSupported Compilers are:\n\n    GCC 4.8 and above\n    Clang 3.4 and above\n    MSVC 2015, 2017, 2019\n\nOther compilers might work, but are not officially supported.\nThe bgslibrary requires some features from the ISO C++ 2014 standard.\n\n* Graphical User Interface:\n\n*  * [C++ QT](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-QT) ***(Official)***\n*  * [C++ MFC](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-MFC) ***(Deprecated)***\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Graphical-User-Interface:-Java) ***(Obsolete)***\n\n* Wrappers:\n\n*  * [Python](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Python)\n*  * [MATLAB](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-MATLAB)\n*  * [Java](https://github.com/andrewssobral/bgslibrary/wiki/Wrapper:-Java)\n\n* [Docker images](https://github.com/andrewssobral/bgslibrary/wiki/Docker-images)\n* [How to integrate BGSLibrary in your own CPP code](https://github.com/andrewssobral/bgslibrary/wiki/How-to-integrate-BGSLibrary-in-your-own-CPP-code)\n* [How to contribute](https://github.com/andrewssobral/bgslibrary/wiki/How-to-contribute)\n* [List of collaborators](https://github.com/andrewssobral/bgslibrary/wiki/List-of-collaborators)\n* [Release notes](https://github.com/andrewssobral/bgslibrary/wiki/Release-notes)\n\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/andrewssobral/bgslibrary.svg)](https://starchart.cc/andrewssobral/bgslibrary)\n\n\nCitation\n--------\n\nIf you use this library for your publications, please cite it as:\n```\n@inproceedings{bgslibrary,\nauthor    = {Sobral, Andrews},\ntitle     = {{BGSLibrary}: An OpenCV C++ Background Subtraction Library},\nbooktitle = {IX Workshop de Vis\u00e3o Computacional (WVC'2013)},\naddress   = {Rio de Janeiro, Brazil},\nyear      = {2013},\nmonth     = {Jun},\nurl       = {https://github.com/andrewssobral/bgslibrary}\n}\n```\nA chapter about the BGSLibrary has been published in the handbook on [Background Modeling and Foreground Detection for Video Surveillance](https://sites.google.com/site/backgroundmodeling/).\n```\n@incollection{bgslibrarychapter,\nauthor    = {Sobral, Andrews and Bouwmans, Thierry},\ntitle     = {BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation},\nbooktitle = {Background Modeling and Foreground Detection for Video Surveillance},\npublisher = {CRC Press, Taylor and Francis Group.}\nyear      = {2014},\n}\n```\n\n\nDownload PDF:\n* Sobral, Andrews. BGSLibrary: An OpenCV C++ Background Subtraction Library. IX Workshop de Vis\u00e3o Computacional (WVC'2013), Rio de Janeiro, Brazil, Jun. 2013. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in brazilian-portuguese containing an english abstract).\n\n* Sobral, Andrews; Bouwmans, Thierry. \"BGS Library: A Library Framework for Algorithm\u2019s Evaluation in Foreground/Background Segmentation\". Chapter on the handbook \"Background Modeling and Foreground Detection for Video Surveillance\", CRC Press, Taylor and Francis Group, 2014. ([PDF](http://www.researchgate.net/publication/257424214_BGSLibrary_An_OpenCV_C_Background_Subtraction_Library) in english).\n\n\nSome references\n---------------\n\nSome algorithms of the BGSLibrary were used successfully in the following papers: \n\n* (2014) Sobral, Andrews; Vacavant, Antoine. A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos. Computer Vision and Image Understanding (CVIU), 2014. ([Online](http://dx.doi.org/10.1016/j.cviu.2013.12.005)) ([PDF](http://www.researchgate.net/publication/259340906_A_comprehensive_review_of_background_subtraction_algorithms_evaluated_with_synthetic_and_real_videos))\n\n* (2013) Sobral, Andrews; Oliveira, Luciano; Schnitman, Leizer; Souza, Felippe. (**Best Paper Award**) Highway Traffic Congestion Classification Using Holistic Properties. In International Conference on Signal Processing, Pattern Recognition and Applications (SPPRA'2013), Innsbruck, Austria, Feb 2013. ([Online](http://dx.doi.org/10.2316/P.2013.798-105)) ([PDF](http://www.researchgate.net/publication/233427564_HIGHWAY_TRAFFIC_CONGESTION_CLASSIFICATION_USING_HOLISTIC_PROPERTIES))\n\n\nVideos\n------\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=_UbERwuQ0OU\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_qt_gui_video.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=Ccqa9KBO9_U\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/bgslibrary/master/docs/images/bgslibrary_youtube.png\" width=\"600\" border=\"0\" />\n</a>\n</p>\n"
 },
 {
  "repo": "nagadomi/lbpcascade_animeface",
  "language": null,
  "readme_contents": "# lbpcascade_animeface\n\nThe face detector for anime/manga using OpenCV.\n\nOriginal release since 2011 at [OpenCV\u306b\u3088\u308b\u30a2\u30cb\u30e1\u9854\u691c\u51fa\u306a\u3089lbpcascade_animeface.xml](http://ultraist.hatenablog.com/entry/20110718/1310965532) (in Japanese)\n\n## Usage\n\nDownload and place the cascade file into your project directory.\n\n    wget https://raw.githubusercontent.com/nagadomi/lbpcascade_animeface/master/lbpcascade_animeface.xml\n\n### Python Example\n\n```python\nimport cv2\nimport sys\nimport os.path\n\ndef detect(filename, cascade_file = \"../lbpcascade_animeface.xml\"):\n    if not os.path.isfile(cascade_file):\n        raise RuntimeError(\"%s: not found\" % cascade_file)\n\n    cascade = cv2.CascadeClassifier(cascade_file)\n    image = cv2.imread(filename, cv2.IMREAD_COLOR)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.equalizeHist(gray)\n    \n    faces = cascade.detectMultiScale(gray,\n                                     # detector options\n                                     scaleFactor = 1.1,\n                                     minNeighbors = 5,\n                                     minSize = (24, 24))\n    for (x, y, w, h) in faces:\n        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n    cv2.imshow(\"AnimeFaceDetect\", image)\n    cv2.waitKey(0)\n    cv2.imwrite(\"out.png\", image)\n\nif len(sys.argv) != 2:\n    sys.stderr.write(\"usage: detect.py <filename>\\n\")\n    sys.exit(-1)\n    \ndetect(sys.argv[1])\n```\nRun\n\n    python detect.py imas.jpg\n\n![result](https://user-images.githubusercontent.com/287255/43184241-ed3f1af8-9022-11e8-8800-468b002c73d9.png)\n\n## Note\nI am providing similar project at https://github.com/nagadomi/animeface-2009. animeface-2009 is my original work that was made before libcascade_animeface. The detection accuracy is higher than this project. However, installation of that is a bit complicated. Also I am providing a face cropping script using animeface-2009.\n"
 },
 {
  "repo": "kongqw/OpenCVForAndroid",
  "language": "Java",
  "readme_contents": "# OpenCV 3.2.0\n\n\u8fd0\u884c\u524d\u8bf7\u5148\u5b89\u88c5[OpenCV Manager(\u5fc5\u987b)](https://github.com/kongqw/FaceDetectLibrary/tree/opencv3.2.0/OpenCVManager)\u3002\n\n\u672c\u793a\u4f8b\u57fa\u4e8eOpenCV 3.2.0 \u7248\u672c\u5728Android\u5e73\u53f0\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u548c\u76ee\u6807\u8ffd\u8e2a\n\n## \u76ee\u6807\u68c0\u6d4b\n\n- \u4eba\u8138\u68c0\u6d4b\n- \u773c\u775b\u68c0\u6d4b\n- \u5fae\u7b11\u68c0\u6d4b\n- \u4e0a\u534a\u8eab\u68c0\u6d4b\n- \u4e0b\u534a\u8eab\u68c0\u6d4b\n- \u5168\u8eab\u68c0\u6d4b\n\n## \u76ee\u6807\u8ffd\u8e2a\n\n- CamShift\u7b97\u6cd5\u5b9e\u73b0\u76ee\u6807\u8ffd\u8e2a\n\n### \u6548\u679c\u56fe\n\n\u76ee\u6807\u68c0\u6d4b\n\n![\u76ee\u6807\u68c0\u6d4b](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectDetecting.gif)\n\n\u76ee\u6807\u8ffd\u8e2a\n\n![\u76ee\u6807\u8ffd\u8e2a](https://github.com/kongqw/OpenCVForAndroid/blob/opencv3.2.0/gif/ObjectTracking.gif)\n\n## \u4eba\u8138\u8bc6\u522b\n\n\u4eba\u8138\u8bc6\u522b\uff08\u5bf9\u6bd4\uff09\u8bf7\u5207\u6362\u5230[master](https://github.com/kongqw/FaceDetectLibrary/tree/master)\u5206\u652f\uff0c\u57fa\u4e8eOpenCV 2.4.11\u3002\n"
 },
 {
  "repo": "opentrack/opentrack",
  "language": "C++",
  "readme_contents": "## Intro\n\n[<img src=\"https://ci.appveyor.com/api/projects/status/n0j9h38jnif5qbe9/branch/unstable?svg=true\"/>](https://ci.appveyor.com/project/sthalik/opentrack/branch/unstable)\n\nopentrack project home is located at <<http://github.com/opentrack/opentrack>>.\n\nFor the latest **downloads** visit <<https://github.com/opentrack/opentrack/releases>> Download an `.exe` installer or a `.7z` archive. Currently installers and portable versions for Windows are available for each release. It supports [USB stick truly \"portable\" installations](https://github.com/opentrack/opentrack/wiki/portable-mode-for-USB-sticks)\n\nPlease first refer to <<https://github.com/opentrack/opentrack/wiki>>\nfor [new user guide](https://github.com/opentrack/opentrack/wiki/Quick-Start-Guide-(WIP)), [frequent answers](https://github.com/opentrack/opentrack/wiki/common-issues), specific tracker/filter\ndocumentation. See also the [gameplay video](https://www.youtube.com/watch?v=XI73ul_FnBI) with opentrack set up.\n\n## Looking for railway planning software?\n\n**Railway planning software** <<http://opentrack.ch>> had the name `opentrack` first. Apologies for the long-standing naming conflict.\n\n## Usage\n\n`opentrack` is an application dedicated to tracking user's head\nmovements and relaying the information to games and flight simulation\nsoftware.\n\n`opentrack` allows for output shaping, filtering, and operating with many input and output devices and protocols; the codebase runs Microsoft Windows, Apple OSX (currently unmaintained), and GNU/Linux.\n\nDon't be afraid to submit an **issue/feature request** if you have any problems! We're a friendly bunch.\n\n## Tracking input\n\n- PointTracker by Patrick Ruoff, freetrack-like light sources\n- Oculus Rift DK1, DK2, CV, and legacy/knockoff versions (Windows only)\n- Paper [marker support](https://github.com/opentrack/opentrack/wiki/Aruco-tracker)\n  via the ArUco library <<https://github.com/opentrack/aruco>>\n- Razer Hydra\n- Relaying via UDP from a different computer\n- Relaying UDP via FreePIE-specific Android app\n- Joystick analog axes (Windows)\n- Windows Phone [tracker](https://github.com/ZanderAdam/OpenTrack.WindowsPhone/wiki) over opentrack UDP protocol\n- Arduino with custom firmware\n- Intel RealSense 3D cameras (Windows)\n- BBC micro:bit, LEGO, sensortag support via Smalltalk<sup>[(1)](https://en.wikipedia.org/wiki/Smalltalk)[(2)](https://en.wikipedia.org/wiki/Alan_Kay)</sup>\n  [S2Bot](http://www.picaxe.com/Teaching/Other-Software/Scratch-Helper-Apps/)\n- Wiimote (Windows)\n\n## Output protocols\n\n- SimConnect for newer Microsoft Flight Simulator (Windows)\n- freetrack implementation (Windows)\n- Relaying UDP to another computer\n- Virtual joystick output (Windows, Linux, OSX)\n- Wine freetrack glue protocol (Linux, OSX)\n- X-Plane plugin (Linux)\n- Tablet-like mouse output (Windows)\n- FlightGear\n- FSUIPC for Microsoft Flight Simulator 2002/2004 (Windows)\n- SteamVR through a bridge (Windows; see <<https://github.com/r57zone/OpenVR-OpenTrack>> by @r57zone)\n\n## Credits, in chronological order\n\n- Stanis\u0142aw Halik (maintainer)\n- Wim Vriend -- author of [FaceTrackNoIR](http://facetracknoir.sourceforge.net/) that served as the initial codebase for `opentrack`. While the  code was almost entirely rewritten, we still hold on to many of `FaceTrackNoIR`'s ideas.\n- Chris Thompson (aka mm0zct, Rift and Razer Hydra author and maintainer)\n- Patrick Ruoff (PT tracker author)\n- Xavier Hallade (Intel RealSense tracker author and maintainer)\n- furax49 (hatire tracker author)\n- Michael Welter (contributor)\n- Alexander Orokhovatskiy (Russian translation; profile repository maintenance; providing hardware; translating reports from the Russian community)\n- Attila Csipa (Micro:Bit author)\n- Eike \"e4z9\" (OSX joystick output driver)\n- Wei Shuai (Wiimote tracker)\n- St\u00e9phane Lenclud (Kinect Face Tracker, Easy Tracker)\n\n## Thanks\n\n- uglyDwarf (high CON)\n- Andrzej Czarnowski (FreePIE tracker and\n  [Google Cardboard](https://github.com/opentrack/opentrack/wiki/VR-HMD-goggles-setup-----google-cardboard,-colorcross,-opendive)\n  assistance, testing)\n- Wim Vriend (original codebase author and maintainer)\n- Ryan Spicer (OSX tester, contributor)\n- Ries van Twisk (OSX tester, OSX Build Fixes, contributor)\n- Donovan Baarda (filtering/control theory expert)\n- Mathijs Groothuis (@MathijsG, dozens of bugs and other issues reported; NL translation)\n- The Russian community from the [IL-2 Sturmovik forums](https://forum.il2sturmovik.ru/) (reporting bugs, requesting important features)\n- OpenCV authors and maintainers <<https://github.com/opencv/opencv/>>.\n\n## Contributing\n\nCode, translations, \n\nPlease see [basic rules for contributing](https://github.com/opentrack/opentrack/blob/unstable/CONTRIBUTING.md). There's also a guide for [working with core code](https://github.com/opentrack/opentrack/wiki/Hacking-opentrack). For writing input and output modules you don't need this guide except for \n\n## License and warranty\n\nAlmost all code is licensed under the [ISC license](https://en.wikipedia.org/wiki/ISC_license). There are very few proprietary dependencies. There is no copyleft code. See individual files for licensing and authorship information.\n\nSee [WARRANTY.txt](WARRANTY.txt) for applying warranty terms (that is, disclaiming possible pre-existing warranty) that are in force unless the software author specifies their own warranty terms. In short, we disclaim all possible warranty and aren't responsible for any possible damage or losses.\n\nThe code is held to a high-quality standard and written with utmost care; consider this a promise without legal value. Despite doing the best we can not to injure users' equipment, software developers don't want to be dragged to courts for imagined or real issues. Disclaiming warranty is a standard practice in the field, even for expensive software like operating systems.\n\n## Building opentrack from source\n\nOn Windows, use either mingw-w64 or MS Visual Studio 2015 Update 3/newer. On other platforms use GNU or LLVM. Refer to [Visual C++ 2015 build instructions](https://github.com/opentrack/opentrack/wiki/Building-under-MS-Visual-C---2017-and-later).\n"
 },
 {
  "repo": "tebelorg/RPA-Python",
  "language": "Python",
  "readme_contents": "# RPA for Python :snake:\n\n[**Use Cases**](#use-cases)&ensp;|&ensp;[**API Reference**](#api-reference)&ensp;|&ensp;[**About & Credits**](#about--credits)&ensp;|&ensp;[**PyCon Video**](https://www.youtube.com/watch?v=F2aQKWx_EAE)&ensp;|&ensp;[**v1.27**](https://github.com/tebelorg/RPA-Python/releases)\n\n>_This tool was previously known as TagUI for Python. [More details](https://github.com/tebelorg/RPA-Python/issues/100) on the name change, which is backward compatible so existing scripts written with `import tagui as t` and `t.function()` will continue to work._\n\n![RPA for Python demo in Jupyter notebook](https://raw.githubusercontent.com/tebelorg/Tump/master/tagui_python.gif)\n\nTo install this Python package for RPA (robotic process automation) -\n```\npip install rpa\n```\n\nTo use it in Jupyter notebook, Python script or interactive shell -\n```python\nimport rpa as r\n```\n\nNotes on different operating systems and optional visual automation mode -\n- :rainbow_flag: **Windows -** if visual automation is cranky, try setting your display zoom level to recommended % or 100%\n- :apple: **macOS -** Catalina update introduces tighter app security, see solutions for [PhantomJS](https://github.com/tebelorg/RPA-Python/issues/79) and [Java popups](https://github.com/tebelorg/RPA-Python/issues/78)\n- :penguin: **Linux -** visual automation mode requires special setup on Linux, see how to [install OpenCV and Tesseract](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html)\n\n# Use Cases\n\nRPA for Python's simple and powerful API makes robotic process automation fun! You can use it to quickly automate repetitive time-consuming tasks, whether the tasks involve websites, desktop applications, or the command line.\n\n#### WEB AUTOMATION&ensp;:spider_web:\n```python\nr.init()\nr.url('https://www.google.com')\nr.type('//*[@name=\"q\"]', 'decentralization[enter]')\nprint(r.read('result-stats'))\nr.snap('page', 'results.png')\nr.close()\n```\n\n#### VISUAL AUTOMATION&ensp;:see_no_evil:\n```python\nr.init(visual_automation = True)\nr.dclick('outlook_icon.png')\nr.click('new_mail.png')\n...\nr.type('message_box.png', 'message')\nr.click('send_button.png')\nr.close()\n```\n\n#### OCR AUTOMATION&ensp;\ud83e\uddff\n```python\nr.init(visual_automation = True, chrome_browser = False)\nprint(r.read('pdf_window.png'))\nprint(r.read('image_preview.png'))\nr.hover('anchor_element.png')\nprint(r.read(r.mouse_x(), r.mouse_y(), r.mouse_x() + 400, r.mouse_y() + 200))\nr.close()\n```\n\n#### KEYBOARD AUTOMATION&ensp;:musical_keyboard:\n```python\nr.init(visual_automation = True, chrome_browser = False)\nr.keyboard('[cmd][space]')\nr.keyboard('safari[enter]')\nr.keyboard('[cmd]t')\nr.keyboard('joker[enter]')\nr.wait(2.5)\nr.snap('page.png', 'results.png')\nr.close()\n```\n\n#### MOUSE AUTOMATION&ensp;:mouse:\n```python\nr.init(visual_automation = True)\nr.type(600, 300, 'open source')\nr.click(900, 300)\nr.snap('page.bmp', 'results.bmp')\nr.hover('button_to_drag.bmp')\nr.mouse('down')\nr.hover(r.mouse_x() + 300, r.mouse_y())\nr.mouse('up')\nr.close()\n```\n\n# API Reference\n\nCheck out [sample Python script](https://github.com/tebelorg/RPA-Python/blob/master/sample.py), [RPA Challenge solution](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610518196), and [RedMart groceries example](https://github.com/tebelorg/RPA-Python/issues/24). To automate Chrome browser invisibly, see this [simple hack](https://github.com/tebelorg/RPA-Python/issues/133#issuecomment-634113838). To run 20-30X faster, without normal UI interaction delays, [see this advanced hack](https://github.com/tebelorg/RPA-Python/issues/120#issuecomment-610532082).\n\n#### ELEMENT IDENTIFIERS\nAn element identifier helps to tell RPA for Python exactly which element on the user interface you want to interact with. For example, //\\*[@id=\"email\"] is an XPath pointing to the webpage element having the id attribute \"email\".\n\n- :globe_with_meridians: For web automation, the web element identifier can be XPath selector, CSS selector, or the following attributes - id, name, class, title, aria-label, text(), href, in decreasing order of priority. Recommend writing XPath manually or simply using attributes. There is automatic waiting for an element to appear before timeout happens, and error is returned that the element cannot be found. To change the default timeout of 10 seconds, use timeout() function.\n\n- :camera_flash: An element identifier can also be a .png or .bmp image snapshot representing the UI element (can be on desktop applications, terminal window or web browser). If the image file specified does not exist, OCR will be used to search for that text on the screen to act on the UI element containing the text, eg r.click('Submit Form.png'). Transparency (0% opacity) is supported in .png images. x, y coordinates of elements on the screen can be used as well.\n\n- :page_facing_up: A further image identifier example is an image of the window (PDF viewer, MS Word, textbox etc) with the center content of the image set as transparent. This allows using read() and snap() to perform OCR and save snapshots of application windows, containers, frames, textboxes with varying content. Also for read() and snap(), x1, y1, x2, y2 coordinates pair can be used to define the region of interest on the screen to perform OCR or capture snapshot.\n\n#### CORE FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\ninit()|visual_automation = False, chrome_browser = True|start TagUI, auto-setup on first run\nclose()||close TagUI, Chrome browser, SikuliX\npack()||for deploying package without internet\nupdate()||for updating package without internet\n\n>_to print and log debug info to rpa_python.log use debug(True), to switch off use debug(False)_\n\n#### BASIC FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nurl()|webpage_url (no parameter to return current URL)|go to web URL\nclick()|element_identifier (or x, y using visual automation)| left-click on element\nrclick()|element_identifier (or x, y using visual automation)|right-click on element\ndclick()|element_identifier (or x, y using visual automation)|double-click on element\nhover()|element_identifier (or x, y using visual automation)|move mouse to element\ntype()|element_identifier (or x, y), text_to_type ('[enter]', '[clear]')|enter text at element\nselect()|element_identifier (or x, y), option_value (or x, y)|choose dropdown option\nread()|element_identifier (page = web page) (or x1, y1, x2, y2)|fetch & return element text\nsnap()|element_identifier (page = web page), filename_to_save|save screenshot to file\nload()|filename_to_load|load & return file content\ndump()|text_to_dump, filename_to_save|save text to file\nwrite()|text_to_write, filename_to_save|append text to file\nask()|text_to_prompt|ask & return user input\n\n>_to wait for an element to appear until timeout() value, use hover(). to drag-and-drop, [you can do this](https://github.com/tebelorg/RPA-Python/issues/58#issuecomment-570778431)_\n\n#### PRO FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nkeyboard()|keys_and_modifiers (using visual automation)|send keystrokes to screen\nmouse()|'down' or 'up' (using visual automation)|send mouse event to screen\nwait()|delay_in_seconds (default 5 seconds)|explicitly wait for some time\ntable()|element_identifier (XPath only), filename_to_save|save basic HTML table to CSV\nupload()|element_identifier (CSS only), filename_to_upload|upload file to web element\ndownload()|download_url, filename_to_save(optional)|download from URL to file\nunzip()|file_to_unzip, unzip_location (optional)|unzip zip file to specified location\nframe()|main_frame id or name, sub_frame (optional)|set web frame, frame() to reset\npopup()|string_in_url (no parameter to reset to main page)|set context to web popup tab\nrun()|command_to_run (use ; between commands)|run OS command & return output\ndom()|statement_to_run (JS code to run in browser)|run code in DOM & return output\nvision()|command_to_run (Python code for SikuliX)|run custom SikuliX commands\ntimeout()|timeout_in_seconds (blank returns current timeout)|change wait timeout (default 10s)\n\nkeyboard() modifiers and special keys -\n>_[shift] [ctrl] [alt] [cmd] [win] [meta] [clear] [space] [enter] [backspace] [tab] [esc] [up] [down] [left] [right] [pageup] [pagedown] [delete] [home] [end] [insert] [f1] .. [f15] [printscreen] [scrolllock] [pause] [capslock] [numlock]_\n\n#### HELPER FUNCTIONS\nFunction|Parameters|Purpose\n:-------|:---------|:------\nexist()|element_identifier|return True or False if element exists before timeout\npresent()|element_identifier|return True or False if element is present now\ncount()|element_identifier|return number of web elements as integer\nclipboard()|text_to_put or no parameter|put text or return clipboard text as string\nmouse_xy()||return '(x,y)' coordinates of mouse as string\nmouse_x()||return x coordinate of mouse as integer\nmouse_y()||return y coordinate of mouse as integer\ntitle()||return page title of current web page as string\ntext()||return text content of current web page as string\ntimer()||return time elapsed in sec between calls as float\n\n>_to type large amount of text quickly, use clipboard() and keyboard() to paste instead of type()_\n\n# About & Credits\n\nTagUI is the leading open-source RPA software :robot: with thousands of active users. It was created in 2016-2017 when I left DBS Bank as a test automation engineer, to embark on a one-year sabbatical to Eastern Europe. Most of its code base was written in Novi Sad Serbia. My wife and I also spent a couple of months in Budapest Hungary, as well as Chiang Mai Thailand for visa runs. In 2018, I joined AI Singapore to continue development of TagUI.\n\nOver the past few months I take on a daddy role full-time, taking care of my newborn baby girl and wife :cowboy_hat_face:\ud83e\udd31. In between the nannying, I use my time pockets to create this Python package that's built on TagUI. I hope that RPA for Python and ML frameworks would be good friends, and `pip install rpa` would make life easier for Python users.\n\nLastly, at only ~1k lines of code, it would make my day to see developers of other languages port this project over to their favourite programming language. See ample comments in this [single-file package](https://github.com/tebelorg/RPA-Python/blob/master/tagui.py), and its intuitive architecture -\n\n![RPA for Python architecture](https://raw.githubusercontent.com/tebelorg/Tump/master/TagUI-Python/architecture.png)\n\nI would like to credit and express my appreciation below :bowing_man:, and you are invited to [connect on LinkedIn](https://www.linkedin.com/in/kensoh) -\n\n- [TagUI](https://github.com/kelaberetiv/TagUI/tree/pre_v6) - AI Singapore from Singapore / [@aisingapore](https://www.aisingapore.org)\n- [SikuliX](https://github.com/RaiMan/SikuliX1) - Raimund Hocke from Germany / [@RaiMan](https://github.com/RaiMan)\n- [CasperJS](https://github.com/casperjs/casperjs) - Nicolas Perriault from France / [@n1k0](https://github.com/n1k0)\n- [PhantomJS](https://github.com/ariya/phantomjs) - Ariya Hidayat from Indonesia / [@ariya](https://github.com/ariya)\n- [SlimerJS](https://github.com/laurentj/slimerjs) - Laurent Jouanneau from France / [@laurentj](https://github.com/laurentj)\n\n# License\nRPA for Python is open-source software released under Apache 2.0 license\n"
 },
 {
  "repo": "changwookjun/StudyBook",
  "language": null,
  "readme_contents": "# Study E-Book(ComputerVision DeepLearning MachineLearning Math NLP Python ReinforcementLearning)\n\nContents  \n* [Computer Vision Books](https://github.com/changwookjun/StudyBook/tree/master/ComputerVisionBooks)   \n  + [Machine Learning for OpenCV.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Machine%20Learning%20for%20OpenCV.pdf)   \n  + [Computer Vision- Algorithms and Applications.pdf](https://github.com/changwookjun/StudyBook/blob/master/ComputerVisionBooks/Computer%20Vision-%20Algorithms%20and%20Applications.pdf)   \n* [Deep Learning Books](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks)   \n  + [Deep Learning - Josh Patterson & Adam Gibson.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20-%20Josh%20Patterson%20%26%20Adam%20Gibson.pdf)   \n  + [Deep Learning with Python A Hands-on Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep%20Learning%20with%20Python%20A%20Hands-on%20Introduction.pdf)   \n  + [Fundamentals of Deep Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Fundamentals%20of%20Deep%20Learning.pdf)   \n  + [Introduction to Deep Learning Using R.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Introduction%20to%20Deep%20Learning%20Using%20R.pdf)   \n  + [Learning TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Learning%20TensorFlow.pdf)   \n  + [deeplearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning.pdf)   \n  + [deeplearningbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook.pdf)   \n  + [deeplearningbook_bookmarked.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearningbook_bookmarked.pdf)   \n  + [oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/oreilly-hands-on-machine-learning-with-scikit-learn-and-tensorflow-1491962291.pdf)   \n  + [CS 20_Tensorflow for Deep Learning Research](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research)     \n      - [01 _ Lecture slide _ Overview of Tensorflow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/01%20_%20Lecture%20slide%20_%20Overview%20of%20Tensorflow.pdf)     \n      - [02_Lecture slide_TensorFlow Operations.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/02_Lecture%20slide_TensorFlow%20Operations.pdf)     \n      - [03 _ Lecture slide _ Basic Models in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/03%20_%20Lecture%20slide%20_%20Basic%20Models%20in%20TensorFlow.pdf)     \n      - [04 Eager Execution + word2vec.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/04%20Eager%20Execution%20%2B%20word2vec.pdf)     \n      - [05_Slide_Managing your experiment.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/05_Slide_Managing%20your%20experiment.pdf)     \n      - [06_Introduction to Computer Vision and convolutional network.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/06_Introduction%20to%20Computer%20Vision%20and%20convolutional%20network.pdf)     \n      - [07 _ Covnets in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/07%20_%20Covnets%20in%20TensorFlow.pdf)     \n      - [08_Style transfer.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/08_Style%20transfer.pdf)     \n      - [10_Lecture_Slides_VAE in TensorFlow.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/10_Lecture_Slides_VAE%20in%20TensorFlow.pdf)     \n      - [11 _ Slides _ Introduction to RNNs.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/11%20_%20Slides%20_%20Introduction%20to%20RNNs.pdf)     \n      - [12_Slides_Machine Translation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/12_Slides_Machine%20Translation.pdf)    \n      - [14_Slides_A TensorFlow Chatbot.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/14_Slides_A%20TensorFlow%20Chatbot.pdf)    \n      - [16_Slides_Tensor2Tensor.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/16_Slides_Tensor2Tensor.pdf)    \n      - [CS20_intro_to_RL.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/CS20_intro_to_RL.pdf)   \n      - [march9guestlecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/CS%2020_Tensorflow%20for%20Deep%20Learning%20Research/march9guestlecture.pdf)    \n  + [DeepLearning_chapter-wise-pdf](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf)     \n      - [table-of-contents.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B1%5Dtable-of-contents.pdf)  \n      - [acknowledgements.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B2%5Dacknowledgements.pdf)  \n      - [notation.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B3%5Dnotation.pdf)  \n      - [chapter-1-introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B4%5Dchapter-1-introduction.pdf)  \n      - [part-1-basics.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B5%5Dpart-1-basics.pdf)  \n      - [part-1-chapter-2.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B6%5Dpart-1-chapter-2.pdf)  \n      - [part-1-chapter-3.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B7%5Dpart-1-chapter-3.pdf)  \n      - [part-1-chapter-4.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B8%5Dpart-1-chapter-4.pdf)  \n      - [part-1-chapter-5.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B9%5Dpart-1-chapter-5.pdf)  \n      - [part-2-deep-network-modern-practices.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B10%5Dpart-2-deep-network-modern-practices.pdf)  \n      - [part-2-chapter-6.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B11%5Dpart-2-chapter-6.pdf)  \n      - [part-2-chapter-7.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B12%5Dpart-2-chapter-7.pdf)  \n      - [part-2-chapter-8.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B13%5Dpart-2-chapter-8.pdf)  \n      - [part-2-chapter-9.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B14%5Dpart-2-chapter-9.pdf)        \n      - [part-2-chapter-10.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B15%5Dpart-2-chapter-10.pdf)        \n      - [part-2-chapter-11.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B16%5Dpart-2-chapter-11.pdf)        \n      - [part-2-chapter-12.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B17%5Dpart-2-chapter-12.pdf)  \n      - [part-3-deep-learning-research.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B18%5Dpart-3-deep-learning-research.pdf) \n      - [part-3-chapter-13.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B19%5Dpart-3-chapter-13.pdf) \n      - [part-3-chapter-14.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B20%5Dpart-3-chapter-14.pdf) \n      - [part-3-chapter-15.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B21%5Dpart-3-chapter-15.pdf) \n      - [part-3-chapter-16.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B22%5Dpart-3-chapter-16.pdf) \n      - [part-3-chapter-17.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B23%5Dpart-3-chapter-17.pdf) \n      - [part-3-chapter-18.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B24%5Dpart-3-chapter-18.pdf) \n      - [part-3-chapter-19.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B25%5Dpart-3-chapter-19.pdf) \n      - [part-3-chapter-20.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B26%5Dpart-3-chapter-20.pdf) \n      - [bibliography.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B27%5Dbibliography.pdf) \n      - [index.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/DeepLearning_chapter-wise-pdf/%5B28%5Dindex.pdf) \n  + [d2l-en.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/d2l-en.pdf) \n  + [Dive into DeepLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Dive_into_Deep_Learning.pdf)   \n  + [ee559 Deep learning](https://github.com/changwookjun/StudyBook/tree/master/DeepLearningBooks/ee559-Deeplearning)     \n  + [Hands-on-Machine-Learning-with-Scikit-2E.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Hands-on-Machine-Learning-with-Scikit-2E.pdf)  \n  + [deeplearning_2019_spring.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/deeplearning_2019_spring.pdf)    \n  + [Deep-Learning-with-PyTorch.pdf](https://github.com/changwookjun/StudyBook/blob/master/DeepLearningBooks/Deep-Learning-with-PyTorch.pdf)  \n\n* [Machine Learning Books](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks)      \n  + [30_03_atelierdatamining.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/30_03_atelierdatamining.pdf)  \n  + [Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)  \n  + [Building Machine Learning Systems with Python, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Building%20Machine%20Learning%20Systems%20with%20Python%2C%202nd%20Edition.pdf)   \n  + [MATLAB Machine Learning by Michael Paluszek.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/MATLAB%20Machine%20Learning%20by%20Michael%20Paluszek.pdf)  \n  + [Machine Learning in Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine%20Learning%20in%20Python.pdf)  \n  + [Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Machine_Learning.pdf)  \n  + [Mastering Feature Engineering.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Feature%20Engineering.pdf)  \n  + [Mastering Machine Learning with scikit-learn, 2nd Edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Mastering%20Machine%20Learning%20with%20scikit-learn%2C%202nd%20Edition.pdf)  \n  + [NG_MLY.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Ng_MLY.pdf) \n  + [Practical Machine Learning A New Look at Anomaly Detection.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Practical%20Machine%20Learning%20A%20New%20Look%20at%20Anomaly%20Detection.pdf) \n  + [Practical Machine Learning with H2O.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Data%20Analytics.pdf) \n  + [Python Real World Machine Learning - Prateek Joshi.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Python%20Real%20World%20Machine%20Learning%20-%20Prateek%20Joshi.pdf) \n  + [Gaussian Processes for Machine Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/RW.pdf) \n  + [The Elements of Statistical Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/ESLII_print12.pdf) \n  + [Foundations of Data Science.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science.pdf) \n  + [cs229-cheatsheet](https://github.com/changwookjun/StudyBook/tree/master/MachineLearningBooks/cs229-cheatsheet) \n  + [Automatic_Machine_Learning.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Automatic_Machine_Learning.pdf) \n  + [DataScienceHandbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/DataScienceHandbook.pdf) \n  + [Python Data Science Handbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/pythondatasciencehandbook.pdf) \n  + [Foundations of Data Science(Microsoft).pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Foundations%20of%20Data%20Science(Microsoft).pdf) \n  + [Bayesian_Data_Analysis_Third_edition.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Bayesian_Data_Analysis_Third_edition.pdf)  \n  + [Joseph K. Blitzstein, Jessica Hwang-Introduction to Probability.pdf](https://github.com/changwookjun/StudyBook/blob/master/MachineLearningBooks/Joseph%20K.%20Blitzstein%2C%20Jessica%20Hwang-Introduction%20to%20Probability.pdf)   \n  \n* [Math Books](https://github.com/changwookjun/StudyBook/tree/master/MathBooks)     \n  + [MIT18_657F15_LecNote.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/MIT18_657F15_LecNote.pdf) \n  + [Mathematics for Machine Learnin.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Mathematics%20for%20Machine%20Learnin.pdf) \n  + [mathandcomp.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mathandcomp.pdf) \n  + [Introduction to Applied Linear Algebra.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/Introduction%20to%20Applied%20Linear%20Algebra.pdf) \n  + [matrixcookbook.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/matrixcookbook.pdf) \n  + [mml-book](https://github.com/changwookjun/StudyBook/tree/master/MathBooks/mml-book)   \n  + [MATHEMATICS FOR MACHINE LEARNING.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/mml-book.pdf)  \n  + [LINEAR ALGEBRA.pdf](https://github.com/changwookjun/StudyBook/blob/master/MathBooks/LINEAR%20ALGEBRA.pdf)  \n\n* [NLP Books](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks)     \n  + [Applied Text Analysis with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Applied%20Text%20Analysis%20with%20Python.pdf) \n  + [Natural Language Processing with Python.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Natural%20Language%20Processing%20with%20Python.pdf) \n  + [Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from your Data.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Text%20Analytics%20with%20Python%20A%20Practical%20Real-World%20Approach%20to%20Gaining%20Actionable%20Insights%20from%20your%20Data.pdf) \n  + [The Text Mining HandBook.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/The%20Text%20Mining%20HandBook.pdf) \n  + [eisenstein-nlp-notes.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/eisenstein-nlp-notes.pdf)\n  + [oxford-cs-deepnlp-2017](https://github.com/changwookjun/StudyBook/tree/master/NLPBooks/oxford-cs-deepnlp-2017)\n    - [Lecture 1a - Introduction.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201a%20-%20Introduction.pdf)    \n    - [Lecture 1b - Deep Neural Networks Are Our Friends.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%201b%20-%20Deep%20Neural%20Networks%20Are%20Our%20Friends.pdf)   \n    - [Lecture 2a- Word Level Semantics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202a-%20Word%20Level%20Semantics.pdf) \n    - [Lecture 2b - Overview of the Practicals.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%202b%20-%20Overview%20of%20the%20Practicals.pdf) \n    - [Lecture 3 - Language Modelling and RNNs Part 1.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%203%20-%20Language%20Modelling%20and%20RNNs%20Part%201.pdf) \n    - [Lecture 4 - Language Modelling and RNNs Part 2.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%204%20-%20Language%20Modelling%20and%20RNNs%20Part%202.pdf)   \n    - [Lecture 5 - Text Classification.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%205%20-%20Text%20Classification.pdf)   \n    - [Lecture 6 - Nvidia RNNs and GPUs.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%206%20-%20Nvidia%20RNNs%20and%20GPUs.pdf)   \n    - [Lecture 7 - Conditional Language Modeling.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%207%20-%20Conditional%20Language%20Modeling.pdf)   \n    - [Lecture 8 - Conditional Language Modeling with Attention.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)   \n    - [Lecture 9 - Speech Recognition.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%209%20-%20Speech%20Recognition.pdf)   \n    - [Lecture 10 - Text to Speech.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2010%20-%20Text%20to%20Speech.pdf)   \n    - [Lecture 11 - Question Answering.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2011%20-%20Question%20Answering.pdf)   \n    - [Lecture 12- Memory Lecture.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2012-%20Memory%20Lecture.pdf)    \n    - [Lecture 13 - Linguistics.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/oxford-cs-deepnlp-2017/Lecture%2013%20-%20Linguistics.pdf)      \n  + [Speech and Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Speech_and_Language_Processing.pdf) \n  + [Embeddings in Natural Language Processing.pdf](https://github.com/changwookjun/StudyBook/blob/master/NLPBooks/Embeddings%20in%20Natural%20Language%20Processing.pdf)  \n\n\n* [Python Books](https://github.com/changwookjun/StudyBook/tree/master/PythonBooks)   \n  + [Learn Python The Hard Way 3rd Edition free pdf download.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Learn%20Python%20The%20Hard%20Way%203rd%20Edition%20free%20pdf%20download.pdf)\n  + [SciPy and NumPy.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/SciPy%20and%20NumPy.pdf)\n  + [ScipyLectures-simple.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/ScipyLectures-simple.pdf)\n  + [Shaw Z.A. - Learn Python the Hard Way, 2nd Edition [2011, PDF, ENG].pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Shaw%20Z.A.%20-%20Learn%20Python%20the%20Hard%20Way%2C%202nd%20Edition%20%5B2011%2C%20PDF%2C%20ENG%5D.pdf)\n  + [Understanding GIL.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/Understanding%20GIL.pdf)\n  + [scipy-ref-0.17.0.pdf](https://github.com/changwookjun/StudyBook/blob/master/PythonBooks/scipy-ref-0.17.0.pdf)\n  \n* [Reinforcement Learning Books](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks)  \n  + [RLAlgsInMDPs.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLAlgsInMDPs.pdf)\n  + [RLbook2018.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/RLbook2018.pdf)\n  + [Dissecting Reinforcement Learning](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning)    \n    - [Dissecting Reinforcement Learning-Part1.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part1.pdf)   \n    - [Dissecting Reinforcement Learning-Part2.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part2.pdf) \n    - [Dissecting Reinforcement Learning-Part3.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part3.pdf) \n    - [Dissecting Reinforcement Learning-Part4.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part4.pdf) \n    - [Dissecting Reinforcement Learning-Part5.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part5.pdf)   \n    - [Dissecting Reinforcement Learning-Part6.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part6.pdf)   \n    - [Dissecting Reinforcement Learning-Part7.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/Dissecting%20Reinforcement%20Learning/Dissecting%20Reinforcement%20Learning-Part7.pdf)  \n  + [AI_CRASH_COURSE.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AI_CRASH_COURSE.pdf)   \n  \n  + [UCL Course on RL d.silver](https://github.com/changwookjun/StudyBook/tree/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver)\n    - [Lecture 1: Introduction to Reinforcement Learning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/intro_RL.pdf)  \n    - [Lecture 2: Markov Decision Processes](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MDP.pdf)      \n    - [Lecture 3: Planning by Dynamic Programming](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/DP.pdf)  \n    - [Lecture 4: Model-Free Prediction](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/MC-TD.pdf)      \n    - [Lecture 5: Model-Free Control](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/control.pdf)      \n    - [Lecture 6: Value Function Approximation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/FA.pdf)      \n    - [Lecture 7: Policy Gradient Methods](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/pg.pdf)      \n    - [Lecture 8: Integrating Learning and Planning](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/dyna.pdf)      \n    - [Lecture 9: Exploration and Exploitation](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/XX.pdf)      \n    - [Lecture 10: Case Study: RL in Classic Games](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/games.pdf)  \n    - [Lecture 11: Case Study: Deep RL](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/UCL%20Course%20on%20RL%20d.silver/deep_rl_tutorial.pdf)      \n    - [Video-lectures available here](https://www.youtube.com/watch?v=2pWv7GOvuf0)          \n  + [AnIntroductiontoDeepReinforcementLearning.pdf](https://github.com/changwookjun/StudyBook/blob/master/ReinforcementLearningBooks/AnIntroductiontoDeepReinforcementLearning.pdf)  \n\n# Author\nChangWookJun / @changwookjun (changwookjun@gmail.com)\n"
 },
 {
  "repo": "abidrahmank/OpenCV2-Python-Tutorials",
  "language": "Python",
  "readme_contents": "OpenCV2-Python-Guide\n====================\n\nThis repo contains tutorials on OpenCV-Python library using new cv2 interface\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n=======================================================================\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\n**IMP - This tutorial is meant for OpenCV 3x version. Not OpenCV 2x**\n\nPlease try the examples with OpenCV 3x before sending any bug reports\n\nData files\n-----------\n\nThe input data used in these tutorials are given in **Data** folder\n\nOnline\n---------\n\n* **For official tutorials, please visit : http://docs.opencv.org/trunk/doc/py_tutorials/py_tutorials.html**\n* https://opencv-python-tutroals.readthedocs.org/en/latest/index.html - This is only for checking. May contain lots of errors, please stick to the official tutorials.\n\nOffline\n---------\nTo build docs from source,\n* Install sphinx\n* Download/Clone this repo and navigate to the base folder\n* run command : `make html` , html docs will be available in **build/html/** folder\n"
 },
 {
  "repo": "datitran/object_detector_app",
  "language": "Python",
  "readme_contents": "# Object-Detector-App\n\nA real-time object recognition application using [Google's TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) and [OpenCV](http://opencv.org/).\n\n## Getting Started\n1. `conda env create -f environment.yml`\n2. `python object_detection_app.py` / `python object_detection_multithreading.py`\n    Optional arguments (default value):\n    * Device index of the camera `--source=0`\n    * Width of the frames in the video stream `--width=480`\n    * Height of the frames in the video stream `--height=360`\n    * Number of workers `--num-workers=2`\n    * Size of the queue `--queue-size=5`\n    * Get video from HLS stream rather than webcam '--stream-input=http://somertmpserver.com/hls/live.m3u8'\n    * Send stream to livestreaming server '--stream-output=--stream=http://somertmpserver.com/hls/live.m3u8'\n\n## Tests\n```\npytest -vs utils/\n```\n\n## Requirements\n- [Anaconda / Python 3.5](https://www.continuum.io/downloads)\n- [TensorFlow 1.2](https://www.tensorflow.org/)\n- [OpenCV 3.0](http://opencv.org/)\n\n## Notes\n- OpenCV 3.1 might crash on OSX after a while, so that's why I had to switch to version 3.0. See open issue and solution [here](https://github.com/opencv/opencv/issues/5874).\n- Moving the `.read()` part of the video stream in a multiple child processes did not work. However, it was possible to move it to a separate thread.\n\n## Copyright\n\nSee [LICENSE](LICENSE) for details.\nCopyright (c) 2017 [Dat Tran](http://www.dat-tran.com/).\n"
 },
 {
  "repo": "RubensZimbres/Repo-2017",
  "language": "Python",
  "readme_contents": "# Python Codes in Data Science\n\nCodes in NLP, Deep Learning, Reinforcement Learning and Artificial Intelligence\n\n<b> Welcome to my GitHub repo. </b>\n\nI am a Data Scientist and I code in R, Python and Wolfram Mathematica. Here you will find some Machine Learning, Deep Learning, Natural Language Processing and Artificial Intelligence models I developed.\n\n<b> Outputs of the models can be seen at my portfolio: </b> https://drive.google.com/file/d/0B0RLknmL54khdjRQWVBKeTVxSHM/view?usp=sharing\n\n----------------\nKeras version used in models: keras==1.1.0\n\n<b> Autoencoder for Audio  </b> is a model where I compressed an audio file and used Autoencoder to reconstruct the audio file, for use in phoneme classification.\n\n<b> Collaborative Filtering  </b> is a Recommender System where the algorithm predicts a movie review based on genre of movie and similarity among people who watched the same movie.\n\n<b> Convolutional NN Lasagne  </b> is a Convolutional Neural Network model in Lasagne to solve the MNIST task.\n\n<b> Ensembled Machine Learning </b> is a .py file where 7 Machine Learning algorithms are used in a classification task with 3 classes and all possible hyperparameters of each algorithm are adjusted. Iris dataset of scikit-learn.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Ensembled.MachineLearning.png?raw=true>\n</p>\n\n<b> GAN Generative Adversarial  </b> are models of Generative Adversarial Neural Networks.\n\n<b> Hyperparameter Tuning RL  </b> is a model where hyperparameters of Neural Networks are adjusted via Reinforcement Learning. According to a reward, hyperparameter tuning (environment) is changed through a policy (mechanization of knowledge) using the Boston Dataset. Hyperparameters tuned are: learning rate, epochs, decay, momentum, number of hidden layers and nodes and initial weights.\n\n<b> Keras Regularization L2  </b> is a Neural Network model for regression made with Keras where a L2 regularization was applied to prevent overfitting.\n\n<b> Lasagne Neural Nets Regression  </b> is a Neural Network model based in Theano and Lasagne, that makes a linear regression with a continuous target variable and reaches 99.4% accuracy. It uses the DadosTeseLogit.csv sample file.\n\n<b> Lasagne Neural Nets + Weights  </b> is a Neural Network model based in Theano and Lasagne, where is possible to visualize weights between X1 and X2 to hidden layer. Can also be adapted to visualize weights between hidden layer and output. It uses the DadosTeseLogit.csv sample file.\n\n<b> Multinomial Regression  </b> is a regression model where target variable has 3 classes.\n\n<b> Neural Networks for Regression  </b> shows multiple solutions for a regression problem, solved with sklearn, Keras, Theano and Lasagne. It uses the Boston dataset sample file from sklearn and reaches more than 98% accuracy.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/HiddenLayers.jpg?raw=true>\n</p>\n\n<b> NLP + Naive Bayes Classifier  </b> is a model where movie reviews were labeled as positive and negative and the algorithm then classifies a totally new set of reviews using Logistic Regression, Decision Trees and Naive Bayes, reaching an accuracy of 92%.\n\n<b> NLP Anger Analysis  </b> is a Doc2Vec model associated with Word2Vec model to analyze level of anger using synonyms in consumer complaints of a U.S. retailer in Facebook posts.\n\n<b> NLP Consumer Complaint  </b> is a model where Facebook posts of a U.S. computer retailer were scraped, tokenized, lemmatized and applied Word2Vec. After that, t-SNE and Latent Dirichlet Allocation were developed in order to classify the arguments and weights of each keyword used by a consumer in his complaint. The code also analyzes frequency of words in 100 posts.\n\n<b> NLP Convolutional Neural Network </b> is a Convolutional Neural Network for Text in order to classify movie reviews.\n\n<b> NLP Doc2Vec  </b> is a Natural Language Procesing file where cosine similarity among phrases is measured through Doc2Vec.\n\n<b> NLP Document Classification  </b> is a code for Document Classification according to Latent Dirichlet Allocation.\n\n<b> NLP Facebook Analysis  </b> analyzes Facebook posts regarding Word Frequency and Topic Modelling using LDA.\n\n<b> NLP Facebook Scrap  </b> is a Python code for scraping data from Facebook.\n\n<b> NLP - Latent Dirichlet Allocation  </b> is a Natural Language Processing model where a Wikipedia page on Statistical Inference is classified regarding topics, using Latent Dirichlet Allocation with Gensim, NLTK, t-SNE and K-Means.\n\n<b> NLP Probabilistic ANN  </b> is a Natural Language Processing model where sentences are vectorized by Gensim and a probabilistic Neural Network model is deveoped using Gensim, for sentiment analysis.\n\n<b> NLP Semantic Doc2Vec + Neural Network  </b> is a model where positive and negative movie reviews were extracted and semantically classified with NLTK and BeautifulSoup, then labeled as positive or negative. Text was then used as an input for the Neural Network model training. After training, new sentences are entered in the Keras Neural Network model and then classified. It uses the zip file.\n\n<b> NLP Sentiment Positive  </b> is a model that identifies website content as positive, neutral or negative using BeautifulSoup and NLTK libraries, plotting the results.\n\n<b> NLP Twitter Analysis ID #  </b> is a model that extracts posts from Twitter based in ID of user or Hashtag.\n\n<b> NLP Twitter Scrap  </b> is a model that scraps Twitter data and shows the cleaned text as output.\n\n<b> NLP Twitter Streaming  </b> is a model of analysis of real-time data from Twitter (under development).\n\n<b> NLP Twitter Streaming Mood  </b> is a model where the evolution of mood Twitter posts is measured during a period of time.\n\n<b> NLP Wikipedia Summarization  </b> is a Python code that summarizes any given page in a few sentences.\n\n<b> NLP Word Frequency  </b> is a model that calculates the frequency of nouns, verbs, words in Facebook posts.\n\n<b> Probabilistic Neural Network  </b> is a Probabilistic Neural Network for Time Series Prediction.\n\n<b> REAL-TIME Twitter Analysis  </b> is a model where Twitter streaming is extracted, words and sentences tokenized, word embeddings were created, topic modeling was made and classified using K-Means. Then, NLTK SentimentAnalyzer was used to classify each sentence of the streaming into positive, neutral or negative. Accumulated sum was used to generate the plot and the code loops each 1 second, collecting new tweets.\n\n<b> RESNET-2  </b> is a Deep Residual Neural Network.\n\n<b> ROC Curve Multiclass  </b> is a .py file where Naive Bayes was used to solve the IRIS Dataset task and ROC curve of different classes are plotted.\n\n<b> SQUEEZENET  </b> is a simplified version of the AlexNet.\n\n<b> Stacked Machine Learning  </b> is a .py notebook where t-SNE, Principal Components Analysis and Factor Analysis were applied to reduce dimensionality of data. Classification performances were measured after applying K-Means.\n\n<b> Support Vector Regression  </b> is a SVM model for non linear regression in an artificial dataset.\n\n<b> Text-to-Speech  </b> is a .py file where Python speaks any given text and saves it as an audio .wav file.\n\n<b> Time Series ARIMA </b>  is a ARIMA model to forecast time series, with an error margin of 0.2%.\n\n<b> Time Series Prediction with Neural Networks - Keras </b>  is a Neural Network model to forecast time series, using Keras with an adaptive learning rate depending upon derivative of loss.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/ARIMA.10Period.png?raw=true> \n</p>\n\n<b> Variational Autoencoder  </b> is a VAE made with Keras.\n\n<b> Web Crawler  </b> is a code that scraps data from different URLs of a hotel website.\n\n<b> t-SNE Dimensionality Reduction  </b> is a t-SNE model for dimensionality reduction which is compared to Principal Components Analysis regarding its discriminatory power.\n\n<b> t-SNE PCA + Neural Networks  </b> is a model that compares performance or Neural Networks made after t-SNE, PCA and K-Means.\n\n<b> t-SNE PCA LDA embeddings </b> is a model where t-SNE, Principal Components Analysis, Linear Discriminant Analysis and Random Forest embeddings are compared in a task to classify clusters of similar digits.\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/Doc2Vec.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/raw/master/Pictures%20-%20Formulas/t_SNE_Lk.png?raw=true>\n</p>\n\n<p align=\"center\">\n<img src=https://github.com/RubensZimbres/Repo-2017/blob/master/Pictures%20-%20Formulas/RESNET_Me.jpg?raw=true>\n</p>\n\n"
 },
 {
  "repo": "Breakthrough/PySceneDetect",
  "language": "Python",
  "readme_contents": "\n![PySceneDetect](https://raw.githubusercontent.com/Breakthrough/PySceneDetect/master/docs/img/pyscenedetect_logo_small.png)\n==========================================================\nVideo Scene Cut Detection and Analysis Tool\n----------------------------------------------------------\n\n[![Build Status](https://img.shields.io/travis/com/Breakthrough/PySceneDetect)](https://travis-ci.com/github/Breakthrough/PySceneDetect) [![PyPI Status](https://img.shields.io/pypi/status/scenedetect.svg)](https://pypi.python.org/pypi/scenedetect/) [![PyPI Version](https://img.shields.io/pypi/v/scenedetect?color=blue)](https://pypi.python.org/pypi/scenedetect/)  [![PyPI License](https://img.shields.io/pypi/l/scenedetect.svg)](http://pyscenedetect.readthedocs.org/en/latest/copyright/)\n\n\n### Latest Release: v0.5.4 (September 14, 2020)\n\n**Main Webpage**:  [py.scenedetect.com](http://py.scenedetect.com)\n\n**Documentation**:  [manual.scenedetect.com](http://manual.scenedetect.com)\n\n**Installation and Dependencies**: https://pyscenedetect.readthedocs.io/en/latest/download/\n\n----------------------------------------------------------\n\n**Quick Install**: To install PySceneDetect via `pip` with all dependencies:\n\n    pip install scenedetect[opencv]\n\nTo enable video splitting support, you will also need to have `mkvmerge` or `ffmpeg` installed on your system. See the documentation on [Video Splitting Support](https://pyscenedetect.readthedocs.io/en/latest/examples/video-splitting/) after installation for details.\n\nRequires Python modules `numpy`, OpenCV `cv2`, and (optional) `tqdm` for displaying progress.\n\n----------------------------------------------------------\n\n**Quick Start (Command Line)**:\n\nSplit the input video wherever a new scene is detected:\n\n    scenedetect -i video.mp4 detect-content split-video\n\nSkip the first 10 seconds of the input video, and output a list of scenes to the terminal:\n\n    scenedetect -i video.mp4 time -s 10s detect-content list-scenes\n\nTo show a summary of all other options and commands:\n\n    scenedetect help\n\nYou can find more examples [on the website](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/) or [in the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/cli/global_options.html).\n\n**Quick Start (Python API)**:\n\nIn the code example below, we create a function `find_scenes()` which will\nload a video, detect the scenes, and return a list of tuples containing the\n(start, end) timecodes of each detected scene.  Note that you can modify\nthe `threshold` argument to modify the sensitivity of the scene detection.\n\n```python\n# Standard PySceneDetect imports:\nfrom scenedetect import VideoManager\nfrom scenedetect import SceneManager\n\n# For content-aware scene detection:\nfrom scenedetect.detectors import ContentDetector\n\ndef find_scenes(video_path, threshold=30.0):\n    # Create our video & scene managers, then add the detector.\n    video_manager = VideoManager([video_path])\n    scene_manager = SceneManager()\n    scene_manager.add_detector(\n        ContentDetector(threshold=threshold))\n\n    # Base timestamp at frame 0 (required to obtain the scene list).\n    base_timecode = video_manager.get_base_timecode()\n\n    # Improve processing speed by downscaling before processing.\n    video_manager.set_downscale_factor()\n\n    # Start the video manager and perform the scene detection.\n    video_manager.start()\n    scene_manager.detect_scenes(frame_source=video_manager)\n\n    # Each returned scene is a tuple of the (start, end) timecode.\n    return scene_manager.get_scene_list(base_timecode)\n```\n\nTo get started, try printing the result from calling `find_scenes` on a small video clip:\n\n```python\n    scenes = find_scenes('video.mp4')\n    print(scenes)\n```\n\nSee [the manual](https://pyscenedetect.readthedocs.io/projects/Manual/en/latest/api.html) for the full PySceneDetect API documentation.\n\n----------------------------------------------------------\n\nPySceneDetect is a command-line tool and Python library, which uses OpenCV to analyze a video to find scene changes or cuts.  If `ffmpeg` or `mkvmerge` is installed, the video can also be split into scenes automatically.  A frame-by-frame analysis can also be generated for a video, to help with determining optimal threshold values or detecting patterns/other analysis methods for a particular video.  See [the Usage documentation](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/) for details.\n\nThere are two main detection methods PySceneDetect uses: `detect-threshold` (comparing each frame to a set black level, useful for detecting cuts and fades to/from black), and `detect-content` (compares each frame sequentially looking for changes in content, useful for detecting fast cuts between video scenes, although slower to process).  Each mode has slightly different parameters, and is described in detail below.\n\nIn general, use `detect-threshold` mode if you want to detect scene boundaries using fades/cuts in/out to black.  If the video uses a lot of fast cuts between content, and has no well-defined scene boundaries, you should use the `detect-content` mode.  Once you know what detection mode to use, you can try the parameters recommended below, or generate a statistics file (using the `-s` / `--statsfile` flag) in order to determine the correct paramters - specifically, the proper threshold value.\n\nNote that PySceneDetect is currently in beta; see Current Features & Roadmap below for details.  For help or other issues, you can contact me on [my website](http://www.bcastell.com/about/), or we can chat in #pyscenedetect on Freenode.  Feel free to submit any bugs or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues) here on Github.\n\n\nUsage\n----------------------------------------------------------\n\n - [Basic Usage](https://pyscenedetect.readthedocs.io/en/latest/examples/usage/)\n - [PySceneDetect Manual](https://pyscenedetect-manual.readthedocs.io/en/latest/), covers `scenedetect` command and Python API\n - [Example: Detecting and Splitting Scenes in Movie Clip](https://pyscenedetect.readthedocs.io/en/latest/examples/usage-example/)\n\n\nCurrent Features & Roadmap\n----------------------------------------------------------\n\nYou can [view the latest features and version roadmap on Readthedocs](http://pyscenedetect.readthedocs.org/en/latest/features/).\nSee [`docs/changelog.md`](https://github.com/Breakthrough/PySceneDetect/blob/master/docs/changelog.md) for a list of changes in each version, or visit [the Releases page](https://github.com/Breakthrough/PySceneDetect/releases) to download a specific version.  Feel free to submit any bugs/issues or feature requests to [the Issue Tracker](https://github.com/Breakthrough/PySceneDetect/issues).\n\nAdditional features being planned or in development can be found [here (tagged as `feature`) in the issue tracker](https://github.com/Breakthrough/PySceneDetect/issues?q=is%3Aissue+is%3Aopen+label%3Afeature).  You can also find additional information about PySceneDetect at [http://www.bcastell.com/projects/PySceneDetect/](http://www.bcastell.com/projects/PySceneDetect/).\n\n\n----------------------------------------------------------\n\nLicensed under BSD 3-Clause (see the `LICENSE` file for details).\n\nCopyright (C) 2014-2020 Brandon Castellano.\nAll rights reserved.\n\n"
 },
 {
  "repo": "QianMo/OpenCV3-Intro-Book-Src",
  "language": "C++",
  "readme_contents": "\u300aOpenCV3\u7f16\u7a0b\u5165\u95e8\u300b\u4e66\u672c\u914d\u5957\u6e90\u4ee3\u7801\n==============================\n#### \u300aIntroduction to OpenCV3 Programming\u300bBook Source Code<br>\n\n![](http://img.blog.csdn.net/20150325155409850)  \n<br>\u672c\u4e66\u6709OpenCV2\u3001OpenCV3\u4e24\u5957\u72ec\u7acb\u7684\u4e66\u672c\u914d\u5957\u793a\u4f8b\u7a0b\u5e8f\u4f9b\u9009\u62e9\u4f7f\u7528\u3002\n<br>  \u67094\u4e2a\u90e8\u520611\u7ae0\uff0c\u5171\u670995\u4e2a\u4e3b\u7ebf\u793a\u4f8b\u7a0b\u5e8f\uff0c\u4e3a\u65b9\u4fbf\u5927\u5bb6\u67e5\u9605\u548c\u5b66\u4e60\uff0c\u603b\u7ed3\u6210\u5982\u4e0b\u3002\n# \u6b63\u6587\u90e8\u5206\u6e90\u4ee3\u7801\n## \u7b2c\u4e00\u90e8\u5206 \u5feb\u901f\u4e0a\u624bOpenCV\n\t\t1\tOpenCV\u73af\u5883\u914d\u7f6e\u7684\u6d4b\u8bd5\u7528\u4f8b\t1.3.8\n\t\t2\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e00\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u663e\u793a\t1.4.1\n\t\t3\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e8c\u4e2a\u7a0b\u5e8f\uff1a\u56fe\u50cf\u8150\u8680\t1.4.2\n\t\t4\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u4e09\u4e2a\u7a0b\u5e8f\uff1ablur\u56fe\u50cf\u6a21\u7cca\t1.4.3\n\t\t5\t\u5feb\u901f\u4e0a\u624bOpenCV\u7684\u7b2c\u56db\u4e2a\u7a0b\u5e8f\uff1acanny\u8fb9\u7f18\u68c0\u6d4b\t1.4.4\n\t\t6\t\u8bfb\u53d6\u5e76\u64ad\u653e\u89c6\u9891\t1.5.1\n\t\t7\t\u8c03\u7528\u6444\u50cf\u5934\u91c7\u96c6\u56fe\u50cf\t1.5.2\n\t\t8\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5f69\u8272\u76ee\u6807\u8ddf\u8e2a\uff1aCamshift\t2.1.1\n\t\t9\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u5149\u6d41\uff1aoptical flow\t2.1.2\n\t\t10\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u70b9\u8ffd\u8e2a\uff1alkdemo\t2.1.3\n\t\t11\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u4eba\u8138\u8bc6\u522b\uff1aobjectDetection\t2.1.4\n\t\t12\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u652f\u6301\u5411\u91cf\u673a\u5f15\u5bfc\t2.1.5\n\t\t13\t\u5b98\u65b9\u4f8b\u7a0b\u5f15\u5bfc\u3001\u8d4f\u6790\u4e4b\u652f\u6301\u5411\u91cf\u673a\uff1a\u5904\u7406\u7ebf\u6027\u4e0d\u53ef\u5206\u6570\u636e\t2.1.5\n\t\t14\tprintf\u51fd\u6570\u7684\u7528\u6cd5\u793a\u4f8b\t2.6.2\n\t\t15\t\u7528imwrite\u51fd\u6570\u751f\u6210png\u900f\u660e\u56fe\t3.1.8\n\t\t16\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u56fe\u50cf\u7684\u8f7d\u5165\u3001\u663e\u793a\u4e0e\u8f93\u51fa\t3.1.9\n\t\t17\t\u4e3a\u7a0b\u5e8f\u754c\u9762\u6dfb\u52a0\u6ed1\u52a8\u6761\t3.2.1\n\t\t18\t\u9f20\u6807\u64cd\u4f5c\u793a\u4f8b\t3.3\n## \u7b2c\u4e8c\u90e8\u5206 \u521d\u63a2core\u7ec4\u4ef6\t\n\t\t19\t\u57fa\u7840\u56fe\u50cf\u5bb9\u5668Mat\u7c7b\u7684\u4f7f\u7528\t4.1.7\n\t\t20\t\u7528OpenCV\u8fdb\u884c\u57fa\u672c\u7ed8\u56fe\t4.3\n\t\t21\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e00\uff1a\u7528\u6307\u9488\u8bbf\u95ee\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t22\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e8c\uff1a\u7528\u8fed\u4ee3\u5668\u64cd\u4f5c\u50cf\u7d20\t5.1.5\u30015.1.6\n\t\t23\t\u64cd\u4f5c\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u65b9\u6cd5\u4e09\uff1a\u52a8\u6001\u5730\u5740\u8ba1\u7b97\t5.1.5\u30015.1.6\n\t\t24\t\u904d\u5386\u56fe\u50cf\u4e2d\u50cf\u7d20\u768414\u79cd\u65b9\u6cd5\t5.1.6\n\t\t25\t\u521d\u7ea7\u56fe\u50cf\u6df7\u5408\t5.2.4\n\t\t26\t\u591a\u901a\u9053\u56fe\u50cf\u6df7\u5408\t5.3.3\n\t\t27\t\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u3001\u4eae\u5ea6\u503c\u8c03\u6574\t5.4.3\n\t\t28\t\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\t5.5.8\n\t\t29\tXML\u548cYAML\u6587\u4ef6\u7684\u5199\u5165\t5.6.3\n\t\t30\tXML\u548cYAML\u6587\u4ef6\u7684\u8bfb\u53d6\t5.6.4\u3001\n## \u7b2c\u4e09\u90e8\u5206 \u638c\u63e1imgproc\u7ec4\u4ef6\t\n\t\t31\t\u65b9\u6846\u6ee4\u6ce2\uff1aboxFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t32\t\u5747\u503c\u6ee4\u6ce2\uff1ablur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t33\t\u9ad8\u65af\u6ee4\u6ce2\uff1aGaussianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.1.11\n\t\t34\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u7ebf\u6027\u6ee4\u6ce2\t6.1.12\n\t\t35\t\u4e2d\u503c\u6ee4\u6ce2\uff1amedianBlur\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t36\t\u53cc\u8fb9\u6ee4\u6ce2\uff1abilateralFilter\u51fd\u6570\u7684\u4f7f\u7528\t6.2.4\n\t\t37\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u6ee4\u6ce2\t6.2.5\n\t\t38\t\u81a8\u80c0\uff1adilate\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t39\t\u8150\u8680\uff1aerode\u51fd\u6570\u7684\u4f7f\u7528\t6.3.5\n\t\t40\t\u7efc\u5408\u793a\u4f8b\uff1a\u8150\u8680\u4e0e\u81a8\u80c0\t6.3.6\n\t\t41\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u81a8\u80c0\t6.4.8\n\t\t42\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u8150\u8680\t6.4.8\n\t\t43\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u5f00\u8fd0\u7b97\t6.4.8\n\t\t44\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u95ed\u8fd0\u7b97\t6.4.8\n\t\t45\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u68af\u5ea6\t6.4.8\n\t\t46\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9876\u5e3d\u201d\t6.4.8\n\t\t47\t\u7528morphologyEx()\u51fd\u6570\u5b9e\u73b0\u5f62\u6001\u5b66\u201c\u9ed1\u5e3d\u201d\t6.4.8\n\t\t48\t\u7efc\u5408\u793a\u4f8b\uff1a\u5f62\u6001\u5b66\u6ee4\u6ce2\t6.4.9\n\t\t49\t\u6f2b\u6c34\u586b\u5145\u7b97\u6cd5\uff1afloodFill\u51fd\u6570\t6.5.3\n\t\t50\t\u7efc\u5408\u793a\u4f8b\uff1a\u6f2b\u6c34\u586b\u5145\t6.5.4\n\t\t51\t\u5c3a\u5bf8\u8c03\u6574\uff1aresize()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.5\n\t\t52\t\u5411\u4e0a\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrUp()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t53\t\u5411\u4e0b\u91c7\u6837\u56fe\u50cf\u91d1\u5b57\u5854\uff1apyrDown()\u51fd\u6570\u7684\u4f7f\u7528\t6.6.6\n\t\t54\t\u7efc\u5408\u793a\u4f8b\uff1a\u56fe\u50cf\u91d1\u5b57\u5854\u4e0e\u56fe\u7247\u5c3a\u5bf8\u7f29\u653e\t6.6.7\n\t\t55\t\u793a\u4f8b\u7a0b\u5e8f\uff1a\u57fa\u672c\u9608\u503c\u64cd\u4f5c\t6.7.3\n\t\t56\tCanny\u8fb9\u7f18\u68c0\u6d4b\t7.1.2\n\t\t57\tSobel \u7b97\u5b50\u7684\u4f7f\u7528\t7.1.3\n\t\t58\tLaplacian\u7b97\u5b50\u7684\u4f7f\u7528\t7.1.4\n\t\t59\tScharr\u6ee4\u6ce2\u5668\t7.1.5\n\t\t60\t\u7efc\u5408\u793a\u4f8b\uff1a\u8fb9\u7f18\u68c0\u6d4b\t7.1.6\n\t\t61\t\u6807\u51c6\u970d\u592b\u53d8\u6362\uff1aHoughLines()\u51fd\u6570\u7684\u4f7f\u7528\t7.2.4\n\t\t62\t\u7d2f\u8ba1\u6982\u7387\u970d\u592b\u53d8\u6362\uff1aHoughLinesP()\u51fd\u6570\t7.2.5\n\t\t63\t\u970d\u592b\u5706\u53d8\u6362\uff1aHoughCircles()\u51fd\u6570\t7.2.8\n\t\t64\t\u7efc\u5408\u793a\u4f8b\uff1a\u970d\u592b\u53d8\u6362\t7.2.9\n\t\t65\t\u5b9e\u73b0\u91cd\u6620\u5c04\uff1aremap()\u51fd\u6570\t7.3.3\n\t\t66\t\u7efc\u5408\u793a\u4f8b\u7a0b\u5e8f\uff1a\u5b9e\u73b0\u591a\u79cd\u91cd\u6620\u5c04\t7.3.4\n\t\t67\t\u4eff\u5c04\u53d8\u6362\t7.4.5\n\t\t68\t\u76f4\u65b9\u56fe\u5747\u8861\u5316\t7.5.3\n\t\t69\t\u8f6e\u5ed3\u67e5\u627e\t8.1.3\n\t\t70\t\u67e5\u627e\u5e76\u7ed8\u5236\u8f6e\u5ed3\t8.1.4\n\t\t71\t\u51f8\u5305\u68c0\u6d4b\u57fa\u7840\t8.2.3\n\t\t72\t\u5bfb\u627e\u548c\u7ed8\u5236\u7269\u4f53\u7684\u51f8\u5305\t8.2.4\n\t\t73\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u77e9\u5f62\u8fb9\u754c\t8.3.6\n\t\t74\t\u521b\u5efa\u5305\u56f4\u8f6e\u5ed3\u7684\u5706\u5f62\u8fb9\u754c\t8.3.7\n\t\t75\t\u4f7f\u7528\u591a\u8fb9\u5f62\u5305\u56f4\u8f6e\u5ed3\t8.3.8\n\t\t76\t\u56fe\u50cf\u8f6e\u5ed3\u77e9\t8.4.4\n\t\t77\t\u5206\u6c34\u5cad\u7b97\u6cd5\u7684\u4f7f\u7528\t8.5.2\n\t\t78\t\u5b9e\u73b0\u56fe\u50cf\u4fee\u8865\t8.6.2\n\t\t79\tH-S\u4e8c\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.3\n\t\t80\t\u4e00\u7ef4\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.4\n\t\t81\tRGB\u4e09\u8272\u76f4\u65b9\u56fe\u7684\u7ed8\u5236\t9.2.5\n\t\t82\t\u76f4\u65b9\u56fe\u5bf9\u6bd4\t9.3.2\n\t\t83\t\u53cd\u5411\u6295\u5f71\t9.4.7\n\t\t84\t\u6a21\u677f\u5339\u914d\t9.5.3\n\t\t\n## \u7b2c\u56db\u90e8\u5206 \u6df1\u5165featrue2d\u7ec4\u4ef6\t\n\t\t85\t\u5b9e\u73b0Harris\u89d2\u70b9\u68c0\u6d4b\uff1acornerHarris()\u51fd\u6570\u7684\u4f7f\u7528\t10.1.4\n\t\t86\tharris\u89d2\u70b9\u68c0\u6d4b\u4e0e\u7ed8\u5236\t10.1.5\n\t\t87\tShi-Tomasi\u89d2\u70b9\u68c0\u6d4b\t10.2.3\n\t\t88\t\u4e9a\u50cf\u7d20\u7ea7\u89d2\u70b9\u68c0\u6d4b\t10.3.3\n\t\t89\tSURF\u7279\u5f81\u70b9\u68c0\u6d4b\t11.1.6\n\t\t90\tSURF\u7279\u5f81\u63d0\u53d6\t11.2.3\n\t\t91\t\u4f7f\u7528FLANN\u8fdb\u884c\u7279\u5f81\u70b9\u5339\u914d\t11.3.3\n\t\t92\tFLANN\u7ed3\u5408SURF\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u548c\u5339\u914d\t11.3.4\n\t\t93\tSIFT\u914d\u5408\u66b4\u529b\u5339\u914d\u8fdb\u884c\u5173\u952e\u70b9\u63cf\u8ff0\u548c\u63d0\u53d6\t11.3.5\n\t\t94\t\u5bfb\u627e\u5df2\u77e5\u7269\u4f53\t11.4.3\n\t\t95\t\u5229\u7528ORB\u7b97\u6cd5\u8fdb\u884c\u5173\u952e\u70b9\u7684\u63cf\u8ff0\u4e0e\u5339\u914d\t11.5.4\n\n\n\n# \u989d\u5916\u7684\u9644\u8d60\u7a0b\u5e8f\u4e00\u89c8\n\n\u9664\u4e66\u672c\u672c\u8eab\u7684\u793a\u4f8b\u7a0b\u5e8f\u4e4b\u5916\uff0c\u989d\u5916\u9644\u52a0\u4e86OpenCV2\u7248\u768421\u4e2a\u76f8\u8f83\u4e8e\u6b63\u6587\u4e3b\u7ebf\u7684\u793a\u4f8b\u4ee3\u7801\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u7684\u7a0b\u5e8f\u6e90\u4ee3\u7801\u3002\n\u73b0\u5c06\u9644\u52a0\u768421\u4e2a\u793a\u4f8b\u7a0b\u5e8f\u5217\u4e3e\u5982\u4e0b\uff1a\n\n\t\t1\t\u968f\u673a\u56fe\u5f62\u548c\u6587\u5b57\u751f\u6210\u793a\u4f8b\uff08randomtext\uff09\n\t\t2\t\u751f\u6210\u5f69\u8272\u8272\u6761\uff08gencolors\uff09\t\n\t\t3\t\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08kalman\uff09\t\n\t\t4\t\u6e10\u53d8\u8fc7\u6e21\u5404\u79cd\u56fe\u5f62\u6ee4\u6ce2\uff08median_blur\uff09\n\t\t5\t\u8ddd\u79bb\u53d8\u6362\uff08distanceTransform\uff09\n\t\t6\t\u628a\u56fe\u50cf\u6620\u5c04\u5230\u6781\u6307\u6570\u7a7a\u95f4\uff08Log Polar\uff09\n\t\t7\tfilter2D\u6ee4\u6ce2\u5668\u7684\u7528\u6cd5\t\n\t\t8\tgrabCut\u56fe\u50cf\u5206\u5272\u793a\u4f8b\t\n\t\t9\tMeanShift\u56fe\u50cf\u5206\u5272\u793a\u4f8b\n\t\t10\t\u7528\u6ed1\u52a8\u63a7\u5236\u56fe\u50cf\u76f4\u65b9\u56fe\n\t\t11\t\u627e\u5230\u56fe\u50cf\u6700\u5c0f\u7684\u5c01\u95ed\u8f6e\u5ed3\n\t\t12\tRetina\u7279\u5f81\u70b9\u68c0\u6d4b\n\t\t13\t\u6444\u50cf\u5934\u5e27\u6570\u68c0\u6d4b\n\t\t14\t\u89c6\u9891\u622a\u56fe\n\t\t15\t\u5bf9\u89c6\u9891\u7684\u5feb\u901f\u89d2\u70b9\u68c0\u6d4b\n\t\t16\t\u89c6\u9891\u7b80\u5355\u8272\u5f69\u68c0\u6d4b\n\t\t17\t\u8ddf\u8e2a\u5206\u5272\u89c6\u9891\u4e2d\u8fd0\u52a8\u7684\u7269\u4f53\n\t\t18\t\u89c6\u9891\u7684\u76f4\u65b9\u56fe\u53cd\u5411\u6295\u5f71\u3002\n\t\t19\t\u8ba1\u7b97\u89c6\u9891\u4e2d\u4e24\u4e2a\u56fe\u50cf\u533a\u57df\u7684\u76f8\u4f3c\u5ea6\n\t\t20\t\u89c6\u9891\u524d\u540e\u80cc\u666f\u5206\u79bb\n\t\t21\t\u7528\u9ad8\u65af\u80cc\u666f\u5efa\u6a21\u5206\u79bb\u80cc\u666f\n\n\n<br>\n\n\n# \u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\n\n[\u3010\u4e66\u672c\u52d8\u8bef&\u7ef4\u62a4\u535a\u6587\u3011](http://blog.csdn.net/poem_qianmo/article/details/44416709)\n\n![](http://img.blog.csdn.net/20150325202951885)  \n<br>Please Enjoy~\n\n"
 },
 {
  "repo": "ANYbotics/grid_map",
  "language": "C++",
  "readme_contents": "# Grid Map\n\n## Overview\n\nThis is a C++ library with [ROS] interface to manage two-dimensional grid maps with multiple data layers. It is designed for mobile robotic mapping to store data such as elevation, variance, color, friction coefficient, foothold quality, surface normal, traversability etc. It is used in the [Robot-Centric Elevation Mapping](https://github.com/anybotics/elevation_mapping) package designed for rough terrain navigation.\n\nFeatures:\n\n* **Multi-layered:** Developed for universal 2.5-dimensional grid mapping with support for any number of layers.\n* **Efficient map re-positioning:** Data storage is implemented as two-dimensional circular buffer. This allows for non-destructive shifting of the map's position (e.g. to follow the robot) without copying data in memory.\n* **Based on Eigen:** Grid map data is stored as [Eigen] data types. Users can apply available Eigen algorithms directly to the map data for versatile and efficient data manipulation.\n* **Convenience functions:** Several helper methods allow for convenient and memory safe cell data access. For example, iterator functions for rectangular, circular, polygonal regions and lines are implemented.\n* **ROS interface:** Grid maps can be directly converted to and from ROS message types such as PointCloud2, OccupancyGrid, GridCells, and our custom GridMap message. Conversion packages provide compatibility with [costmap_2d], [PCL], and [OctoMap] data types.\n* **OpenCV interface:** Grid maps can be seamlessly converted from and to [OpenCV] image types to make use of the tools provided by [OpenCV].\n* **Visualizations:** The *grid_map_rviz_plugin* renders grid maps as 3d surface plots (height maps) in [RViz]. Additionally, the *grid_map_visualization* package helps to visualize grid maps as point clouds, occupancy grids, grid cells etc.\n* **Filters:** The *grid_map_filters* provides are range of filters to process grid maps as a sequence of filters. Parsing of mathematical expressions allows to flexibly setup powerful computations such as thresholding, normal vectors, smoothening, variance, inpainting, and matrix kernel convolutions.\n\nThis is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.\n\nThe source code is released under a [BSD 3-Clause license](LICENSE).\n\n**Author: P\u00e9ter Fankhauser<br />\nAffiliation: [ANYbotics](https://www.anybotics.com/)<br />\nMaintainer: P\u00e9ter Fankhauser, pfankhauser@anybotics.com<br />**\nWith contributions by: Tanja Baumann, Jeff Delmerico, Remo Diethelm, Perry Franklin, Dominic Jud, Ralph Kaestner, Philipp Kr\u00fcsi, Alex Millane, Daniel Stonier, Elena Stumm, Martin Wermelinger, Christos Zalidis, Edo Jelavic, Ruben Grandia, Simone Arreghini, Magnus G\u00e4rtner\n\nThis projected was initially developed at ETH Zurich (Autonomous Systems Lab & Robotic Systems Lab).\n\n[This work is conducted as part of ANYmal Research, a community to advance legged robotics.](https://www.anymal-research.org/)\n\n![Grid map example in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin_example.png)\n\n## Publications\n\nIf you use this work in an academic context, please cite the following publication:\n\n> P. Fankhauser and M. Hutter,\n> **\"A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation\"**,\n> in Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1), A. Koubaa (Ed.), Springer, 2016. ([PDF](http://www.researchgate.net/publication/284415855))\n\n\n    @incollection{Fankhauser2016GridMapLibrary,\n      author = {Fankhauser, P{\\'{e}}ter and Hutter, Marco},\n      booktitle = {Robot Operating System (ROS) \u2013 The Complete Reference (Volume 1)},\n      title = {{A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation}},\n      chapter = {5},\n      editor = {Koubaa, Anis},\n      publisher = {Springer},\n      year = {2016},\n      isbn = {978-3-319-26052-5},\n      doi = {10.1007/978-3-319-26054-9{\\_}5},\n      url = {http://www.springer.com/de/book/9783319260525}\n    }\n\n## Documentation\n\nAn introduction to the grid map library including a tutorial is given in [this book chapter](http://www.researchgate.net/publication/284415855).\n\nThe C++ API is documented here:\n* [grid_map_core](http://docs.ros.org/kinetic/api/grid_map_core/html/index.html)\n* [grid_map_ros](http://docs.ros.org/kinetic/api/grid_map_ros/html/index.html)\n* [grid_map_costmap_2d](http://docs.ros.org/kinetic/api/grid_map_costmap_2d/html/index.html)\n* [grid_map_cv](http://docs.ros.org/kinetic/api/grid_map_cv/html/index.html)\n* [grid_map_filters](http://docs.ros.org/kinetic/api/grid_map_filters/html/index.html)\n* [grid_map_octomap](http://docs.ros.org/kinetic/api/grid_map_octomap/html/index.html)\n* [grid_map_pcl](http://docs.ros.org/kinetic/api/grid_map_pcl/html/index.html)\n\n## Installation\n\n### Installation from Packages\n\nTo install all packages from the grid map library as Debian packages use\n\n    sudo apt-get install ros-$ROS_DISTRO-grid-map\n\n### Building from Source\n\n#### Dependencies\n\nThe *grid_map_core* package depends only on the linear algebra library [Eigen].\n\n    sudo apt-get install libeigen3-dev\n\nThe other packages depend additionally on the [ROS] standard installation (*roscpp*, *tf*, *filters*, *sensor_msgs*, *nav_msgs*, and *cv_bridge*). Other format specific conversion packages (e.g. *grid_map_cv*, *grid_map_pcl* etc.) depend on packages described below in *Packages Overview*.\n\n#### Building\n\nTo build from source, clone the latest version from this repository into your catkin workspace and compile the package using\n\n    cd catkin_ws/src\n    git clone https://github.com/anybotics/grid_map.git\n    cd ../\n    catkin_make\n\nTo maximize performance, make sure to build in *Release* mode. You can specify the build type by setting\n\n    catkin_make -DCMAKE_BUILD_TYPE=Release\n\n\n### Packages Overview\n\nThis repository consists of following packages:\n\n* ***grid_map*** is the meta-package for the grid map library.\n* ***grid_map_core*** implements the algorithms of the grid map library. It provides the `GridMap` class and several helper classes such as the iterators. This package is implemented without [ROS] dependencies.\n* ***grid_map_ros*** is the main package for [ROS] dependent projects using the grid map library. It provides the interfaces to convert grid maps from and to several [ROS] message types.\n* ***grid_map_demos*** contains several nodes for demonstration purposes.\n* ***grid_map_filters*** builds on the [ROS Filters] package to process grid maps as a sequence of filters.\n* ***grid_map_msgs*** holds the [ROS] message and service definitions around the [grid_map_msg/GridMap] message type.\n* ***grid_map_rviz_plugin*** is an [RViz] plugin to visualize grid maps as 3d surface plots (height maps).\n* ***grid_map_visualization*** contains a node written to convert GridMap messages to other [ROS] message types for example for  visualization in [RViz].\n\nAdditional conversion packages:\n\n* ***grid_map_costmap_2d*** provides conversions of grid maps from [costmap_2d] map types.\n* ***grid_map_cv*** provides conversions of grid maps from and to [OpenCV] image types.\n* ***grid_map_octomap*** provides conversions of grid maps from OctoMap ([OctoMap]) maps.\n* ***grid_map_pcl*** provides conversions of grid maps from Point Cloud Library ([PCL](http://pointclouds.org/)) polygon meshes and point clouds. For details, see the grid map pcl package [README](grid_map_pcl/README.md).\n\n### Unit Tests\n\nRun the unit tests with\n\n    catkin_make run_tests_grid_map_core run_tests_grid_map_ros\n\nor\n\n    catkin build grid_map --no-deps --verbose --catkin-make-args run_tests\n\nif you are using [catkin tools](http://catkin-tools.readthedocs.org/).\n\n## Usage\n\n### Demonstrations\n\nThe *grid_map_demos* package contains several demonstration nodes. Use this code to verify your installation of the grid map packages and to get you started with your own usage of the library.\n\n* *[simple_demo](grid_map_demos/src/simple_demo_node.cpp)* demonstrates a simple example for using the grid map library. This ROS node creates a grid map, adds data to it, and publishes it. To see the result in RViz, execute the command\n\n        roslaunch grid_map_demos simple_demo.launch\n\n* *[tutorial_demo](grid_map_demos/src/tutorial_demo_node.cpp)* is an extended demonstration of the library's functionalities. Launch the *tutorial_demo* with\n\n        roslaunch grid_map_demos tutorial_demo.launch\n\n* *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* showcases the usage of the grid map iterators. Launch it with\n\n        roslaunch grid_map_demos iterators_demo.launch\n\n* *[image_to_gridmap_demo](grid_map_demos/src/ImageToGridmapDemo.cpp)* demonstrates how to convert data from an [image](grid_map_demos/data/eth_logo.png) to a grid map. Start the demonstration with\n\n        roslaunch grid_map_demos image_to_gridmap_demo.launch\n\n    ![Image to grid map demo result](grid_map_demos/doc/image_to_grid_map_demo_result.png)\n    \n* *[grid_map_to_image_demo](grid_map_demos/src/GridmapToImageDemo.cpp)* demonstrates how to save a grid map layer to an image. Start the demonstration with\n\n        rosrun grid_map_demos grid_map_to_image_demo _grid_map_topic:=/grid_map _file:=/home/$USER/Desktop/grid_map_image.png\n\n* *[opencv_demo](grid_map_demos/src/opencv_demo_node.cpp)* demonstrates map manipulations with help of [OpenCV] functions. Start the demonstration with\n\n        roslaunch grid_map_demos opencv_demo.launch\n\n    ![OpenCV demo result](grid_map_demos/doc/opencv_demo_result.gif)\n\n* *[resolution_change_demo](grid_map_demos/src/resolution_change_demo_node.cpp)* shows how the resolution of a grid map can be changed with help of the [OpenCV] image scaling methods. The see the results, use\n\n        roslaunch grid_map_demos resolution_change_demo.launch\n\n* *[filters_demo](grid_map_demos/src/FiltersDemo.cpp)* uses a chain of [ROS Filters] to process a grid map. Starting from the elevation of a terrain map, the demo uses several filters to show how to compute surface normals, use inpainting to fill holes, smoothen/blur the map, and use math expressions to detect edges, compute roughness and traversability. The filter chain setup is configured in the [`filters_demo_filter_chain.yaml`](grid_map_demos/config/filters_demo_filter_chain.yaml) file. Launch the demo with\n\n        roslaunch grid_map_demos filters_demo.launch\n\n    [![Filters demo results](grid_map_demos/doc/filters_demo_preview.gif)](grid_map_demos/doc/filters_demo.gif)\n\n For more information about grid map filters, see [grid_map_filters](#grid_map_filters).\n\n* *[interpolation_demo](grid_map_demos/src/InterpolationDemo.cpp)* shows the result of different interpolation methods on the resulting surface. The start the demo, use\n\n        roslaunch grid_map_demos interpolation_demo.launch\n\n<img src=\"grid_map_core/doc/interpolationSineWorld.gif\" width=\"256\" height=\"252\">\n<img src=\"grid_map_core/doc/interpolationGaussWorld.gif\" width=\"256\" height=\"252\">\n\nThe user can play with different worlds (surfaces) and different interpolation settings in the [`interpolation_demo.yaml`](grid_map_demos/config/interpolation_demo.yaml) file. The visualization displays the ground truth in green and yellow color. The interpolation result is shown in red and purple colors. Also, the demo computes maximal and average interpolation errors, as well as the average time required for a single interpolation query.\n\nGrid map features four different interpolation methods (in order of increasing accuracy and increasing complexity):\n* **NN** - Nearest Neighbour (fastest, but least accurate).\n* **Linear** - Linear interpolation.\n* **Cubic convolution** - Piecewise cubic interpolation. Implemented using the cubic convolution algorithm.\n* **Cubic** - Cubic interpolation (slowest, but most accurate).\n\nFor more details check the literature listed in  [`CubicInterpolation.hpp`](grid_map_core/include/grid_map_core/CubicInterpolation.hpp) file.\n\n### Conventions & Definitions\n\n[![Grid map layers](grid_map_core/doc/grid_map_layers.png)](grid_map_core/doc/grid_map_layers.pdf)\n\n[![Grid map conventions](grid_map_core/doc/grid_map_conventions.png)](grid_map_core/doc/grid_map_conventions.pdf)\n\n\n### Iterators\n\nThe grid map library contains various iterators for convenience.\n\nGrid map | Submap | Circle | Line | Polygon\n:---: | :---: | :---: | :---: | :---:\n[![Grid map iterator](grid_map_core/doc/iterators/grid_map_iterator_preview.gif)](grid_map_core/doc/iterators/grid_map_iterator.gif) | [![Submap iterator](grid_map_core/doc/iterators/submap_iterator_preview.gif)](grid_map_core/doc/iterators/submap_iterator.gif) | [![Circle iterator](grid_map_core/doc/iterators/circle_iterator_preview.gif)](grid_map_core/doc/iterators/circle_iterator.gif) | [![Line iterator](grid_map_core/doc/iterators/line_iterator_preview.gif)](grid_map_core/doc/iterators/line_iterator.gif) | [![Polygon iterator](grid_map_core/doc/iterators/polygon_iterator_preview.gif)](grid_map_core/doc/iterators/polygon_iterator.gif)\n__Ellipse__ | __Spiral__\n[![Ellipse iterator](grid_map_core/doc/iterators/ellipse_iterator_preview.gif)](grid_map_core/doc/iterators/ellipse_iterator.gif) | [![Spiral iterator](grid_map_core/doc/iterators/spiral_iterator_preview.gif)](grid_map_core/doc/iterators/spiral_iterator.gif)\n\nUsing the iterator in a `for` loop is common. For example, iterate over the entire grid map with the `GridMapIterator` with\n\n    for (grid_map::GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        cout << \"The value at index \" << (*iterator).transpose() << \" is \" << map.at(\"layer\", *iterator) << endl;\n    }\n\nThe other grid map iterators follow the same form. You can find more examples on how to use the different iterators in the *[iterators_demo](grid_map_demos/src/IteratorsDemo.cpp)* node.\n\nNote: For maximum efficiency when using iterators, it is recommended to locally store direct access to the data layers of the grid map with `grid_map::Matrix& data = map[\"layer\"]` outside the `for` loop:\n\n    grid_map::Matrix& data = map[\"layer\"];\n    for (GridMapIterator iterator(map); !iterator.isPastEnd(); ++iterator) {\n        const Index index(*iterator);\n        cout << \"The value at index \" << index.transpose() << \" is \" << data(index(0), index(1)) << endl;\n    }\n\nYou can find a benchmarking of the performance of the iterators in the `iterator_benchmark` node of the `grid_map_demos` package which can be run with\n\n    rosrun grid_map_demos iterator_benchmark\n\nBeware that while iterators are convenient, it is often the cleanest and most efficient to make use of the built-in [Eigen] methods. Here are some examples:\n\n* Setting a constant value to all cells of a layer:\n\n        map[\"layer\"].setConstant(3.0);\n\n* Adding two layers:\n\n        map[\"sum\"] = map[\"layer_1\"] + map[\"layer_2\"];\n\n* Scaling a layer:\n\n        map[\"layer\"] = 2.0 * map[\"layer\"];\n\n* Max. values between two layers:\n\n        map[\"max\"] = map[\"layer_1\"].cwiseMax(map[\"layer_2\"]);\n\n* Compute the root mean squared error:\n\n        map.add(\"error\", (map.get(\"layer_1\") - map.get(\"layer_2\")).cwiseAbs());\n        unsigned int nCells = map.getSize().prod();\n        double rootMeanSquaredError = sqrt((map[\"error\"].array().pow(2).sum()) / nCells);\n\n\n### Changing the Position of the Map\n\nThere are two different methods to change the position of the map:\n* `setPosition(...)`: Changes the position of the map without changing data stored in the map. This changes the corresponce between the data and the map frame.\n* `move(...)`: Relocates the grid map such that the corresponce between data and the map frame does not change. Data in the overlapping region before and after the position change remains stored. Data that falls outside of the map at its new position is discarded. Cells that cover previously unknown regions are emptied (set to nan). The data storage is implemented as two-dimensional circular buffer to minimize computational effort.\n\n`setPosition(...)` | `move(...)`\n:---: | :---:\n![Grid map iterator](grid_map_core/doc/setposition_method.gif) | ![Submap iterator](grid_map_core/doc/move_method.gif)|\n\n\n## Packages\n\n### grid_map_rviz_plugin\n\nThis [RViz] plugin visualizes a grid map layer as 3d surface plot (height map). A separate layer can be chosen as layer for the color information.\n\n![Grid map visualization in RViz](grid_map_rviz_plugin/doc/grid_map_rviz_plugin.png)\n\n\n### grid_map_visualization\n\nThis node subscribes to a topic of type [grid_map_msgs/GridMap] and publishes messages that can be visualized in [RViz]. The published topics of the visualizer can be fully configure with a YAML parameter file. Any number of visualizations with different parameters can be added. An example is [here](grid_map_demos/config/tutorial_demo.yaml) for the configuration file of the *tutorial_demo*.\n\nPoint cloud | Vectors | Occupancy grid | Grid cells\n--- | --- | --- | ---\n[![Point cloud](grid_map_visualization/doc/point_cloud_preview.jpg)](grid_map_visualization/doc/point_cloud.jpg) | [![Vectors](grid_map_visualization/doc/vectors_preview.jpg)](grid_map_visualization/doc/vectors.jpg) | [![Occupancy grid](grid_map_visualization/doc/occupancy_grid_preview.jpg)](grid_map_visualization/doc/occupancy_grid.jpg) | [![Grid cells](grid_map_visualization/doc/grid_cells_preview.jpg)](grid_map_visualization/doc/grid_cells.jpg)\n\n#### Parameters\n\n* **`grid_map_topic`** (string, default: \"/grid_map\")\n\n    The name of the grid map topic to be visualized. See below for the description of the visualizers.\n\n\n#### Subscribed Topics\n\n* **`/grid_map`** ([grid_map_msgs/GridMap])\n\n    The grid map to visualize.\n\n\n#### Published Topics\n\nThe published topics are configured with the [YAML parameter file](grid_map_demos/config/tutorial_demo.yaml). Possible topics are:\n\n* **`point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a point cloud. Select which layer to transform as points with the `layer` parameter.\n\n        name: elevation\n        type: point_cloud\n        params:\n         layer: elevation\n         flat: false # optional\n\n* **`flat_point_cloud`** ([sensor_msgs/PointCloud2])\n\n    Shows the grid map as a \"flat\" point cloud, i.e. with all points at the same height *z*. This is convenient to visualize 2d maps or images (or even video streams) in [RViz] with help of its `Color Transformer`. The parameter `height` determines the desired *z*-position of the flat point cloud.\n\n        name: flat_grid\n        type: flat_point_cloud\n        params:\n         height: 0.0\n\n    Note: In order to omit points in the flat point cloud from empty/invalid cells, specify the layers which should be checked for validity with `setBasicLayers(...)`.\n\n* **`vectors`** ([visualization_msgs/Marker])\n\n    Visualizes vector data of the grid map as visual markers. Specify the layers which hold the *x*-, *y*-, and *z*-components of the vectors with the `layer_prefix` parameter. The parameter `position_layer` defines the layer to be used as start point of the vectors.\n\n        name: surface_normals\n        type: vectors\n        params:\n         layer_prefix: normal_\n         position_layer: elevation\n         scale: 0.06\n         line_width: 0.005\n         color: 15600153 # red\n\n* **`occupancy_grid`** ([nav_msgs/OccupancyGrid])\n\n    Visualizes a layer of the grid map as occupancy grid. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bound with `data_min` and `data_max`.\n\n        name: traversability_grid\n        type: occupancy_grid\n        params:\n         layer: traversability\n         data_min: -0.15\n         data_max: 0.15\n\n* **`grid_cells`** ([nav_msgs/GridCells])\n\n    Visualizes a layer of the grid map as grid cells. Specify the layer to be visualized with the `layer` parameter, and the upper and lower bounds with `lower_threshold` and `upper_threshold`.\n\n        name: elevation_cells\n        type: grid_cells\n        params:\n         layer: elevation\n         lower_threshold: -0.08 # optional, default: -inf\n         upper_threshold: 0.08 # optional, default: inf\n\n* **`region`** ([visualization_msgs/Marker])\n\n    Shows the boundary of the grid map.\n\n        name: map_region\n        type: map_region\n        params:\n         color: 3289650\n         line_width: 0.003\n\n*Note: Color values are in RGB form as concatenated integers (for each channel value 0-255). The values can be generated like [this](http://www.wolframalpha.com/input/?i=BitOr%5BBitShiftLeft%5Br%2C16%5D%2C+BitShiftLeft%5Bg%2C8%5D%2C+b%5D+where+%7Br%3D0%2C+g%3D255%2C+b%3D0%7D) as an example for the color green (red: 0, green: 255, blue: 0).*\n\n### grid_map_filters\n\nThe *grid_map_filters* package containts several filters which can be applied a grid map to perform computations on the data in the layers. The grid map filters are based on [ROS Filters], which means that a chain of filters can be configured as a YAML file. Furthermore, additional filters can be written and made available through the ROS plugin mechanism, such as the [`InpaintFilter`](grid_map_cv/include/grid_map_cv/InpaintFilter.hpp) from the `grid_map_cv` package.\n\nSeveral basic filters are provided in the *grid_map_filters* package:\n\n* **`gridMapFilters/ThresholdFilter`**\n\n    Set values below/above a threshold to a specified value.\n\n        name: lower_threshold\n        type: gridMapFilters/ThresholdFilter\n        params:\n          layer: layer_name\n          lower_threshold: 0.0 # alternative: upper_threshold\n          set_to: 0.0 # # Other uses: .nan, .inf\n\n* **`gridMapFilters/MeanInRadiusFilter`**\n\n    Compute for each cell of a layer the mean value inside a radius.\n\n        name: mean_in_radius\n        type: gridMapFilters/MeanInRadiusFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.06 # in m.\n* **`gridMapFilters/MedianFillFilter`**\n\n    Compute for each _NaN_ cell of a layer the median (of finites) inside a patch with radius. \n    Optionally, apply median calculations for values that are already finite, the patch radius for these points is given by existing_value_radius. \n\n        name: median\n        type: gridMapFilters/MedianFillFilter\n        params:\n          input_layer: input\n          output_layer: output\n          fill_hole_radius: 0.11 # in m. \n          filter_existing_values: false # Default is false. If enabled it also does a median computation for existing values. \n          existing_value_radius: 0.2 # in m. Note that this option only has an effect if filter_existing_values is set true. \n    \n* **`gridMapFilters/NormalVectorsFilter`**\n\n    Compute the normal vectors of a layer in a map.\n\n        name: surface_normals\n        type: gridMapFilters/NormalVectorsFilter\n        params:\n          input_layer: input\n          output_layers_prefix: normal_vectors_\n          radius: 0.05\n          normal_vector_positive_axis: z\n\n* **`gridMapFilters/NormalColorMapFilter`**\n\n    Compute a new color layer based on normal vectors layers.\n\n        name: surface_normals\n        type: gridMapFilters/NormalColorMapFilter\n        params:\n          input_layers_prefix: normal_vectors_\n          output_layer: normal_color\n\n* **`gridMapFilters/MathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression with layers of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/MathExpressionFilter\n        params:\n          output_layer: output\n          expression: acos(normal_vectors_z) # Slope.\n          # expression: abs(elevation - elevation_smooth) # Surface roughness.\n          # expression: 0.5 * (1.0 - (slope / 0.6)) + 0.5 * (1.0 - (roughness / 0.1)) # Weighted and normalized sum.\n\n* **`gridMapFilters/SlidingWindowMathExpressionFilter`**\n\n    Parse and evaluate a mathematical matrix expression within a sliding window on a layer of a grid map. See [EigenLab] for the documentation of the expressions.\n\n        name: math_expression\n        type: gridMapFilters/SlidingWindowMathExpressionFilter\n        params:\n          input_layer: input\n          output_layer: output\n          expression: meanOfFinites(input) # Box blur\n          # expression: sqrt(sumOfFinites(square(input - meanOfFinites(input))) ./ numberOfFinites(input)) # Standard deviation\n          # expression: 'sumOfFinites([0,-1,0;-1,5,-1;0,-1,0].*elevation_inpainted)' # Sharpen with kernel matrix\n          compute_empty_cells: true\n          edge_handling: crop # options: inside, crop, empty, mean\n          window_size: 5 # in number of cells (optional, default: 3), make sure to make this compatible with the kernel matrix\n          # window_length: 0.05 # instead of window_size, in m\n\n* **`gridMapFilters/DuplicationFilter`**\n\n    Duplicate a layer of a grid map.\n\n        name: duplicate\n        type: gridMapFilters/DuplicationFilter\n        params:\n          input_layer: input\n          output_layer: output\n\n* **`gridMapFilters/DeletionFilter`**\n\n    Delete layers from a grid map.\n\n        name: delete\n        type: gridMapFilters/DeletionFilter\n        params:\n          layers: [color, score] # List of layers.\n\nAdditionally, the *grid_map_cv* package provides the following filters:\n\n* **`gridMapCv/InpaintFilter`**\n\n    Use OpenCV to inpaint/fill holes in a layer.\n\n        name: inpaint\n        type: gridMapCv/InpaintFilter\n        params:\n          input_layer: input\n          output_layer: output\n          radius: 0.05 # in m\n\n\n## Build Status\n\n### Devel Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idev__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idev__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldev__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldev__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdev__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdev__grid_map__ubuntu_bionic_amd64/) |\n| doc | [![Build Status](http://build.ros.org/buildStatus/icon?job=Idoc__grid_map__ubuntu_trusty_amd64)](http://build.ros.org/job/Idoc__grid_map__ubuntu_trusty_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kdoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Kdoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ldoc__grid_map__ubuntu_xenial_amd64)](http://build.ros.org/job/Ldoc__grid_map__ubuntu_xenial_amd64/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mdoc__grid_map__ubuntu_bionic_amd64)](http://build.ros.org/job/Mdoc__grid_map__ubuntu_bionic_amd64/) |\n\n### Release Job Status\n\n| | Indigo | Kinetic | Lunar | Melodic |\n| --- | --- | --- | --- | --- |\n| grid_map | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map__ubuntu_bionic_amd64__binary/) |\n| grid_map_core | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_core__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_core__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_core__ubuntu_bionic_amd64__binary/) |\n| grid_map_costmap_2d | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_costmap_2d__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_costmap_2d__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_costmap_2d__ubuntu_bionic_amd64__binary/) |\n| grid_map_cv | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_cv__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_cv__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_cv__ubuntu_bionic_amd64__binary/) |\n| grid_map_demos | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_demos__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_demos__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_demos__ubuntu_bionic_amd64__binary/) |\n| grid_map_filters | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_filters__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_filters__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_filters__ubuntu_bionic_amd64__binary/) |\n| grid_map_loader | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_loader__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_loader__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_loader__ubuntu_bionic_amd64__binary/) |\n| grid_map_msgs | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_msgs__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_msgs__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_msgs__ubuntu_bionic_amd64__binary/) |\n| grid_map_octomap | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_octomap__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_octomap__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_octomap__ubuntu_bionic_amd64__binary/) |\n| grid_map_pcl | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_pcl__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_pcl__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_pcl__ubuntu_bionic_amd64__binary/) |\n| grid_map_ros | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_ros__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_ros__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_ros__ubuntu_bionic_amd64__binary/) |\n| grid_map_rviz_plugin | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_rviz_plugin__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_rviz_plugin__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_rviz_plugin__ubuntu_bionic_amd64__binary/) |\n| grid_map_sdf | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_sdf__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_sdf__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_sdf__ubuntu_bionic_amd64__binary/) |\n| grid_map_visualization | [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary)](http://build.ros.org/job/Ibin_uT64__grid_map_visualization__ubuntu_trusty_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Kbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary)](http://build.ros.org/job/Lbin_uX64__grid_map_visualization__ubuntu_xenial_amd64__binary/) | [![Build Status](http://build.ros.org/buildStatus/icon?job=Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary)](http://build.ros.org/job/Mbin_uB64__grid_map_visualization__ubuntu_bionic_amd64__binary/) |\n\n\n## Bugs & Feature Requests\n\nPlease report bugs and request features using the [Issue Tracker](https://github.com/anybotics/grid_map/issues).\n\n[ROS]: http://www.ros.org\n[RViz]: http://wiki.ros.org/rviz\n[Eigen]: http://eigen.tuxfamily.org\n[OpenCV]: http://opencv.org/\n[OctoMap]: https://octomap.github.io/\n[PCL]: http://pointclouds.org/\n[costmap_2d]: http://wiki.ros.org/costmap_2d\n[grid_map_msgs/GridMapInfo]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMapInfo.html\n[grid_map_msgs/GridMap]: http://docs.ros.org/api/grid_map_msgs/html/msg/GridMap.html\n[grid_map_msgs/GetGridMap]: http://docs.ros.org/api/grid_map_msgs/html/srv/GetGridMap.html\n[sensor_msgs/PointCloud2]: http://docs.ros.org/api/sensor_msgs/html/msg/PointCloud2.html\n[visualization_msgs/Marker]: http://docs.ros.org/api/visualization_msgs/html/msg/Marker.html\n[geometry_msgs/PolygonStamped]: http://docs.ros.org/api/geometry_msgs/html/msg/PolygonStamped.html\n[nav_msgs/OccupancyGrid]: http://docs.ros.org/api/nav_msgs/html/msg/OccupancyGrid.html\n[nav_msgs/GridCells]: http://docs.ros.org/api/nav_msgs/html/msg/GridCells.html\n[ROS Filters]: http://wiki.ros.org/filters\n[EigenLab]: https://github.com/leggedrobotics/EigenLab\n"
 },
 {
  "repo": "emgucv/emgucv",
  "language": "C#",
  "readme_contents": "==================================================================\n\nA cross platform .Net wrapper for the OpenCV image-processing library. Allows OpenCV functions to be called from .NET compatible languages. The wrapper can be compiled by Visual Studio, Xamarin Studio and Unity, it can run on Windows, Linux, Mac OS, iOS and Android.\n\nPlease visit our project webpage for more information:\nhttp://www.emgu.com/wiki/index.php/Main_Page\n\nBuild instructions can be found here:\nhttp://www.emgu.com/wiki/index.php/Download_And_Installation#Building_from_Git\n"
 },
 {
  "repo": "atduskgreg/opencv-processing",
  "language": "Java",
  "readme_contents": "## OpenCV for Processing\n\n**A Processing library for the [OpenCV](http://opencv.org/) computer vision library.**\n\nOpenCV for Processing is based on OpenCV's official Java bindings. It attempts to provide convenient wrappers for common OpenCV functions that are friendly to beginners and feel familiar to the Processing environment.\n\nSee the included examples below for an overview of what's possible and links to the relevant example code. Complete documentation is available here:\n\n**[OpenCV for Processing reference](http://atduskgreg.github.io/opencv-processing/reference/)**\n\nOpenCV for Processing is based on the officially supported [OpenCV Java API](http://docs.opencv.org/java/), currently at version 2.4.5. In addition to using the wrapped functionality, you can import OpenCV modules and use any of its documented functions: [OpenCV javadocs](http://docs.opencv.org/java/). See the advanced examples (HistogramSkinDetection, DepthFromStereo, and Marker Detection) below for details. (This style of API was inspired by Kyle McDonald's [ofxCv addon](https://github.com/kylemcdonald/ofxCv) for OpenFrameworks.) \n\nContributions welcome.\n\n### Installing\n\nOpenCV for Processing currently supports Mac OSX, 32-bit and 64-bit Windows, 32- and 64-bit Linux. Android support is hopefully coming soon (pull requests welcome).\n\n_NB: When running on the Mac, make sure you have Processing set to 64-bit mode in the Preferences_\n\nSee [here](https://github.com/atduskgreg/opencv-processing/releases) for the latest release.\n\n### Examples\n\n#### LiveCamTest\n\nAccess a live camera and do image processing on the result, specifically face detection.\n\nCode: [LiveCamTest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/LiveCamTest/LiveCamTest.pde)\n\n_Note: There's a bug that prevents live camera access in current versions of Processing 2.0 on machines with a Retina display._\n\n#### FaceDetection\n\nDetect faces in images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8634017624/\" title=\"Screen Shot 2013-04-08 at 1.22.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8543/8634017624_35f7ef05ce.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-08 at 1.22.18 PM\"></a>\n\nCode: [FaceDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FaceDetection/FaceDetection.pde)\n\n#### BrightnessContrast\n\nAdjust the brightness and contrast of color and gray images.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9155239258/\" title=\"brightness and contrast by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2841/9155239258_41a7df36c6.jpg\" width=\"500\" height=\"358\" alt=\"brightness and contrast\"></a>\n\nCode: [BrightnessContrast.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightnessContrast/BrightnessContrast.pde)\n\n#### FilterImages\n\nBasic filtering operations on images: threshold, blur, and adaptive thresholds.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8643666252/\" title=\"Screen Shot 2013-04-12 at 1.42.30 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8240/8643666252_be0da1c751.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-12 at 1.42.30 PM\"></a>\n\nCode: [FilterImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FilterImages/FilterImages.pde)\n\n#### FindContours\n\nFind contours in images and calculate polygon approximations of the contours (i.e., the closest straight line that fits the contour).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9024663015/\" title=\"contours with polygon approximations by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3719/9024663015_f419b117b1.jpg\" width=\"500\" height=\"208\" alt=\"contours with polygon approximations\"></a>\n\nCode: [FindContours.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindContours/FindContours.pde)\n\n#### FindEdges\n\nThree different edge-detection techniques: Canny, Scharr, and Sobel.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8635989723/\" title=\"Screen Shot 2013-04-10 at 2.03.59 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8109/8635989723_170b69dca0.jpg\" width=\"500\" height=\"358\" alt=\"Screen Shot 2013-04-10 at 2.03.59 AM\"></a>\n\nCode: [FindEdges.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindEdges/FindEdges.pde)\n\n#### FindLines\n\nFind straight lines in the image using Hough line detection.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9263329608/\" title=\"Hough line detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3781/9263329608_735ce228bb.jpg\" width=\"486\" height=\"500\" alt=\"Hough line detection\"></a>\n\nCode: [HoughLineDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HoughLineDetection/HoughLineDetection.pde)\n\n#### BrightestPoint\n\nFind the brightest point in an image.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9199572469/\" title=\"finding the brightest point by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7407/9199572469_4a25c83062.jpg\" width=\"500\" height=\"366\" alt=\"finding the brightest point\"></a>\n\nCode: [BrightestPoint.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BrightestPoint/BrightestPoint.pde)\n\n#### RegionOfInterest\n\nAssign a sub-section (or Region of Interest) of the image to be processed. Video of this example in action here: [Region of Interest demo on Vimeo](https://vimeo.com/69009345).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9077805277/\" title=\"region of interest by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3795/9077805277_084d87a3a5.jpg\" width=\"500\" height=\"358\" alt=\"region of interest\"></a>\n\nCode: [RegionOfInterest.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/RegionOfInterest/RegionOfInterest.pde)\n\n#### ImageDiff\n\nFind the difference between two images in order to subtract the background or detect a new object in a scene.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8640005799/\" title=\"Screen Shot 2013-04-11 at 2.10.35 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8114/8640005799_44b48e01ae.jpg\" width=\"500\" height=\"409\" alt=\"Screen Shot 2013-04-11 at 2.10.35 PM\"></a>\n\nCode: [ImageDiff.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ImageDiff/ImageDiff.pde)\n\n#### DilationAndErosion\n\nThin (erode) and expand (dilate) an image in order to close holes. These are known as \"morphological\" operations.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9075875005/\" title=\"dilation and erosion by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2818/9075875005_8f7cde3ed7.jpg\" width=\"496\" height=\"500\" alt=\"dilation and erosion\"></a>\n\nCode: [DilationAndErosion.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DilationAndErosion/DilationAndErosion.pde)\n\n#### BackgroundSubtraction\n\nDetect moving objects in a scene. Use background subtraction to distinguish background from foreground and contour tracking to track the foreground objects.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9220336868/\" title=\"Background Subtraction by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7292/9220336868_bed3498528.jpg\" width=\"500\" height=\"369\" alt=\"Background Subtraction\"></a>\n\nCode: [BackgroundSubtraction.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/BackgroundSubtraction/BackgroundSubtraction.pde)\n\n\n#### WorkingWithColorImages\n\nDemonstration of what you can do color images in OpenCV (threshold, blur, etc) and what you can't (lots of other operations).\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9136033334/\" title=\"color operations: threshold and blur by atduskgreg, on Flickr\"><img src=\"http://farm6.staticflickr.com/5451/9136033334_3345dfa057.jpg\" width=\"500\" height=\"358\" alt=\"color operations: threshold and blur\"></a>\n\nCode: [WorkingWithColorImages.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WorkingWithColorImages/WorkingWithColorImages.pde)\n\n#### ColorChannels ####\n\nSeparate a color image into red, green, blue or hue, saturation, and value channels in order to work with the channels individually.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9246157901/\" title=\"ColorChannels by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2847/9246157901_08ccf19e7d.jpg\" width=\"488\" height=\"500\" alt=\"ColorChannels\"></a>\n\nCode: [ColorChannels](https://github.com/atduskgreg/opencv-processing/blob/master/examples/ColorChannels/ColorChannels.pde)\n\n#### FindHistogram\n\nDemonstrates use of the findHistogram() function and the Histogram class to get and draw histograms for grayscale and individual color channels.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9174190443/\" title=\"gray, red, green, blue histograms by atduskgreg, on Flickr\"><img src=\"http://farm8.staticflickr.com/7287/9174190443_224a740ce8.jpg\" width=\"500\" height=\"355\" alt=\"gray, red, green, blue histograms\"></a>\n\nCode: [FindHistogram.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/FindHistogram/FindHistogram.pde)\n\n#### HueRangeSelection\n\nDetect objects based on their color. Demonstrates the use of HSV color space as well as range-based image filtering.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9193745547/\" title=\"Hue-based color detection by atduskgreg, on Flickr\"><img src=\"http://farm4.staticflickr.com/3799/9193745547_8f09e55a39.jpg\" width=\"500\" height=\"397\" alt=\"Hue-based color detection\"></a>\n\nCode: [HueRangeSelection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HueRangeSelection/HueRangeSelection.pde)\n\n#### CalibrationDemo (in progress)\n\nAn example of the process involved in calibrating a camera. Currently only detects the corners in a chessboard pattern.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8706849024/\" title=\"Screen Shot 2013-05-04 at 2.03.23 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8267/8706849024_f2d938ec51.jpg\" width=\"500\" height=\"382\" alt=\"Screen Shot 2013-05-04 at 2.03.23 AM\"></a>\n\nCode: [CalibrationDemo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/CalibrationDemo/CalibrationDemo.pde)\n\n#### HistogramSkinDetection\n\nA more advanced example. Detecting skin in an image based on colors in a region of color space. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8707167599/\" title=\"Screen Shot 2013-05-04 at 2.25.18 PM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8135/8707167599_d38fbdfe30.jpg\" width=\"500\" height=\"171\" alt=\"Screen Shot 2013-05-04 at 2.25.18 PM\"></a>\n\nCode: [HistogramSkinDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/HistogramSkinDetection/HistogramSkinDetection.pde)\n\n#### DepthFromStereo\n\nAn advanced example. Calculates depth information from a pair of stereo images. Warning: uses un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642493130/\" title=\"Screen Shot 2013-04-12 at 2.27.30 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8260/8642493130_f99dd76f3d.jpg\" width=\"500\" height=\"404\" alt=\"Screen Shot 2013-04-12 at 2.27.30 AM\"></a>\n\nCode: [DepthFromStereo.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/DepthFromStereo/DepthFromStereo.pde)\n\n#### WarpPerspective (in progress)\n\nUn-distort an object that's in perspective. Coming to the real API soon.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/9279197332/\" title=\"Warp Perspective by atduskgreg, on Flickr\"><img src=\"http://farm3.staticflickr.com/2861/9279197332_ca6beb3760.jpg\" width=\"500\" height=\"416\" alt=\"Warp Perspective\"></a>\n\nCode: [WarpPerspective.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/WarpPerspective/WarpPerspective.pde)\n\n#### MarkerDetection\n\nAn in-depth advanced example. Detect a CV marker in an image, warp perspective, and detect the number stored in the marker. Many steps in the code. Uses many un-wrapped OpenCV objects and functions.\n\n<a href=\"http://www.flickr.com/photos/unavoidablegrain/8642309968/\" title=\"Screen Shot 2013-04-12 at 12.20.17 AM by atduskgreg, on Flickr\"><img src=\"http://farm9.staticflickr.com/8522/8642309968_257e397db2.jpg\" width=\"500\" height=\"225\" alt=\"Screen Shot 2013-04-12 at 12.20.17 AM\"></a>\n\nCode: [MarkerDetection.pde](https://github.com/atduskgreg/opencv-processing/blob/master/examples/MarkerDetection/MarkerDetection.pde)\n\n#### MorphologyOperations\n\nOpen and close an image, or do more complicated morphological transformations.\n\n<a href=\"https://flic.kr/p/tazj7r\" title=\"Morphology operations\"><img src=\"https://farm6.staticflickr.com/5340/17829980821_1734e8bab8_z_d.jpg\" width=\"640\" height=\"393\" alt=\"Morphology operations\"></a>\n\nCode: [MorphologyOperations.pde](examples/MorphologyOperations/MorphologyOperations.pde)\n"
 },
 {
  "repo": "go-opencv/go-opencv",
  "language": "Go",
  "readme_contents": "Go OpenCV binding\n==================\n\n[![Join the chat at https://gitter.im/lazywei/go-opencv](https://badges.gitter.im/lazywei/go-opencv.svg)](https://gitter.im/lazywei/go-opencv?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nA Golang binding for [OpenCV](http://opencv.org/).\n\nOpenCV 1.x C API bindings through CGO, and OpenCV 2+ C++ API ([`GoCV`](gocv/)) through SWIG.\n\n-------------------\n\n## Disclaimer\n\nThis is a fork of [chai's go-opencv](https://github.com/chai2010/opencv), which has only OpenCV1 support through CGO, and all credits for OpenCV1 wrapper (except files in `gocv/` folder) should mainly go to Chai. At the time of the fork (Dec 9, 2013) the original project was inactive and was hosted on Google Code, which was a little inconvenient for community contribution. Hence, I decided to host a fork on Github so people can contribute to this project easily. Since then, some patches were added by community, and some experimental OpenCV 2 wrappers were added as well. That means this fork went on a little bit divergent way comparing to the origin project. However, now the origin project seems to be active again and be moved to GitHub starting from Aug 25, 2014. Efforts to merge the two projects are very welcome.\n\n-------------------\n\n## Install\n\n### Linux & Mac OS X\n\nInstall Go and OpenCV, you might want to install both of them via `apt-get` or `homebrew`.\n\nYou can reference the [link](https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html) to install required packages.\n\n```\ngo get github.com/go-opencv/go-opencv\ncd $GOPATH/src/github.com/go-opencv/go-opencv/samples\ngo run hellocv.go\n```\n\n### Windows\n\n- Install Go and MinGw\n- install OpenCV-2.4.x to MinGW dir\n\n```\n# libopencv*.dll --> ${MinGWRoot}\\bin\n# libopencv*.lib --> ${MinGWRoot}\\lib\n# include\\opencv --> ${MinGWRoot}\\include\\opencv\n# include\\opencv2 --> ${MinGWRoot}\\include\\opencv2\n\ngo get github.com/go-opencv/go-opencv\ncd ${GoOpenCVRoot}/trunk/samples && go run hellocv.go\n```\n\n## [WIP] OpenCV2 (GoCV)\n\nAfter OpenCV 2.x+, the core team no longer develop and maintain C API. Therefore, CGO will not be used in CV2 binding. Instead, we are using SWIG for wrapping. The support for OpenCV2 is currently under development, and whole code will be placed under `gocv` package.\n\nIf you want to use CV2's API, please refer to the code under `gocv/` directory. There is no too many documents for CV2 wrapper yet, but you can still find the example usages in `*_test.go`.\n\nPlease also note that the basic data structures in OpenCV (e.g., `cv::Mat`, `cv::Point3f`) are wrapped partially for now. For more detail on how to use these types, please refer to [GoCV's README](gocv/README.md).\n\n*Requirement*: we will build the wrappers based on [mat64](https://godoc.org/github.com/gonum/matrix/mat64), given it is much easier to manipulate the underlaying data. In most case, it is not necessary to access the original CV data, e.g., `cv::Mat` can be converted from/to `*mat64.Dense`.\n\n## Example\n\n### OpenCV2's initCameraMatrix2D\n\n```go\npackage main\n\nimport . \"github.com/go-opencv/go-opencv/gocv\"\nimport \"github.com/gonum/matrix/mat64\"\n\nfunc main() {\n\n\tobjPts := mat64.NewDense(4, 3, []float64{\n\t\t0, 25, 0,\n\t\t0, -25, 0,\n\t\t-47, 25, 0,\n\t\t-47, -25, 0})\n\n\timgPts := mat64.NewDense(4, 2, []float64{\n\t\t1136.4140625, 1041.89208984,\n\t\t1845.33190918, 671.39581299,\n\t\t302.73373413, 634.79998779,\n\t\t1051.46154785, 352.76107788})\n\n\tcamMat := GcvInitCameraMatrix2D(objPts, imgPts)\n\tfmt.Println(camMat)\n}\n```\n\n\n### Resizing\n\n```go\npackage main\n\nimport opencv \"github.com/go-opencv/go-opencv/opencv\"\n\nfunc main() {\n\tfilename := \"bert.jpg\"\n\tsrcImg := opencv.LoadImage(filename)\n\tif srcImg == nil {\n\t\tpanic(\"Loading Image failed\")\n\t}\n\tdefer srcImg.Release()\n\tresized1 := opencv.Resize(srcImg, 400, 0, 0)\n\tresized2 := opencv.Resize(srcImg, 300, 500, 0)\n\tresized3 := opencv.Resize(srcImg, 300, 500, 2)\n\topencv.SaveImage(\"resized1.jpg\", resized1, nil)\n\topencv.SaveImage(\"resized2.jpg\", resized2, nil)\n\topencv.SaveImage(\"resized3.jpg\", resized3, nil)\n}\n```\n\n### Webcam\n\nYet another cool example is created by @saratovsource which demos how to use webcam:\n\n```\ncd samples\ngo run webcam.go\n```\n\n### More\n\nYou can find more samples at: https://github.com/go-opencv/go-opencv/tree/master/samples\n\n## How to contribute\n\n- Fork this repo\n- Clone the main repo, and add your fork as a remote\n\n  ```\n  git clone https://github.com/go-opencv/go-opencv.git\n  cd go-opencv\n  git remote rename origin upstream\n  git remote add origin https://github.com/your_github_account/go-opencv.git\n  ```\n\n- Create new feature branch\n\n  ```\n  git checkout -b your-feature-branch\n  ```\n\n- Commit your change and push it to your repo \n\n  ```\n  git commit -m 'new feature'\n  git push origin your-feature-branch\n  ```\n\n- Open a pull request!\n\n"
 },
 {
  "repo": "yangkun19921001/Blog",
  "language": "HTML",
  "readme_contents": "<p align=\"center\">\n<a href=\"https://github.com/yangkun19921001/Blog\" target=\"_blank\">\n\t<img src=\"https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200308171526.png\" width=\"300px\"/>\n</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"XXX\"><img src=\"https://img.shields.io/badge/\u9605\u8bfb-read-brightgreen.svg\" alt=\"\u5728\u7ebf\u9605\u8bfb\"></a>\n</p>\n\n\n\n   * [Blog](#blog)\n      * [\u9762\u8bd5](#\u9762\u8bd5)\n      * [Flutter \u7cfb\u5217](#flutter-\u7cfb\u5217)\n      * [\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217](#\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217)\n      * [Java \u6e90\u7801\u5206\u6790](#java-\u6e90\u7801\u5206\u6790)\n      * [Android \u6e90\u7801\u5206\u6790](#android-\u6e90\u7801\u5206\u6790)\n      * [\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790](#\u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790)\n      * [\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217](#\u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217)\n      * [\u8bbe\u8ba1\u6a21\u5f0f](#\u8bbe\u8ba1\u6a21\u5f0f)\n      * [\u9ad8\u7ea7 UI \u7cfb\u5217](#\u9ad8\u7ea7-ui-\u7cfb\u5217)\n      * [\u97f3\u89c6\u9891](#\u97f3\u89c6\u9891)\n      * [\u5f00\u6e90\u9879\u76ee](#\u5f00\u6e90\u9879\u76ee)\n      * [\u82f1\u8bed](#\u82f1\u8bed)\n      * [\u5173\u4e8e\u6211](#\u5173\u4e8e\u6211)\n\n# Blog\n\n\u7528\u4e8e\u8bb0\u5f55\u751f\u6d3b\u3001\u5b66\u4e60\u3001\u5de5\u4f5c\u7b49\u5185\u5bb9\u3002\n\n## \u9762\u8bd5\n\n- [Android \u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5b9d\u5178](https://github.com/yangkun19921001/Blog/blob/master/\u7b14\u8bd5\u9762\u8bd5/Android\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u9762\u8bd5\u5fc5\u5907/README.md)\n- [1307 \u9875\u5b57\u8282\u8df3\u52a8 Android \u9762\u8bd5\u5168\u5957\u771f\u9898\u89e3\u6790\u5728\u4e92\u8054\u7f51\u706b\u4e86 \uff0c\u5b8c\u6574\u7248\u5f00\u653e\u4e0b\u8f7d](https://mp.weixin.qq.com/s/Crty_REXRVMEhI20XAeLGw)\n\n## Flutter \u7cfb\u5217\n\n- [Google \u4e3a\u4ec0\u4e48\u4ee5 Flutter \u4f5c\u4e3a\u539f\u751f\u7a81\u7834\u53e3](https://juejin.im/post/5c91f0f25188256b7463868e)\n- [Flutter (\u4e00) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3(\u53d8\u91cf\u3001\u5185\u7f6e\u7c7b\u578b\u3001\u51fd\u6570\u3001\u64cd\u4f5c\u7b26\u3001\u6d41\u7a0b\u63a7\u5236\u8bed\u53e5)](https://juejin.im/post/5c91ed15518825573578c31f)\n- [Flutter (\u4e8c) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 \uff08\u5f02\u5e38,\u7c7b,Mixin, \u6cdb\u578b,\u5e93\uff09](https://juejin.im/post/5c939b275188252d863cc797)\n- [Flutter (\u4e09) Dart \u8bed\u8a00\u57fa\u7840\u8be6\u89e3 (\u5f02\u6b65,\u751f\u6210\u5668,\u9694\u79bb,\u5143\u6570\u636e,\u6ce8\u91ca)](https://juejin.im/post/5c962b356fb9a0710e47e361)\n- [Flutter (\u56db) \u57fa\u7840 Widgets\u3001Material Components Widget \u5168\u9762\u4ecb\u7ecd](https://juejin.im/post/5cbedc816fb9a03202221a37)\n\n## \u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5\u7cfb\u5217\n\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e00)\u5192\u6ce1\u4e0e\u9009\u62e9\u6392\u5e8f](https://juejin.im/post/5c9442cb5188252da9013153)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e8c)\u7ebf\u6027\u8868\u7684\u94fe\u5f0f\u5b58\u50a8\u7ed3\u6784](https://juejin.im/post/5c9449dd5188252da22508e3)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e09)\u6808\u4e0e\u6808\u7684\u5e94\u7528](https://juejin.im/post/5c9453965188252db02e4be6)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u56db)\u54c8\u5e0c\u8868\u7684\u601d\u60f3\u548c\u4e8c\u53c9\u6811\u5165\u95e8](https://juejin.im/post/5c9456f25188252d971438a9)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e94) \u5206\u6cbb\u6cd5 (\u4e8c\u5206\u67e5\u627e\u3001\u5feb\u901f\u6392\u5e8f\u3001\u5f52\u5e76\u6392\u5e8f)](https://juejin.im/post/5c945c245188252d863cc969)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u516d)\u4e8c\u53c9\u6392\u5e8f\u6811](https://juejin.im/post/5c9460e25188252d971438c4)\n- [\u4ece\u96f6\u5f00\u59cb\u5b66\u6570\u636e\u7ed3\u6784\u548c\u7b97\u6cd5(\u4e03) huffman \u6811\u4e0e AVL \u6811](https://juejin.im/post/5c9464515188252d7e34df85)\n\n## Java \u6e90\u7801\u5206\u6790\n\n- [\u6e90\u7801\u5206\u6790 (\u4e00) ArrayList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c94695c5188252daa18f487)\n- [\u6e90\u7801\u5206\u6790 (\u4e8c) LinkedList JDK 1.8 \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946b555188252d7941fef2)\n- [\u6e90\u7801\u5206\u6790 (\u4e09) Stack \u6e90\u7801\u5206\u6790](https://juejin.im/post/5c946d525188252d5f0fd9ee)\n- [\u9762\u8bd5\u5b98: \u6211\u5fc5\u95ee\u7684\u5bb9\u5668\u77e5\u8bc6\u70b9!](https://juejin.im/post/5e88afce518825085d6ced2e)\n\n## Android \u6e90\u7801\u5206\u6790\n\n- [\u4ece setContentView \u5165\u53e3\uff0c\u5168\u65b9\u4f4d\u5206\u6790 LayoutInflater](https://juejin.im/post/5d6a7f2be51d4561e43a6ce8)\n- [\u5206\u6790\u5e7f\u64ad \u7684 registerReceiver\u3001sendBroadcast\u3001 onReceive \u7cfb\u7edf\u5230\u5e95\u505a\u4e86\u4ec0\u4e48?](https://juejin.im/post/5d752aad518825346e5f2b31)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e00) SystemServer \u8fdb\u7a0b\u542f\u52a8](https://juejin.im/post/5db3f95ee51d4529e83947f9)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e8c) Launcher \u542f\u52a8](https://juejin.im/post/5db5565cf265da4d0f14053c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e09) \u5e94\u7528\u7a0b\u5e8f\u8fdb\u7a0b\u521b\u5efa\u5230\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u7684\u8fc7\u7a0b](https://juejin.im/post/5db599bc6fb9a0203b234b08)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u56db) Activity \u542f\u52a8](https://juejin.im/post/5db85da4e51d4529f73e27fb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e94) Service \u542f\u52a8](https://juejin.im/post/5dbb0507f265da4cf406f735)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516d) BroadcastReceiver \u542f\u52a8](https://juejin.im/post/5dbd5144e51d456eec1830af)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e03) ContentProvider \u542f\u52a8](https://juejin.im/post/5dbe8e6ce51d456f0006634a)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u516b) ActivityManagerService](https://juejin.im/post/5dc4339c5188254e7a15585c)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u4e5d) WindowManager](https://juejin.im/post/5dc7d729f265da4cf85d7feb)\n- [Android 8.0 \u6e90\u7801\u5206\u6790 (\u5341) WindowManagerService \u7684\u7a97\u53e3\u7ba1\u7406](https://juejin.im/post/5dcab476f265da4d0a68e3ab)\n\n## \u7b2c\u4e09\u65b9\u6d41\u884c\u6846\u67b6\u6e90\u7801\u5206\u6790\n\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e00) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d89e9c051882509662c5620)\n- [Android \u56fe\u7247\u52a0\u8f7d\u6846\u67b6 Glide 4.9.0 (\u4e8c) \u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Glide \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5d8c83836fb9a04dec52f19d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Rxjava2 \u7684\u57fa\u672c\u6267\u884c\u6d41\u7a0b\u3001\u7ebf\u7a0b\u5207\u6362\u539f\u7406](https://juejin.im/post/5d9b489251882560e87e620e)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e00) \u540c\u6b65\u3001\u5f02\u6b65\u6267\u884c\u6d41\u7a0b](https://juejin.im/post/5d9ef57c51882514316fe33a)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e8c) \u62e6\u622a\u5668\u7684\u9b45\u529b](https://juejin.im/post/5da306965188252ba420a15d)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 OKHttp3 (\u4e09) \u7f13\u5b58\u7b56\u7565](https://juejin.im/post/5da5dcd551882544432558f8)\n- [\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5206\u6790 Retrofit \u7f51\u7edc\u8bf7\u6c42\uff0c\u5305\u542b RxJava + Retrofit + OKhttp \u8bf7\u6c42\u8bb2\u89e3](https://juejin.im/post/5da802d051882508866e9463)\n\n## \u6027\u80fd\u4f18\u5316\u7cfb\u5217\n\n- [\u6027\u80fd\u4f18\u5316(\u4e00)APP \u542f\u52a8\u4f18\u5316\uff08\u4e0d\u6562\u8bf4\u79d2\u5f00\uff0c\u4f46\u662f\u6700\u7ec8\u4f18\u5316\u5b8c\u771f\u4e0d\u5230 1s\uff09](https://juejin.im/post/5cc19374e51d456e781f2036)\n- [\u6027\u80fd\u4f18\u5316(\u4e8c) UI \u7ed8\u5236\u4f18\u5316](https://juejin.im/post/5cc2dfc7e51d456e845b4260)\n- [\u6027\u80fd\u4f18\u5316(\u4e09)\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0,\u81f3\u5c11\u89e3\u51b3 APP \u4e2d 90 % \u7684\u5185\u5b58\u5f02\u5e38\u95ee\u9898](https://juejin.im/post/5cd82a3ee51d456e781f20ce)\n- [\u6027\u80fd\u4f18\u5316(\u56db) ubuntu \u5b8c\u7f8e\u7f16\u8bd1 libjpeg \u56fe\u50cf\u538b\u7f29\u5e93\uff0c\u5ab2\u7f8e\u5fae\u4fe1\u56fe\u7247\u538b\u7f29\u7b97\u6cd5](https://juejin.im/post/5ce15d0ce51d45106e5e6dac)\n- [\u6027\u80fd\u4f18\u5316 (\u4e94) \u957f\u56fe\u4f18\u5316\uff0c\u4eff\u5fae\u535a\u52a0\u8f7d\u957f\u56fe\u65b9\u5f0f](https://juejin.im/post/5ce96da06fb9a07ee4633f50)\n- [\u6027\u80fd\u4f18\u5316 (\u516d) \u8001\u677f\u95ee\u4f60\u54b1\u4eec APP \u8017\u7535\u91cf\uff0c\u770b\u5b8c\u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u80fd\u77e5\u9053\u8fd8\u80fd\u505a\u51fa\u5bf9\u5e94\u4f18\u5316\u3002](https://juejin.im/post/5ce9088f6fb9a07ee4633ef3)\n- [\u6027\u80fd\u4f18\u5316 (\u4e03) APK \u52a0\u56fa\u4e4b Dex \u52a0\u89e3\u5bc6\uff0c\u53cd\u7f16\u8bd1\u90fd\u770b\u4e0d\u5230\u9879\u76ee\u4e3b\u8981\u4ee3\u7801\u3002](https://juejin.im/post/5cf3ee295188256aa76bb1e1)\n- [\u6027\u80fd\u4f18\u5316 (\u516b) APK \u52a0\u56fa\u4e4b\u52a8\u6001\u66ff\u6362 Application](https://juejin.im/post/5cf69d30f265da1b897abd53)\n- [\u6027\u80fd\u4f18\u5316 (\u4e5d) APP \u7a33\u5b9a\u6027\u4e4b\u70ed\u4fee\u590d\u539f\u7406\u63a2\u7d22](https://juejin.im/post/5cfce989f265da1b6c5f6991)\n- [\u6027\u80fd\u4f18\u5316 (\u5341) APP \u6301\u7eed\u8fd0\u884c\u4e4b\u8fdb\u7a0b\u4fdd\u6d3b\u5b9e\u73b0](https://juejin.im/post/5cffe4d4f265da1b695d55d4)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e00) ProGuard \u5bf9\u4ee3\u7801\u548c\u8d44\u6e90\u538b\u7f29](https://juejin.im/post/5d05dab06fb9a07ea9446e21)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e8c) APK \u6781\u9650\u538b\u7f29(\u8d44\u6e90\u8d8a\u591a,\u6548\u679c\u8d8a\u663e\u8457)](https://juejin.im/post/5d0627f7f265da1bd4247e76)\n- [\u6027\u80fd\u4f18\u5316 (\u5341\u4e09) \u6709\u4e86 breakpad , native \u5d29\u6e83\u518d\u4e5f\u4e0d\u6015\u4e86](https://juejin.im/post/5d811f82518825446d0d15e1)\n- [\u9762\u8bd5\u5b98: \u8bf4\u4e00\u4e0b\u4f60\u505a\u8fc7\u54ea\u4e9b\u6027\u80fd\u4f18\u5316?](https://juejin.im/post/5e7f12ba518825736d2780a0)\n\n## \u79fb\u52a8\u67b6\u6784\u5e08\u7cfb\u5217\n\n- [\u79fb\u52a8\u67b6\u6784 (\u4e00) \u67b6\u6784\u7b2c\u4e00\u6b65\uff0c\u5b66\u4f1a\u753b\u5404\u79cd UML \u56fe\u3002](https://juejin.im/post/5d2e048cf265da1b9163c7c8)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e8c) Android \u4e2d Handler \u67b6\u6784\u5206\u6790\uff0c\u5e76\u5b9e\u73b0\u81ea\u5df1\u7b80\u6613\u7248\u672c Handler \u6846\u67b6](https://juejin.im/post/5d30b4a8f265da1b855c8f45)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e09) AMS \u6e90\u7801\u5206\u6790](https://juejin.im/post/5d3463b4e51d45109725ff47)\n- [\u79fb\u52a8\u67b6\u6784 (\u56db) EventBus 3.1.1 \u6e90\u7801\u5206\u6790\u53ca\u5b9e\u73b0\u81ea\u5df1\u7684\u8f7b\u91cf\u7ea7 EventBus \u6846\u67b6\uff0c\u6839\u636e TAG \u53d1\u9001\u63a5\u6536\u4e8b\u4ef6\u3002](https://juejin.im/post/5d3c5b965188252c9c52beba)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e94) \u4ec5\u4ec5\u5bf9 Java Bean \u7684\u64cd\u4f5c\uff0c\u5c31\u80fd\u5b8c\u6210\u5bf9\u6570\u636e\u6301\u4e45\u5316\u3002](https://juejin.im/post/5d49a6c9518825056564a074)\n- [\u79fb\u52a8\u67b6\u6784 (\u516d) \u8f7b\u91cf\u7ea7\u8fdb\u7a0b\u95f4\u901a\u4fe1\u6846\u67b6\u8bbe\u8ba1](https://juejin.im/post/5d4fe70d518825168d37a740)\n- [\u79fb\u52a8\u67b6\u6784 (\u4e03) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u7ec4\u4ef6\u5316\u6846\u67b6\u6a21\u578b](https://juejin.im/post/5d5bcb85f265da03e369839d)\n- [\u79fb\u52a8\u67b6\u6784 (\u516b) \u4eba\u4eba\u90fd\u80fd\u770b\u5f97\u61c2\u7684\u52a8\u6001\u5316\u52a0\u8f7d\u63d2\u4ef6\u6280\u672f\u6a21\u578b\u5b9e\u73b0](https://juejin.im/post/5d6246d36fb9a06b0f23ed6e)\n\n## \u8bbe\u8ba1\u6a21\u5f0f\n\n- [\u901a\u8fc7\u4ee3\u7801\u793a\u4f8b\u6765\u5b66\u4e60\u9762\u5411\u5bf9\u8c61\u516d\u5927\u539f\u5219](https://juejin.im/post/5d669bfc6fb9a06b1b19d25e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e00) \u901a\u8fc7\u7406\u8bba + \u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d\u5355\u4f8b\u6a21\u5f0f\u6765\u5b66\u4e60\u5355\u4f8b](https://juejin.im/post/5d6a8121e51d4561e6237193)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e8c ) \u7ed3\u5408\u4ee3\u7801\u793a\u4f8b + Android \u6e90\u7801\u4e2d Builder \u6765\u5b66\u4e60\u5efa\u9020\u8005\u6a21\u5f0f](https://juejin.im/post/5d6bcd0ee51d4561d41d2e36)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u4e09 ) \u539f\u578b\u6a21\u5f0f](https://juejin.im/post/5d6e7eaa5188250d9432b463)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u56db ) \u5de5\u5382\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7125d5f265da03d7283ce9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e94) \u62bd\u8c61\u5de5\u5382\u6a21\u5f0f](https://juejin.im/post/5d71278ef265da03d063c265)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516d) \u7b56\u7565\u6a21\u5f0f](https://juejin.im/post/5d7273abf265da03b31bf1ec)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e03) \u72b6\u6001\u6a21\u5f0f](https://juejin.im/post/5d738f40e51d4561c41fb8a6)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u516b) \u8d23\u4efb\u94fe\u6a21\u5f0f](https://juejin.im/post/5d749589f265da03d871e36e)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u4e5d) \u89c2\u5bdf\u8005\u6a21\u5f0f](https://juejin.im/post/5d7501f36fb9a06ac93cf457)\n- [\u8bbe\u8ba1\u6a21\u5f0f ( \u5341 ) \u5907\u5fd8\u5f55\u6a21\u5f0f](https://juejin.im/post/5d77ab1de51d4561c83e7cd9)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e00) \u8fed\u4ee3\u5668\u6a21\u5f0f](https://juejin.im/post/5d791e176fb9a06ae61ae3cc)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e8c) \u6a21\u677f\u65b9\u6cd5\u6a21\u5f0f](https://juejin.im/post/5d7a759fe51d4561c02a25db)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e09) \u8bbf\u95ee\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b24b1e51d4561d41d2e96)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u56db) \u4e2d\u4ecb\u8005\u6a21\u5f0f](https://juejin.im/post/5d7b63b3e51d4561ea1a94ed)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e94) \u4ee3\u7406\u6a21\u5f0f](https://juejin.im/post/5d7c6bc7f265da03f3338254)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516d) \u7ec4\u5408\u6a21\u5f0f](https://juejin.im/post/5d7cbda7f265da03d2116f64)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u4e03) \u9002\u914d\u5668\u3001\u88c5\u9970\u3001\u4eab\u5143\u6a21\u5f0f](https://juejin.im/post/5d7dfff751882539aa5ad79c)\n- [\u8bbe\u8ba1\u6a21\u5f0f (\u5341\u516b) \u5916\u89c2\u3001\u6865\u63a5\u6a21\u5f0f](https://juejin.im/post/5d7e01f4f265da03b5747aac)\n\n## \u9ad8\u7ea7 UI \u7cfb\u5217\n\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e00) View \u7684\u57fa\u7840\u77e5\u8bc6\u4f60\u5fc5\u987b\u77e5\u9053](https://juejin.im/post/5dcff9d3f265da0bd20af0da)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e8c) \u6df1\u5165\u7406\u89e3 Android 8.0 View \u89e6\u6478\u4e8b\u4ef6\u5206\u53d1\u673a\u5236](https://juejin.im/post/5dd7a4796fb9a07a8f412d17)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e09) \u7406\u89e3 View \u5de5\u4f5c\u539f\u7406\u5e76\u5e26\u4f60\u5165\u81ea\u5b9a\u4e49 View \u95e8](https://juejin.im/post/5ddff234518825793218d2e4)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u56db) Paint \u6e32\u67d3/\u6ee4\u955c/xfermode \u4f7f\u7528](https://juejin.im/post/5de36c43f265da05de5881e8)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e94) \u770b\u5b8c\u8be5\u7bc7\u6587\u7ae0 Canvas \u4f60\u5e94\u8be5\u4f1a\u4e86](https://juejin.im/post/5de514fcf265da060115e02d)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u516d) PathMeasure \u5236\u4f5c\u8def\u5f84\u52a8\u753b](https://juejin.im/post/5de789dce51d4557e76a4a39)\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e03) SVG \u57fa\u7840\u4f7f\u7528 + \u7ed8\u5236\u4e2d\u56fd\u5730\u56fe](https://juejin.im/post/5deb6d41e51d4558052f16ac)\n\n## \u97f3\u89c6\u9891\n\n- [\u97f3\u89c6\u9891\u4e4b\u8fdb\u7a0b\u95f4\u4f20\u9012 YUV \u683c\u5f0f\u89c6\u9891\u6d41\uff0c\u89e3\u51b3\u4e0d\u80fd\u540c\u65f6\u8c03\u7528 Camera \u95ee\u9898](https://juejin.im/post/5cf345ddf265da1b8c19731a)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e00) C \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5df8c917f265da339772a5d1)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e8c) C++ \u8bed\u8a00\u5165\u95e8](https://juejin.im/post/5e1347775188253a6c3966fd)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e09) JNI \u4ece\u5165\u95e8\u5230\u638c\u63e1](https://juejin.im/post/5e1606e0f265da5d2d0ffbdb)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u56db) \u4ea4\u53c9\u7f16\u8bd1\u52a8\u6001\u5e93\u3001\u9759\u6001\u5e93\u7684\u5165\u95e8\u5b66\u4e60](https://juejin.im/post/5e1ad6806fb9a02ff076e103)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e94) Shell \u811a\u672c\u5165\u95e8](https://juejin.im/post/5e1c0a4ce51d451c8771c487)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516d) FFmpeg 4.2.2 \u4ea4\u53c9\u7f16\u8bd1](https://juejin.im/post/5e1eace16fb9a02fec66474e)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e03) \u638c\u63e1\u97f3\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 AudioTrack\u3001OpenSL ES \u6e32\u67d3 PCM \u6570\u636e](https://juejin.im/post/5e3fcc5bf265da57685db2a9)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u516b) \u638c\u63e1\u89c6\u9891\u57fa\u7840\u77e5\u8bc6\u5e76\u4f7f\u7528 OpenGL ES 2.0 \u6e32\u67d3 YUV \u6570\u636e](https://juejin.im/post/5e4581476fb9a07cd80f15e0)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u4e5d) \u4ece 0 ~ 1 \u5f00\u53d1\u4e00\u6b3e Android \u7aef\u64ad\u653e\u5668(\u652f\u6301\u591a\u534f\u8bae\u7f51\u7edc\u62c9\u6d41/\u672c\u5730\u6587\u4ef6)](https://juejin.im/post/5e495ec1e51d452713551017)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341) \u57fa\u4e8e Nginx \u642d\u5efa(rtmp\u3001http)\u76f4\u64ad\u670d\u52a1\u5668](https://juejin.im/post/5e4ec66c5188254967067502)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e00) Android \u7aef\u5b9e\u73b0 rtmp \u63a8\u6d41](https://juejin.im/post/5e5d17276fb9a07cc01a29d3)\n- [\u97f3\u89c6\u9891\u5b66\u4e60 (\u5341\u4e8c) \u57fa\u4e8e FFmpeg + OpenSLES \u5b9e\u73b0\u97f3\u9891\u4e07\u80fd\u64ad\u653e\u5668](https://juejin.im/post/5eb1880be51d454de7772152)\n- [WebRTC \u5b66\u4e60\u8bb0\u5f55 (\u4e00) \u4e91\u670d\u52a1\u5668\u642d\u5efa AppRTC \u73af\u5883](https://juejin.im/post/5e8f4a606fb9a03c7a331bd3)\n\n## \u5f00\u6e90\u9879\u76ee\n\n- [\u70ed\u4fee\u590d DexEncryptionDecryption](https://github.com/yangkun19921001/DexEncryptionDecryption)\n- [\u56fe\u7247\u538b\u7f29 LIBJPEG_SAMPLE](https://github.com/yangkun19921001/LIBJPEG_SAMPLE)\n- [\u8fdb\u7a0b\u4fdd\u6d3b KeepAlive](https://github.com/yangkun19921001/KeepAlive)\n- [Java/Native \u5f02\u5e38\u6355\u83b7 YKCrash](https://github.com/yangkun19921001/YKCrash)\n- [\u63d2\u4ef6\u5316 YKPluginAPK](https://github.com/yangkun19921001/YKPluginAPK)\n- [YUV \u64ad\u653e YUVPlay](https://github.com/yangkun19921001/YUVPlay)\n- [\u9632\u5fae\u535a\u957f\u56fe\u52a0\u8f7d long_picture_view](https://github.com/yangkun19921001/long_picture_view)\n- [\u8fdb\u7a0b\u95f4\u901a\u4fe1 YKProBus](https://github.com/yangkun19921001/YKProBus)\n- [EventBus YEventBus](https://github.com/yangkun19921001/YEventBus)\n- [\u8fdb\u7a0b\u95f4\u5927\u6570\u636e\u4f20\u8f93 MemoryFileWriteBytesYUV](https://github.com/yangkun19921001/MemoryFileWriteBytesYUV)\n- [ Kotlin GitHub App](https://juejin.im/post/5dc294d5f265da4d4434afc9)\n- [Android rtmp\u63a8\u6d41\u3001\u62c9\u6d41](https://github.com/yangkun19921001/NDK_AV_SAMPLE/tree/master/ykav_common/src/main/cpp)\n\n## \u82f1\u8bed\n\n\u5f85\u66f4\u65b0...\n\n## \u5173\u4e8e\u6211\n\n- Email: yang1001yk@gmail.com\n- [\u4e2a\u4eba\u535a\u5ba2](https://www.devyk.top/)\n- [\u6398\u91d1](https://juejin.im/user/578259398ac2470061f3a3fb)\n- [GitHub](https://github.com/yangkun19921001)\n\n\u626b\u7801\u5173\u6ce8\u6211\u7684\u516c\u4f17\u53f7\uff0c\u8ba9\u6211\u4eec\u79bb\u5f97\u66f4\u8fdb\u4e00\u4e9b!\n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200328235020.jpg)\n\n\n\n## \u8d5e\u8d4f\n\n\u5982\u679c\u8fd9\u4e2a\u9762\u8bd5\u9898\u5e93\u5bf9\u4f60\u5f88\u6709\u5e2e\u52a9\uff0c\u53ef\u4ee5\u626b\u63cf\u4e0b\u65b9\u4e8c\u7ef4\u7801\u7ed9\u4f5c\u8005\u4e00\u70b9\u9f13\u52b1\u3002\u91d1\u989d\u968f\u610f, \u8c22\u8c22! \n\n![](https://devyk.oss-cn-qingdao.aliyuncs.com/blog/20200330103229.png)\n\n\n\n\n\n\n\n"
 },
 {
  "repo": "Image-Py/imagepy",
  "language": "Python",
  "readme_contents": "# Introduction\n\nImagePy is an open source image processing framework written in Python. Its UI interface, image data structure and table data structure are wxpython-based, Numpy-based and pandas-based respectively. Furthermore, it supports any plug-in based on Numpy and pandas, which can talk easily between scipy.ndimage, scikit-image, simpleitk, opencv and other image processing libraries.\n\n![newdoc01](http://idoc.imagepy.org/imgs/newdoc01.png)\n<div align=center>Overview, mouse measurement, geometric transformation, filtering, segmentation, counting, etc.</div><br>\n\n![newdoc02](http://idoc.imagepy.org/imgs/newdoc02.png)\n<div align=center>If you are more a IJ-style user, try `Windows -> Windows Style` to switch</div><br>\n\nImagePy:\n- has a user-friendly  interface\n- can read/save a variety of image data formats\n- supports ROI settings, drawing, measurement and other mouse operations\n- can perform image filtering, morphological operations and other routine operations\n- can do image segmentation, area counting, geometric measurement and density analysis.\n- is able to perform data analysis, filtering, statistical analysis and others related to the parameters extracted from the image.\n\nOur long-term goal of this project is to be used as ImageJ + SPSS (although not achieved yet)\n\n## Installation\n\n__OS support\uff1awindows, linux, mac, with python3.x__\n\n1.  ImagePy is a ui framework based on wxpython, which can not be installed\n    with pip on Linux. You need download\u00a0[the whl according to your\n    Linux system](https://wxpython.org/pages/downloads/).\n2.  On Linux and Mac, there may be permission denied promblem, for\n    ImagePy will write some config information, so please\u00a0start with\n    sudo. If you install with pip, please add \\--user parameter like\n    this:\u00a0pip install --user imagepy\n3.  If you install ImagePy in an Anaconda virtual environment, you may\n    get a error when starting like this:\u00a0This program needs access to the\n    screen. Please run with a Framework build of python, and only when\n    you are logged in on the main display, if so, please start with\n    pythonw -m imagepy.\n\n### - Pre-compiled package\nThis is the simplest option to run ImagePy.  \nA precompiled archive can be downloaded from the [release tab](https://github.com/Image-Py/imagepy/releases) of the repository.  \nSimply unzip the archive and run the ImagePy.bat file.  \nThis will open a command line window and open the GUI of ImagePy.\n\n### - Using pip\nIn a command-prompt type `pip install imagepy`.\n~~On Windows you currently need to first install shapely using conda.~~ This should also work for windows, now that shapely is available via pip.\nOnce installed, ImagePy can be run by typing `python -m imagepy` in a command prompt.\n\n\n\n## Citation\uff1a\n[ImagePy: an open-source, Python-based and platform-independent software package for bioimage analysis](https://academic.oup.com/bioinformatics/article/34/18/3238/4989871)\n\n## Forum\n\nImagePy is a community partner of forum.image.sc, Anything about the usage and development of ImagePy could be discussed in https://forum.image.sc.\n\n\n\n## Contribute\n\n**Contribute Manual:** All markdown file under [doc folder](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc) be parsed as manual. Plugins and manual are paired by plugins's title and manual's file name. We can browse document from the parameter dialog's Help button. We need more manual contributors, just pull request markdown file [here](https://github.com/Image-Py/imagepy/tree/master/imagepy/doc).\n\n**Contribute Plugins:** Here is a [demo plugin](https://github.com/Image-Py/demoplugin) repositories with document to show how to write plugins and publish on ImagePy. You are wellcom and feel free to contact with us if you need help.\n\n**Improve Main Framework:** Just fork ImagePy, then give Pull Request. But if you want to add some new feature, Please have a issue with us firstly.\n\n## Basic operations\uff1a\n\nImagePy has a very rich set of features, and here, we use a specific example to show you a glimpse of the capacity of ImagePy. We choose the official coin split of scikit-image, since this example is simple and comprehensive.\n\n### Open image\n\n`menu: File -> Local Samples -> Coins` to open the sample image within ImagePy.\n_PS: ImagePy supports bmp, jpg, png, gif, tif and other commonly used file formats. By installing ITK plug-in\uff0cdicom\uff0cnii and other medical image formats can also be read/saved. It is also possible to read/write wmv, avi and other video formats by installing OpenCV._\n\n![newdoc03](http://idoc.imagepy.org/imgs/newdoc03.png)\n<div align=center>Coins</div><br>\n\n\n### Filtering & Segmentation\n\n`menu\uff1aProcess -> Hydrology -> Up And Down Watershed` Here, a composite filter is selected to perform sobel gradient extraction on the image, and then the upper and lower thresholds are used as the mark, and finally we watershed on the gradient map.\nFiltering and segmentation are the crucial skills in the image processing toolkit, and are the key to the success or failure of the final measurement.\nSegmentation methods such as adaptive thresholds, watersheds and others are also supported.\n\n![newdoc04](http://idoc.imagepy.org/imgs/newdoc04.png)\n<div align=center>Up And Down Watershed</div><br>\n\n![newdoc05](http://idoc.imagepy.org/imgs/newdoc05.png)\n<div align=center>Mask</div><br>\n\n### Binarization\n\n`menu\uff1aProcess -> Binary -> Binary Fill Holes` After the segmentation, we obtained a relatively clean mask image, but there is still some hollowing out, as well as some impurities, which will interfere with counting and measurement.\n_ImagePy supports binary operations such as erode, dilate, opening and closing, as well as skeletonization, central axis extraction, and distance transformation._\n\n![newdoc06](http://idoc.imagepy.org/imgs/newdoc06.png)\n<div align=center>Fill Holes</div><br>\n\n### Geometry filtering\n\n`menu\uff1aAnalysis -> Region Analysis -> Geometry Filter` ImagePy can perform geometric filtering based on :__the area, the perimeter, the topology, the solidity, the eccentricity__ and other parameters. You can also use multiple conditions for filtering. Each number can be positive|negative, which indicates the kept object will have the corresponding parameter greater|smaller than the value respectively. The kept objects will be set to the front color, the rejected ones will be set to the back color. In this demo, the back color is set to 100 in order to see which ones are filtered out. Once satisfied with the result, set the back color to 0 to reject them. In addition, ImagePy also supports gray density filtering, color filtering, color clustering and other functions.\n\n![newdoc07](http://idoc.imagepy.org/imgs/newdoc07.png)\n<div align=center>Geometry filtering (the area is over-chosen to emphasize the distinction)</div><br>\n\n\n### Geometry Analysis\n\n`menu\uff1aProcess -> Region Analysis -> Geometry Analysis` Count the area and analyze the parameters. By choosing the `cov` option, ImagePy will fit each area with an ellipse calculated via the covariance.\nThe parameters such as area, perimeter, eccentricity, and solidity shown in the previous step are calculated here. In fact, the filtering of the previous step is a downstream analysis of this one.\n\n![newdoc08](http://idoc.imagepy.org/imgs/newdoc08.png)\n<div align=center>Geometry Analysis</div><br>\n\n![newdoc09](http://idoc.imagepy.org/imgs/newdoc09.png)\n<div align=center>Generate the result table (dark to emphasize the ellipse)</div><br>\n\n\n### Sort Table by area\n\n`menu\uff1aTable -> Statistic -> Table Sort By Key` Select the major key as area, and select descend. The table will be sorted in descending order of area. A table is another important piece of data other than an image. In a sense, many times we need to get the required information on the image and then post-process the data in the form of a table. ImagePy supports table I/O (xls, xlsx, csv), filtering, slicing, statistical analysis, sorting and more.  (Right click on the column header to set the text color, decimal precision, line style, etc.)\n\n![newdoc10](http://idoc.imagepy.org/imgs/newdoc10.png)\n<div align=center>Table</div><br>\n\n\n### Charts\n\n`menu\uff1aTable -> Chart -> Hist Chart` From tabular data, we often need to draw a graph. Here, we plot the histograms of the area and the perimeter columns. ImagePy's tables can be used to draw common charts such as line charts, pie charts, histograms, and scatter plots (matplotlib-based). The chart comes with zooming, moving and other functions. The table can also be saved as an image.\n\n![newdoc11](http://idoc.imagepy.org/imgs/newdoc11.png)\n<div align=center>Histograms</div><br>\n\n\n### 3D chart\n\n`menu\uff1aKit3D -> Viewer 3D -> 2D Surface` Surface reconstruction of the image. This image shows the three reconstructed results including, sobel gradient map, high threshold and low threshold. It shows how the Up And Down Watershed works:\n- calculate the gradient.\n- mark the coin and background through the high and low thresholds,\n- simulate the rising water on the dem diagram to form the segmentation.\n\nImagePy can perform 3D filtering of images, 3D skeletons, 3D topological analysis, 2D surface reconstruction, and 3D surface visualization. The 3D view can be freely dragged, rotated, and the image results can be saved as a .stl file.\n\n![newdoc12](http://idoc.imagepy.org/imgs/newdoc12.png)\n<div align=center>3D visualisation</div><br>\n\n\n\n### Macro recording and execution\n\n`menu\uff1aWindow -> Develop Tool Suite` Macro recorder is shown in the develop tool panel. We have manually completed an image segmentation. However, batch processing more than 10 images can be tedious. So, assuming that these steps are highly repeatable and robust for dealing with such problems, we can record a macro to combine several processes into a one-click program. The macro recorder is similar to a radio recorder. When it is turned on, each step of the operation will be recorded. We can click the pause button to stop recording, then click the play button to execute. When the macro is running, the recorded commands will be executed sequentially, therefore achieving simplicity and reproducibility.\n\nMacros are saved into .mc files. drag and drop the file to the status bar at the bottom of ImagePy, the macro will be executed automatically. we can also copy the .mc file to the submenu of the menus under the ImagePy file directory. When ImagePy is started, the macro file will be parsed into a menu item at the corresponding location. By clicking the menu, the macro will also be executed.\n\n![newdoc13](http://idoc.imagepy.org/imgs/newdoc13.png)\n<div align=center>Macro Recording</div><br>\n\n\n### Workflow\n\nA macro is a sequence of predefined commands. By recording a series of fixed operations into macros, you can improve your work efficiency. However, the disadvantage is the lack of flexibility. For example, sometimes the main steps are fixed, but the parameter tuning needs human interaction. In this case, the workflow is what you want. A workflow in ImagePy is a flow chart that can be visualized, divided into two levels: __chapters and sections__.\nThe chapter corresponds to a rectangular area in the flow chart, and the section is a button in the rectangular area, which is also a command and is accompanied by a graphic explanation. The message window on the right will display the corresponding function description, while mousing hovering above. Click on the `Detail Document` in the top right corner to see the documentation of the entire process.\n\n\nThe workflow is actually written in MarkDown (a markup language), but it needs to be written respecting several specifications, as follows:\n\n```markdown\nTitle\n=====\n## Chapter1\n1. Section1\nsome coment for section1 ...\n2. ...\n## Chapter 2\n\t...\n```\n![newdoc14](http://idoc.imagepy.org/imgs/newdoc14.png)\n<div align=center>Workflow</div><br>\n\n### Report Plugin\n\nSometimes we need to make a report to print or generate a PDF document. ImagePy can generate report from a xlsx template. We just need put specific mark in some cells, ImagePy will parse the template and generate a parameter dialog, then we can input some information, or give image/table in, the report will be generated! more about how to make template please see [here](https://github.com/Image-Py/demoplugin/blob/master/doc/report.md).\n\n![newdoc14](http://idoc.imagepy.org/demoplugin/38.png)\n\n<div align=center>generate report</div><br>\n\n### Filter Plugin\n\nWe introduced macros and workflows in the last sections, using macros and workflows to connect existing functions is convenient. But sometimes we need to create new features. In this section, we are trying to add a new feature to ImagePy. ImagePy can easily access any Numpy-based function. Let's take the Canny operator of scikit-image as an example.\n\n```python\nfrom skimage import feature\nfrom imagepy.core.engine import Filter\n\nclass Plugin(Filter):\n    title = 'Canny'\n    note = ['all', 'auto_msk', 'auto_snap', 'preview']\n    para = {'sigma':1.0, 'low_threshold':10, 'high_threshold':20}\n\n    view = [(float, 'sigma', (0,10), 1, 'sigma', 'pix'),\n            ('slide', 'low_threshold', (0,50), 4, 'low_threshold'),\n            ('slide', 'high_threshold', (0,50), 4, 'high_threshold')]\n\ndef run(self, ips, snap, img, para = None):\n    return feature.canny(snap, para['sigma'], para['low_threshold'],\n        para['high_threshold'], mask=ips.get_msk())*255\n```\n![newdoc15](http://idoc.imagepy.org/imgs/newdoc15.png)\n<div align=center>Canny Filter Demo</div><br>\n\n#### Steps to create a your own filter:\n\n1. Import the package(s), often third party.\n2. Inherit the __`Filter`__ class\u3002\n3. The __`title`__ will be used as the name of the menu and the title of the parameter dialog, also as a command for macro recording.\n4. Tell the framework what needs to do for you in __`Note`__, whether to do type checking, to support the selection, to support _UNDO_, etc.\n5. __`Para`__ is the a dictionary of parameters, including needed parameters for the\n   functions.\n6. Define the interaction method for each of the parameters in __`View`__, the framework will automatically generate the dialog for parameter tuning by reading these information.\n7. Write the core function __`run`__. `img` is the current image, `para` is the result entre by user. if `auto_snap` is set in `note`, `snap` will be a duplicate of `img`. We can process the `snap`, store the result in `img`. <span style=\"color:red\">If the function does not support the specified output</span>, we can also return the result, and the framework will help us copy the result to img and display it.\n8. Save the file as `xxx_plg.py` and copy to the `menu` folder, restart ImagePy.\n   It will be loaded as a menu item.\n\n#### What did the framework do for us?\n\nThe framework unifies the complex tasks in a formal manner and helps us to perform:\n- type checking. If the current image type does not meet the requirements in the note, the analysis is terminated.\n- according to the `para`, generate automatically a dialog box to detect the input legality from the `view`.\n- Real-time preview\n- automatic ROI support\n- undo support\n- parallelization support\n- image stack support\n- etc.\n\n### Table\n\nAs mentioned earlier, the table is another very important data type other than the image. Similarly, ImagePy also supports the extension of table. Here we give an example of sorting-by-key used in the previous description.\n\n```python\nfrom imagepy.core.engine import Table\nimport pandas as pd\n\nclass Plugin(Table):\n    title = 'Table Sort By Key'\n    para = {'major':None, 'minor':None, 'descend':False}\n\n    view = [('field', 'major', 'major', 'key'),\n    \t    ('field', 'minor', 'minor', 'key'),\n    \t    (bool, 'descend', 'descend')]\n\ndef run(self, tps, data, snap, para=None):\n    by = [para['major'], para['minor']]\n    data.sort_values(by=[i for i in by if i != 'None'],\n        axis=0, ascending = not para['descend'], inplace=True)\n```\n![newdoc16](http://idoc.imagepy.org/imgs/newdoc16.png)\n<div align=center>Table Sort Demo</div><br>\n\n#### How Table works\n\nSame as `Filter`\uff0c`Table` also has parameters such as `title`\uff0c`note`\uff0c`para`\uff0c`view`.\nWhen the plugin is running, the framework will generate a dialog box according to `para`\nand `view`. After the parameters are chosen, they are passed to the `run` together with the current table and be processed. The table data is a pandas.DataFrame object in the current table, stored in `tps`. Other information, such as `tps.rowmsk`, `tps.colmsk` can also be retrieved from `tps` to get the row and column mask of the current selected table.\n\n### Other type of plugins\n\nThe `Filter` and `Table` described above are the two most important plugins, but ImagePy also supports some other types of plugin extensions. There are currently ten, they are:\n\n1. `Filter`: mainly for image processing\n2. `Simple`: similar to `Filter`, but focus on the overall characteristics of the image, such as the operation of the ROI, the operation of the false color, the area measurement, or the three-dimensional analysis of the entire image stack, visualization, and so on.\n3. `Free`: operate that are independant of image. Used to open image, close software etc.\n4. `Tool`: use the mouse to interact on the diagram and show small icons on the toolbar, such as a brush.\n5. `Table`: operate on the table, such as statistics analysis, sorting, plotting.\n6. `Widget`: widgets that are displayed in panels, such as the navigation bar on the right, the macro recorder, and others.\n7. `Markdown`: markup language, when clicked, a separate window will pop up to display the document.\n8. `Macros`\uff1acommand sequence file for serially fixed operational procedures.\n9. `Workflow`: combination of macro and MarkDown to create an interactive guidance process.\n10. `Report`: a xlsx template with specific mark, rename as `.rpt`, used to auto generate report.\n\n## Motivation & Goal\n\nPython is a simple, elegant, powerful language, and has very rich third-party libraries for scientific computing. Based on the universal matrix structure and the corresponding rules, numpy-based libraries such as scipy, scikit-image, scikit-learn and other scientific computing libraries have brought great convenience to scientific research. On the other hand, more and more problems in biology, material science and other scientific research can be efficiently and accurately solved via scientific computing, image processing.\n\nHowever there are still many researchers that lack programming skills. Thus it is a crucial to make the Numpy-based scientific computing libraries available to more researchers. ImagePy brings the computing capacities closer to the non-programmer researchers, so that they won't need to be concerned about the UI and interaction design, and focus exclusively on the algorithm itself, and finally, accelerate open-source tool building or even commercial products incubation. These tools, meanwhile, can let more researchers, who are not good at programming, gain, promote and popularize scientific knowledge such as image processing and statistics.\n"
 },
 {
  "repo": "joelibaceta/video-to-ascii",
  "language": "Python",
  "readme_contents": "<div align=center>\n\n  ![Logo](./images/logo.svg)\n\n<p>\n\n  It's a simple python package to play videos in a terminal using [ASCII](https://en.wikipedia.org/wiki/ASCII) characters.\n\n  [![Financial Contributors on Open Collective](https://opencollective.com/video-to-ascii/all/badge.svg?label=financial+contributors)](https://opencollective.com/video-to-ascii) [![PyPI version](https://badge.fury.io/py/video-to-ascii.svg)](https://badge.fury.io/py/video-to-ascii)\n  [![Maintainability](https://api.codeclimate.com/v1/badges/a5fcdf2b0cab41654ca3/maintainability)](https://codeclimate.com/github/joelibaceta/video-to-terminal/maintainability)\n  [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/joelibaceta/video-to-ascii)\n  [![HitCount](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii.svg)](http://hits.dwyl.io/joelibaceta/https://github.com/joelibaceta/video-to-ascii)\n\n</p>\n\n![Screenshot](./images/Simpsons.apng)\n\n</div>\n\n<details><summary><b>Translations</b></summary>\n<p>\n\n- [\ud83c\uddfa\ud83c\uddf8 English](./README.md)\n- [\ud83c\uddea\ud83c\uddf8 Espa\u00f1ol](./translations/README_es.md)\n- [\ud83c\uddf9\ud83c\uddfc \u7e41\u9ad4\u4e2d\u6587](./translations/README_zh-TW.md)\n\n<p>\n</details>\n\n## Requirements\n\n- Python3\n- PortAudio (_Only required for installation with audio support_)\n- FFmpeg (_Only required for installation with audio support_)\n\n## Installation\n\nStandard installation\n\n```bash\n$ pip3 install video-to-ascii\n```\n\nWith audio support installation\n\n```bash\n$ pip3 install video-to-ascii --install-option=\"--with-audio\"\n```\n\n## How to use\n\nJust run `video-to-ascii` in your terminal\n\n```bash\n$ video-to-ascii -f myvideo.mp4\n```\n\n### Options\n\n**`--strategy`**\nAllow to choose an strategy to render the output.\n\n![Render Strategies](./images/Strategies.png)\n\n**`-o --output`**\nExport the rendering output to a bash file to share with someone.\n\n![Exporting](./images/export.png)\n\n**`-a --with-audio`**\nIf an installation with audio support was made, you can use this option to play the audio track while rendering the video ascii characters.\n\n## How it works\n\nEvery video is composed by a set of frames that are played at a certain frame rate.\n\n![Video Frames](./images/imgVideoFrames.png)\n\nSince a terminal has a specific number of rows and columns, we have to resize our video to adjust to the terminal size limitations.\n\n![Terminal](./images/imgTerminal.png)\n\nTo reach a correct visualization of an entire frame we need to adjust the _frame height_ to match the _terminal rows_, avoiding using more _characters_ than the number of _terminal columns_.\n\n![Resizing](./images/imgResizing.png)\n\nWhen picking a character to represent a pixel we need to measure the relevance of that pixel's color in the frame, based on that we can then select the most appropriate character based on the [relative luminance](https://en.wikipedia.org/wiki/Relative_luminance) in colorimetric spaces, using a simplify version of the luminosity function.\n\n<p align=\"center\">\n  <img src=\"./images/Luminosity.svg\">\n</p>\n\n> Green light contributes the most to the intensity perceived by humans, and blue light the least.\n\nThis function returns an integer in the range from 0 to 255, we assign a character according to density to show more colored surface for areas with more intense color (highest values).\n\n```python\nCHARS_LIGHT \t= [' ', ' ', '.', ':', '!', '+', '*', 'e', '$', '@', '8']\nCHARS_COLOR \t= ['.', '*', 'e', 's', '@']\nCHARS_FILLED    = ['\u2591', '\u2592', '\u2593', '\u2588']\n```\n\nThe reduced range of colors supported by the terminal is a problem we need to account for. Modern terminals support up to 256 colors, so we need to find the closest 8 bit color that matches the original pixel in 16 or 24 bit color, we call this set of 256 colors [ANSI colors](https://stackoverflow.com/questions/4842424/list-of-ansi-color-escape-sequences).\n\n![The Mapping of RGB and ANSI Colors](./images/imgPixelSection.png)\n\n![8 Bits Color Table](./images/8-bit_color_table.png)\n\nFinally, when putting it all together, we will have an appropriate character for each pixel and a new color.\n\n![Frame Image by Characters](../images/imgPixelImage.png)\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](./CONTRIBUTING.md)].\n\n<a href=\"https://github.com/joelibaceta/video-to-ascii/graphs/contributors\"><img src=\"https://opencollective.com/video-to-ascii/contributors.svg?width=890&button=false\" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/video-to-ascii/contribute/)].\n\nOr maybe just [buy me a coffee](https://ko-fi.com/joelibaceta).\n\n#### Individuals\n\n<a href=\"https://opencollective.com/video-to-ascii#backers\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/individuals.svg?width=890\"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/video-to-ascii/contribute)]\n\n<a href=\"https://opencollective.com/video-to-ascii/organization/0/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/0/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/1/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/2/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/3/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/4/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/5/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/6/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/7/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/8/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/video-to-ascii/organization/9/website\" target=\"_blank\" rel=\"noopener\"><img src=\"https://opencollective.com/video-to-ascii/organization/9/avatar.svg\"></a>\n"
 },
 {
  "repo": "shanren7/real_time_face_recognition",
  "language": "Python",
  "readme_contents": "# real_time_face_detection and recognition\nThis is a real time face detection and recognition project base  on opencv/tensorflow/mtcnn/facenet. Chinese version of description is [here](https://zhuanlan.zhihu.com/p/25025596) .Face detection is based on [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html).Face embedding is based on [Facenet](https://arxiv.org/abs/1503.03832).\n##Workflow\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/real%20time%20face%20detection%20and%20recognition.jpg)\n\n##Inspiration\nThe code was inspired by several projects as follows:\n\n1.[OpenFace](https://github.com/cmusatyalab/openface). The main idea was inspired by openface. However, I prefer python and tensorflow,so there comes this project.\n\n2.[davidsandberg/facenet](https://github.com/davidsandberg/facenet).\n\n   facenet.py was taken from https://github.com/davidsandberg/facenet/blob/master/facenet/src/facenet.py\n    \n   nn4.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/models/nn4.py\n    \n   detect_face.py was taken from https://github.com/davidsandberg/facenet/blob/master/src/align/detect_face.py\n    \n3.[yobibyte/yobiface](https://github.com/yobibyte/yobiface).\n\n##Dependencies\n1.tensorflow\n2.opencv with python bindings (cv2)\n3.jupyter notebook for running .ipynb examples\n\n##Running\n1.Downloading pre-trained facenet from https://github.com/yobibyte/yobiface/blob/master/model/model-20160506.ckpt-500000 and putting in model_check_point folder.\n\n2.Running [real time face detection and recognition.ipynb](https://github.com/shanren7/real_time_face_recognition/blob/master/real%20time%20face%20detection%20and%20%20recognition.ipynb) with jupyter notebook\n\n##Results\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_20.jpg)\n![](https://github.com/shanren7/real_time_face_recognition/blob/master/images/video_guai_2192.jpg)\n"
 },
 {
  "repo": "ahmetozlu/tensorflow_object_counting_api",
  "language": "Python",
  "readme_contents": "# TensorFlow Object Counting API\nThe TensorFlow Object Counting API is an open source framework built on top of TensorFlow and Keras that makes it easy to develop object counting systems. ***Please contact if you need professional object detection & tracking & counting project with the super high accuracy.***\n\n## QUICK DEMO\n\n---\n### Cumulative Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=411>\n</p>\n\n---\n### Real-Time Counting Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42237325-1f964e82-7f06-11e8-966b-dfde98701c66.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/42238435-77ac0d34-7f09-11e8-9609-e7c3c2c5af74.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42241094-14163cc8-7f12-11e8-83ed-68021b5e3b33.gif\" | width=430><img src=\"https://user-images.githubusercontent.com/22610163/42237904-d6a3ac22-7f07-11e8-88f8-5f21430d9503.gif\" | width=430>\n</p>\n\n---\n\n---\n### Object Tracking Mode (TensorFlow implementation):\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389634-4682ea00-19d3-11ea-84a2-3996a43e98fe.gif\" | width=430> <img src=\"https://user-images.githubusercontent.com/22610163/70389738-6bc42800-19d4-11ea-971f-f19cb5b90140.gif\" | width=430>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/70389764-e9883380-19d4-11ea-8c54-80935811c3fa.gif\" | width=680>\n</p>\n\n- Tracking module was built on top of [this approach](https://github.com/kcg2015/Vehicle-Detection-and-Tracking).\n\n---\n\n### Object Counting On Single Image (TensorFlow implementation):\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/47524870-7c830e80-d8a4-11e8-8fd1-741193615a04.png\" | width=750></p>\n\n---\n\n### Object Counting based R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969852-0569b080-7983-11e9-8051-07d6766ca0e4.png\" | width=750></p>\n\n### Object Segmentation & Counting based Mask R-CNN ([Keras and TensorFlow implementation](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counter)):\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/22610163/57969871-1c100780-7983-11e9-9660-7b8571b01ff7.png\" | width=750></p>\n\n---\n\n### BONUS: Custom Object Counting Mode (TensorFlow implementation):\n\nYou can train TensorFlow models with your own training data to built your own custom object counter system! If you want to learn how to do it, please check one of the sample projects, which cover some of the theory of transfer learning and show how to apply it in useful projects, are given at below.\n\n**Sample Project#1: Smurf Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/smurf_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62861574-9d6e0080-bd0c-11e9-9e38-b63226df8aa1.gif\" | width=750>\n</p>\n\n**Sample Project#2: Barilla-Spaghetti Counting**\n\nMore info can be found in [**here**](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/mask_rcnn_counting_api_keras_tensorflow/barilla_spaghetti_counter_training)!\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/62903429-46e3df00-bd6b-11e9-9f97-4de477fa8769.png\" | width=750>  \n</p>\n\n---\n\n***The development is on progress! The API will be updated soon, the more talented and light-weight API will be available in this repo!***\n\n- ***Detailed API documentation and sample jupyter notebooks that explain basic usages of API will be added!***\n\n**You can find a sample project - case study that uses TensorFlow Object Counting API in [*this repo*](https://github.com/ahmetozlu/vehicle_counting_tensorflow).**\n\n---\n\n## USAGE\n\n### 1.) Usage of \"Cumulative Counting Mode\"\n\n#### 1.1) For detecting, tracking and counting *the pedestrians* with disabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"pedestrian counting\" case:*\n\n    fps = 30 # change it with your input video fps\n    width = 626 # change it with your input video width\n    height = 360 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 385 # roi line position\n    deviation = 1 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_x_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"pedestrian counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166945-c0744de0-8fa0-11e8-8985-9f863c59e859.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"pedestrian counting case-study\": [pedestrian_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/pedestrian_counting.py)**\n\n---\n\n**1.2)** For detecting, tracking and counting *the vehicles* with enabled color prediction\n\n*Usage of \"Cumulative Counting Mode\" for the \"vehicle counting\" case:*\n\n    fps = 24 # change it with your input video fps\n    width = 640 # change it with your input video width\n    height = 352 # change it with your input vide height\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    roi = 200 # roi line position\n    deviation = 3 # the constant that represents the object counting area\n\n    object_counting_api.cumulative_object_counting_y_axis(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height, roi, deviation) # counting all the objects\n    \n*Result of the \"vehicle counting\" case:*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/43166455-45964aac-8f9f-11e8-9ddf-f71d05f0c7f5.gif\" | width=700>\n</p>\n\n---\n\n**Source code of \"vehicle counting case-study\": [vehicle_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/vehicle_counting.py)**\n\n---\n\n### 2.) Usage of \"Real-Time Counting Mode\"\n\n#### 2.1) For detecting, tracking and counting the *targeted object/s* with disabled color prediction\n \n *Usage of \"the targeted object is bicycle\":*\n \n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"bicycle\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n    \n *Result of \"the targeted object is bicycle\":*\n \n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411751-1ae1d3f0-820a-11e8-8465-9ec9b44d4fe7.gif\" | width=700>\n</p>\n\n*Usage of \"the targeted object is person\":*\n\n    is_color_recognition_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    targeted_objects = \"person\"\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n \n *Result of \"the targeted object is person\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411749-1a80362c-820a-11e8-864e-acdeed85b1f2.gif\" | width=700>\n</p>\n\n*Usage of \"detecting, counting and tracking all the objects\":*\n\n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n \n *Result of \"detecting, counting and tracking all the objects\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411750-1aae0d72-820a-11e8-8726-4b57480f4cb8.gif\" | width=700>\n</p>\n\n---\n*Usage of \"detecting, counting and tracking **the multiple targeted objects**\":*\n\n    targeted_objects = \"person, bicycle\" # (for counting targeted objects) change it with your targeted objects\n    fps = 25 # change it with your input video fps\n    width = 1280 # change it with your input video width\n    height = 720 # change it with your input video height\n    is_color_recognition_enabled = 0\n\n    object_counting_api.targeted_object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, targeted_objects, fps, width, height) # targeted objects counting\n---\n \n#### 2.2) For detecting, tracking and counting \"all the objects with disabled color prediction\"\n\n*Usage of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n    \n    is_color_prediction_enabled = 0 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with disabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411748-1a5ab49c-820a-11e8-8648-d78ffa08c28c.gif\" | width=700>\n</p>\n\n\n*Usage of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n    is_color_prediction_enabled = 1 # set it to 1 for enabling the color prediction for the detected objects\n    fps = 24 # change it with your input video fps\n    width = 854 # change it with your input video width\n    height = 480 # change it with your input vide height    \n\n    object_counting_api.object_counting(input_video, detection_graph, category_index, is_color_recognition_enabled, fps, width, height) # counting all the objects\n    \n *Result of detecting, counting and tracking \"all the objects with enabled color prediction\":*\n\n <p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/42411747-1a215e4a-820a-11e8-8aef-faa500df6836.gif\" | width=700>\n</p>\n\n### 3.) Usage of \"Object Tracking Mode\"\n\nJust run [object_tracking.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/object_tracking.py)\n\n---\n\n**For sample usages of \"Real-Time Counting Mode\": [real_time_counting.py](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/real_time_counting.py)**\n\n---\n\n*The minimum object detection threshold can be set [in this line](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/utils/visualization_utils.py#L443) in terms of percentage. The default minimum object detecion threshold is 0.5!*\n\n## General Capabilities of The TensorFlow Object Counting API\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421361-6662c280-e76d-11e8-9680-ec86e245fdac.jpg\" | width = 720>\n</p>\n\nHere are some cool capabilities of TensorFlow Object Counting API:\n\n- Detect just the targeted objects\n- Detect all the objects\n- Count just the targeted objects\n- Count all the objects\n- Predict color of the targeted objects\n- Predict color of all the objects\n- Predict speed of the targeted objects\n- Predict speed of all the objects\n- Print out the detection-counting result in a .csv file as an analysis report\n- Save and store detected objects as new images under [detected_object folder](www)\n- Select, download and use state of the art [models that are trained by Google Brain Team](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n- Use [your own trained models](https://www.tensorflow.org/guide/keras) or [a fine-tuned model](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb) to detect spesific object/s\n- Save detection and counting results as a new video or show detection and counting results in real time\n- Process images or videos depending on your requirements\n\nHere are some cool architectural design features of TensorFlow Object Counting API:\n\n- Lightweigth, runs in real-time\n- Scalable and well-designed framework, easy usage\n- Gets \"Pythonic Approach\" advantages\n- It supports REST Architecture and RESTful Web Services\n\nTODOs:\n\n- Kalman Filter based object tracker util will be developed.\n- Autonomus Training Image Annotation Tool will be developed.\n\n## Theory\n\n### System Architecture\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48421362-6662c280-e76d-11e8-9b63-da9698626f75.jpg\" | width=720>\n</p>\n\n- Object detection and classification have been developed on top of TensorFlow Object Detection API, [see](https://github.com/tensorflow/models/tree/master/research/object_detection) for more info.\n\n- Object color prediction has been developed using OpenCV via K-Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features, [see](https://github.com/ahmetozlu/tensorflow_object_counting_api/tree/master/utils/color_recognition_module) for more info.\n\n[TensorFlow\u2122](https://www.tensorflow.org/) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n[OpenCV (Open Source Computer Vision Library)](https://opencv.org/about.html) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n\n### Tracker\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/41812993-a4b5a172-7735-11e8-89f6-083ec0625f21.png\" | width=700>\n</p>\n\nSource video is read frame by frame with OpenCV. Each frames is processed by [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) is developed on TensorFlow. This is a loop that continue working till reaching end of the video. The main pipeline of the tracker is given at the above Figure.\n\n### Models\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48481757-b1d5a900-e81f-11e8-824b-4317115fe5b4.png\">\n</p>\n\nBy default I use an [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) in this project. You can find more information about SSD in [here](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab). \n\nPlease, See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies. You can easily select, download and use state-of-the-art models that are suitable for your requeirements using TensorFlow Object Detection API.\n\nYou can perform transfer learning on trained TensorFlow models to build your custom object counting systems!\n\n## Project Demo\n\nDemo video of the project is available on [My YouTube Channel](https://www.youtube.com/watch?v=bas6c8d1JyU).\n\n## Installation\n\n### Dependencies\n\nTensorflow Object Counting API depends on the following libraries:\n\n- TensorFlow Object Detection API\n- Protobuf 3.0.0\n- Python-tk\n- Pillow 1.0\n- lxml\n- tf Slim (which is included in the \"tensorflow/models/research/\" checkout)\n- Jupyter notebook\n- Matplotlib\n- Tensorflow\n- Cython\n- contextlib2\n- cocoapi\n\nFor detailed steps to install Tensorflow, follow the [Tensorflow installation instructions](https://www.tensorflow.org/install/). \n\nTensorFlow Object Detection API have to be installed to run TensorFlow Object Counting API, for more information, please see [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).\n\n## Citation\nIf you use this code for your publications, please cite it as:\n\n    @ONLINE{tfocapi,\n        author = \"Ahmet \u00d6zl\u00fc\",\n        title  = \"TensorFlow Object Counting API\",\n        year   = \"2018\",\n        url    = \"https://github.com/ahmetozlu/tensorflow_object_counting_api\"\n    }\n\n## Author\nAhmet \u00d6zl\u00fc\n\n## License\nThis system is available under the MIT license. See the LICENSE file for more info.\n\n\n\n\n\n\n"
 },
 {
  "repo": "christopher5106/FastAnnotationTool",
  "language": "C++",
  "readme_contents": "# Fast Image Data Annotation Tool (FIAT)\n\nFIAT enables image data annotation, data augmentation, data extraction, and result visualisation/validation.\n\n- annotate images for image classification, optical character reading (digit classification, letter classification), ...\n\n- extract data into different format (Caffe LMDB, OpenCV Cascade Classifiers, Tesseract ... ) with data augmentation (resizing, noise in translation / rotation / scaling, pepper noise , gaussian noise, rectangle merging, line extraction ...)\n\nThe philosophy of this tool is\n\n- to enable fast annotation : annotate data just by selecting the diagonal of the object, for a fixed ratio. Press Enter if the class is always the same. Type the letter of the class otherwise,\n\n- to be re-usable for different scenarios, and leave you free to build any other tool as input of the annotation process, using any pre-existing weaky classifier (depending on your case) or bounding box proposal algorithm such as selective search, to facilitate annotation with a list of rectangles to select or discard, by just typing the letter of the class or ESCAPE KEY,\n\n- to have the extraction tool act like a monad, so that you can apply transformation in any order, at any stage of your process, the format remaining the same : given a directory and a CSV file, extraction tool will produce a new directory and CSV file, in the same format, by default,\n\n- to feed any classification / training process,\n\n- to be usable for visualisation and export : a visual check that the data is correctly annotated, after manual annotation, extraction, or even after your own bounding box prediction algorithm if it uses the Output class to its produce results.\n\nRequires OPENCV 3 and Google Protobuf.\n\n### Build on Ubuntu 18.04\n    sudo apt-get install caffe-cpu caffe-doc caffe-tools-cpu libcaffe-cpu-dev libcaffe-cpu1 python3-caffe-cpu\n    sudo apt-get install libgoogle-glog-dev\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libprotobuf-dev libprotoc-dev protobuf-compiler\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install libopencv-contrib-dev libopencv-dev libopencv-photo-dev libopencv-shape-dev\n    sudo apt-get install liblmdb-dev\n    make all\n\n### File format\n\nRectangle extraction tools create annotations CSV files in the RotatedRect file format. [This blog post will give you the reasons motivating this choice](http://christopher5106.github.io/computer/vision/2015/12/26/file-format-for-computer-vision-annotations.html).\n\n\n### Annotation tool\n\n```bash\n./bin/annotateRect [FLAGS] input_dir output_file.csv\n```\n\nA tool used to annotate rectangles or to show the results (rectangles with `--init` option).\n\n![data annotation](http://christopher5106.github.io/img/annotator_erase.png)\n\n\n**Meaning of the colors** :\n\n- blue rectangle : currently active rectangle to annotate\n\n- green rectangles : remaining rectangles to annotate. Use `--init init_file.csv` option to feed rectangles to annotate to the annotation tool.\n\n- yellow rectangles : annotated rectangles\n\n**Add a rectangle or modify the rectangle** with the following keys :\n\n- Click with the mouse to set the center of the active rectangle (blue) or create a new active rectangle at this position.\n\n- Use arrow keys to move the active rectangle (blue)\n\n    - move left (left arrow key)\n    - move right (right arrow key)\n    - move up (up arrow key)\n    - move down (down arrow key)\n\n- Press **FN** while using arrow keys to change orientation/scale of the active rectangle (blue)\n\n    - rotate left (left arrow key)\n    - rotate right (right arrow key)\n    - augment size (up arrow key)\n    - downsize (down arrow key)\n\nFor platforms for which pressing FN with arrow key does not change key value, press **Space bar** to change into \"Rotation/Scale\" mode and use arrow keys.\n\n- **BACKSPACE**: erase the currently active rectangle.\n\n- **ESC**: next init rectangle or next image (without save)\n\n**Annotate the class** with :\n\n- **Any letter**: save the currently active rectangle (blue) with this letter as category / class . For example \"0\", if there is only one category. The blue rectangle will become yellow.\n\n- **ENTER**: save the letter with the same class as previously.\n\n\nFLAGS :\n\n- `--ratio 1.0` is the ratio height/width of the annotation rectangles.\n\n- `--cross` display a cross instead of a rectangle for non-current rectangles, ie previous (yellow) or future (green) rectangles. This is a useful display option when rectangles are too close together.\n\n![](http://christopher5106.github.io/img/annotation_cross.png)\n\n- `--init init_file.csv` to initialize the rectangles instead of selecting them manually (appear in green). The first one of them will be use as the currently active rectangle (blue). You can still add new rectangles when all init rectangles (green or blue) have been annotated.\n\n- `--export=output_dir` will not display annotation interface. Saves the annotated images to directory.\n\nNOTES :\n\n- the annotation tool can be stopped and launched again : it will resume the work from *output_file.csv*, previously annotated rectangles appear in yellow.\n\n- in case the init rectangles are bigger than the image, a white border is added to the image to show the rectangles outside the image.\n\n- although annotation tool can read images in an output_file.csv or init_file.csv outside current exe directory (by adding the CSV dir to the image path), it will save images with the input_dir as base path. So, when annotating, execute the command in the same directory as the output_file.csv.\n\nEXAMPLE :\n\nAnnotate images in the current directory :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nA first image will appear :\n\n![](pic.jpg)\n\nTo annotate fast, just select the diagonal, by clicking first the top left corner of the object, then the bottom right corner, as shown here with the red arrows :\n\n![](tutorial/pic_selection.jpg)\n\nImage is now selected and appear in blue:\n\n![](tutorial/selected.png)\n\nPress the key corresponding to its class, for example 'a'. Now it appears in yellow.\n\n![](tutorial/labeled.png)\n\nSelect the second book.\n\n![](tutorial/second_selected.png)\n\nAnd press the key corresponding to its class, for example 'e'.\n\n![](tutorial/result.png)\n\nThe output annotation file in CSV format *out.csv* will look like :\n\n```\n./pic.jpg,a,709,816,826,1116,-14.6958\n./pic.jpg,e,1510,607,741,1001,6.32224\n```\n\nAt any time, you can view how the annotations are, and potentially add new annotations with the same command :\n\n```bash\n./bin/annotateRect . out.csv -ratio 1.35\n```\n\nThe `--export` option is also very useful : it allows to export the images with the rectangles, without having the burden of the annotation interface and to facilitate results sharing for instance.\n\n### Extraction tool\n\nUse annotation information to extract a version of the images :\n\n```bash\n./bin/extractRect [FLAGS] annotations.csv output_dir\n```\n\nMoreove, the tool will create an output CSV file listing the new rectangle coordinates in the format `path,label,center_x,center_y,width,height,rotation,noise_x,noise_y,noise_rotation,noise_scale`.\n\nExtraction extracts at best quality possible.\n\nImage will be rotated so that annotation window will be parallel to the image borders.\n\nFor example in the previous example,\n\n```bash\n./bin/extractRect out.csv out\n```\n\nwill create an output directory *out* with two subdirectories corresponding to each label, *out/a* and *out/b* and its corresponding extracted objects, and a CSV file *out/results.csv* with image path, labels and new rectangle coordinates after the extraction.\n\n![](tutorial/a.jpg) ![](tutorial/e.jpg)\n\nExtract with a noise in rotation with `./bin/extractRect out3.csv out8 --noise_rotation=30`\n\n![](tutorial/a_with_rotation_noise.jpg) ![](tutorial/e_with_rotation_noise.jpg)\n\nSeveral options such as noise in translation, scale, or pepper/gaussian noise are available.\n\nThe `--full_image` option makes transformation available without extracting the rectangle.\n\nSee below for more options.\n\nINPUT SELECTION FLAGS :\n\n- `--input_class_filter` select entries of the specified class only.\n\n- `--limit` limit the number of annotation rectangles to consider, good for debuging purposes.\n\nEDIT RECTANGLES\n\n- `--skip_rotation` skips rotation information in annotation (all set to 0.0).\n\n- `--factor=1.2` extends the extraction box by a factor of the width and height. `--factor_width` and `--factor_height` do the same on one axis only and are cumulative with `--factor` (multiplication). In the example, `./bin/extractRect out.csv out7 --factor 1.2`\n\n![](tutorial/a_with_factor.jpg) ![](tutorial/e_with_factor.jpg)\n\n\n- `--offset_x` and `--offset_y` add an offset on each axis of the rectangle, in percentage of the width of the rectangle. In the example, select the titles of the book with the command `./bin/extractRect out3.csv out12 --offset_y=0.5 --factor_height=0.3`\n\n![](tutorial/a_subselection.jpg) ![](tutorial/e_subselection.jpg)\n\n- `--merge` : if multiple bounding box per images, will extract the global bounding box containing all rectangles in each image. In the example : `./bin/extractRect out.csv out --merge` will produce :\n\n![](tutorial/merge.jpg)\n\n\n- `--merge_line` : If multiple rectangle per images, merge rectangles that are roughly on the same line.\n\n- `--correct_ratio` : corrects the ratio of the annotated rectangles to the specified `--ratio` by augmenting one of the two dimensions (height or width). Default is false.\n\n- `--add_borders=true` : adds borders to the extracted image to fit the ratio. Default is false. Available only when `--resize_width` not zero.\n\n- `--ratio=1.0` is used in combination to `--correct_ratio` or `--resize-width` options. It defines the ratio (height/width) of the window to extract. The use of `--resize-width` option without `--correct_ratio` will stretch the image to final dimensions.\n\nNOISE FLAGS\n\n- `--noise_translation=0.2` adds a noise in translation of 20% of the width/height. Not taken into consideration for negatives generation. `--noise_translation_offset` can be used to specify a mininum noise (for negative generation for example).\n\n- `--noise_rotation=30` adds a noise in rotation in `[-noise_rotation\u00b0,noise_rotation\u00b0]`.\n\n- `--noise_zoomin=2 --noise_zoomout=3` adds a noise in scale factor uniformly distributed in `[1/3,200%]`. Not taken into consideration for negatives generation and in `--full_image` mode.\n\n- `--pepper_noise=0.1 --gaussian_noise=30` adds a pixel noise\n\n- `--samples` is the number of sample to extract per image. Default is 1. Useful in combination with noise option.\n\n\nOUTPUT FLAGS\n\n- `--full_image` will not extract the rectangle along the given annotation. Always true in `--backend=opencv` output mode.\n\n- `--resize_width=400` resizes the output to a width of `resize_width` and a height of `resize_width*ratio`. `--resize_width=0` will not resize the output. Default value is no resize. Not available in `--full_image` and `--backend=opencv` mode.\n\n- `--gray=true` extracts as a gray image. Default is false. Always true in `--backend=opencv` output mode.\n\n- `--backend=directory` defines the output format for storing the results. Possible values are : directory, lmdb, tesseract, opencv. Default value is directory.\n\n- `--output_class` override the class by the specified class\n\n- `--output_by_label=false` avoids creation of different output directories per label. Available for `--backend=directory` only.\n\n- `--append` append new extracts to an existing directory. Available for `--backend=directory` only.\n\n\nNEGATIVE GENERATION\n\n- `--neg_per_pos` defines the number of negative samples per positives to extract. By default, no negative (0).\n\n- `--neg_width=0.2` defines the width of negative samples to extract, in pourcentage to the largest image dimension (width or height).\n\n\n# License conditions\n\nCopyright (c) 2016 Christopher5106\n\nThis tool has been developped for a work at Axa, and is a contribution to OpenSource by Axa.\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n"
 },
 {
  "repo": "RaiMan/SikuliX1",
  "language": "Java",
  "readme_contents": "[![RaiMan's Stuff](https://raw.github.com/RaiMan/SikuliX-2014-Docs/master/src/main/resources/docs/source/RaiManStuff64.png)](http://sikulix.com) SikuliX\n============\n\n**What is SikuliX**<br>SikuliX automates anything you see on the screen of your desktop computer \nrunning Windows, Mac or some Linux/Unix. It uses image recognition powered by OpenCV to identify \nGUI components and can act on them with mouse and keyboard actions.\nThis is handy in cases when there is no easy access to a GUI's internals or \nthe source code of the application or web page you want to act on. [More details](http://sikulix.com)\n\n<hr>\n\n**You need at least Java 8, but it works on Java 9 up to latest (currently 14)**\n\n**Windows:** Works out of the box ([for exceptions look here](https://github.com/RaiMan/SikuliX1/wiki/Windows:-Problems-with-libraries-OpenCV-or-Tesseract))\n\n**Mac:** you have to make Tesseract OCR available ([for HowTo look here](https://github.com/RaiMan/SikuliX1/wiki/macOS-Linux:-Support-libraries-for-Tess4J-Tesseract-4-OCR)). **Java 14: open problems with Tesseract**\n\n**Linux:** you have to make OpenCV and Tesseract OCR available ([for HowTo look here](https://sikulix-2014.readthedocs.io/en/latest/newslinux.html#version-1-1-4-special-for-linux-people)).\n\n<hr>\n\n**Latest stable version is 2.0.4** (branch `release_2.0.x` - [see what is fixed](https://github.com/RaiMan/SikuliX1/wiki/ZZZ-Bug-Fixes))\n\n**Development version 2.1.0 currently not useable until further notice**<br>\nNew features will only be available in new major versions (currently 2.1.0, branches master and/or dev_...). \n<br>Until release of a stable 2.1.0, there will be nightly builds and snapshots available (see below).\n\n[Here you can read about the changes/enhancements](https://sikulix-2014.readthedocs.io/en/latest/news.html)\n\n**Get SikuliX ready to use**\n- [SikuliX IDE for editing and running scripts](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixide-2.0.4.jar)\n  - [Jython support for the IDE](https://repo1.maven.org/maven2/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar)\n  - [JRuby support for the IDE](https://repo1.maven.org/maven2/org/jruby/jruby-complete/9.2.0.0/jruby-complete-9.2.0.0.jar)\n  - download all needed to one folder and run sikulix-2.0.x.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages](https://launchpad.net/sikuli/sikulix/2.0.4/+download/sikulixapi-2.0.4.jar)\n  - for use in non-Maven projects\n \nFor use in **Java Maven projects** the dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.0.4</version>\n</dependency>\n```\n<hr>\n\n**Current development version is 2.1.0** (branch `master` nightly builds / snapshots):<br>\n[![Build Status](https://travis-ci.org/RaiMan/SikuliX1.svg?branch=master)](https://travis-ci.org/RaiMan/SikuliX1)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FRaiMan%2FSikuliX1?ref=badge_shield)\n\n[Read about fixes, enhancements and new features](https://github.com/RaiMan/SikuliX1/wiki/About-fixes-and-enhancements-in-2.1.0)\n\n**Get the nightly builds ready to use** \n- [SikuliX IDE for editing and running scripts]()\n  - [Jython support for the IDE]()\n  - [JRuby support for the IDE]()\n  - download all needed to one folder and run sikulix-2.1.0.jar\n  <br><br>\n- [SikuliX Java API for programming in Java or Java aware languages]()\n  - for use in non-Maven projects\n\nFor use in **Java Maven projects** use the SNAPSHOT dependency information:<br><br>\nThe repository URL:\n```\n<repositories>\n  <repository>\n    <id>sonatype-ossrh</id>\n    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n  </repository>\n</repositories>\n```\nThe dependency coordinates are:\n```\n<dependency>\n  <groupId>com.sikulix</groupId>\n  <artifactId>sikulixapi</artifactId>\n  <version>2.1.0-SNAPSHOT</version>\n</dependency>\n```\n<hr>\n\n**Development environment**\n\n - Java 11 (current JDK LTS release)\n - Java 8 (Oracle) for comatibility test\n - Source and target level for Java is version 8 as long as supported by Oracle\n - Maven project\n - Windows 10 latest (Pro 64-Bit)\n - Mac 10.15 latest\n - Ubuntu 18.04 in WSL on Windows 10 (basic tests only, headless)\n - Ubuntu 18.04 running in Oracle VM VirtualBox 6.1 on Windows 10\n - Using IntelliJ IDEA CE in all environments\n\n<hr>\n\n#### Contributions are welcome and appreciated\n - for `bugreports and requests for features or enhancements` use the issue tracker here\n - for `bugfixes` related to the latest release version you should create a pull request against the release branch (currently `release_2.0.x`), so your fix will be in the next bug-fix release (see milestones).\n- for `smaller bugfixes and/or feature enhancements` related to the running development (currently branch master as version 2.1.0-SNAPSHOT and dev_... branches) you should create a pull request against the target branch\n- a pull request should target only one branch. It is the resposibility and job of the maintainer to apply the changes to other branches in case \n- for `more complex revisions and/or enhancements` you should ask for a development branch together with a short description of your ideas\n \n **Please respect the following rules and guidelines when contributing**\n  - Start with smaller fixes. E.g. choose an issue from the issue tracker and try to fix it. Or fix issues you encounter while using SikuliX.\n  - Only fix cosmetic stuff if it's related to an issue you want to fix.\n  - Before you change stuff like dependencies / overall code style and so on, talk with the maintainer beforehand.<br>Sometimes there is a a reason that things are as they are (... and sometimes not :-)).\n  - Try to accept the individual coding styles of the acting contributors, even if some of the stuff might be sub-optimal in your eyes.<br>But feel free to talk about your ideas and the reasons behind.\n\n \n"
 },
 {
  "repo": "ryfeus/lambda-packs",
  "language": "Python",
  "readme_contents": "# lambda-packs \n\nPrecompiled packages for AWS lambda\n\n## How to start\n\n1. https://aws.amazon.com/lambda/ and create/sign in into account\n2. Lambda > Functions - Create lambda function\n3. Blank function\n4. Configure triggers - Next\n5. Configure function\n  - Runtime - Python 2.7\n6. Lambda function handler and role\n  - Handler - service.handler\n  - Role - Create new role from template(s)\n  - Role name - test\n  - Policy templates - Simple Microservice Permissions\n7. Advanced settings\n  - Memory (MB) 128\n  - Timeout 1 min 0 sec\n6. Code entry type - Upload a .ZIP file - choose Pack.zip from rep\n7. Test -> Save and test\n\n## How to modify\n\n1. Modify service.py file from sources folder\n2. Choose all files in sources folder when compressing, don't put it in one folder\n3. Upload zip file on function page\n\n## Current packs\n\n### Selenium PhantomJS\n\n#### Intro\n\nSelenium on PhantomJS. In fact - a ready-made tool for web scraping. For example, the demo now opens a random page in Wikipedia and sends its header. (PhantomJS at the same time disguises itself as a normal browser, knows how to log in, click and fill out forms) Also added requests, so you can do API requests for different resources to discard / take away the information.\n\nUseful for web testing and scraping.\n\n#### Demo\n\nCurrent demo opens random page from wiki (https://en.wikipedia.org/wiki/Special:Random) and prints title.\n\n#### Serverless start\n\n```\ngit clone https://github.com/ryfeus/lambda-packs.git\ncd lambda-packs/Selenium_PhantomJS/source/\nserverless deploy\nserverless invoke --function main --log\n```\n\nYou can also see the results from the API Gateway endpoint in a web browser.\n\n#### Documentation\n\nhttps://selenium-python.readthedocs.io/\n\n\n\n---\n### Pyresttest + WRK\n\n#### Intro\n\nWhat does the lambda have to do with it? In a nutshell on AWS in one region you can simultaneously run 200 lambdas (more if you write to support). Lambda works in 11 regions. So you can run in parallel more than 2000 lambdas, each of which will conduct load testing of your service. Five minutes of such testing will cost just one dollar.\n\n#### Demo\n\nDemo in this package tries to send requests to github.com for 5 seconds with 1 connection and also conduct pyresttest dummy test.\n\n#### Tools\n\n1. WRK (https://github.com/wg/wrk) - the main tool for load testing. It works with multiple threads, you can specify the number of connections and length of the load. For more fine-tuning, you can use LuaJIT scripts (https://www.lua.org/).\n2. Pyrestest (https://github.com/svanoort/pyresttest) is a handy tool for testing the full pipeline of the API. For example, the user registers, then uses the api key to create tasks / make notes / downloads files, then reads them, then deletes them.\n\n#### Documentation\n\nhttps://github.com/wg/wrk\n\nhttps://github.com/svanoort/pyresttest\n\n---\n### Lxml + requests\n\n#### Intro\n\nPackage for parsing static HTML pages. Difference here is that it works faster and consumes less memory than PhantomJS but is limited in terms websites it can parse and other features.\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/Lxml_requests/source -n lxml-requests\ncd lxml-requests\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\n```\nwget https://github.com/ryfeus/lambda-packs/blob/master/Lxml_requests/buildPack.sh\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\n#### Tools\n\nLxml 3.7.1\n\n#### Documentation\n\nhttp://lxml.de/\n\n---\n### Tensorflow\n\n#### Intro \n\nOpen source library for Machine Intelligence. Basically revolutionized AI and made it more accessible. Using tensorflow on lambda is not as bad as it may sound - for some simple models it is the simplest and the cheapest way to deploy.\n\n#### Demo\n\nAs hello world code I used recognition of images trained on imagenet (https://www.tensorflow.org/tutorials/image_recognition). Given the price tag lambda one run (recognition of one picture) will cost $0.00005. Therefore for a dollar you can recognize 20,000 images. It is much cheaper than almost any alternatives, though completely scalable (200 functions can be run in parallel), and can be easily integrated into cloud infrastructure. Current demo downloads image from link 'imagelink' from event source ( if empty - then downloads https://s3.amazonaws.com/ryfeuslambda/tensorflow/imagenet/cropped_panda.jpg)\n\n#### Tools\n\nTensorflow 1.4.0\n\n#### Documentation\n\nhttps://www.tensorflow.org/tutorials/image_recognition\n\n#### Nightly version\n\nNightly version archive is more than 50 MB in size but it is still eligible for using with AWS Lambda (though you need to upload pack through S3). For more read here:\n\nhttps://hackernoon.com/exploring-the-aws-lambda-deployment-limits-9a8384b0bec3\n\n#### Serverless start\n\n```\nserverless install -u https://github.com/ryfeus/lambda-packs/tree/master/tensorflow/source -n tensorflow\ncd tensorflow\nserverless deploy\nserverless invoke --function main --log\n```\n\n#### Build pack\n\nfor Python2:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack.sh\n```\n\nfor Python3:\n\n```bash\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/buildPack_py3.sh\nwget https://raw.githubusercontent.com/ryfeus/lambda-packs/master/Tensorflow/index_py3.py\ndocker pull amazonlinux:latest\ndocker run -v $(pwd):/outputs --name lambdapackgen -d amazonlinux:latest tail -f /dev/null\ndocker exec -i -t lambdapackgen /bin/bash /outputs/buildPack_py3.sh\n```\n\n> Note: Remember You should set `python3.6` for AWS Lambda function environment.\n\n#### Layer ARN\n\narn:aws:lambda:us-east-1:339543757547:layer:tensorflow-pack\n\n---\n### Sklearn\n\n#### Intro\n\nPackage for fans of machine learning, building models and the like. I doubt that there is a more convenient way to deploy model to the real world.\n\n#### Tools\n\n1. Scikit-learn 0.17.1\n2. Scipy 0.17.0\n\n#### Documentation\n\nhttp://scikit-learn.org/\n\n---\n### Skimage\n\n#### Intro\n\nPackage of image processing tools, and not only to style image, but also a large set of computer vision algorithms.\n\nThere are currently two zipped packs available, Pack.zip and Pack_nomatplotlib.zip, you probably want to use Pack_nomatplotlib.zip. See https://github.com/ryfeus/lambda-packs/issues/5 for more information.\n\n#### Tools\n\nScikit-image 0.12.3\n\n#### Documentation\n\nhttp://scikit-image.org/\n\n---\n### OpenCV + PIL\n\n#### Intro\n\nAnother package of image processing tools, and not only to style image, but also a large set of Computer vision algorithms.\n\n#### Tools\n\n1. OpenCV 3.1.0\n2. PIL 4.0.0\n\n#### Documentation\n\nhttps://pillow.readthedocs.io/\n\nhttp://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n\n---\n### Pandas\n\n#### Intro\n\nPackage for fans of statistics, data scientists and data engineers. RAM at lambda is 1.5 gigabytes, and the maximum operating time - 5 minutes. I am sure that will be enough for most tasks.\n\n#### Tools\n\nPandas 0.19.0\n\n#### Documentation\n\nhttp://pandas.pydata.org/\n\n---\n### Spacy\n\n#### Intro\n\nOpensource library for Natural Language Processing in python.\n\n#### Tools\n\n1. Spacy 2.0.11\n\n#### Documentation\n\nhttps://spacy.io/\n\n#### Example\n\nExample code loads language model from S3 and uses it to analyze sentence.\n\n---\n### Tesseract\n\n#### Intro\n\nOCR (optical character recognition) library for text recognition from the image.\n\n#### Documentation\n\nhttps://github.com/tesseract-ocr/tesseract\n\n---\n\n### PDF generator + Microsoft office file generator (docx, xlsx, pptx) + image generator (jpg, png) + book generator (epub)\n\n#### Intro\n\n\"Hello world\" code in package creates example of every document. Basically these libs are low memory (less than 128MB) and high speed (less than 0.5 seconds) so it's something like ~1m documents generated per 1$ in terms of AWS Lambda pricing.\n\n#### Tools\n\n- docx (python-docx - https://pypi.python.org/pypi/python-docx)\n- xlsx (XlsxWriter - https://pypi.python.org/pypi/XlsxWriter)\n- pptx (python-pptx - https://pypi.python.org/pypi/python-pptx)\n- pdf (Reportlab - https://pypi.python.org/pypi/reportlab)\n- epub (EbookLib - https://pypi.python.org/pypi/EbookLib)\n- png/jpg/... (Pillow - https://pypi.python.org/pypi/Pillow)\n\n\n---\n\n### Satellite imagery processing (rasterio + OSGEO + pyproj + shapely + PIL)\n\n#### Intro\n\nAWS Lambda pack in Python for processing satellite imagery. Basically it enables to deploy python code in an easy and cheap way for processing satellite imagery or polygons. In \u201chello world\u201d code of the pack I download red, green, blue Landsat 8 bands from AWS, make True Color image out of it and upload it to S3. It takes 35 seconds and 824MB of RAM for it so ~2500 scenes can be processed for 1$.\n\n#### Tools\n\n- Rasterio (https://github.com/mapbox/rasterio 0.36)\n- OSGEO (https://trac.osgeo.org/gdal/wiki/GdalOgrInPython)\n- Pyproj (https://github.com/jswhit/pyproj)\n- Shapely (https://github.com/Toblerity/Shapely)\n- PIL (https://pillow.readthedocs.io/)\n\n---\n\n### PyTorch\n\nPython 3.6 based PyTorch\n\n#### Tools\n\n- PyTorch 1.0.1 (CPU)\n- torchvision 0.2.1\n\n#### Installed Packages (deps)\n\n- numpy-1.16.1 \n- pillow-5.4.1 \n- six-1.12.0 \n- torchvision-0.2.1\n\n#### Build Pack\n\n```bash\n# You need `docker` before run\n./build-with-docker.sh\n```\n"
 },
 {
  "repo": "zhengyima/DeepNude_NoWatermark_withModel",
  "language": "Python",
  "readme_contents": "\n# DeepNude\n\nDeepNude Source Code \n\n[The README.md in English](https://github.com/zhengyima/DeepNude_NoWatermark_withModel/blob/master/README_EN.md)\n\nDeepNude\u6e90\u4ee3\u7801\n\n\u53bb\u6c34\u5370 \n\n\u5e26\u4e09\u4e2a\u6a21\u578b.lib\u6587\u4ef6\u4e0b\u8f7d\u5730\u5740\n\n\u4f9b\u5e7f\u5927\u7a0b\u5e8f\u5458\u6280\u672f\u4ea4\u6d41\u4f7f\u7528\n\n~~[demo\u5730\u5740](http://39.105.149.229/dn): demo\u5f88\u539f\u59cb\u8106\u5f31\u4e0d\u9c81\u68d2\uff0c\u6240\u4ee5\u611f\u5174\u8da3\u7684\u8bdd\u5c3d\u91cf\u8fd8\u662f\u81ea\u5df1\u53bb\u8dd1\u4ee3\u7801\u5427\u3002\u4e0d\u8981\u5bf9demo\u505a\u574f\u4e8b\u54e6\uff0c\u4e0d\u7136\u5c31\u5173\u6389= =~~\n\n# Preinstallation\n\nBefore launch the script install these packages in your **Python3** environment:\n- numpy\n- Pillow\n- setuptools\n- six\n- pytorch \n- torchvision\n- wheel\n```\npip3 install numpy pillow setuptools six pytorch torchvision wheel\n```\n\n\u5efa\u8bae\u4f7f\u7528Conda\u5b89\u88c5 :) \n\n\n```\n conda create -n deepnude python=3.6 numpy Pillow setuptools six pytorch torchvision wheel\n conda activate deepnude\n```\n\n**\u6ce8\uff1a\u5982\u679c\u61d2\u5f97\u6298\u817ePython\u73af\u5883\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\uff0c\u89c1\u4e0b**\n\n\u611f\u8c22\u7f51\u53cb[\u98de\u54e5](https://github.com/fizzday)\u63d0\u4f9bdocker\u4e00\u952e\u8fd0\u884c\u90e8\u5206\u6280\u672f\u652f\u6301\n\n## \u4f7f\u7528docker\u4e00\u952e\u8fd0\u884c\n```bash\ncd ~\n\ngit clone https://github.com/zhengyima/DeepNude_NoWatermark_withModel.git --depth 1 deepnude\n\ncd deepnude\n\ndocker run --rm -it -v $PWD:/app:rw ababy/python-deepnude /bin/bash\n\npython main.py\n```\n> \u6ce8\u610f: docker\u8fd0\u884c\u53ea\u80fd\u4f7f\u7528cpu,\u6240\u4ee5,\u9700\u8981\u4fee\u6539gpu\u8fd0\u884c\u4e3acpu, \u4fee\u6539\u65b9\u6cd5\u8bf7\u53c2\u8003 [#GPU](#gpu). \u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u4e5f\u6162\u4e0d\u4e86\u591a\u5c11.  \n\n> \u5bf9\u5e94\u7684\u4e09\u4e2a .lib \u6587\u4ef6\u9700\u8981\u81ea\u5df1\u624b\u52a8\u4e0b\u8f7d\u540e, \u6dfb\u52a0\u5230\u9879\u76ee\u6839\u76ee\u5f55 `checkpoints` \u76ee\u5f55\u4e0b, \u624d\u80fd\u6b63\u5e38\u8fd0\u884c, \u7531\u4e8e\u6587\u4ef6\u592a\u5927, \u5c31\u6ca1\u6709\u653e\u5165docker\u955c\u50cf\n\n# Models\n\n\u5728\u8fd0\u884c\u4e4b\u524d\u9700\u4e0b\u8f7d\u4e09\u4e2a.lib\u6587\u4ef6\uff0c\u4e4b\u540e\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u65b0\u5efacheckpoints\u76ee\u5f55\uff0c\u5c06\u4e0b\u8f7d\u7684\u4e09\u4e2a\u6587\u4ef6\u653e\u81f3checkpoints\u76ee\u5f55\u4e0b\u3002\n\n\u53cb\u60c5\u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u4e0b\u8f7d\u6e20\u9053\uff1a\n\n\n* [Link](http://39.105.149.229/dn.zip)\n\n* [Google Drive](https://drive.google.com/drive/folders/1OKuIp0nxMUucgEScTc2vESvlpKzIWav4?usp=sharing)\n\n\n# Launch the script\n\n\u73af\u5883\u914d\u597d\uff0c\u6a21\u578b\u4e0b\u597d\u4e4b\u540e\u4fbf\u53ef\u4ee5\u8fd0\u884c\u4ee3\u7801\u4e86\uff01\n\n```\n python main.py\n```\n\nThe script will transform *input.png* to *output.png*.\n\n\n\n\n# GPU\n\n\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528id\u4e3a0\u7684GPU\u8fd0\u884c\u3002\n\n\u82e5\u8fd0\u884c\u73af\u5883\u4e0d\u5e26GPU\uff0c\u5219\u62a5\u9519\u3002\u5982\u679c\u6ca1\u6709GPU\u6216\u60f3\u4f7f\u7528CPU\u8fd0\u884c\u7a0b\u5e8f\uff0c\u8bf7\u5c06gan.py\u4e2d\n\n```\nself.gpu_ids = [0] #FIX CPU\n```\n\n\u6539\u4e3a (\n\n```\nself.gpu_ids = [] #FIX CPU\n```\n\n## Links\n- https://pytorch.org/\n\n"
 },
 {
  "repo": "kylemcdonald/FaceTracker",
  "language": "C++",
  "readme_contents": "# FaceTracker\n\n**This repository is no longer maintained, due to challenges of upgrading to OpenCV 4.**\n\nFaceTracker is a library for deformable face tracking written in C++ using OpenCV 2, authored by [Jason Saragih](http://jsaragih.org/) and maintained by [Kyle McDonald](http://kylemcdonald.net/).\n\nAny publications arising from the use of this software, including but not limited to academic journal and conference publications, technical reports and manuals, should cite the following work: `J. M. Saragih, S. Lucey, and J. F. Cohn. Face Alignment through Subspace Constrained Mean-Shifts. International Conference of Computer Vision (ICCV), September, 2009.`\n\n## FAQ\n\n1. **\"I successfully compiled the code, so why is my app is crashing?\"** Make sure that your model files are in the right location. If you see the error `Assertion failed: s.is_open()` when running your app, that means you forgot to put the model files in the right directory.\n2. **\"Is there an example of using FaceTracker on a mobile device?\"** There is no official example. But there is an example of using ofxFaceTracker on iOS [here](https://github.com/kylemcdonald/ofxFaceTracker-iOS) and a native Android example [here](https://github.com/ajdroid/facetrackerapp).\n3. **\"Why is the tracking is slow, and why is there high CPU usage?\"** The face detection step (finding the general location of the face) can be slow. If this is causing an issue, you might want to put the tracking in a separate thread. If the detection is very slow you might try using a face detector that is native to your platform, and initializing FaceTracker with that rectangle.\n4. **Can I use this for my commercial project/product?** Yes. FaceTracker was re-licensed under the MIT license on April 8, 2020. Previously it was available under a custom non-commercial use license, with a separate license for commercial use available for purchase.\n\nWrappers are available for:\n\n* Android: [facetrackerapp](https://github.com/ajdroid/facetrackerapp)\n* [openFrameworks](http://www.openframeworks.cc/): [ofxFaceTracker](https://github.com/kylemcdonald/ofxFaceTracker)\n* [Cinder](http://libcinder.org/): [ciFaceTracker](https://github.com/Hebali/ciFaceTracker)\n* Python: [pyfacetracker](https://bitbucket.org/amitibo/pyfacetracker)\n\n## Installation\n\nThese instructions are for compiling the code on OS X and Ubuntu, but it should be possible to compile on other platforms.\n\nFirst, install OpenCV3 (if you're using OpenCV2, use the [opencv2](https://github.com/kylemcdonald/FaceTracker/tree/opencv2) branch of this repo). On OSX you can use [homebrew](http://brew.sh/):\n\n```\n$ brew tap homebrew/science\n$ brew install opencv3\n```\n\nAnd on Ubuntu use:\n\n```\n$ sudo apt-get install libcv-dev libopencv-dev\n```\n\nAlternatively, you can download [OpenCV from the GitHub](https://github.com/opencv/opencv) and compile it manually.\n\nAfter installing OpenCV, clone this repository with `git clone git://github.com/kylemcdonald/FaceTracker.git`. This repository contains a few subdirectories within the root directory:\n   - src (contains all source code)\n   - model (contains a pre-trained tracking model)\n   - bin (will contain the executable after building)\n\nNext, make sure that your copy of OpenCV is located in `/usr/local` (this should be the case if you used `apt-get`). If it isn't located there, modify the `OPENCV_PATH` in the `Makefile`. If you installed with Homebrew, it should be set to `/usr/local/opt/opencv3`.\n\nOptionally, you can also add `-fopenmp` to the `CFLAGS` and `-lgomp` to the `LIBRARIES` variable to compile with [OpenMP](http://openmp.org/) support.\n\nFrom the root `FaceTracker` directory, build the library and example by running `make`.\n\nTo test the demo, `cd bin` and `./face_tracker`. Because many webcams are 1280x720, try running `./face_tracker -s .25` to rescale the image before processing for a smoother framerate.\n\n## `face_tracker` Usage\n\n````\nUsage: face_tracker [options]\nOptions:\n-m <string> : Tracker model (default: ../model/face2.tracker)\n-c <string> : Connectivity (default: ../model/face.con)\n-t <string> : Triangulation (default: ../model/face.tri)\n-s <double> : Image scaling (default: 1)\n-d <int>    : Frames/detections (default: -1)\n--check     : Check for failure \n--help      : Print help\n-?          : Print help\n````\n"
 },
 {
  "repo": "jayrambhia/Install-OpenCV",
  "language": "Shell",
  "readme_contents": "Install-OpenCV\n==============\n\nshell scripts to install different version of OpenCV in different distributions of Linux\n\n### Ubuntu\nif your system is Ubuntu, run the commands below.\n```\n$ cd Ubuntu\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### RedHat\nif your system is RedHat, run the commands below.\n```\n$ cd RedHat\n$ chmod +x * \n$ ./opencv_latest.sh\n```\n\n\n### ArchLinux\nif your system is ArchLinux, run the commands below.\n```\n$ cd ArchLinux\n$ chmod +x * \n$ ./opencv2_4_0.sh\n```\n"
 },
 {
  "repo": "trishume/eyeLike",
  "language": "C++",
  "readme_contents": "## eyeLike\nAn OpenCV based webcam gaze tracker based on a simple image gradient-based eye center algorithm by Fabian Timm.\n\n## DISCLAIMER\n**This does not track gaze yet.** It is basically just a developer reference implementation of Fabian Timm's algorithm that shows some debugging windows with points on your pupils.\n\nIf you want cheap gaze tracking and don't mind hardware check out [The Eye Tribe](https://theeyetribe.com/).\nIf you want webcam-based eye tracking contact [Xlabs](http://xlabsgaze.com/) or use their chrome plugin and SDK.\nIf you're looking for open source your only real bet is [Pupil](http://pupil-labs.com/) but that requires an expensive hardware headset.\n\n## Status\nThe eye center tracking works well but I don't have a reference point like eye corner yet so it can't actually track\nwhere the user is looking.\n\nIf anyone with more experience than me has ideas on how to effectively track a reference point or head pose\nso that the gaze point on the screen can be calculated contact me.\n\n## Building\n\nCMake is required to build eyeLike.\n\n### OSX or Linux with Make\n```bash\n# do things in the build directory so that we don't clog up the main directory\nmkdir build\ncd build\ncmake ../\nmake\n./bin/eyeLike # the executable file\n```\n\n### On OSX with XCode\n```bash\nmkdir build\n./cmakeBuild.sh\n```\nthen open the XCode project in the build folder and run from there.\n\n### On Windows\nThere is some way to use CMake on Windows but I am not familiar with it.\n\n## Blog Article:\n- [Using Fabian Timm's Algorithm](http://thume.ca/projects/2012/11/04/simple-accurate-eye-center-tracking-in-opencv/)\n\n## Paper:\nTimm and Barth. Accurate eye centre localisation by means of gradients.\nIn Proceedings of the Int. Conference on Computer Theory and\nApplications (VISAPP), volume 1, pages 125-130, Algarve, Portugal,\n2011. INSTICC.\n\n(also see youtube video at http://www.youtube.com/watch?feature=player_embedded&v=aGmGyFLQAFM)\n"
 },
 {
  "repo": "jhansireddy/AndroidScannerDemo",
  "language": "C++",
  "readme_contents": "# ScanLibrary\nScanLibrary is an android document scanning library built on top of OpenCV, using the app you will be able to select the exact edges and crop the document accordingly from the selected 4 edges and change the perspective transformation of the cropped image.\n\n# Screenshots\n\n<div align=\"center\">\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanInput.png\" alt=\"Scan Input\" title=\"Scan Input\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" alt=\"Scan Points\" title=\"Scan Points\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/blackWhiteScannedResult.png\" alt=\"After Scan\" title=\"After Scan\"></img>\n\n<a href=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" />\n<img width=\"23%\" src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/returned_scan_result.png\" alt=\"Scanned Result\" title=\"Scanned Result\"></img>\n\n</div>\n\n# Videos\n\n<div align=\"center\" >\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Kl7rRZ79m6k\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/jhansireddy/AndroidScannerDemo/master/ScanDemoExample/screenshots/scanPoints.png\" \nalt=\"Scan Video\" width=\"40%\" border=\"10\" /></a>\n</div>\n\n# Using it in your project\n\n* `git clone https://github.com/jhansireddy/AndroidScannerDemo.git` into a standalone dir;\n* Create or using your project: `File -> New -> Import module...`;\n* As source directory point to: `~/_dirWhereYouClonedAndroidScannerDemo_/AndroidScannerDemo/ScanDemoExample/scanlibrary` and confirm;\n\n- Add the dependency to your main app build.gradle this way: \n```\t    \ncompile project(':scanlibrary')\n```\n- In your activity or fragment when you want to give an option of document scanning to user then:\nStart the scanlibrary ScanActivity, with this the app will go to library, below is the sample code snippet:\nNote: preference can be one of OPEN_CAMERA or OPEN_MEDIA or left empty, based on the passed preference the scan library decides to open camera or media or open the scan home page.\n```java\n       int REQUEST_CODE = 99;\n       int preference = ScanConstants.OPEN_CAMERA;\n       Intent intent = new Intent(this, ScanActivity.class);\n       intent.putExtra(ScanConstants.OPEN_INTENT_PREFERENCE, preference);\n       startActivityForResult(intent, REQUEST_CODE);\n```\n\n- Once the scanning is done, the application is returned from scan library to main app, to retrieve the scanned image, add onActivityResult in your activity or fragment from where you have started startActivityForResult, below is the sample code snippet:\n```java\n    @Override\n    protected void onActivityResult(int requestCode, int resultCode, Intent data) {\n        super.onActivityResult(requestCode, resultCode, data);\n        if (requestCode == REQUEST_CODE && resultCode == Activity.RESULT_OK) {\n            Uri uri = data.getExtras().getParcelable(ScanConstants.SCANNED_RESULT);\n            Bitmap bitmap = null;\n            try {\n                bitmap = MediaStore.Images.Media.getBitmap(getContentResolver(), uri);\n                getContentResolver().delete(uri, null, null);\n                scannedImageView.setImageBitmap(bitmap);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n```\n# License\n\n\tCopyright (c) 2016 Jhansi Karee\n\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\n\n- **IMPORTANT:** This project uses the OPENCV Framework. Download the newest version here 'http://opencv.org/.\n"
 },
 {
  "repo": "abreheret/PixelAnnotationTool",
  "language": "C++",
  "readme_contents": "PixelAnnotationTool\n============================\n\n-----------------\n| **` Linux/MAC `** | **` Windows `** | **` Donate  `** | \n|-----------------|---------------------|---------------------|\n| [![Build Status](https://api.travis-ci.org/abreheret/PixelAnnotationTool.svg?branch=master)](https://travis-ci.org/abreheret/PixelAnnotationTool) | [![Appveyor Build Status](https://img.shields.io/appveyor/ci/abreheret/pixelannotationtool.svg)](https://ci.appveyor.com/project/abreheret/pixelannotationtool) |  [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) |\n\n\n\nSoftware that allows you to manually and quickly annotate images in directories.\nThe method is pseudo manual because it uses the algorithm [watershed marked](http://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1) of OpenCV. The general idea is to manually provide the marker with brushes and then to launch the algorithm. If at first pass the segmentation needs to be corrected, the user can refine the markers by drawing new ones on the erroneous areas (as shown on video below).\n\n[![gif_file](giphy.gif)](https://youtu.be/wxi2dInWDnI)\n\nExample :\n\n<img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road.jpg\" width=\"300\"/> <img src=\"https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road_color_mask.png\" width=\"300\"/>\n\nLittle example from an user ([tenjumh](https://github.com/tenjumh/Pixel-Annotation-Tool)) of PixelAnnotationTools : https://www.youtube.com/watch?v=tX-xcg5wY4U\n\n----------\n\n### Building Dependencies :\n* [Qt](https://www.qt.io/download-open-source/)  >= 5.x\n* [CMake](https://cmake.org/download/) >= 2.8.x \n* [OpenCV](http://opencv.org/releases.html) >= 2.4.x \n* For Windows Compiler : Works under Visual Studio >= 2015\n\nHow to build go to [here](scripts_to_build)\n\n### Download binaries :\nGo to release [page](https://github.com/abreheret/PixelAnnotationTool/releases)\n\n### Donate :\nIf you like, donate !\n\n\nDonating is very simple - and secure. Please click [here](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=8K79VKWBS7352) to make a donation. \n\n**Thank you!**\n\nYour donation will help me to maintain and update PixelAnnotationTool.\n\n### License :\n\nGNU Lesser General Public License v3.0 \n\nPermissions of this copyleft license are conditioned on making available complete source code of licensed works and modifications under the same license or the GNU GPLv3. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. However, a larger work using the licensed work through interfaces provided by the licensed work may be distributed under different terms and without source code for the larger work.\n\n[more](https://github.com/abreheret/PixelAnnotationTool/blob/master/LICENSE)\n\n### Citation :\n\n```bib\n  @MISC{Breheret:2017\n    author = {Amaury Br{\\'e}h{\\'e}ret},\n    title = {{Pixel Annotation Tool}},\n    howpublished = \"\\url{https://github.com/abreheret/PixelAnnotationTool}\",\n    year = {2017},\n  }\n```\n\n\n"
 },
 {
  "repo": "movidius/ncappzoo",
  "language": "Python",
  "readme_contents": "# Neural Compute Application Zoo (ncappzoo) \n[![Stable release](https://img.shields.io/badge/For_OpenVINO\u2122_Version-2020.1-green.svg)](https://github.com/opencv/dldt/releases/tag/2020.1)\n[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\nWelcome to the Neural Compute Application Zoo (_ncappzoo_). This repository is a place for any interested developers to share their projects (code and Neural Network content) that make use of the [Intel&reg; Neural Compute Stick 2 (Intel&reg; NCS2)](https://software.intel.com/en-us/neural-compute-stick)  or the original [Intel&reg; Movidius&trade; Neural Compute Stick](https://software.intel.com/en-us/movidius-ncs) and the Deep Learning Deployment Toolkit (DLDT) portion of the [OpenVINO&trade; Toolkit](https://software.intel.com/en-us/openvino-toolkit).\n \nThe _ncappzoo_ is a community repository with many content owners and maintainers. All _ncappzoo_ content is open source and being made available in this central location for others to download, experiment with, modify, build upon, and learn from.\n\n## _ncappzoo_ Quick Start\nIf you have an  Intel&reg; NCS2 (or the first generation Intel&reg; Movidus&trade; NCS) device and want to jump into the _ncappzoo_, follow these steps: \n\nClone the repo with the following command:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\nRun this command inside of any app/network folder to check your system software dependencies for that particular sample:\n```bash\nmake install_reqs\n```\nIf the script returns successfully, you're ready to run the app or network sample!\n\n## _ncappzoo_ Apps and Networks\nExplore apps by opening a terminal window navigating to any directory under **_ncappzoo_/apps** and execute this command:\n```bash\nmake run\n```\nExplore the neural networks by navigating to any network directory under **_ncappzoo_/networks** and execute the same command:\n```bash\nmake run\n```\nThats it! All of the network and app directories have simple consistent makefiles. To see other make targets supported from these directories just execute this command:\n```bash\nmake help\n```\n\n\n## _ncappzoo_ Repository Branches\nThere are main three branches in the repository; their descriptions are below.  **The master branch is the one most developers will want.**  The others are provided for legacy compatibility.\n\n- **master** branch: This is the most current branch, and the content relies on the DLDT from the OpenVINO&trade; Toolkit.  This is the only branch that is compatible with the Intel&reg; NCS2 however, it is also compatible with the original Intel&reg; Movidius&trade; NCS device.\n- **ncsdk2** branch: This branch is a legacy branch and the content relies on the NCSDK 2.x tools and APIs rather than the OpenVINO&trade; toolkit. This branch is only compatible with the original Intel&reg; Movidius&trade; NCS device and is **NOT** compatible with the Intel&reg; NCS2 device.\n- **ncsdk1** branch: This branch is a legacy branch and the content relies on the NCSDK 1.x tools and APIs rather than OpenVINO&trade; toolkit.  This branch is only compatible with the original Intel&reg; Movidius&trade; Neural Compute Stick and is **NOT** compatible with the Intel&reg; NCS2 device.\n\nYou can use the following git command to use the master branch of the repo:\n```bash\ngit clone https://github.com/movidius/ncappzoo.git\n```\n\n## _ncappzoo_ Compatibility Requirements\n\n### Hardware compatibility\nThe projects in the _ncappzoo_ are periodically tested on Intel&reg; x86-64 Systems unless otherwise stated in the project's README.md file.  Although not tested on other harware platforms most projects should also work on any hardware which can run the OpenVINO&trade; toolkit including the Raspberry Pi 3/3B/3B+/4B hardware<br><br>\nThe projects in the _ncappzoo_ work on both the Intel&reg; NCS2 and the original Intel&reg; Movidius NCS devices.\n\n\n### Operating System Compatibility\nThe projects in the _ncappzoo_ are tested and known to work on the **Ubuntu 18.04** OS.  These projects will likely work on other Linux based operating systems as well but they aren't tested on those unless explicitly stated in the project's README.md file and there may be some tweaks required as well.  If any specific issues are found for other OSes please submit a pull request as broad compatibility is desirable.\n\n### OpenVINO and DLDT Compatibility\nThe projects in the **master branch** depend on the Deep Learning Deployment Toolkit (DLDT) portion of the OpenVINO&trade; toolkit.  There are two flavors of the the OpenVINO&trade; toolkit's DLDT:  \n- The [Intel&reg; Distribution of the OpenVINO&trade; toolkit](https://software.intel.com/en-us/openvino-toolkit) is a binary installation available for  supported platforms.  Here are some links regarding the Intel Distribution of the OpenVINO&trade; Toolkit and the Intel&reg; NCS2\n  - Getting started web page: https://software.intel.com/en-us/articles/get-started-with-neural-compute-stick\n  - Getting Started Video for Linux: https://youtu.be/AeEjQKKkPzg?list=PL61cFkSnEEmOF3AJvLtlDTSbwjlCP4iCs\n  - OpenVINO Toolkit documentation: https://docs.openvinotoolkit.org/latest/index.html\n- The [open source distribution of the OpenVINO&trade; toolkit DLDT](https://github.com/opencv/dldt).  This is the means by which the Intel&reg; NCS2 device can be used with most single board computers on the market and is also helpful for other non-Ubuntu development systems.  Here are some links regarding the open source distribution of the OpenVINO&trade; with the Intel&reg;  NCS2: \n  - Applies to all target system: https://software.intel.com/en-us/articles/intel-neural-compute-stick-2-and-open-source-openvino-toolkit\n  - ARMv7: https://software.intel.com/en-us/articles/ARM-sbc-and-NCS2\n  - ARM64: https://software.intel.com/en-us/articles/ARM64-sbc-and-NCS2\n  - Python on all: https://software.intel.com/en-us/articles/python3-sbc-and-ncs2\n\n**Note:** When using the open source distribution of the OpenVINO&trade; toolkit, you may need to modify your shell's path and environment variables to point to the toolkit's directories.\n\nThe projects in the _ncappzoo_ work with both flavors of the OpenVINO&trade; Toolkit and unless otherwise specified in a project's README.md file all projects are targeted for the **OpenVINO&trade; Toolkit 2020.1 release**.\n\n### OpenCV Compatibility\nSome projects also rely on OpenCV. For these projects, OpenCV distributed with the OpenVINO&trade; release is the recommended version.  Other versions may also work but are not tested an may require tweaks to get working.  \n\n### Python Compatibility\nThe Python projects in the _ncappzoo_ rely on Python 3.5, unless otherwise stated in the individual project's README.\n\n### Raspberry Pi Compatibility\nThe _ncappzoo_ is compatible with the Raspberry Pi 3 B+ and the Raspberry Pi 4. Some additional configuration steps are required:\n\n#### Intel&reg; Distribution of OpenVINO&trade; for Raspbian* OS\nThe **Intel&reg; Distribution of OpenVINO&trade; toolkit for Raspbian OS** does not include the Model Optimizer. To use the _ncappzoo_, you must clone the open source version of the OpenVINO&trade; Deep Learning Development Toolkit (DLDT) and use that version of the Model Optimizer. Clone the repository, install dependencies for TensorFlow* and Caffe*, and set up your **PATH** and **PYTHONPATH** variables to point to the Model Optimizer:\n```\ncd ~\ngit clone https://github.com/opencv/dldt.git\ncd dldt/model-optimizer\npip3 install -r requirements_tf.txt\npip3 install -r requirements_caffe.txt\nexport PATH=~/dldt/model-optimizer:$PATH\nexport PYTHONPATH=~/dldt/model-optmizer:$PYTHONPATH\n```\n#### Open Source OpenVINO&trade; Deep Learning Development Toolkit (DLDT)\nTo setup the open source version of OpenVINO&trade; with your Raspberry Pi, add to the PATH, PYTHONPATH, and LD_LIBRARY_PATH environment variables the location of the build Inference Engine libraries and Python API.\n\n## _ncappzoo_ Repository Layout\nThe _ncappzoo_ contains the following top-level directories.  See the README file in each of these directories or just click on the links below to explore the contents of the _ncappzoo_.\n- **[apps](apps/README.md)** : Applications built to use the Intel&reg; Movidius Intel&reg; NCS and Intel&reg; Neural Compute Stick 2.  **This is a great place to start in the _ncappzoo_!**\n- **[networks](networks/README.md)** : Scripts to download models and optimize neural networks based on any framework for use with the Intel&reg; NCS and Intel&reg; NCS2.\n- **[caffe](caffe/README.md)** : Scripts to download caffe models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[tensorflow](tensorflow/README.md)** : Scripts to download TensorFlow&trade; models and optimize neural networks for use with the Intel&reg; NCS and Intel&reg; NCS2.  Note: this is a legacy directory and new networks will be in the _networks_ directory.\n- **[data](data/README.md)** : Data and scripts to download data for use with models and applications that use the Intel&reg; NCS and Intel&reg; NCS2\n\nThe top-level directories above have subdirectories that hold project content. Each of these project subdirectories has one or more owners that assumes responsibility for it. The [OWNERS](OWNERS) file contains the mapping of subdirectory to owner. \n\n## Contributing to the ncappzoo\nThe _ncappzoo_ is meant to explore and teach features available for the Intel&reg; Movidius&trade; Neural Compute Stick and Intel&reg; Neural Compute Stick 2 with the Intel&reg; OpenVINO&trade; toolkit. The more contributions to the ncappzoo, the more successful this community will be! We always encourage everyone with Neural Compute Stick related content to share by contributing their applications and model-related work to the ncappzoo. It's easy to do, and even when contributing new content you will be the owner and maintainer of the content.\n\nIf your inclusion is an opportunity to explore a new idea in computer vision, add as much documentation about the functionality and your process in creating your app or network, including smartly commenting your code. This will give others - and you! - a chance to learn from your addition. Your addition will help grow our community and improve all of our AI and computer vision skills. Most importantly, the insights you get from releasing your app into the wild here will only help you down the line if you ever want to commercialize your idea. As always, your work in the _ncappzoo_ should be properly attributed so that its ownership will always be managed by you and those you grant additional rights to.\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file for instructions and guidelines for contributing.\n\n## Licensing\nAll content in the _ncappzoo_ is licensed via the [MIT license](https://opensource.org/licenses/MIT) unless specifically stated otherwise in lower-level projects. Individual model and code owners maintain the copyrights for their content, but provide it to the community in accordance with the MIT License.\n\nSee the [LICENSE](LICENSE) file in the top-level directory for all licensing details, including reuse and redistribution of content in the _ncappzoo_ repository.\n\nIntel and the Intel logo are trademarks of Intel Corporation or its subsidiaries.\n\nOpenVINO is a trademark of Intel Corporation or its subsidiaries.\n\nRaspberry Pi is a trademark of the Raspberry Pi Foundation.\n\nTensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc.\n\n"
 },
 {
  "repo": "Paperspace/DataAugmentationForObjectDetection",
  "language": "Jupyter Notebook",
  "readme_contents": "# Data Augmentation For Object Detection\n\nAccompanying code for the [Paperspace tutorial series on adapting data augmentation methods for object detection tasks](https://blog.paperspace.com/data-augmentation-for-bounding-boxes/)\n\n## Dependencies\n1. OpenCV 3.x\n2. Numpy\n3. Matplotlib\n\nWe support a variety of data augmentations, like.\n\n### Horizontal Flipping\n![Horizontal Flip](Images/hflip.png)\n\n### Scaling\n![Scaling](Images/scale_aug.png)\n\n### Translation\n![Translation](Images/transl_aug.png)\n\n### Rotation\n![Rotation](Images/rotate.png)\n\n### Shearing\n![Shearing](Images/shear_box.png)\n\n### Resizing\n![Resizing](Images/resize.png)\n\n\n## Quick Start\nA quick start tutorial can be found in the file `quick-start.ipynb` in this repo.\n\n## Documentation\nA list of all possible transforms and extensive documentation can be found in by opening `docs/build/html/index.html` in your browser or at this [link.](https://augmentationlib.paperspace.com/)\n"
 },
 {
  "repo": "airob0t/idcardgenerator",
  "language": "Python",
  "readme_contents": "# idcardgenerator\n\u3010\u4ec5\u505a\u7814\u7a76\u4f7f\u7528\uff0c\u8bf7\u9075\u5b88\u5f53\u5730\u6cd5\u5f8b\u6cd5\u89c4\uff0c\u6cd5\u5f8b\u540e\u679c\u81ea\u8d1f\u3011\n\n\u8eab\u4efd\u8bc1\u56fe\u7247\u751f\u6210\u5de5\u5177,\u586b\u5165\u4fe1\u606f\uff0c\u9009\u62e9\u4e00\u5f20\u5934\u50cf\u56fe\u7247,\u5373\u53ef\u751f\u6210\u9ed1\u767d\u548c\u5f69\u8272\u8eab\u4efd\u8bc1\u56fe\u7247\u3002\n\n\u53ef\u4ee5\u9009\u62e9\u662f\u5426\u81ea\u52a8\u62a0\u56fe\uff0c\u81ea\u52a8\u62a0\u56fe\u76ee\u524d\u4ec5\u652f\u6301\u7eaf\u8272\u80cc\u666f\uff0c\u5bf9\u81ea\u52a8\u62a0\u56fe\u6548\u679c\u4e0d\u6ee1\u610f\u53ef\u4ee5\u624b\u52a8\u62a0\u56fe\u3002\n\n\u5728\u7ebf\u62a0\u56fe\u5730\u5740:(https://burner.bonanza.com/)\n\n(https://www.gaoding.com/koutu)\n\n## \u66f4\u65b0:\n- \u81ea\u52a8\u6539\u53d8\u5934\u50cf\u5927\u5c0f\n- \u81ea\u52a8\u4ece\u7eaf\u8272\u80cc\u666f\u4e2d\u62a0\u56fe\n- \u652f\u6301pip\u5b89\u88c5\n\n## ToDo\n- \u81ea\u52a8\u4ece\u590d\u6742\u80cc\u666f\u4e0b\u62a0\u56fe\n\n## \u73af\u5883\n- numpy\n- pillow\n- opencv\n\n## \u4e0b\u8f7d\n## pip\u5b89\u88c5\n`pip install idcardgenerator`\n\n```\nfrom idcardgenerator import gui\ngui.run()\n```\n\u6587\u4ef6\u4f1a\u751f\u6210\u5728\u8fd0\u884c\u76ee\u5f55\n\n### Windows\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/win_v1.3/idcardgenerator.exe)\n### Mac\n[\u4e0b\u8f7d](https://github.com/airob0t/idcardgenerator/releases/download/v1.1/idcardgenerator)\n\n## \u6253\u5305\u7a0b\u5e8f\n\n\u5b89\u88c5pyinstaller\n\n`pip install pyinstaller`\n\nMac\u6253\u5305(\u6253\u5305\u6210Mac app\u5c1a\u6709\u95ee\u9898\u672a\u89e3\u51b3)\n\n    pyinstaller -i usedres/ico.icns --windowed --clean --noconfirm --onefile --add-data ./usedres:./usedres idcardgenerator.py\n\nWindows\u6253\u5305\n\n    pyinstaller -i usedres/ico.ico --windowed --clean --noconfirm --onefile --add-data \"usedres;usedres\" idcardgenerator.py\n\n## \u53c2\u7167\u6807\u51c6\uff1a\n\n### \u6b63\u9762\n\u3000\u3000\u5de6\u4e0a\u89d2\u4e3a\u56fd\u5fbd\uff0c\u7528\u7ea2\u8272\u6cb9\u58a8\u5370\u5237;\u5176\u53f3\u4fa7\u4e3a\u8bc1\u4ef6\u540d\u79f0\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\uff0c\u5206\u4e0a\u4e0b\u4e24\u6392\u6392\u5217\uff0c\u5176\u4e2d\u4e0a\u6392\u7684\u201c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u201d\u4e3a4\u53f7\u5b8b\u4f53\u5b57\uff0c\u4e0b\u6392\u7684\u201c\u5c45\u6c11\u8eab\u4efd\u8bc1\u201d\u4e3a2\u53f7\u5b8b\u4f53\u5b57;\u201c\u7b7e\u53d1\u673a\u5173\u201d\u3001\u201c\u6709\u6548\u671f\u9650\u201d\u4e3a6\u53f7\u52a0\u7c97\u9ed1\u4f53\u5b57;\u7b7e\u53d1\u673a\u5173\u767b\u8bb0\u9879\u91c7\u7528\uff0c\u201cxx\u5e02\u516c\u5b89\u5c40\u201d;\u6709\u6548\u671f\u9650\u91c7\u7528\u201cxxxx.xx-xxxx.xx.xx\u201d\u683c\u5f0f\uff0c\u4f7f\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff0c\u5168\u90e8\u7528\u9ed1\u8272\u6cb9\u58a8\u5370\u5237\u3002\n\n### \u80cc\u9762\n\u3000\u3000\u201c\u59d3\u540d\u201d\u3001\u201c\u6027\u522b\u201d\u3001\u201c\u6c11\u65cf\u201d\u3001\u201c\u51fa\u751f\u5e74\u6708\u65e5\u201d\u3001\u201c\u4f4f\u5740\u201d\u3001\u201c\u516c\u6c11\u8eab\u4efd\u53f7\u7801\u201d\u4e3a6\u53f7\u9ed1\u4f53\u5b57\uff0c\u7528\u84dd\u8272\u6cb9\u58a8\u5370\u5237\uff1b\u767b\u8bb0\u9879\u76ee\u4e2d\u7684\u59d3\u540d\u9879\u75285\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u5176\u4ed6\u9879\u76ee\u5219\u7528\u5c0f5\u53f7\u9ed1\u4f53\u5b57\u5370\u5237\uff1b\u51fa\u751f\u5e74\u6708\u65e5 \u65b9\u6b63\u9ed1\u4f53\u7b80\u4f53\u5b57\u7b26\u5927\u5c0f\uff1a\u59d3\u540d\uff0b\u53f7\u7801\uff0811\u70b9\uff09\u5176\u4ed6\uff089\u70b9\uff09\u5b57\u7b26\u95f4\u8ddd\uff08AV\uff09\uff1a\u53f7\u7801\uff0850\uff09\u5b57\u7b26\u884c\u8ddd\uff1a\u4f4f\u5740\uff0812\u70b9\uff09\uff1b\u8eab\u4efd\u8bc1\u53f7\u7801\u5b57\u4f53   OCR-B 10 BT   \u6587\u5b57 \u534e\u6587\u7ec6\u9ed1\u3002\n"
 },
 {
  "repo": "xiangjiana/Android-MS",
  "language": null,
  "readme_contents": "[**\u7248\u6743\u58f0\u660e**](#\u7248\u6743\u58f0\u660e)\n\n# Android \u9ad8\u7ea7\u9762\u8bd5\n\n### ![\u9762\u8bd5](img/2020Android\u6700\u65b0\u6280\u672f\u8be6\u89e3.png)\n#### \u6700\u65b0\u66f4\u6587\uff1a\n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n\n#### \u5199\u7ed9Android\u7684\u4e00\u5c01\u4fe1\n### \u5bf9\u4e8e\u8eab\u8fb9\u6b63\u5728\u9762\u8bd5\u548c\u9762\u8bd5\u4e2d\u7684\u4eba\uff0c\u52a0\u4e0a\u6211\u4ee5\u5f80\u7684\u9762\u8bd5\u7ecf\u5386\u6574\u7406\u4e86\u5982\u4e0b\u9762\u8bd5\u5907\u8003\u8def\u7ebf\uff0c\u548cPDF\u7248\uff08\u6709\u76f8\u5e94\u7684\u89c6\u9891\u6559\u7a0b\u5728\u540e\u9762\uff09\n### ![MS](img/MS.png)\n### ![PDF](img/PDF.png)\n\n\u6700\u8fd1\u534a\u5e74\uff0c\u5e38\u5e38\u6709\u4eba\u95ee\u6211 \u201cAndroid\u5c31\u4e1a\u5e02\u573a\u7a76\u7adf\u600e\u4e48\u6837\uff0c\u6211\u8fd8\u80fd\u4e0d\u80fd\u575a\u6301\u4e0b\u53bb ?\u201d\n\n\u73b0\u5728\u60f3\u60f3\uff0c\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u53d1\u5c55\u4e0d\u77e5\u4e0d\u89c9\u5df2\u7ecf\u5341\u591a\u5e74\u4e86\uff0cMobile First \u4e5f\u5df2\u7ecf\u53d8\u6210\u4e86 AI First\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u518d\u662f\u201c\u98ce\u53e3\u4e0a\u7684\u732a\u201d\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u5149\u73af\u548c\u6ea2\u4ef7\u5f00\u59cb\u6162\u6162\u6d88\u5931\uff0c\u5e76\u4e14\u6b63\u5728\u5411 AI\u3001\u533a\u5757\u94fe\u7b49\u65b0\u7684\u9886\u57df\u8f6c\u79fb\u3002\u79fb\u52a8\u5f00\u53d1\u7684\u65b0\u9c9c\u8840\u6db2\u4e5f\u5df2\u7ecf\u53d8\u5c11\uff0c\u6700\u660e\u663e\u7684\u662f\u56fd\u5185\u5e94\u5c4a\u751f\u90fd\u7eb7\u7eb7\u6d8c\u5411\u4e86 AI \u65b9\u5411\u3002\n\n\u200b       \u53ef\u4ee5\u8bf4\uff0c\u56fd\u5185\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u7ea2\u5229\u671f\u5df2\u7ecf\u8fc7\u53bb\u4e86\uff0c\u73b0\u5728\u662f\u589e\u91cf\u4e0b\u964d\u3001\u5b58\u91cf\u53ae\u6740\uff0c\u4ece\u4e89\u593a\u7528\u6237\u5230\u4e89\u593a\u65f6\u957f\u3002\u6bd4\u8f83\u660e\u663e\u7684\u662f\u624b\u673a\u5382\u5546\u7eb7\u7eb7\u4e92\u8054\u7f51\u5316\uff0c\u4e0e\u4f20\u7edf\u4e92\u8054\u7f51\u4f01\u4e1a\u76f4\u63a5\u7ade\u4e89\u3002\u53e6\u5916\u4e00\u65b9\u9762\uff0c\u8fc7\u53bb\u6e20\u9053\u7684\u6253\u6cd5\u5931\u7075\uff0c\u5c0f\u7a0b\u5e8f\u3001\u5feb\u5e94\u7528\u7b49\u65b0\u5174\u6e20\u9053\u5d1b\u8d77\uff0c\u65e0\u8bba\u662f\u624b\u673a\u5382\u5546\uff0c\u8fd8\u662f\u5404\u5927 App \u90fd\u628a\u51fa\u6d77\u6446\u5230\u4e86\u6218\u7565\u7684\u4f4d\u7f6e\u3002\n\n\u5404\u5927\u57f9\u8bad\u5e02\u573a\u4e5f\u4e0d\u518d\u57f9\u8badAndroid\uff0c**\u4f5c\u4e3a\u5f00\u53d1Android\u7684\u6211\u4eec\u8be5\u4f55\u53bb\u4f55\u4ece\uff1f**\n\n\u200b        \u5176\u5b9e\u5982\u679c\u4f60\u6280\u672f\u6df1\u5ea6\u8db3\u591f\uff0c\u5927\u5fc5\u4e0d\u7528\u4e3a\u5c31\u4e1a\u800c\u5fe7\u6101\u3002\u6bcf\u4e2a\u884c\u4e1a\u4f55\u5c1d\u4e0d\u662f\u8fd9\u6837\uff0c\u6700\u5f00\u59cb\u7684\u98ce\u53e3\uff0c\u5230\u6162\u6162\u7684\u6210\u719f\u3002Android\u521d\u7ea7\u57282019\u5e74\u7684\u65e5\u5b50\u91cc\u98ce\u5149\u4e0d\u518d\uff0c \u9760\u4f1a\u56db\u5927\u7ec4\u4ef6\u5c31\u80fd\u591f\u83b7\u53d6\u5230\u6ee1\u610f\u85aa\u8d44\u7684\u65f6\u4ee3\u4e00\u53bb\u4e0d\u590d\u8fd4\u3002**\u7ecf\u8fc7\u4e00\u6ce2\u4e00\u6ce2\u7684\u6dd8\u6c70\u4e0e\u6d17\u724c\uff0c\u5269\u4e0b\u7684\u90fd\u662f\u6280\u672f\u7684\u91d1\u5b50\u3002\u5c31\u50cf\u5927\u6d6a\u892a\u53bb\uff0c\u88f8\u6cf3\u7684\u4f1a\u6162\u6162\u4e0a\u5cb8\u3002**\u800c\u771f\u6b63\u575a\u6301\u4e0b\u6765\u7684\u4e00\u5b9a\u4f1a\u53d6\u5f97\u4e0d\u9519\u6210\u7ee9\u3002\u6bd5\u7adfAndroid\u5e02\u573a\u662f\u5982\u6b64\u4e4b\u5927\u3002\u4eceAndroid\u9ad8\u7ea7\u7684\u84ec\u52c3\u7684\u5c31\u4e1a\u5c97\u4f4d\u9700\u6c42\u6765\u770b\uff0c\u80fd\u575a\u4fe1\u6211\u4eec\u6bcf\u4e00\u4f4dAndroid\u5f00\u53d1\u8005\u7684\u68a6\u60f3 \u3002\n\n### ![2020\u9762\u8bd5\u4e13\u9898](img/2020\u9762\u8bd5\u4e13\u9898.png)\n### ![2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55](img/2020\u9762\u8bd5\u4e13\u9898\u76ee\u5f55.png)\n### ![23\u79cd\u8bbe\u8ba1\u6a21\u5f0f](img/23\u79cd\u8bbe\u8ba1\u6a21\u5f0f.png)\n\n \u63a5\u4e0b\u6765\u6211\u4eec\u9488\u5bf9Android\u9ad8\u7ea7\u5c55\u5f00\u7684\u5b8c\u6574\u9762\u8bd5\u9898 \n ### \u4e00\u4e36kotlin(\u89c6\u9891\uff09\n - [kotlin\u5927\u51681-10\u89c6\u9891\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/Kotlin_%E5%89%AF%E6%9C%AC.png)\n ### \u4e8c\u4e36flutter\uff08\u89c6\u9891\uff09\n - [flutter\u89c6\u9891\u5305](https://github.com/xiangjiana/Android-MS/blob/master/img/flutter.png)\n ### \u4e09\u4e36\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u4ece0\u5f00\u59cb\u8ba4\u8bc6\uff08\u89c6\u9891\uff09\n - [\u97f3\u89c6\u9891\u9ad8\u624b\u5f00\u53d1\u7cfb\u5217\u89c6\u98911-10](https://github.com/xiangjiana/Android-MS/blob/master/img/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%911-10%E8%A7%86%E9%A2%91.png)\n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08\u89c6\u9891\u89e3\u7801\u57fa\u7840-\u5c01\u88dd\u683c\u5f0f\uff09](https://mp.weixin.qq.com/s/vNKkl7xXsZhgALu7VigcDg) \n \n - [\u6df1\u5165\u8bb2\u89e3\u97f3\u89c6\u9891\u7f16\u7801\u539f\u7406\uff0cH264\u7801\u6d41\u8be6\u89e3\uff08H264\u7f16\u7801-\u5e27\u5185\u9884\u6d4b\uff09](https://mp.weixin.qq.com/s/OA6DV_hnCoWnn_0lcheVbg)\n \n \n \n ### \u56db\u4e36\u6700\u65b0\u89c6\u9891\u66f4\u65b0\uff1a\n - [1.90\u5206\u949f\u641e\u5b9a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide\uff0c\u9762\u8bd5\u5b9e\u6218\u4e00\u6761\u9f99](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n - [2.\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [3.\u963f\u91cc\u9762\u8bd5\u9898\uff1a\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [4.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u5f00\u53d1\u6280\u80fd-\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [5.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u8fdb\u9636\uff0c\u5b9e\u73b0\u8d44\u6e90\u66f4\u65b0\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [6.Binder\u673a\u5236\u8be6\u89e3\uff0c\u7528Binder\u4e2dmmap\u601d\u60f3\u89e3\u51b3\u4f60\u7684APP\u5361\u987f\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [7.\u5317\u4e0a\u5e7f\u6df110\u5e74\u9762\u8bd5\u7ecf\u9a8c\u8001\u53f8\u673a\u503e\u56ca\u76f8\u6388\uff0c\u8ba9\u4f60\u5c11\u8d705-10\u5e74\u5f2f\u8def\u7684\u9762\u8bd5\u79c1\u623f\u8bfe\uff08\u804c\u4e1a\u8def\u5f84.\u7b80\u5386\u89c4\u5212.\u9762\u8bd5\u5b98\u5fc3\u7406\u5206\u6790.\u6280\u672f\u9762\u8bd5\u5b9e\u6218\uff0cGlide\uff0cOkhttp\uff0c\u4f18\u5316\u9762\u8bd5\u9898\u52a9\u4f60\u6253\u901a\u4efb\u7763\u4e8c\u8109\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [8.\u4e0d\u4f1aNDK\u600e\u4e48\u73a9\u70ed\u4fee\u590d\uff1f\u4eca\u665a\u6559\u4f60\u4eceJava\u5c42\u5b9e\u73b0\u817e\u8bafTinker\u70ed\u4fee\u590d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [9.\u8fc8\u5411\u67b6\u6784\u5e08\u7684\u7b2c\u4e00\u6b65-\u4ece\u6253\u9020\u81ea\u5df1\u7684\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u5f00\u59cb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [10.Android\u9879\u76ee\u7684\u6700\u7ec8\u8fdb\u5316,\u63d2\u4ef6\u5316\u5f00\u53d1\u8ba9\u4f60\u7684\u5e94\u7528\u52a0\u8f7d\u6d77\u91cf\u63d2\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [11.\u544a\u522b\u201c\u642c\u8fd0\u5de5\u201d\u624b\u5199\u5fae\u4fe1\uff0cQQ\u90fd\u5728\u7528\u7684\u6570\u636e\u6846\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [12.JVM\u865a\u62df\u673a\u5c42\u770bKlass \u5bf9\u8c61\u751f\u6210\u673a\u5236\uff0c\u63ed\u79d8\u4ece\u672a\u770b\u8fc7\u7684\u7ec6\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [13.Android\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u6218\uff0c\u6253\u9020\u5168\u81ea\u52a8\u6ce8\u5165\u6846\u67b6ButterKnife](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [14.\u6027\u80fd\u4f18\u4ece\u53ea\u4f1a\u5f00\u53d1\u5230\u5168\u76d8\u638c\u63a7App\u6027\u80fd\uff0c\u53ea\u9700\u8981\u4ece\u8fd9\u8282\u8bfe\u5f00\u59cb\u5316](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [15.\u8001\u53f8\u673a\u6253\u7834Bitmap\u5e38\u89c4\u601d\u7ef4\uff0c\u4eceSkia\u5f15\u64ce\u770bBitmap\u52a0\u8f7d\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [16.\u4f60\u7684\u5e94\u7528\u505a\u4e86\u57cb\u70b9\u4e0a\u4f20\u5417\uff1f\u624b\u5199\u7f16\u8bd1\u65f6\u4eca\u65e5\u5934\u6761\u7684\u57cb\u70b9\u67b6\u6784](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [17.NDK\u5927\u725b\u5e26\u4f60\u4e00\u5802\u8bfe\u641e\u5b9a\u4e00\u7ebf\u5927\u5382\u97f3\u89c6\u9891\u9762\u8bd5\u96c6\u5408](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [18.\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\u5417\uff1f\uff0c\u5982\u679c\u8fde\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6\u90fd\u62e7\u4e0d\u6e05\u600e\u4e48\u884c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [19.\u9762\u8bd5\u4e13\u9898-Okhttp\u76f8\u5173\u9762\u8bd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [20.\u9762\u8bd5\u4e13\u9898-Okhttp\u9762\u8bd5\u4e13\u9898\u5b8c\u7ed3\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [21.\u9762\u8bd5\u4e13\u9898\u4e4b\u6253\u901aGlide\u6e90\u7801\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [22.UI\u4f18\u5316\u662f\u4e0d\u662f\u53ea\u4f1a\u8bf4\u5e03\u5c40\u5c42\u7ea7\u4e0d\u80fd\u592a\u6df1\uff1f\u6765\uff0c\u8fd9\u91cc\u6709\u5168\u76d8\u6df1\u5ea6\u5206\u6790\uff01](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [23.\u6ce8\u89e3\u53cd\u5c04\u7684\u9ad8\u7ea7\u6280\u5de7\uff0c\u8ba9\u4f60\u5f7b\u5e95\u4e86\u89e3EventBus\u662f\u5982\u4f55\u8fdb\u884c\u7ec4\u4ef6\u901a\u4fe1\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [24.\u542c\u8bf4\u4f60\u60f3\u6210\u4e3a\u67b6\u6784\u5e08\uff1f\u90a3\u4f60\u6709\u67b6\u6784style\u6ca1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [25.Android\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u7684\u9ad8\u7ea7\u8fdb\u9636\uff0c\u624b\u5199\u5b9e\u73b0\u7f51\u6613\u4e91\u4e3b\u9898\u6362\u80a4\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [26.Binder\u4e13\u9898\uff08\u4e00\uff09\u76f4\u6363Binder\u56db\u5c42\u6846\u67b6\uff0c\u8da3\u8bb2Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [27.Binder\u4e13\u9898\uff08\u4e8c\uff09\u8fdb\u7a0b\u901a\u4fe1\u7684\u6838\u5fc3\u5185\u5b58\u7ba1\u7406\u4e0e\u8c03\u5ea6\uff0c\u6df1\u5165\u7406\u89e3Binder\u4e2d\u5185\u5b58\u64cd\u4f5c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [28.Binder\u4e13\u9898\uff08\u4e09\uff09\u57fa\u4e8eBinder\u7684\u5e95\u5c42\u5b9e\u73b0\uff0c\u624b\u5199Binder\u8fdb\u7a0b\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [29.Jetpack\u4e4b\u540e\u4f60\u8fd8\u6ca1\u6709\u89e3\u9501LiveData\uff1f\u6765\uff0c\u4e00\u8282\u8bfe\u5e26\u4f60\u89e3\u9501\u5e76\u4e14\u8fd0\u7528\u5230\u9879\u76ee\u4e2d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [30.\u4e3a\u4ec0\u4e48\u9009\u62e9Glide\u4f5c\u4e3a\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u4e0eFresco\uff0cPascco\u4f18\u52bf\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [31.\u5373\u5b66\u5373\u7528\u7684Android\u9ad8\u7ea7\u6280\u80fd\uff0c\u5927\u957f\u56fe\u52a0\u8f7d\u539f\u7406\u53ca\u624b\u5199\u5b9e\u73b0(\u54c8\u592b\u66fc\u7b97\u6cd5\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [32.\u9ad8\u7ea7UI\u8981\u4e0d\u8981\u4e86\u89e3\u4e0b\uff1f\u8001\u53f8\u673a\u5e26\u4f60\u6765\u4e00\u5802\u81ea\u5b9a\u4e49ViewGroup\u5b9e\u6218\u8bfe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [33.\u9762\u8bd5\u65f6\u603b\u88ab\u5185\u5b58\u95ee\u9898\u8650\u5343\u904d\uff1f\u7406\u8bba\u77e5\u8bc6\u53c8\u770b\u4e0d\u61c2\uff1f\u6765\uff0c\u8fd9\u8282\u8bfe\u5f7b\u5e95\u641e\u5b9a\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [34.Android\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u5230\u5e95\u6709\u591a\u7b80\u5355\uff0c\u56db\u4e2a\u6838\u5fc3\u7c7b\u9610\u8ff0\u5176\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Android\u5e38\u7528\u7ec4\u4ef6\u901a\u4fe1\u65b9\u5f0f\u9610\u8ff0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ce8\u89e3\u4e0e\u53cd\u5c04\u5728\u4e8b\u4ef6\u603b\u7ebf\u6846\u67b6\u4e2d\u7684\u4f7f\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u4ece\u5b9e\u6218\u4e2d\u4e86\u89e3\u89e3\u8026\u7684\u6838\u5fc3\u601d\u60f3\u4e0e\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [35.\u865a\u62df\u673a\u8be6\u89e3\u5185\u5b58\u7ed3\u6784\u539f\u7406\uff0c\u7528\u4ee3\u7801\u7684\u89d2\u5ea6\u5206\u6790\u5185\u5b58\u5206\u5e03](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u65b9\u6cd5\u533a\uff0c\u5806\u533a\uff0c\u6808\u533a\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5b57\u8282\u7801\u6307\u4ee4\u4e0eDex\u6307\u4ee4\u96c6\u6267\u884c\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5bc4\u5b58\u5668\u4e0e\u865a\u62df\u6808\u6307\u4ee4\u6d41\u7a0b\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [36.\u5927\u5382\u67b6\u6784\u5e08\u5e26\u4f60\u624b\u5199Glide\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\uff0c\u8ba9\u4f60\u79bb\u67b6\u6784\u5e08\u7684\u8ddd\u79bb\u66f4\u8fd1\u4e00\u6b65](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.Glide\u6846\u67b6\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u8981\u600e\u6837\u5c01\u88c5?](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u600e\u4e48\u5904\u7406\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u7684\u9ad8\u5e76\u53d1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.Glide\u7684\u4e09\u7ea7\u7f13\u5b58\u673a\u5236\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [37.\u5e26\u4f60\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u89e3\u8bfbHandler\u6838\u5fc3\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n  - [1.Handler\u6e90\u7801\u5206\u6790\u7684\u4e09\u6761\u4e3b\u7ebf](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6e90\u7801\u4e2d\u9690\u85cf\u7684\u90a3\u4e9b\u4e0d\u80fd\u5ffd\u89c6\u7684\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.Handler\u76f8\u5173\u9762\u8bd5\u9898\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [38.FFmpeg\u548cMediaCodec\u7684API\u770b\u4e0d\u61c2\uff1f\u5e94\u8be5\u4ece\u97f3\u89c6\u9891H264\u7f16\u7801\u539f\u7406\u5165\u624b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.H264\u539f\u7406\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4fe1\u6e90\u7f16\u7801\u5668\u662f\u5982\u4f55\u5bf9\u89c6\u9891\u5e27\u8fdb\u884c\u7f16\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3. slice(\u5207\u7247) Nal(\u5b8f\u5757) \uff08\u50cf\u7d20\u9884\u6d4b\uff09\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [39.\u5982\u679c\u4f60\u662f\u67b6\u6784\u5e08\uff1f\u600e\u6837\u80fd\u8ba9\u4f60\u7684\u5e94\u7528\u53ea\u670910M\u7684\u4f53\u79ef\u786e\u62e5\u67091000M\u7684\u529f\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5982\u4f55\u8ba9\u4f60\u7684\u9879\u76ee\u4f53\u79ef\u5c0f\u529f\u80fd\u591a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u6ca1\u6709\u5b89\u88c5\u7684APK\u5305\u6211\u4eec\u600e\u4e48\u8ba9\u5b83\u201c\u52a8\u8d77\u6765\u201d](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u67b6\u6784\u5e08\u8be5\u5e72\u7684\u6d3b\u5c31\u662f\u628a\u4e0d\u53ef\u80fd\u53d8\u6210\u53ef\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u624b\u5199\u5b9e\u73b0\u5927\u5382\u90fd\u5728\u73a9\u7684\u63d2\u4ef6\u5316\u5f00\u53d1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [40.\u6296\u97f3\u89c6\u9891\u526a\u8f91\u539f\u7406\uff0c\u624b\u5199\u89c6\u9891\u526a\u8f91\u4e0e\u80cc\u666f\u97f3\u4e50\u5408\u6210](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  - [1.MeidiaCodec\u7f16\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u4e0d\u66ff\u6362\u89c6\u9891\u97f3\u4e50\u524d\u63d0\u4e0b\uff0c\u5c06\u6b4c\u66f2\u5408\u6210\u5230\u89c6\u9891\u58f0\u97f3  ](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u767b\u9876IT\u754c\u7684\u7687\u51a0-\u624b\u5199\u89c6\u9891\u526a\u8f91\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [41.Android\u9ad8\u7ea7\u6280\u80fd-\u5927\u56fe\u52a0\u8f7d\uff0c\u800c\u4f60\u5728\u672c\u8282\u8bfe\u5b66\u5230\u7684\u53ef\u4e0d\u6b62\u8fd9\u4e00\u4e2a\u70b9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u5927\u56fe\u52a0\u8f7d\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u5185\u5b58\u590d\u7528\uff0c\u56fe\u7247\u5360\u7528\u5185\u5b58\u5206\u6790\u4ee5\u53ca\u4ee3\u7801\u5b9e\u73b0](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u5982\u4f55\u521b\u5efa\u5927\u56fe\u800c\u4e0dOOM\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u989d\u5916\u5206\u4eab\uff1a\u9762\u8bd5\u9047\u5230\u5b8c\u5168\u4e0d\u4f1a\u7684\u95ee\u9898\u600e\u4e48\u529e\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [42. jepg\u56fe\u50cf\u5f15\u64ce\u5b9e\u73b0\u8d85\u8fc7\u539f\u751f\u7684\u56fe\u7247\u538b\u7f29\u6027\u80fd](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u54c8\u592b\u66fc\u538b\u7f29\u7b97\u6cd5\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.Bitmap\u6e90\u7801\u89e3\u8bfb](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.native\u5c42\u8bfb\u53d6\u56fe\u7247](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [43.\u7834\u89e3\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7aa5\u63a2\u963f\u91ccARouter\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u8def\u7531\u6846\u67b6\u7a76\u7adf\u662f\u4ec0\u4e48\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u963f\u91ccARouter\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.APT\u6280\u672f\u5b9e\u73b0\u624b\u5199Arouter\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [44.\u963f\u91ccP7\u5c97\u9762\u8bd5\u5173\u4e8eRecyclerView\u7684\u8fde\u73af\u70ae\uff0c\u4e00\u5c0f\u65f6\u89e3\u51b3RecyclerView\u6240\u6709\u5e95\u5c42\u7591\u60d1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.RecyclerView\u7684\u590d\u7528\u673a\u5236\uff0c\u7b80\u5355\u8bf4\u8bf4View\u56de\u6536\u4e0e\u590d\u7528\u7684\u8fc7\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.RecyclerView\u652f\u6301\u591a\u4e2a\u4e0d\u540c\u7c7b\u578b\u5e03\u5c40\uff0c\u4ed6\u4eec\u600e\u4e48\u7f13\u5b58\uff0c\u5e76\u4e14\u67e5\u627e\u7684\u5462](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.\u8bf4\u4e00\u8bf4RecyclerView\u9002\u914d\u5668\u7684\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u7406\u6e05RecyclerView\u67b6\u6784\u601d\u60f3\uff0c\u624b\u5199RecyclerView\u81ea\u5b9a\u4e49\u63a7\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n \n- [45.\u5343\u4e07\u7ea7\u5e94\u7528\u7f8e\u56e2Robust\u4fee\u590d\u539f\u7406\uff0c\u624b\u5199\u5b57\u8282\u7801\u63d2\u4ef6\u6280\u672f\u6280\u672f\u70b9\uff1a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.\u7f8e\u56e2robust\u4fee\u590d\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [2.robust\u4f9d\u8d56\u7684\u63d2\u4ef6\u5b9e\u73b0\u65b9\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.apk\u7f16\u8bd1\u539f\u7406\uff0cgroovy\u5b9e\u73b0\u52a8\u6001\u63d2\u5165\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [4.\u81ea\u5df1\u5b9e\u73b0robust\u63d2\u4ef6\uff0c\u52a8\u6001\u6539\u52a8\u4e3b\u5de5\u7a0b\u4ee3\u7801](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [46.\u67b6\u6784\u5e08\u4fee\u70bc\u4e4b\u8def-\u7ad9\u5728\u67b6\u6784\u5e08\u7684\u89d2\u5ea6\u5982\u4f55\u5999\u7528\u81ea\u5b9a\u4e49\u6ce8\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [1.\u4e3a\u4ec0\u4e48\u4e0d\u7528EventBus\u4e86](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [2. \u4e8b\u4ef6\u81a8\u80c0\u5982\u4f55\u89e3\u51b3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [3. \u7ec4\u4ef6\u901a\u4fe1\u8fd8\u80fd\u600e\u4e48\u73a9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n  \n  \n \n ### \u4e94\u4e36\u4e13\u9898\u7bc7\n #### 2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898\n  - [2020\u5e74\u6700\u65b0Android\u5927\u5382\u9762\u8bd5\u8bfe \u53ea\u4e3a\u4f60\u8fdbBAT\u589e\u52a050%\u7684\u6210\u529f\u7387\u9762\u8bd5\u4e13\u9898](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n - [1.\u4ec0\u4e48\u662f\u6ca1\u6709\u65b9\u5411\u5c31\u505c\u4e0b\u6765\uff0c\u4e86\u89e3Android\u53d1\u5c55\u624d\u80fd\u7a33\u6b65\u524d\u884c\u7ec4\u4ef6\u5316\uff1f\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\u5f00\u53d1](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u7528\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1Android\u5f00\u53d1\u8005\u7684\u524d\u666f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u8ba9offer\u8ffd\u7740\u4f60\u7684\u79d8\u8bc0\u662f\u4ec0\u4e48\uff1f\u7b80\u5386\u5168\u76d8\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5236\u5b9a\u804c\u4e1a\u89c4\u5212\uff0c\u4e3a\u672a\u6765\u94fa\u5e73\u9053\u8def](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [2.\u5de5\u6b32\u5584\u5176\u4e8b\u5fc5\u5148\u5229\u5176\u5668-OkHttp\u6e90\u7801\u89e3\u8bfb](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4e3a\u4ec0\u4e48OkHttp\u4f7f\u7528Socket\u800c\u4e0d\u662fHttpUrlConnection](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Okhttp\u6d41\u7a0b\uff0c\u6838\u5fc3\u7c7b\u5168\u89e3\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u4ece\u6e90\u7801\u89d2\u5ea6\u4e86\u89e3Okhttp\u4e2d\u6784\u5efa\u8005\u4e0e\u8d23\u4efb\u94fe\u6a21\u5f0f](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [\u5982\u4f55\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u7f51\u7edc\u8bbf\u95ee\u6846\u67b6](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    \n - [3.\u6280\u80fd\u6301\u7eed\u8fdb\u9636-Glide\u8be6\u89e3\uff0c\u8ba9\u4f60\u5bf9\u56fe\u7247\u52a0\u8f7d\u6846\u67b6\u77e5\u6839\u77e5\u5e95](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u6e90\u7801\u4e09\u6761\u4e3b\u7ebf\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n    - [Glide\u751f\u547d\u5468\u671f\u7ba1\u7406\u7b56\u7565\u5206\u6790](https://mp.weixin.qq.com/s/n2Y-86BG9p3EFkU6x7dSOg)\n\n\n #### \u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab\n  - [\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\uff0c\u624b\u6dd8\u9879\u76ee\u5185\u90e8\u67b6\u6784\u5206\u4eab](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n - [1.\u544a\u522b\u4f20\u7edf\u5355\u4e00\u6a21\u5757\u5f00\u53d1\uff0c\u62e5\u62b1\u7ec4\u4ef6\u5316\u5f00\u53d1\u6a21\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4ec0\u4e48\u662f\u7ec4\u4ef6\u5316\uff0c\u4e3a\u4ec0\u4e48\u8981\u53bb\u5c06\u9879\u76ee\u7ec4\u4ef6\u5316\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u600e\u6837\u5bf9\u9879\u76ee\u4e2d\u6240\u6709\u7684\u4e1a\u52a1\u6a21\u5757\u8fdb\u884c\u79d1\u5b66\u7ba1\u7406\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u600e\u6837\u8fdb\u884c\u7a97\u4f53\u8df3\u8f6c\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u7684\u539f\u7406\u89e3\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u7ec4\u4ef6\u5316\u5f00\u53d1\u7684\u6838\u5fc3\u5bc6\u7801\uff0c\u7ec4\u4ef6\u5316\u7406\u7531\u6846\u67b6\u624b\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b8c\u7f8e\u5229\u7528](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7c7b\u751f\u6210\u795e\u5668Java poet\u795e\u52a9\u653b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u8def\u7531\u8868\u7684\u7a7a\u865a\u5bc2\u5bde\u51b7\u8ba9\u6211\u4eec\u5982\u4f55\u6ee1\u8db3\u5b83](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u624b\u5199\u5b9e\u73b0\u7ec4\u4ef6\u5316\u8def\u7531\u6846\u67b6\u8ba9\u7ec4\u4ef6\u5316\u9879\u76ee\u5982\u864e\u6dfb\u7ffc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3.\u7ec4\u4ef6\u5316\u5f00\u53d1\u4e2d\u6ca1\u6709\u8026\u5408\u7684\u4e1a\u52a1\u6a21\u5757\u8981\u5982\u4f55\u901a\u4fe1\uff1f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u9762\u5bf9Android\u5e02\u573a\u7684N\u4e2d\u901a\u4fe1\u65b9\u5f0f\u6211\u4eec\u8be5\u5982\u4f55\u6289\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Jetpack\u4e2d\u7684liveData\u6253\u9020\u7ec4\u4ef6\u5316\u901a\u4fe1\u6846\u67b6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n #### Android\u7cfb\u7edf\u6e90\u7801FrameWork\u5b9e\u6218\u4e13\u9898\n - [1.Android10.0 9.0 8.0\u6df1\u5165\u865a\u62df\u673a\u5e95\u5c42\u4e2d\u8bb2\u89e3\u5185\u5b58\u5206\u5e03\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [30\u5206\u949f\u7406\u6e05\u7a0b\u5e8f\u5458\u5bf9\u5185\u5b58\u7684\u6050\u60e7\uff0c\u5e73\u65f6\u5199\u7684\u53d8\u91cf\uff0c\u5bf9\u8c61\u5728\u5c4b\u91cc\u5185\u5b58\u7684\u5206\u90e8](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u65b9\u6cd5\u533a\uff0c\u6808\u533a\uff0c\u5806\u533a\u8fd0\u884c\u673a\u5236\u548c\u6d41\u7a0b\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u57fa\u4e8e\u5bc4\u5b58\u5668\u4e0eJVM\u57fa\u4e8eJava\u6808\u7684\u533a\u522b\u548c\u539f\u7406-\u5bc4\u5b58\u5668\u7684\u4f5c\u7528\u4e8e\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u53cd\u7f16\u8bd1dex\u6587\u4ef6\u4e4barm\u6307\u4ee4\u96c6\u5206\u6790\uff0c\u770b\u770bnew\u4e00\u4e2a\u5bf9\u8c61\u6700\u7ec8\u53d8\u6210\u4e86\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [2.\u67e5\u770bAndroid\u7cfb\u7edf\u6e90\u7801\uff0cdex\u4e2dARM\u516c\u53f8\u7684\u6307\u4ee4\u96c6\u52a0\u8f7d\u4e0e\u6267\u884c\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6df1\u5165\u7406\u89e3Android\u6838\u5fc3\u5173\u952e\u5b57 new synchronized volatile final\u5e95\u5c42\u5b9e\u73b0\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Java\u5bf9\u8c61\u5728\u5806\u533a\u5927\u5c0f\u4e3a8\u4e2a\u5b57\u8282\u53ca\u6bcf\u4e00\u4e2a\u5206\u90e8\u8be6\u89e3-hashcode\u51fd\u6570\u6267\u884c\u673a\u5236\uff0chashcode\u5728\u5185\u5b58\u4e2d\u5b58\u5728\u54ea\u91cc](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [class\u7c7b\u5728\u65b9\u6cd5\u533a\u4e3a\u4f55\u662f426\u4e2a\u5b57\u8282\uff0c\u65b9\u6cd5\u4e3a\u4f55\u4f1a\u589e\u52a04\u4e2a\u5b57\u8282](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u4eceJNIEnv\u5165\u624b\u627e\u5230\u7c7b\u52a0\u8f7d\u4e0eclass\u7c7b\u6784\u9020\u673a\u5236](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    \n - [3. sophix\u70ed\u4fee\u590d\u524d\u8eab--Arm\u6307\u4ee4\u96c6\u66ff\u6362\u5b9e\u73b0java\u65b9\u6cd5\u52a8\u6001\u4fee\u590d\u6280\u672f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [artMethod\u7ed3\u6784\u4f53 \u8be6\u7ec6\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u6267\u884c\u5f15\u64ce\u4e2dclass\u5982\u4f55\u88ab\u52a0\u8f7d\uff0c\u4e09\u90e8\u66f2\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Findclass\u51fd\u6570\uff0c\u7c7b\u5982\u4f55\u5728\u865a\u62df\u673a\u5c42\u53ea\u52a0\u8f7d\u4e00\u6b21](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Defineclass\u51fd\u6570\uff0c\u5b9a\u4e49\u4e00\u4e2a\u7a7a\u7684class](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n     - [Loadclass\u51fd\u6570\uff0cdex\u6587\u4ef6\u4e2d\u5c06\u7c7b\u6709\u78c1\u76d8\u7f13\u5b58\u5230\u5185\u5b58](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n  - [\u865a\u62df\u673a\u5c42\u66ff\u6362\u6267\u884c\u5f15\u64ce\u4e2darm\u6307\u4ee4](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [Android\u865a\u62df\u673a\u5c42Java\u65b9\u6cd5\u66ff\u6362\u539f\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n    - [\u7cfb\u7edf\u6e90\u7801\u79fb\u690d\u5230AS\u7f16\u8bd1\u4e0e\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n\n### \u516d\u4e362020\u6700\u65b0Android\u6587\u7ae0\u7cfb\u5217\uff1a\n - [\u9762\u8bd5\u5b98\uff1a\u4f60\u6709\u7528\u8fc7Flutter\u5417? Flutte\u5c06\u901a\u4fe1\u6846\u67b6\u96c6\u6210\u5230\u7ec4\u4ef6\u5316\u9879\u76ee\u4e2d\u5b9e\u73b0\u6700\u7ec8\u6548\u679c\u67b6\u6784\u662f\u600e\u4e48\u6837\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u6bd4\u5176\u4ed6\u5982ReactNative\u597d](https://www.jianshu.com/p/3e2d9b23cfd6)\n \n - [\u5982\u4f55\u52a0\u8f7d100M\u7684\u56fe\u7247\u5374\u4e0d\u6491\u7206\u5185\u5b58,\u4e00\u5f20 100M \u7684\u5927\u56fe\uff0c\u5982\u4f55\u9884\u9632 OOM\uff1f](https://www.jianshu.com/p/878e4ddaa51b)\n  \n - [\u5b9d\u5b9d\u5df4\u58eb\uff1aKotlin\u4e3a\u4ec0\u4e48\u4f1a\u706b\u8d77\u6765\uff0c\u6709\u4ec0\u4e48\u7279\u70b9\uff0c\u8ddfJava\u533a\u522b](https://www.jianshu.com/p/dd9c0b9af2a1)\n  \n - [IGG\uff1aAndroid\u5185\u5b58\u56de\u6536\u673a\u5236\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/2b2642ce379f)\n  \n - [\u6012\u5237Android\u9762\u8bd5100\u9898\uff0c\u518d\u4e5f\u4e0d\u62c5\u5fc3\u4e0d\u80fd\u540a\u6253\u9762\u8bd5\u5b98\u4e86](https://www.jianshu.com/p/c01c3d0b1ee9)\n \n - [\u5b57\u8282\u8df3\u52a8:Android R\u5982\u4f55\u8bbf\u95ee\u6587\u4ef6\uff0c\u4fee\u6539\u6587\u4ef6\uff0c\u4f60\u4eec\u5bf9R\u9002\u914d\u4e86\u5417](https://www.jianshu.com/p/994b72f06af9)\n  \n  - [\u5b57\u8282\u8df3\u52a8\uff1aIO\u4f18\u5316\u662f\u600e\u4e48\u505a\u7684\uff0c\u4f7f\u7528 SharedPreferences\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u5361\uff0cmmkv\u539f\u7406\u662f\u4ec0\u4e48](https://www.jianshu.com/p/12428890ae1e)\n   \n  - [2020\u5e74Android\u6700\u5148\u9762\u8bd5\u4e13\u9898\u52a9\u4f60\u65a9\u83b7offer\uff0c\u4ee5\u53ca\u6559\u4f60\u5982\u4f55\u4e00\u6b65\u6b65\u7b80\u5386](https://www.jianshu.com/p/af7938c116bb)\n   \n - [\u9762\u8bd5\u5b98\u8bf4\uff1a\u5927\u5bb6\u90fd\u8bf4 Java \u53cd\u5c04\u6548\u7387\u4f4e\uff0c\u4f60\u77e5\u9053\u539f\u56e0\u5728\u54ea\u91cc\u4e48](https://www.jianshu.com/p/4a32b9b71115)\n \n - [Android \u5f00\u53d1\u4e2d\u7684\u67b6\u6784\u6a21\u5f0f -- MVC / MVP / MVVM](https://www.jianshu.com/p/218f9432ee52)\n \n - [\u5173\u4e8e\u963f\u91cc\u63a8\u51fa\u7684\u8def\u7531\u6846\u67b6ARouter\u6e90\u7801\u89e3\u6790](https://www.jianshu.com/p/7b4d085e23a9)\n  \n - [\u8131\u4e86\u9a6c\u7532\u6211\u4e5f\u8ba4\u8bc6\u4f60: \u804a\u804a Android \u4e2d\u7c7b\u7684\u771f\u5b9e\u5f62\u6001](https://www.jianshu.com/p/1094f6e4444f)\n\n- [Android\u9762\u8bd5\u5206\u6790\u4e00\uff1a\u5173\u4e8eOKhttp\u8be6\u89e3\uff08\u9644\u5e26\u89c6\u9891\u6559\u7a0b\uff09](https://www.jianshu.com/p/f4e353336b86)\n \n - [\u9762\u8bd5\u5b98\uff1a\u5185\u5b58\u6cc4\u6f0f\u8fde\u73af\u95ee\u3002\u88ab\u95ee\u61f5\u4e86\uff1f\u6765\u770b\u770b\u8fd9\u4e2a](https://www.bilibili.com/video/BV1ck4y1r7PK)\n \n  - [\u963f\u91cc\u9762\u8bd5\u5b98\uff1a\u5173\u4e8eRecyclerView\u505a\u4e0b\u5206\u4eab](https://www.jianshu.com/p/e5b2963706c7)\n  \n - [\u6211\u53eb\u5f20\u4e1c\u5347\uff0c\u6211\u662f\u4e00\u540dAndroid\u7a0b\u5e8f\u5458\uff0c\u6211\u6709\u8bdd\u8981\u8bf4](https://www.jianshu.com/p/ca36bf015eee)\n\n - [Android\u5f00\u53d1\u7b2c5\u5e74\u505a\u4e86\u4e00\u4e2a\u4ea7\u54c1\uff0c\u88ab\u9ec4\u6653\u660e\uff0cangelbabay\uff0c\u9ec4\u6e24\u7b49\u4e00\u7ebf\u660e\u661f\u8f6c\u53d1\u540e\uff0c\u6211......](https://www.jianshu.com/p/bc9426831180)\n \n#### \u4e03\u4e36Android\u6027\u80fd\u4f18\u5316\u7bc7\n\n- [Android\u6027\u80fd\u4f18\u5316(1)- \u542f\u52a8\u4f18\u5316](https://mp.weixin.qq.com/s/gZdL4rNuw0cHTXL7RkkJ1A)\n  \n - [Android\u6027\u80fd\u4f18\u5316(2)-UI\u6e32\u67d3\u4f18\u5316](https://mp.weixin.qq.com/s/YeSkBcMB2tf0pVmddxA2Mg)\n   \n - [Android\u6027\u80fd\u4f18\u5316(3)-\u5954\u6e83\u4f18\u5316 ](https://mp.weixin.qq.com/s/J8qPFv9Fbt9_UGKqGxS5YQ)\n    \n - [Android\u6027\u80fd\u4f18\u5316(4)-\u5185\u5b58\u4f18\u5316 ](https://mp.weixin.qq.com/s/Rte_e7R7grfrYUE6RXJYKg)\n     \n - [Android\u6027\u80fd\u4f18\u5316(5)-\u5361\u987f\u4f18\u5316](https://pan.baidu.com/s/15rJ_qEWSJdlODA6ifjHTAw)\n      \n - [Android\u6027\u80fd\u4f18\u5316(6)-\u5b58\u50a8\u4f18\u5316](https://mp.weixin.qq.com/s/6A4jU8spcLJa2BKdMS10yA)\n     \n - [Android\u6027\u80fd\u4f18\u5316(7)-\u7f51\u7edc\u4f18\u5316](https://mp.weixin.qq.com/s/rJChehNdyPFbB_LcKSV_jA)\n\n - [Android\u6027\u80fd\u4f18\u5316(8)-\u8017\u7535\u4f18\u5316](https://mp.weixin.qq.com/s/wJdzAM5a9a6rFHDdJMBfAA)\n         \n - [Android\u6027\u80fd\u4f18\u5316(9)-\u591a\u7ebf\u7a0b\u5e76\u53d1\u4f18\u5316](https://mp.weixin.qq.com/s/sZ1MgTlDlusgGDWGZUZZzw)\n\n - [Android\u6027\u80fd\u4f18\u5316(10)-\u5b89\u88c5\u5305\u4f18\u5316](https://mp.weixin.qq.com/s/Qo0y6xbZ8LFdYvcWdqmKjQ)\n \n #### \u516b\u4e36Android Framework\u5c42\u9762\u8bd5\n \n- [Android Framework\u5c42\u9762\u8bd5\uff081\uff09-\u4e4bActivity\u542f\u52a8\u9762\u8bd5\u8fde\u73af\u70ae](https://mp.weixin.qq.com/s/LYQXe93evbHrleUPs62Jvw)\n  \n - [Android Framework\u5c42\u9762\u8bd5\uff082\uff09-\u4e4bBinder\u901a\u4fe1\u673a\u5236](https://mp.weixin.qq.com/s/Qnf79D54UF3o9k_VmuAIWQ)\n   \n - [Android Framework\u5c42\u9762\u8bd5\uff083\uff09-\u4e4bHandler\u9762\u8bd5\u96c6\u5408](https://mp.weixin.qq.com/s/MAAQLTgMYD3FxVS6ZFPDww)\n    \n - [Android Framework\u5c42\u9762\u8bd5\uff084\uff09-\u4e4b\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u5168\u9762\u6574\u7406 ](https://mp.weixin.qq.com/s/UEL_jxU8nugGGBerYFuN_g)\n     \n - [Android Framework\u6e90\u7801\u9762\u8bd5\uff085\uff09-\u4e4bonMeasure\u6d4b\u91cf\u539f\u7406](https://mp.weixin.qq.com/s/jydw_NT3AMIyLCoiynFFnA)\n      \n - [AndroidFramework\u5c42\u9762\u8bd5\uff086\uff09-\u4e4bAndroid \u5c4f\u5e55\u5237\u65b0\u673a\u5236(\u8bf4\u8bf4\u5361\u987f\u539f\u56e0)](https://mp.weixin.qq.com/s/xqoJRfXHUawQjfbioCcBfw)\n\n  \n\n\n### \u4e5d\u4e36\u4e92\u8054\u7f51\u7684\u5bd2\u51ac\u4e0b\uff0c\u5982\u4f55\u624b\u63e1\u5b89\u535370\u4e07\u5e74\u85aa\u3002\u4e00\u5802\u8bfe\u5e26\u4f60\u8d70\u8fdb\u8c61\u7259\u5854\n\n[\u4e3a\u4ec0\u4e48\u4f1a\u53d1\u751f\u4e92\u8054\u7f51\u7684\u5bd2\u51ac](android/videowhy.md)\n\n[\u97f3\u9891\u964d\u566a\u539f\u7406\uff0c\u97f3\u9891\u539f\u7406](android/voice.md)\n\n[\u97f3\u89c6\u9891\u662f\u4ec0\u4e48\uff0c\u89c6\u9891\u4e3a\u4ec0\u4e48\u9700\u8981\u538b\u7f29](android/videowhy.md)\n\n[\u89c6\u9891\u538b\u7f29\u538b\u7f29\u7684\u662f\u4ec0\u4e48\u4fe1\u606f? \u5e27\u5185\u538b\u7f29\u4e0e\u5e27\u95f4\u538b\u7f29\u539f\u7406](android/videoencode.md)\n\n[\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u4f4e\u5ef6\u65f6\u600e\u4e48\u5b9e\u73b0](android/live-optimitor.md)\n\n[\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n[\u5fae\u4fe1---\u97f3\u89c6\u9891\u901a\u8bdd\u5982\u4f55\u5b9e\u73b0\uff0c\u89c6\u9891\u4f1a\u8bae\u539f\u7406](android/mediacodec.md)\n\n[5G\u65f6\u4ee3\u5982\u4f55\u91cd\u751f\u79fb\u52a8\u4e92\u8054\u7f51,\u5e26\u4f60\u770b\u770b\u4ec0\u4e48\u662f5G\u5e94\u7528]()\n\n[\u5c0f\u7c73--\u4e07\u7269\u4e92\u8054\u5b9e\u73b0\u539f\u7406](android/net.md)\n\n### \u5341\u4e362019Android\u5e74\u9ad8\u7ea7\u9762\u8bd5\n\n * [2019\u5e74Bat\u9762\u8bd5\u96c6\u5408](#2019\u5e74Bat\u9762\u8bd5\u96c6\u5408)\n * [\u67b6\u6784\u76f8\u5173\u9762\u8bd5](#\u67b6\u6784\u76f8\u5173\u9762\u8bd5)\n * [NDK\u76f8\u5173\u9762\u8bd5](#NDK\u76f8\u5173\u9762\u8bd5)\n * [\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5](#\u7b97\u6cd5\u76f8\u5173\u9762\u8bd5)\n * [\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5](#\u9ad8\u7ea7UI\u76f8\u5173\u9762\u8bd5)\n * [\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5](#\u6027\u80fd\u4f18\u5316\u76f8\u5173\u9762\u8bd5)\n * [\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5](#\u4e13\u4e1a\u9886\u57df\u76f8\u5173\u9762\u8bd5)\n * [\u5176\u4ed6](#\u5176\u4ed6)\n\n### \u5341\u4e00\u4e362019\u5e74Bat\u9762\u8bd5\u96c6\u5408\n\n\n\n> \u963f\u91cc\u5df4\u5df4\u9762\u8bd5\u96c6\u5408\n\n- [Android P\u7981\u6b62\u4e86http\u5bf9\u4f60\u4eec\u6709\u5f71\u54cd\u5417\uff1fhttps\u539f\u7406\u4f60\u77e5\u9053\u5417\uff1f](android/https.md)\n\n- [\u4ec0\u4e48\u662f\u5bf9\u79f0\u52a0\u5bc6\uff0c\u4ec0\u4e48\u662f\u975e\u5bf9\u79f0\u52a0\u5bc6\uff0c\u516c\u94a5\u4e0e\u79c1\u94a5\u5c5e\u4e8e\u5bf9\u79f0\u52a0\u5bc6\u5417](android/cert.md)\n\n- [https\u8bf7\u6c42\u4f1a\u4e0d\u4f1a\u5b58\u5728\u88ab\u62e6\u622a\u7684\u53ef\u80fd\uff1f\u4f60\u5bf9\u8fd9\u65b9\u9762\u6709\u8fc7\u7814\u7a76\u5417](android/cert.md)\n\n- [\u4e4b\u524d\u6709\u505a\u8fc7\u76f4\u64ad\u5417?\u4f60\u4eec\u662f\u901a\u8fc7\u4ec0\u4e48\u65b9\u5f0f\u5b9e\u73b0\u76f4\u64ad\u7684? \u76f4\u64ad\u4e92\u52a8\u662f\u5982\u4f55\u505a\u7684](android/live.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u76f4\u64ad\u4e2d \u7f51\u901f\u6bd4\u8f83\u5dee\u7684\u6761\u4ef6\u4e0b\uff0c\u5982\u4f55\u4f7f\u753b\u9762\u4fdd\u8bc1\u6d41\u7545\u7684\u6548\u679c](android/live-optimitor.md)\n\n- [\u817e\u8baf\u8bfe\u5802-\u786c\u7f16\u7801\u4e0e\u8f6f\u7f16\u7801\u533a\u522b\uff0c\u5f55\u5c4f\u65f6\u5982\u4f55\u9009\u53d6\u786c\u7f16\u4e0e\u8f6f\u7f16](android/mediacodec.md)\n\n- [Flutter\u4e3a\u4ec0\u4e48\u4f1a\u505a\u5230\u4e00\u5904\u5199 \u5904\u5904\u8fd0\u884c \u4e0eRN\u7684\u533a\u522b](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [Flutter\u7684\u56fe\u5f62\u5f15\u64ce\u4e0eAndroid\u7684\u6e32\u67d3\u5f15\u64ce\u539f\u7406](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u5bf9\u4e8eTersorflow\u4f60\u600e\u4e48\u7406\u89e3\u7684\uff0c\u6709\u505a\u8fc7\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u5417](android/tersorflow.md)\n\n- [\u4e3a\u4ec0\u4e48Android\u4f1a\u51fa\u73b0\u5361\u987f](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u7ed9\u4f60\u4e00\u4e2aDemo \u4f60\u5982\u4f55\u5feb\u901f\u5b9a\u4f4dANR](android/anr.md)\n\n- [Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n- [Glide\u5bf9Bitmap\u7684\u7f13\u5b58\u4e0e\u89e3\u7801\u590d\u7528\u5982\u4f55\u505a\u5230\u7684](https://github.com/xiangjiana/Android-MS/edit/master/README.md)\n\n- [\u8bf4\u8bf4\u4f60\u5bf9Dalvik\u865a\u62df\u673a\u7684\u8ba4\u8bc6 ](android/dalvik.md)\n\n- [\u63a5\u4e0b\u6765\u8bf4\u8bf4 Android \u865a\u62df\u673aDalvik\u4e0eART\u533a\u522b\u5728\u54ea\u91cc\uff1f](android/artordalvik.md)\n\n- [Handler\u7684\u539f\u7406\u662f\u4ec0\u4e48?\u80fd\u6df1\u5165\u5206\u6790\u4e0b Handler\u7684\u5b9e\u73b0\u673a\u5236\u5417\uff1f](./study/framework/Handler\u673a\u5236\u6e90\u7801.md)\n\n- [ Handler\u4e2d\u6709Loop\u6b7b\u5faa\u73af\uff0c\u4e3a\u4ec0\u4e48\u6ca1\u6709\u963b\u585e\u4e3b\u7ebf\u7a0b\uff0c\u539f\u7406\u662f\u4ec0\u4e48](study/framework/Android\u6d88\u606f\u673a\u5236.md)\n\n  \n\n> \u817e\u8baf\u9762\u8bd5\u96c6\u5408\n\n- [\u8be6\u7ec6\u8bf4\u8bf4Binder\u901a\u4fe1\u539f\u7406\u4e0e\u673a\u5236](android/binder.md)\n\n- [Linux\u81ea\u5e26\u591a\u79cd\u8fdb\u7a0b\u901a\u4fe1\u65b9\u5f0f\uff0c\u4e3a\u4ec0\u4e48Android\u90fd\u6ca1\u91c7\u7528\u4e8c\u504f\u504f\u4f7f\u7528Binder\u901a\u4fe1](android/binder1.md)\n\n- [\u8c08\u4e00\u8c08Binder\u7684\u539f\u7406\u548c\u5b9e\u73b0\u4e00\u6b21\u62f7\u8d1d\u7684\u6d41\u7a0b](android/binder2.md)\n\n- [\u8fdb\u7a0b\u4fdd\u6d3b\u5982\u4f55\u505a\u5230\uff0c\u4f60\u4eec\u4fdd\u6d3b\u7387\u6709\u591a\u9ad8\uff1f](android/process.md)\n\n- [ButterKnife\u4e3a\u4ec0\u4e48\u6267\u884c\u6548\u7387\u4e3a\u4ec0\u4e48\u6bd4\u5176\u4ed6\u6ce8\u5165\u6846\u67b6\u9ad8\uff1f\u5b83\u7684\u539f\u7406\u662f\u4ec0\u4e48](android/butterknife.md)\n\n- [\u7ec4\u4ef6\u5316\u5982\u4f55\u5b9e\u73b0\uff0c\u7ec4\u4ef6\u5316\u4e0e\u63d2\u4ef6\u5316\u7684\u5dee\u522b\u5728\u54ea\u91cc\uff0c\u8be5\u600e\u4e48\u9009\u578b](android/commpont.md)\n\n- [\u8bf4\u4e0b\u7ec4\u4ef6\u4e4b\u95f4\u7684\u8df3\u8f6c\u548c\u7ec4\u4ef6\u901a\u4fe1\u539f\u7406\u673a\u5236](android/commpontrounter.md)\n\n- [\u6709\u6ca1\u6709\u4f7f\u7528\u8fc7\u7ec4\u4ef6\u5316\uff0c\u7ec4\u4ef6\u5316\u901a\u4fe1\u5982\u4f55\u505a\u5230\u7684\uff0cARouter\u6709\u7528\u8fc7\u5417](android/router.md)\n\n- [\u6709\u7528\u8fc7\u63d2\u4ef6\u5316\u5417\uff1f\u8c08\u8c08\u63d2\u4ef6\u5316\u539f\u7406\uff1f](android/plugin.md)\n\n- [\u70ed\u4fee\u590d\u8fde\u73af\u70ae(\u70ed\u4fee\u590d\u662f\u4ec0\u4e48  \u6709\u63a5\u89e6\u8fc7tinker\u5417\uff0ctinker\u539f\u7406\u662f\u4ec0\u4e48)](tencent/tinker.md)\n\n- [\u589e\u91cf\u5347\u7ea7\u4e3a\u4ec0\u4e48\u51cf\u5c11\u5347\u7ea7\u4ee3\u4ef7\uff0c\u589e\u91cf\u5347\u7ea7\u539f\u7406](tencent/update.md)\n\n- [ PMS\u4e4b\u524d\u4e86\u89e3\u8fc7\u5417?\u4f60\u5bf9PMS\u600e\u4e48\u770b\u7684\uff0c\u80fd\u804a\u804aPMS\u7684\u8be6\u7ec6\u5b9e\u73b0\u6d41\u7a0b\u5417](android/pms.md)\n\n- [ AMS\u5728Android\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff0cActivtiy\u542f\u52a8\u8ddfAMS\u6709\u4ec0\u4e48\u5173\u7cfb](android/ams.md)\n\n- [\u4f60\u77e5\u9053\u4ec0\u4e48\u662fAOP\u5417\uff1fAOP\u4e0eOOP\u6709\u4ec0\u4e48\u533a\u522b\uff0c\u8c08\u8c08AOP\u7684\u539f\u7406](android/aop.md)\n\n- [\u7f51\u6613\u4e91--\u624b\u673aQQ\u7684\u6362\u80a4\u662f\u600e\u4e48\u505a\u5230\u7684\uff0c\u4f60\u5bf9\u6362\u80a4\u6709\u4e86\u89e3\u5417\uff1f\u770b\u8fc7\u6362\u80a4\u7684\u539f\u7406\u6ca1\uff1f](android/load.md)\n\n- [\u5783\u573e\u56de\u6536\u673a\u5236\u662f\u5982\u4f55\u5b9e\u73b0\u7684](android/traked.md)\n\n- [\u6570\u636e\u5e93\u7248\u672c\u5982\u4f55\u5355\u72ec\u5347\u7ea7\uff0c\u5e76\u4e14\u5c06\u539f\u6709\u6570\u636e\u8fc1\u79fb\u8fc7\u53bb](tencent/sqlite.md)\n\n- [\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u591a\u7528\u6237\uff0c\u591a\u89d2\u8272\u7684App\u67b6\u6784](android/thread.md)\n\n- [\u8c08\u8c08volatile\u5173\u952e\u5b57\u4e0esynchronized\u5173\u952e\u5b57\u5728\u5185\u5b58\u7684\u533a\u522b](android/volatile.md)\n\n- [synchronize\u5173\u952e\u5b57\u5728\u865a\u62df\u673a\u6267\u884c\u539f\u7406\u662f\u4ec0\u4e48\uff0c\u80fd\u8c08\u4e00\u8c08\u4ec0\u4e48\u662f\u5185\u5b58\u53ef\u89c1\u6027\uff0c\u9501\u5347\u7ea7\u5417](android/synchronize.md)\n\n- [\u7c7b\u6bd4\u4e8e\u5fae\u4fe1\uff0c\u5982\u4f55\u5bf9Apk\u8fdb\u884c\u6781\u9650\u538b\u7f29,\u8c08\u4e0bAndroid\u538b\u7f298\u5927\u6b65 ](android/AndResGuard.md)\n\n- [\u5982\u4f55\u5f7b\u5e95\u9632\u6b62\u53cd\u7f16\u8bd1\uff0cdex\u52a0\u5bc6\u600e\u4e48\u505a ](android/dex.md)\n\n- [\u5e8f\u5217\u5316\u4e0e\u53cd\u5e8f\u5217\u5316\u7684\u539f\u7406\uff0cAndroid\u7684Parcelable\u4e0eSerializable\u533a\u522b\u662f\u4ec0\u4e48](android/herms.md)\n\n- [\u4f60\u66fe\u7ecf\u6709\u6ca1\u6709\u5bf9SqliteDatabase\u505a\u8fc7\u5c01\u88c5\uff0c\u4f60\u81ea\u5df1\u6709\u8bbe\u8ba1\u8fc7\u6570\u636e\u5e93\u6846\u67b6\u5417?\u6216\u8005\u53ea\u662f\u505c\u7559\u5728\u4f7f\u7528ormlite  greenDao\u8fd9\u7c7b\u6846\u67b6](android/sqlite.md)\n\n  \n### \u5341\u4e8c\u4e36\u89c6\u9891\u533a\u57df\n- [\u9879\u76ee\u8d8a\u505a\u8d8a\u590d\u6742\uff1f\u7ec4\u4ef6\u5316\u5f00\u53d1\u66ff\u4f60\u89e3\u51b3\u6240\u6709\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u963f\u91cc\u9762\u8bd5\u9898\u5355\u5229\u6a21\u5f0f\u4e0b\u5f15\u53d1\u7684\u8840\u6848\uff0cDCL\u53cc\u7aef\u9501\u4e0b\u7684CAS\u4e0eABA\u95ee\u9898](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u624b\u5199\u7ec4\u4ef6Lifecycle](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6Navigation\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u7ec4\u4ef6DataBinding-Ex\uff1a\u53cc\u5411\u7ed1\u5b9a\u7bc7](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Android\u9ad8\u7ea7\u67b6\u6784\u5e08-\u6700\u65b0Jetpack\u67b6\u6784\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Foundation\uff08\u6846\u67b6\uff09](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Jetpack\u6e90\u7801\u5206\u6790\u3002\u5de8\u4eba\u662f\u5982\u4f55\u70bc\u6210\u7684](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [MVVM+Jetpack\u5b9e\u73b0\u7684GitHub\u5ba2\u6237\u7aef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f16\u8bd1\u65f6\u6280\u672f\u7684\u5b9e\u8df5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [ButterKnife\u8be6\u89e3\u4e0e\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Dagger2\u6838\u5fc3\u539f\u7406\u5206\u6790](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u997f\u4e86\u4e48\u8fdb\u7a0b\u901a\u4fe1\u6838\u5fc3\u6280\u672fherms\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199handler\uff0c\u5f15\u53d1\u5185\u5b58\u6cc4\u6f0f\u7684\u6839\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7ec4\u4ef6\u5316\u67b6\u6784\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u63d2\u4ef6\u5316\u6846\u67b6\u8bbe\u8ba1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u624b\u5199\u70ed\u4fee\u590d\u6846\u67b6Tinker](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u6613\u4e91\u6362\u80a4\u6280\u672f\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u56fe\u7247\u52a0\u8f7d\u6846\u67b6Glide](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [\u7f51\u7edc\u52a0\u8f7d\u6846\u67b6OKHTTP\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [Rxjava2\u67b6\u6784\u5206\u6790\u4e0e\u6e90\u7801\u8be6\u89e3](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [C/C++\u5165\u95e8\u8bed\u6cd5\u4ee5\u53ca\u57fa\u7840\u77e5\u8bc6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-JNI\u7f16\u7a0b\u5b9e\u6218](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [5G\u65f6\u4ee3\u5f15\u9886\u8005-NDK-\u6784\u5efa\u811a\u672c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n### \u5341\u4e09\u4e36OPCV\u5b66\u4e60\u8d44\u6599\n#### \u7b2c\u4e00\u7ae0 \u9884\u5907\u77e5\u8bc6\n- [1.1.\u7f16\u7a0b\u7684\u6d41\u7a0b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.2.\u4ec0\u4e48\u53eb\u7f16\u8f91](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.3.\u4ec0\u4e48\u53eb\u7f16\u8bd1](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.4.\u4ec0\u4e48\u53eb\u8fde\u63a5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.5.\u4ec0\u4e48\u53eb\u8fd0\u884c](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.6.Visual C++\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.7.\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.8.\u5e93\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.9.OpenCV\u662f\u4ec0\u4e48](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.10.\u4ec0\u4e48\u662f\u547d\u4ee4\u884c\u53c2\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.11.\u5e38\u89c1\u7f16\u8bd1\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.1\u627e\u4e0d\u5230\u5934\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [1.11.2\u62fc\u5199\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [1.12.\u5e38\u89c1\u94fe\u63a5\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [1.13.\u8fd0\u884c\u65f6\u9519\u8bef](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e8c\u7ae0 OpenCV\u4ecb\u7ecd\n\n- [2.1.OpenCV\u7684\u6765\u6e90](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [2.2.OpenCV\u7684\u534f\u8bae](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n#### \u7b2c\u4e09\u7ae0 \u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c\n\n- [3.1.\u56fe\u50cf\u7684\u8868\u793a](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.2.Mat\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.3.\u521b\u5efaMat\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.1\u6784\u9020\u51fd\u6570\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.2create\uff08\uff09\u51fd\u6570\u521b\u5efa\u5bf9\u8c61](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.3.3Matlab\u98ce\u683c\u7684\u521b\u5efa\u5bf9\u8c61\u65b9\u6cd5](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.4.\u77e9\u9635\u7684\u57fa\u672c\u5143\u7d20\u8868\u8fbe](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n\n- [3.5.\u50cf\u7d20\u503c\u7684\u8bfb\u5199](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.1 at()\u51fd\u6570](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.2 \u4f7f\u7528\u8fed\u4ee3\u5668](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.5.3 \u901a\u8fc7\u6570\u636e\u6307\u9488](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n- [3.6.\u9009\u53d6\u56fe\u50cf\u5c40\u90e8\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.1 \u5355\u884c\u6216\u5355\u5217\u9009\u62e9](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.2 \u7528Range\u9009\u62e9\u591a\u884c\u6216\u591a\u5217](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  - [3.6.3 \u611f\u5174\u8da3\u533a\u57df](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n  - [3.6.4 \u53d6\u5bf9\u89d2\u7ebf\u5143\u7d20](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.7.Mat\u8868\u8fbe\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.8.Mat_\u7c7b](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.9.Mat\u7c7b\u7684\u5185\u5b58\u7ba1\u7406](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.10.\u8f93\u51fa](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n  \n - [3.11.Mat\u4e0elpllmage\u548cCvMat\u7684\u8f6c\u6362](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)  \n   - [3.11.1 Mat\u8f6c\u4e3alpllmage\u548cCvMat\u7684\u683c\u5f0f](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)   \n   - [3.11.2 lpllmage\u548cCvMat\u683c\u5f0f\u8f6c\u4e3aMat](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n   \n#### \u7b2c\u56db\u7ae0 \u6570\u636e\u83b7\u53d6\u4e0e\u5b58\u50a8\n\n - [4.1.\u8bfb\u53d6\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.1\u8bfb\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u56fe\u50cf\u6587\u4ef6](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   \n - [4.2\u8bfb\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.2.1\u8bfb\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png)\n   - [4.1.2\u5199\u89c6\u9891](https://github.com/xiangjiana/Android-MS/blob/master/img/VX.png) \n\n   \n  ## \u66f4\u591a\u76f8\u5173\u9762\u8bd5\u5185\u5bb9\uff0c\u89c6\u9891\u6587\u6863\uff0c2020\u9762\u6700\u65b0\u9762\u8bd5\u4e13\u9898PPT\uff0c\u9ad8\u7ea7\u8fdb\u9636\n  ## (\u5907\u6ce8GitHub\uff09 VX\uff1amm14525201314\n  \n  # \u7248\u6743\u58f0\u660e\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"\u77e5\u8bc6\u5171\u4eab\u8bb8\u53ef\u534f\u8bae\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />\u672c\u4f5c\u54c1\u91c7\u7528<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">\u77e5\u8bc6\u5171\u4eab\u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u7981\u6b62\u6f14\u7ece 4.0 \u56fd\u9645\u8bb8\u53ef\u534f\u8bae</a>\u8fdb\u884c\u8bb8\u53ef\u3002\n"
 },
 {
  "repo": "asingh33/CNNGestureRecognizer",
  "language": "Python",
  "readme_contents": "[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/V7V01IK53)\n\nIf you find my work useful, then please do consider supporting me. This will help me keep motivated and do more of such projects. \nThanks !\n\n\n\n\n[![DOI](https://zenodo.org/badge/89872749.svg)](https://zenodo.org/badge/latestdoi/89872749)\n# CNNGestureRecognizer\nGesture recognition via CNN neural network implemented in Keras + Theano + OpenCV\n\n\nKey Requirements:\nPython 3.6.1\nOpenCV 3.4.1\nKeras 2.0.2\nTensorflow 1.2.1\nTheano 0.9.0   (obsolete and not supported any further)\n\nSuggestion: Better to download Anaconda as it will take care of most of the other packages and easier to setup a virtual workspace to work with multiple versions of key packages like python, opencv etc.\n\n# New changes\nI have uploaded few more changes to this repo -\n- Project is Python3 compatible now.\n- Added TensorFlow support, as Theano's development has been stopped.\n- Added a new background subtraction filter, which is by far the best performing filter for this project\n- Added lots of performance improving changes. There is now literally no FPS drop when prediction mode is enabled\n- An in-app graph plotting has been added to observe the probability of the gesture predictions \n \n# Repo contents\n- **trackgesture.py** : The main script launcher. This file contains all the code for UI options and OpenCV code to capture camera contents. This script internally calls interfaces to gestureCNN.py.\n- **gestureCNN.py** : This script file holds all the CNN specific code to create CNN model, load the weight file (if model is pretrained), train the model using image samples present in **./imgfolder_b**, visualize the feature maps at different layers of NN (of pretrained model) for a given input image present in **./imgs** folder.\n- **imgfolder_b** : This folder contains all the 4015 gesture images I took in order to train the model.\n```diff\n- Note: I have replaced ori_4015imgs_weights.hdf5 weight file with these two OS specific weight files. \n```\n- **_pretrained_weights_MacOS.hdf5_** : This is pretrained weight file on MacOS. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1j7K96Dkatz6q6zr5RsQv-t68B3ZOSfh0/view\n- **_pretrained_weights_WinOS.hdf5_** : This is pretrained weight file on Windows. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1PA7rJxHYQsW5IvcZAGeoZ-ExYSttFuGs/view\n- **_imgs_** - This is an optional folder of few sample images that one can use to visualize the feature maps at different layers. These are few sample images from imgfolder_b only.\n- **_ori_4015imgs_acc.png_** : This is just a pic of a plot depicting model accuracy Vs validation data accuracy after I trained it.\n- **_ori_4015imgs_loss.png_** : This is just a pic of a plot depicting model loss Vs validation loss after I training.\n\n# Usage\n**On Mac**\n```bash\neg: With Theano as backend\n$ KERAS_BACKEND=tensorflow python trackgesture.py \n```\n**On Windows**\n```bash\neg: With Tensorflow as backend\n> set \"KERAS_BACKEND=tensorflow\"\n> python trackgesture.py \n```\n\nWe are setting KERAS_BACKEND to change backend to Theano, so in case you have already done it via Keras.json then no need to do that. But if you have Tensorflow set as default then this will be required.\n\n# Features\nThis application comes with CNN model to recognize upto 5 pretrained gestures:\n- OK\n- PEACE\n- STOP\n- PUNCH\n- NOTHING (ie when none of the above gestures are input)\n\nThis application provides following functionalities:\n- Prediction : Which allows the app to guess the user's gesture against pretrained gestures. App can dump the prediction data to the console terminal or to a json file directly which can be used to plot real time prediction bar chart (you can use my other script - https://github.com/asingh33/LivePlot)\n- New Training : Which allows the user to retrain the NN model. User can change the model architecture or add/remove new gestures. This app has inbuilt options to allow the user to create new image samples of user defined gestures if required.\n- Visualization : Which allows the user to see feature maps of different NN layers for a given input gesture image. Interesting to see how NN works and learns things.\n\n\n# Demo \nYoutube link - https://www.youtube.com/watch?v=CMs5cn65YK8\n\n![](https://j.gifs.com/X6zwYm.gif)\n\n# Gesture Input\nI am using OpenCV for capturing the user's hand gestures. In order to simply things I am doing post processing on the captured images to highlight the contours & edges. Like applying binary threshold, blurring, gray scaling.\n\nI have provided two modes of capturing:\n- Binary Mode : In here I first convert the image to grayscale, then apply a gaussian blur effect with adaptive threshold filter. This mode is useful when you have an empty background like a wall, whiteboard etc.\n- SkinMask Mode : In this mode, I first convert the input image to HSV and put range on the H,S,V values based on skin color range. Then apply errosion followed by dilation. Then gaussian blur to smoothen out the noises. Using this output as a mask on original input to mask out everything other than skin colored things. Finally I have grayscaled it. This mode is useful when there is good amount of light and you dont have empty background.\n\n**Binary Mode processing**\n```python\ngray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray,(5,5),2)   \nth3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\nret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n```\n\n![OK gesture in Binary mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiiok160.png)\n\n\n**SkindMask Mode processing**\n```python\nhsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n    \n#Apply skin color range\nmask = cv2.inRange(hsv, low_range, upper_range)\n\nmask = cv2.erode(mask, skinkernel, iterations = 1)\nmask = cv2.dilate(mask, skinkernel, iterations = 1)\n\n#blur\nmask = cv2.GaussianBlur(mask, (15,15), 1)\n#cv2.imshow(\"Blur\", mask)\n\n#bitwise and mask original frame\nres = cv2.bitwise_and(roi, roi, mask = mask)\n# color to grayscale\nres = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n```\n![OK gesture in SkinMask mode](https://github.com/asingh33/CNNGestureRecognizer/blob/master/imgfolder_b/iiok44.png)\n\n\n# CNN Model used\nThe CNN I have used for this project is pretty common CNN model which can be found across various tutorials on CNN. Mostly I have seen it being used for Digit/Number classfication based on MNIST database.\n\n```python\nmodel = Sequential()\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv),\n                    padding='valid',\n                    input_shape=(img_channels, img_rows, img_cols)))\nconvout1 = Activation('relu')\nmodel.add(convout1)\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\nconvout2 = Activation('relu')\nmodel.add(convout2)\nmodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n```\n\nThis model has following 12 layers -\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 32, 198, 198)      320       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 32, 198, 198)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 32, 196, 196)      9248      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 32, 196, 196)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 32, 98, 98)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32, 98, 98)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 307328)            0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               39338112  \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 5)                 0         \n=================================================================\n```\nTotal params: 39,348,325.0\nTrainable params: 39,348,325.0\n\n# Training\nIn version 1.0 of this project I had used 1204 images only for training. Predictions probability was ok but not satisfying. So in version 2.0 I increased the training image set to 4015 images i.e. 803 image samples per class. Also added an additional class 'Nothing' along with the previous 4 gesture classes.\n\nI have trained the model for 15 epochs.\n\n![Training Accuracy Vs Validation Accuracy](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_acc.png)\n\n![Training Loss Vs Validation Loss](https://github.com/asingh33/CNNGestureRecognizer/blob/master/ori_4015imgs_loss.png)\n\n\n# Visualization\nCNN is good in detecting edges and thats why its useful for image classificaion kind of problems. In order to understand how the neural net is understanding the different gesture input its possible to visualize the layer feature map contents.\n\nAfter launching the main script choose option 3 for visualizing different or all layer for a given image (currently it takes images from ./imgs, so change it accordingly)\n```\nWhat would you like to do ?\n    1- Use pretrained model for gesture recognition & layer visualization\n    2- Train the model (you will require image samples for training under .\\imgfolder)\n    3- Visualize feature maps of different layers of trained model\n    3\nWill load default weight file\nImage number 7\nEnter which layer to visualize -1\n(4015, 40000)\nPress any key\nsamples_per_class -  803\nTotal layers - 12\nDumping filter data of layer1 - Activation\nDumping filter data of layer2 - Conv2D\nDumping filter data of layer3 - Activation\nDumping filter data of layer4 - MaxPooling2D\nDumping filter data of layer5 - Dropout\nCan't dump data of this layer6- Flatten\nCan't dump data of this layer7- Dense\nCan't dump data of this layer8- Activation\nCan't dump data of this layer9- Dropout\nCan't dump data of this layer10- Dense\nCan't dump data of this layer11- Activation\nPress any key to continue\n```\n\nTo understand how its done in Keras, check visualizeLayer() in gestureCNN.py\n```python\nlayer = model.layers[layerIndex]\n\nget_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\nactivations = get_activations([input_image, 0])[0]\noutput_image = activations\n```\nLayer 4 visualization for PUNCH gesture\n![Layer 4 visualization for PUNCH gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_4_layer4_MaxPooling2D.png)\n\nLayer 2 visualization for STOP gesture\n![Layer 2 visualization for STOP gesture](https://github.com/asingh33/CNNGestureRecognizer/blob/master/img_7_layer2_Conv2D.png)\n\n\n\n# Conclusion\nSo where to go from here? Well I thought of testing out the responsiveness of NN predictions and games are good benchmark. On MAC I dont have any games installed but then this Chrome Browser Dino Jump game came handy. So I bound the 'Punch' gesture with jump action of the Dino character. Basically can work with any other gesture but felt Punch gesture is easy. Stop gesture was another candidate.\n\nWell here is how it turned out :)\n\nWatch full video - https://www.youtube.com/watch?v=lnFPvtCSsLA&t=49s\n\n![](https://j.gifs.com/58pxVx.gif)\n\n\n\n# In case you want to cite my work\nAbhishek Singh,\u201dasingh33/CNNGestureRecognizer: CNNGestureRecognizer (Version 1.3.0)\u201d, Zenodo. http://doi.org/10.5281/zenodo.1064825, Nov. 2017.  \nDo tell me how you used this work in your project. Would love to see your work. Good Luck!\n\n\n\nDont forget to check out my other github project where I used this framework and applied SuperVised machine learning technique to train the Chrome Browser's TRex character :)\nhttps://github.com/asingh33/SupervisedChromeTrex\nYoutube link - https://youtu.be/ZZgvklkQrss\n\n![](https://j.gifs.com/DRg4mn.gif)\n\n"
 },
 {
  "repo": "foundry/OpenCVSwiftStitch",
  "language": "Objective-C++",
  "readme_contents": "__OpenCV computer vision with iOS: stitching panoramas__  \n\n<img src = \"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/example.big.jpg\">\n\nVersion 4.0 of [OpenCVStitch](http://github.com/foundry/OpenCVStitch) - updated for Swift 4.2  \nSee appropriate branches and tags for Swift 2.x - 3.x\n\nThis project was created to a answer a couple of Stack Overflow questions:  \n[libraries to CAPTURE panorama in iOS](http://stackoverflow.com/q/14062932/1375695)  \n[Can I mix Swift with C++? Like the Objective - C .mm files](http://stackoverflow.com/q/24042774/1375695)    \n\nv2 demonstrates how to mix Swift, Objective-C and C++ in one project whilst keeping the code clearly separate. \n\nThe project AppDelegate and View Controller are written in Swift. Swift cannot talk directly to C++ (which we need for OpenCV), so we provide an Objective-C++ wrapper class to mediate between Swift and C++. We also provide an Objective-C++ category on UIImage to mediate between UIImage and CV::Mat image formats. The CVWrapper header file is pure Objective-C. For [v1](https://github.com/foundry/OpenCVStitch)(which doesn't use Swift) this separation was a matter of clean style. For v2, it is a requirement: if any C++ headers are included in the wrapper, the app will not compile (Swift won't like it).\n\n__Installation__  \nTo run the project you need to install the OpenCV framework using Cocoapods    \n\nAssuming you have first [installed CocoaPods](https://guides.cocoapods.org/using/getting-started.html), run 'pod install' in this directory to install OpenCV for the project. From then on, always open the project in XCode from the `SwiftStitch.xcworkspace` file that the pod install creates. \n\nv3.1.0: The default podfile will install openCV v3.1.0, with a hotfix for arm64 compataibility\n\nv2.4.9: Recomment the podfile:  \n    pod 'OpenCV', '2.4.9-zp'  \n    #pod 'OpenCV', '~> 3.1.0.1'  \nChange the `#include` line for 2.4.9 compatibility as indicated in `stitching.cpp`  \n\n__Use__  \nOpenCVStitch is a very simple iOS/openCV example showing basic use of the Stitcher class. The c++ code is adapted from a sample included with the openCV distribution.  \n\nThe app has almost no user interface. On launch, the stitching code operates on four sample images, displaying the result in a UIScrollView.\n\n__OpenCVStitch Versions__   \n[Version 4.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/4.0)   \nSwift / Objective-C / C++   \nTested with XCode 10.0 / Swift 4.2 for iOS 8.0+  \n[Version 3.0](https://github.com/foundry/OpenCVSwiftStitch/releases/tag/3.0)   \nSwift / Objective-C / C++   \nTested with XCode 8.0 / Swift 3.0 for iOS 8.0+  \n[Version 2.1](https://github.com/foundry/OpenCVSwiftStitch)   \nSwift / Objective-C / C++   \nTested with XCode 7.0 / Swift 2.0 for iOS 7.0+  \n[Version 2.0](https://github.com/foundry/OpenCVSwiftStitch/tree/v2.0)   \nSwift / Objective-C / C++   \nTested with XCode 6.4 / Swift 1.2 for iOS 7.0+  \n[Version 1.0](https://github.com/foundry/OpenCVStitch)  \nObjective-C / C++   \nTested with XCode 4.5.2 -> 6.3 for iOS 5.1 upwards   \n\nProvides a partial answer to: [Libraries to capture panoramas in iOS 6](http://stackoverflow.com/questions/14062932/libraries-to-capture-panorama-in-ios-6/14064788#14064788) (Stack Overflow)\n\n__OpenCV Versions__  \n_OpenCV 3.x_   \nThe podfile installs a hotfixed version of 3.1 as the prebuilt binary provided by openCV  [breaks on arm64 devices](https://github.com/kylefleming/opencv/releases/tag/3.1.0-ios-fix).\n\nThe stitching seems to be much more efficient (85% faster on my iphone 5s). However the quality is noticeably inferior with the test images supplied, so v2.4x remains the default for now [_some improvement with openCV 3.1_].\n\n_OpenCV 2.4.x_  \nThe openCV distribution is not as clean as we would like.  \n2.4.10 - the pod spec and the distribution build for iOS [is broken](http://stackoverflow.com/questions/28331397/cocoapods-opencv-2-4-10-linker-error/28820510)  \n2.4.9 - the pod spec [is broken](http://stackoverflow.com/questions/31005022/cant-install-opencv-with-cocoapods-could-not-resolve-host-hivelocity-dl-sourc). This is likely a result of relying on Sourceforge for hosting.  \n\nTherefore we are using a [self-hosted podspec](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV) (_thanks Matteo!_) until official channels are fixed. Specs are available for [2.4.9](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.9-zp) and [2.4.10](https://github.com/Zi0P4tch0/Specs/tree/master/Specs/OpenCV/2.4.10-zp), but as the latter won't run we use 2.4.9.\n\n_this version of OpenCVStitch opted to use cocoapods to overcome the [notorious](http://stackoverflow.com/q/13905471/1375695) [installation](http://stackoverflow.com/q/15855894/1375695) [issues](http://stackoverflow.com/a/14186883/1375695) with previous versions of the openCV 2.4.x framework.. it seems our optimism was slightly premature!_\n\n__XCode 10__  \n\nProject is now updated for Swift 4.1 and XCode 10. For backwards compatibility checkout the 2.0 / 2.1 branches, 3.0 release or refer to the Objective-C version v1.0.\n\n__Comparisons__\n\n<table><tr>\n<td>OpenCV 2.4.9</td><td>OpenCV 3.0.0</td><td>OpenCV 3.1.0</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset2.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n\n</td>\n</tr><tr>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v2_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n<td>\n<img src=\"https://github.com/foundry/OpenCVSwiftStitch/blob/meta/meta/v3.1_inset.jpg\" width=\"300\" height=\"375\" alt=\"v2_inset2\">\n\n</td>\n</tr></table>\n"
 },
 {
  "repo": "ITCoders/Human-detection-and-Tracking",
  "language": "Python",
  "readme_contents": "# Human detection and Tracking\n\n[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/arpit1997)\n\n## Introduction\n_In this project we have worked on the problem of human detection,face detection, face recognition and tracking an individual. Our project is capable of detecting a human and its face in a given video and storing Local Binary Pattern Histogram (LBPH) features of the detected faces. LBPH features are the key points extracted from an image which is used to recognize and categorize images. Once a human is detected in video, we have tracked that person assigning him a label. We have used the stored LBPH features of individuals to recognize them in any other videos. After scanning through various videos our program gives output like- person labeled as subject1 is seen in video taken by camera1, subject1 is seen in video by camera2. In this way we have tracked an individual by recognizing him/her in the video taken by multiple cameras. Our whole work is based on the application of machine learning and image processing with the help of [openCV](http://opencv.org)._**This code is built on opencv 3.1.1, python 3.4 and C++, other versions of opencv are NOT SUPPORTED.**\n## Requirements\n* **opencv [v3.1.1]**\n\t* **Installation in linux:**\n\t\t\tFor complete installation of opencv in ubuntu you can refer [here](http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/).\n\t* **Installation in windows**\n\t\t\tFor complete installation of opencv in windows you can refer [here](https://putuyuwono.wordpress.com/2015/04/23/building-and-installing-opencv-3-0-on-windows-7-64-bit/)\n* **python3**\n\t* In Ubuntu python 3.4 can be installed via terminal with the command given below:\n\t\t`sudo apt-get install python3`\n* **python libraries:**\n\tHere is a list of all the python dependencies \n\t* Python Image Library (PILLOW)\n\t* Imutils\n\t* numpy\n\n* **C++**\n\n## Approach\n* The code follows the steps given below:\n\t1. First it reads a video and process each frame one by one.\n\t2. For each frame it tries to detect a human. If a human is detected it draws a rectangle around it.\n\t3. after completing step 2 it tries to detect human face.\n\t4. if a human face is detected it tries to recognize it with a pre-trained model file.\n\t5. If human face is recognized it puts the label on that human face else it moves to step 2 again for next frame \n* The repository is structured as follows:\n\t* `main.py` : This is the main python file that detects and recognizes humans.\n\t* `main.cpp` : This is the main C++ file that detects and recognizes humans.\n\t* `create_face_model.py` : This python script is used to create model file using the given data in `data/` folder \n\t* `model.yaml` : This file contains trained model for given data. This trained model contains LBPH features of each and every face for given data.\n\t* `face_cascades/` : This directory contains sample data for testing our codes. This data is prepared by extracting face images of a praticular person from some videos.\n\t* `scripts/` : This directory contains some useful scripts that we used to work on different problems.\n\t* `video/` : This directory contains some of the videos that we used to while testing.\n\n## Installation \n\n## Python\nDon't forget to install the necessary libraries described in the install paragraph above.\n\nFirst you need to run the create_face_model.py file, which uses the images in /data to create a .yaml file\n* In the project folder run \n```sh \npython create_face_model.py\n```\n* To run the python version of the code you have to put all the input videos in one folder and then provide the path of that folder as command line argument:\n```sh\npython3 main.py -v /path/to/input/videos/  \n```\nExample- for our directory structure it is: \n```sh\n python3 main.py -v /video \n```\n\n## C++\n* To compile the C++ version of the code with openCV the command is:\n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename name_of_file.cpp .cpp` name_of_file.cpp `pkg-config --libs opencv` \n```\nExample- for our directory structure it is: \n```sh\n g++ -ggdb `pkg-config --cflags opencv` -o `basename main.cpp .cpp` main.cpp `pkg-config --libs opencv` \n```  \n* To run the C++ version of the code you have to put all the input videos in one folder and then provide the path of that video as command line argument:\n```sh\n./name_of_file /path/to/input/video_file \n```  \nExample- for our directory structure it is: \n```sh\n ./main /video/2.mp4\n```\n* creating your own model file; just follow the steps given below to create your own model file:\n\t* for each individual rename the images as `subjectx.y.jpg` for example for person 1 images should be named as `subject01.0.jpg` , `subject01.1.jpg` and so on.\n\t* put all the images of all the persons in a single folder for example you can see `data\\` folder then run this command given below:\n\t\t`python3 create_face_model.py -i /path/to/persons_images/` \n\n## Performance of code\n* Since this is a computer vision project it requires a lot of computation power and performance of the code is kind of an issue here.\n* The code was tested on two different machines to analyse performace. The input was 30fps 720p video.\n\t* On a machine with AMD A4 dual-core processor we got an output of 4fps which is quite bad.\n\t* on a machine with Intel i5 quad-core processor we got an output of 12fps.\n\n## Results\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/g.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/k.jpg \"Logo Title Text 1\")\n![alt text](https://raw.githubusercontent.com/ITCoders/Human-detection-and-Tracking/master/results/o.jpg \"Logo Title Text 1\")\n\nYou can find project report [here](https://github.com/ITCoders/Human-detection-and-Tracking/raw/master/results/HUMAN%20DETECTION%20ANDaRECOGNITION.pdf)\n## To do\n* improve the performance of the code\n* improve the accuracy of the code and reducing the false positive rate.\n* improve the face recognition accuracy to over 90 percent\n\n## Special Thanks to:\n* [Jignesh S. Bhatt](http://www.iiitvadodara.ac.in/faculty/jsb001.html) - Thank you for mentoring this project\n* [Kamal Awasthi](http://github.com/KamalAwasthi) - Helped in testing the code\n"
 },
 {
  "repo": "opencv/opencv_extra",
  "language": null,
  "readme_contents": "### OpenCV: Open Source Computer Vision Library\n\nThis repository contains extra data for the OpenCV library.\n\n#### Resources\n* Homepage: http://opencv.org\n* Docs: http://docs.opencv.org\n* Q&A forum: http://answers.opencv.org\n* Issue tracking: https://github.com/opencv/opencv/issues\n\n#### Contributing\n\nPlease read before starting work on a pull request: https://github.com/opencv/opencv/wiki/How_to_contribute\n\nSummary of guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the coding style guide.\n"
 },
 {
  "repo": "bethesirius/ChosunTruck",
  "language": "Python",
  "readme_contents": "# <img src=\"https://github.com/bethesirius/ChosunTruck/blob/master/README/Logo.png\" width=\"64\">ChosunTruck\n\n## Introduction\nChosunTruck is an autonomous driving solution for [Euro Truck Simulator 2](https://eurotrucksimulator2.com/).\nRecently, autonomous driving technology has become a big issue and as a result we have been studying technology that incorporates this.\nIt is being developed in a simulated environment called Euro Truck Simulator 2 to allow us to study it using vehicles.\nWe chose Euro Truck Simulator 2 because this simulator provides a good test environment that is similar to the real road.\n\n## Features\n* You can drive a vehicle without handling it yourself.\n* You can understand the principles of autonomous driving.\n* (Experimental/Linux only) You can detect where other vehicles are.\n\n## How To Run It\n### Windows\n\n#### Dependencies\n- OS: Windows 7, 10 (64bit)\n\n- IDE: Visual Studio 2013, 2015\n\n- OpenCV version: >= 3.1\n\n- [Cuda Toolkit 7.5](https://developer.nvidia.com/cuda-75-downloads-archive) (Note: Do an ADVANCED INSTALLATION. ONLY install the Toolkit + Integration to Visual Studio. Do NOT install the drivers + other stuff it would normally give you. Once installed, your project properties should look like this: https://i.imgur.com/e7IRtjy.png)\n\n- If you have a problem during installation, look at our [Windows Installation wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Windows-Installation)\n\n#### Required to allow input to work in Windows:\n- **Go to C:\\Users\\YOURUSERNAME\\Documents\\Euro Truck Simulator 2\\profiles and edit controls.sii from** \n```\nconfig_lines[0]: \"device keyboard `di8.keyboard`\"\nconfig_lines[1]: \"device mouse `fusion.mouse`\"\n```\nto \n```\nconfig_lines[0]: \"device keyboard `sys.keyboard`\"\nconfig_lines[1]: \"device mouse `sys.mouse`\"\n```\n(thanks Komat!)\n- **While you are in controls.sii, make sure your sensitivity is set to:**\n```\n config_lines[33]: \"constant c_rsteersens 0.775000\"\n config_lines[34]: \"constant c_asteersens 4.650000\"\n```\n#### Then:\n- Set controls.sii to read-only\n- Open the visual studio project and build it. \n- Run ETS2 in windowed mode and set resolution to 1024 * 768.(It will work properly with 1920 * 1080 screen resolution and 1024 * 768 window mode ETS2.)\n\n### Linux\n#### Dependencies\n- OS: Ubuntu 16.04 LTS\n\n- [OpenCV version: >= 3.1](http://embedonix.com/articles/image-processing/installing-opencv-3-1-0-on-ubuntu/)\n\n- (Optional) Tensorflow version: >= 0.12.1\n\n### Build the source code with the following command (inside the linux directory).\n```\nmake\n```\n### If you want the car detection function then:\n````\nmake Drive\n````\n#### Then:\n- Run ETS2 in windowed mode and set its resolution to 1024 * 768. (It will work properly with 1920 * 1080 screen resolution and 1024 * 768 windowed mode ETS2)\n- It cannot find the ETS2 window automatically. Move the ETS2 window to the right-down corner to fix this.\n- In ETS2 Options, set controls to 'Keyboard + Mouse Steering', 'left click' to acclerate, and 'right click' to brake.\n- Go to a highway and set the truck's speed to 40~60km/h. (I recommend you turn on cruise mode to set the speed easily)\n- Run this program!\n\n#### To enable car detection mode, add -D or --Car_Detection.\n```\n./ChosunTruck [-D|--Car_Detection]\n```\n## Troubleshooting\nSee [Our wiki page](https://github.com/bethesirius/ChosunTruck/wiki/Troubleshooting).\n\nIf you have some problems running this project, reference the demo video below. Or, [open a issue to contact our team](https://github.com/bethesirius/ChosunTruck/issues).\n\n## Demo Video\nLane Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/vF7J_uC045Q/0.jpg)](http://www.youtube.com/watch?v=vF7J_uC045Q)\n[![youtube link](http://img.youtube.com/vi/qb99czlIklA/0.jpg)](http://www.youtube.com/watch?v=qb99czlIklA)\n\nLane Detection + Vehicle Detection (Youtube link)\n\n[![youtube link](http://img.youtube.com/vi/w6H2eGEvzvw/0.jpg)](http://www.youtube.com/watch?v=w6H2eGEvzvw)\n\n## Todo\n* For better detection performance, Change the Tensorbox to YOLO2.\n* The information from in-game screen have Restrictions. Read ETS2 process memory to collect more driving environment data.\n\n## Founders\n- Chiwan Song, chi3236@gmail.com\n\n- JaeCheol Sim, simjaecheol@naver.com\n\n- Seongjoon Chu, hs4393@gmail.com\n\n## Contributors\n- [zappybiby](https://github.com/zappybiby)\n\n## How To Contribute\nAnyone who is interested in this project is welcome! Just fork it and pull requests!\n\n## License\nChosunTruck, Euro Truck Simulator 2 auto driving solution\nCopyright (C) 2017 chi3236, bethesirius, uoyssim\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n"
 },
 {
  "repo": "youyuge34/Anime-InPainting",
  "language": "Python",
  "readme_contents": "Anime-InPainting: An application Tool based on [Edge-Connect](https://github.com/knazeri/edge-connect)\n------------------------------------------------------------------------------------------------------\n<p align=\"left\">\n\t\t<img src=\"https://img.shields.io/badge/version-0.2-brightgreen.svg?style=flat-square\"\n\t\t\t alt=\"Version\">\n\t\t<img src=\"https://img.shields.io/badge/status-Release-gold.svg?style=flat-square\"\n\t\t\t alt=\"Status\">\n\t\t<img src=\"https://img.shields.io/badge/platform-win | linux-lightgrey.svg?style=flat-square\"\n\t\t\t alt=\"Platform\">\n\t\t<img src=\"https://img.shields.io/badge/PyTorch version-1.0-blue.svg?style=flat-square\"\n\t\t\t alt=\"PyTorch\">\n\t\t<img src=\"https://img.shields.io/badge/License-CC BY\u00b7NC 4.0-green.svg?style=flat-square\"\n\t\t\t alt=\"License\">\n</p>\n\nEnglish | [\u4e2d\u6587\u7248\u4ecb\u7ecd](#jump_zh)\n\n<hr>\n\n### Important\n**2019.3.27 Update:**     \nOur **latest** drawing method [PI-REC](https://github.com/youyuge34/PI-REC) is more powerful.      \n Take a look on it, and I'm sure it won't disappoint you.\n<hr>\n\n<p align=\"center\">\n<img src=\"files/banner.png\" width=\"720\" height=\"240\">\n</p>\n\n## Tool show time \ud83c\udff3\ufe0f\u200d\ud83c\udf08\n#### Outputs\n<p align=\"center\">\n<img src=\"files/show1.jpg\" width=\"720\" height=\"400\">\n</p>\n\n#### Tool operation\n<p align=\"center\">\n<img src=\"files/cut2.gif\" width=\"425\" height=\"425\">\n<img src=\"files/cut3.gif\" width=\"406\" height=\"222\">\n</p>\n\nIntroduction:\n-----\nThis is an optimized application tool which has a frontend based on `Opencv`, whose backend used [Edge-Connect](https://github.com/knazeri/edge-connect).\nMake sure you have read their awesome work and license thoroughly.\nCompared with the original work, this project has such <span id=\"improve\">improvements</span> :\n- Add tool application modes\n- Optimize the training phase\n  - Auto-save and auto-load latest weights files\n  - Add a fast training phase combined with origin phase 2 and 3\n- bugs fixed (most of them are merged into the original work)\n- Add utility files\n- Add configs in `config.yml`\n  - PRINT_FREQUENCY\n  - DEVICE : cpu or gpu\n- ... ...\n\n**You can do the amazing Anime inpainting conveniently here.**\n\n**And detailed [training manual](training_manual.md) is released. You may train your own dataset smoothly now.**\n\n## <span id='pre'>Prerequisites</span>\n- Python 3\n- PyTorch `1.0` (`0.4` is not supported)\n- NVIDIA GPU + CUDA cuDNN\n\n## <span id='ins'>Installation</span>\n- Clone this repo\n- Install PyTorch and dependencies from http://pytorch.org\n- Install python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## Run the Tool\nI want to run the tool! Calm down and follow such step:\n\n\n**Info: The following weights files are trained on anime face dataset which performs not good on a large whole anime character.**\n1. Download the well trained model weights file --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. Unzip the `.7z` and put it under your root directory.\nSo make sure your path now is: `./model/getchu/<xxxxx.pth>`\n3. Complete the above [Prerequisites](#pre) and [Installation](#ins)\n4. (Optional) Check and edit the `./model/getchu/config.yml` config file as you wish\n5. Run the cooool tool:\n\n#### Default Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### Tool with edge window:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### Args help\n```bash\npython tool_patch.py -h\n```\n\n> PS. You can run any well trained model, not only above one. You can download more model weights files\nfrom the original work [Edge-Connect](https://github.com/knazeri/edge-connect). Then you can run the Tool as above.\nOnly one thing to be careful: The `config.yml` in this project has some additional options than the config from the [Edge-Connect](https://github.com/knazeri/edge-connect).\n\n\n## Tool operation\nFor detailed manual, refer to your `terminal` prints or the `__doc__` in `tool_patch.py`.\n\nBelow is the simplified tool operation manual:\n\n\n\nKey | description\n-----|-----\nMouse `Left` | To draw out the defective area in window `input` and to draw the edge in window `edge`\nMouse `Right` | To erase edge in window `edge`\nKey `[` | To make the brush thickness smaller\nKey `]` | To make the brush thickness larger\nKey `0` | Todo\nKey `1` | Todo\nKey `n` | To patch the black part of image, just use input image\nKey `e` | To patch the black part of image, use the input image and edit edge (only work under edge window opened)\nKey `r` | To reset the setup\nKey `s` | To save the output\nKey `q` | To quit\n\n## Training manual\nClick here --> [Training manual by yourself](training_manual.md)\n\n\n\n<span id=\"jump_zh\">\u4e2d\u6587\u7248\u4ecb\u7ecd\ud83c\udde8\ud83c\uddf3 </span>\n-----\n\n<hr>\n\n### \u91cd\u8981\n**2019.3.27 \u66f4\u65b0:**     \n\u6211\u4eec\u7684\u6700\u65b0\u6a21\u578b [PI-REC](https://github.com/youyuge34/PI-REC) \u66f4\u5f3a\u5927.            \n\u5982\u679c\u4f60\u60f3\u7528\u6700\u65b0\u7684AI\u7ed8\u753b\u9ed1\u79d1\u6280\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u4fee\u8865\u56fe\u50cf\uff0c\u8bf7\u70b9\u51fb\u4e0a\u9762\u7684\u94fe\u63a5\ud83d\udc46\n<hr>\n\n\n## \u7b80\u4ecb\nTool\u6548\u679c\u770b\u4e0a\u9762\ud83d\udc46 | Bilibili\u89c6\u9891\u6559\u7a0b\uff1aTO DO\n\n\u8fd9\u662f\u56fe\u50cf\u4fee\u8865\u65b9\u5411\u6700\u65b0\u7814\u7a76\u6210\u679c[Edge-Connect](https://github.com/knazeri/edge-connect)\u7684~~\u963f\u59c6\u65af\u7279\u6717\u6c2e\u6c14\u52a0\u901f\u9b54\u6539~~\uff08\u4f18\u5316\uff09\u7248\u3002\n\u7528`Opencv`\u5199\u4e86\u4e2a\u524d\u7aef\u90e8\u5206\uff0c\u540e\u7aef\u662f[Edge-Connect](https://github.com/knazeri/edge-connect)\uff0c\u65b9\u4fbf\u5f53\u4f5c\u5de5\u5177\u4f7f\u7528\u3002\n\u6b64\u5de5\u5177\u53ef\u4ee5\u7528\u6765\u81ea\u52a8\u56fe\u50cf\u4fee\u8865\uff0c\u53bb\u9a6c\u8d5b\u514b\u2026\u2026\u540c\u6837\u4f18\u5316\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u8fc7\u7a0b\u3002\u5177\u4f53\u4f18\u5316\u5185\u5bb9\u8bf7\u770b[\u82f1\u6587\u7248Improvements](#improve)\u3002\n\n\u66f4\u65b0\uff1a[\u8bad\u7ec3\u624b\u518c](training_manual.md#jump_zh)\u5df2\u7ecf\u586b\u5751\u5b8c\u53d1\u5e03\u4e86\uff01\u4f60\u53ef\u4ee5\u7167\u7740\u6307\u5357\u8bad\u7ec3\u81ea\u5df1\u6570\u636e\u96c6\u4e86~\n\n## \u57fa\u7840\u73af\u5883\n- Python 3\n- PyTorch `1.0` (`0.4` \u4f1a\u62a5\u9519)\n- NVIDIA GPU + CUDA cuDNN \uff08\u5f53\u524d\u7248\u672c\u5df2\u53ef\u9009cpu\uff0c\u8bf7\u4fee\u6539`config.yml`\u4e2d\u7684`DEVICE`\uff09\n\n## \u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\n- Clone this repo\n- \u5b89\u88c5PyTorch\u548ctorchvision --> http://pytorch.org\n- \u5b89\u88c5 python requirements:\n```bash\npip install -r requirements.txt\n```\n\n## \u8fd0\u884cTool\n\u6559\u7ec3\uff01\u6211\u6709\u4e2a\u5927\u80c6\u7684\u60f3\u6cd5\ud83c\ude32\u2026\u2026\u522b\u6025\uff0c\u4e00\u6b65\u6b65\u6765\uff1a\n\n\n**\u6ce8\u610f\uff1a\u4ee5\u4e0b\u6a21\u578b\u662f\u5728\u52a8\u6f2b\u5934\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\uff0c\u6240\u4ee5\u5bf9\u52a8\u6f2b\u5168\u8eab\u5927\u56fe\u4fee\u8865\u6548\u679c\u4e00\u822c\uff0c\u60f3\u81ea\u5df1\u518d\u8bad\u7ec3\u7684\u53c2\u8003\u4e0b\u9762\u7684\u8bad\u7ec3\u6307\u5357**\n1. \u4e0b\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6587\u4ef6 --> [Google Drive](https://drive.google.com/file/d/12I-K7GQEXEL_rEOVJnRv7ecVHyuZE-1-/view?usp=sharing) | [Baidu](https://pan.baidu.com/s/1WkeRtYViGGGw4fUqPo3nsg)\n2. \u89e3\u538b `.7z` \u653e\u5230\u4f60\u7684\u6839\u76ee\u5f55\u4e0b.\n\u786e\u4fdd\u4f60\u7684\u76ee\u5f55\u73b0\u5728\u662f\u8fd9\u6837: `./model/getchu/<xxxxx.pth>`\n3. \u5b8c\u6210\u4e0a\u9762\u7684\u57fa\u7840\u73af\u5883\u548c\u7b2c\u4e09\u65b9\u5e93\u5b89\u88c5\u6b65\u9aa4\n4. (\u53ef\u9009) \u68c0\u67e5\u5e76\u7f16\u8f91 `./model/getchu/config.yml` \u914d\u7f6e\u6587\u4ef6\n5. \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\uff1a\n\n#### \u9ed8\u8ba4Tool:\n\n```bash\npython tool_patch.py --path model/getchu/\n```\n\n#### \u5e26Edge\u7f16\u8f91\u7a97\u53e3\u7684Tool:\n\n```bash\npython tool_patch.py --edge --path model/getchu/\n```\n\n#### \u547d\u4ee4\u884c\u53c2\u6570\u5e2e\u52a9\n```bash\npython tool_patch.py -h\n```\n\n> PS. \u4f60\u4e5f\u80fd\u7528tool\u8dd1\u522b\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5728\u8fd9\u91cc\u4e0b\u8f7d\u539f\u4f5c\u66f4\u591a\u6a21\u578b[Edge-Connect](https://github.com/knazeri/edge-connect).\n\u6587\u4ef6\u7ec4\u7ec7\u65b9\u5f0f\u53c2\u8003\u4e0a\u9762\uff0c\u5176\u4f59\u8fd0\u884c\u547d\u4ee4\u90fd\u4e00\u6837\u3002\u552f\u4e00\u6ce8\u610f\u7684\u662f\u8fd9\u4e2a\u9879\u76ee\u7684 `config.yml` \u6bd4\u539f\u4f5c\u7684\u591a\u4e86\u51e0\u4e2a\u9009\u9879\uff0c\u62a5\u9519\u4e86\u7684\u8bdd\u6ce8\u610f\u4fee\u6539\u3002\n\n## Tool\u64cd\u4f5c\u6307\u5357\n\u8be6\u7ec6\u5185\u5bb9\u8bf7\u7ffb\u770b\u63a7\u5236\u53f0\u7684\u6253\u5370\u5185\u5bb9\uff0c\u6216\u67e5\u770b`tool_patch.py`\u91cc\u7684`__doc__`      \n\u7b80\u7565\u7248tool\u4f7f\u7528\u6307\u5357\uff1a\n\n\u6309\u952e | \u8bf4\u660e\n-----|------\n\u9f20\u6807\u5de6\u952e | Input\u7a97\u53e3\uff1a\u753b\u51fa\u7455\u75b5\u533a\u57df\u7684\u906e\u76d6\uff0cEdge\u7a97\u53e3\uff1a\u624b\u52a8\u753b\u8fb9\u7f18\n\u9f20\u6807\u53f3\u952e | Edge\u7a97\u53e3\uff1a\u6a61\u76ae\u64e6\n\u6309\u952e `[` | \u7b14\u5237\u53d8\u7ec6 \uff08\u63a7\u5236\u53f0\u6253\u5370\u7c97\u7ec6\u5927\u5c0f\uff09\n\u6309\u952e `]` | \u7b14\u5237\u53d8\u7c97\n\u6309\u952e `0` | Todo\n\u6309\u952e `1` | Todo\n\u6309\u952e `n` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u53ea\u4f7f\u7528\u4e00\u5f20\u8f93\u5165\u56fe\u7247\n\u6309\u952e `e` | \u4fee\u8865\u9ed1\u8272\u6d82\u62b9\u533a\u57df\uff0c\u4f7f\u7528\u8f93\u5165\u56fe\u7247\u548c\u8fb9\u7f18\u56fe\u7247\uff08\u4ec5\u5f53edge\u7a97\u53e3\u542f\u52a8\u65f6\u6709\u6548\uff09\n\u6309\u952e `r` | \u5168\u90e8\u91cd\u7f6e\n\u6309\u952e `s` | \u4fdd\u5b58\u8f93\u51fa\u56fe\u7247\n\u6309\u952e `q` | \u9000\u51fa\n\n\n## \u8bad\u7ec3\u6307\u5357\n\u8bad\u7ec3\u6307\u5357 --> [\u9605\u8bfb](training_manual.md#jump_zh)\n\n\n## License\nLicensed under a [Creative Commons Attribution-NonCommercial 4.0 International](https://creativecommons.org/licenses/by-nc/4.0/).\n\nExcept where otherwise noted, this content is published under a [CC BY-NC](https://creativecommons.org/licenses/by-nc/4.0/) license, which means that you can copy, remix, transform and build upon the content as long as you do not use the material for commercial purposes and give appropriate credit and provide a link to the license.\n\n\n## Citation\nIf you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/1901.00212\">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a>:\n\n```\n@inproceedings{nazeri2019edgeconnect,\n  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},\n  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},\n  journal={arXiv preprint},\n  year={2019},\n}\n```\n\n"
 },
 {
  "repo": "mrnugget/opencv-haar-classifier-training",
  "language": "Perl",
  "readme_contents": "# Train your own OpenCV Haar classifier\n\n**Important**: This guide assumes you work with OpenCV 2.4.x. Since I no longer work with OpenCV, and don't have the time to keep up with changes and fixes, this guide is **unmaintained**. Pull requests will be merged of course, and if someone else wants commit access, feel free to ask!\n\nThis repository aims to provide tools and information on training your own\nOpenCV Haar classifier.  Use it in conjunction with this blog post: [Train your own OpenCV Haar\nclassifier](http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html).\n\n\n\n## Instructions\n\n1. Install OpenCV & get OpenCV source\n\n        brew tap homebrew/science\n        brew install --with-tbb opencv\n        wget http://downloads.sourceforge.net/project/opencvlibrary/opencv-unix/2.4.9/opencv-2.4.9.zip\n        unzip opencv-2.4.9.zip\n\n2. Clone this repository\n\n        git clone https://github.com/mrnugget/opencv-haar-classifier-training\n\n3. Put your positive images in the `./positive_images` folder and create a list\nof them:\n\n        find ./positive_images -iname \"*.jpg\" > positives.txt\n\n4. Put the negative images in the `./negative_images` folder and create a list of them:\n\n        find ./negative_images -iname \"*.jpg\" > negatives.txt\n\n5. Create positive samples with the `bin/createsamples.pl` script and save them\nto the `./samples` folder:\n\n        perl bin/createsamples.pl positives.txt negatives.txt samples 1500\\\n          \"opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\\\n          -maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40\"\n\n6. Use `tools/mergevec.py` to merge the samples in `./samples` into one file:\n\n        python ./tools/mergevec.py -v samples/ -o samples.vec\n\n   Note: If you get the error `struct.error: unpack requires a string argument of length 12`\n   then go into your **samples** directory and delete all files of length 0.\n\n7. Start training the classifier with `opencv_traincascade`, which comes with\nOpenCV, and save the results to `./classifier`:\n\n        opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024\n          \n    If you want to train it faster, configure feature type option with LBP:\n\n         opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\\\n          -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\\\n          -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\\\n          -precalcIdxBufSize 1024 -featureType LBP\n\n    After starting the training program it will print back its parameters and then start training. Each stage will print out some analysis as it is trained:\n\n      ```\n      ===== TRAINING 0-stage =====\n      <BEGIN\n      POS count : consumed   1000 : 1000\n      NEG count : acceptanceRatio    600 : 1\n      Precalculation time: 11\n      +----+---------+---------+\n      |  N |    HR   |    FA   |\n      +----+---------+---------+\n      |   1|        1|        1|\n      +----+---------+---------+\n      |   2|        1|        1|\n      +----+---------+---------+\n      |   3|        1|        1|\n      +----+---------+---------+\n      |   4|        1|        1|\n      +----+---------+---------+\n      |   5|        1|        1|\n      +----+---------+---------+\n      |   6|        1|        1|\n      +----+---------+---------+\n      |   7|        1| 0.711667|\n      +----+---------+---------+\n      |   8|        1|     0.54|\n      +----+---------+---------+\n      |   9|        1|    0.305|\n      +----+---------+---------+\n      END>\n      Training until now has taken 0 days 3 hours 19 minutes 16 seconds.\n      ```\n\n    Each row represents a feature that is being trained and contains some output about its HitRatio and FalseAlarm ratio. If a training stage only selects a few features (e.g. N = 2) then its possible something is wrong with your training data.\n\n    At the end of each stage the classifier is saved to a file and the process can be stopped and restarted. This is useful if you are tweaking a machine/settings to optimize training speed.\n\n8. Wait until the process is finished (which takes a long time \u2014 a couple of days probably, depending on the computer you have and how big your images are).\n\n9. Use your finished classifier!\n\n        cd ~/opencv-2.4.9/samples/c\n        chmod +x build_all.sh\n        ./build_all.sh\n        ./facedetect --cascade=\"~/finished_classifier.xml\"\n\n\n## Acknowledgements\n\nA huge thanks goes to Naotoshi Seo, who wrote the `mergevec.cpp` and\n`createsamples.cpp` tools and released them under the MIT licencse. His notes\non OpenCV Haar training were a huge help. Thank you, Naotoshi!\n\n## References & Links:\n\n- [Naotoshi Seo - Tutorial: OpenCV haartraining (Rapid Object Detection With A Cascade of Boosted Classifiers Based on Haar-like Features)](http://note.sonots.com/SciSoftware/haartraining.html)\n- [Material for Naotoshi Seo's tutorial](https://code.google.com/p/tutorial-haartraining/)\n- [OpenCV Documentation - Cascade Classifier Training](http://docs.opencv.org/doc/user_guide/ug_traincascade.html)\n"
 },
 {
  "repo": "RaftLib/RaftLib",
  "language": "C++",
  "readme_contents": "[RaftLib](http://raftlib.io) is a C++ Library for enabling stream/data-flow parallel computation. Using simple right shift operators (just like the C++ streams that you would use for string manipulation), you can link parallel compute kernels together. With RaftLib, we do away with explicit use of pthreads, std::thread, OpenMP, or any other parallel \"threading\" library. These are often mis-used, creating non-deterministic behavior. RaftLib's model allows lock-free FIFO-like access to the communications channels connecting each compute kernel. The full system has many auto-parallelization, optimization, and convenience features that enable relatively simple authoring of performant applications. Feel free to give it a shot, if you have any issues, please create an issue request. Minor issues, \nthe Slack group is the best way to resolve. We take pull requests!! For benchmarking, feel free to send the \nauthors an email. We've started a benchmark collection, however, it's far from complete. We'd love to add your \ncode!! \n\nUser Group / Mailing List: [slack channel](https://join.slack.com/t/raftlib/shared_invite/zt-3sk6ms6f-eEBd23dz98JnoRiXLaRmNw)\n\n=============\n\n### Build status\n\n![CI](https://github.com/RaftLib/RaftLib/workflows/CI/badge.svg?event=push)\n\n### Pre-requisites\n\n#### OS X & Linux\nCompiler: c++14 capable -> Clang, GNU GCC 5.0+, or Intel icc\n\n#### Windows\n* Latest merge from pull request to main should enable compilation on VS on Win10.\n\n### Install\nMake a build directory (for the instructions below, we'll \nwrite [build]). If you want to build the OpenCV example, then\nyou'll need to add to your cmake invocation:\n```bash\n-DBUILD_WOPENCV=true \n```\n\nTo use the [QThreads User space HPC threading library](http://www.cs.sandia.gov/qthreads/) \nyou will need to use the version with the RaftLib org and follow the RaftLib specific readme. \nThis QThreads version has patches for hwloc2.x applied and fixes for test cases. To compile\nRaftLib with QThreads linked, add the following (assumes the QThreads library is in your path):\n```bash\n-DUSEQTHREAD=1\n```\n\nBuilding the examples, benchmarks and tests can be disabled using:\n```bash\n-DBUILD_EXAMPLES=false\n-DBUILD_BENCHMARKS=false\n-DBUILD_TESTS=false\n```\n\nTo build:\n\n```bash\nmkdir [build]\ncd [build]\ncmake ..\nmake && make test\nsudo make install\n```\nNOTE: The default prefix in the makefile is: \n```\nPREFIX ?= /usr/local\n```\n\n## Using\n* When building applications with RaftLib, on Linux it is best to \nuse the **pkg-config** file, as an example, using the _poc.cpp_ example,\n```bash\ng++ `pkg-config --cflags raftlib` poc.cpp -o poc `pkg-config --libs raftlib`\n```\n\nFeel free to substitute your favorite build tool. I use Ninja and make depending on which machine I'm on. To change out, use cmake to generate the appropriate build files with the -Gxxx flag.\n\n### Citation\nIf you use this framework for something that gets published, please cite it as:\n```bibtex\n@article{blc16,\n  author = {Beard, Jonathan C and Li, Peng and Chamberlain, Roger D},\n  title = {RaftLib: A C++ Template Library for High Performance Stream Parallel Processing},\n  year = {2016},\n  doi = {http://dx.doi.org/10.1177/1094342016672542},\n  eprint = {http://hpc.sagepub.com/content/early/2016/10/18/1094342016672542.full.pdf+html},\n  journal = {International Journal of High Performance Computing Applications}\n}\n```\n### Other Info Sources\n* [OpenCV wrappers for RaftLib](https://github.com/RaftLib/RaftOCV)\n* [Project web page](http://raftlib.io)\n* [Project wiki page](https://github.com/jonathan-beard/RaftLib/wiki)\n* [Blog post intro](https://goo.gl/4VDlbr)\n* [Jonathan Beard's thesis](http://goo.gl/obkWUh)\n* [Views on parallel computing, general philosphy](https://goo.gl/R5fQAl)\n* Feel free to e-mail one of the authors of the repo\n"
 },
 {
  "repo": "mukyasa/MMCamScanner",
  "language": "Objective-C",
  "readme_contents": "# MMCamScanner\n#### Simulation to CamScanner app With Custom Camera and Crop Rect Validation \n\n\n------------------\n\n![MMCamScanner](https://github.com/mukyasa/MMCamScanner/blob/master/camscan.gif)\n\n#### Video preview [Here](https://www.youtube.com/watch?v=vO1kA6fjKQ4)\n \n#### ChangeLog(31/7/2015)\n * Crop Feature Tweaked for more control in horizontal and vertical dragging.\n * NOW , Landscape and Potrait both images can be used in scanning the objects in images.\n * Addded Image Filters (Gray Scale, Magic Color ,Black and White)\n * Left Rotate and Right Rotate\n \n\n**Framework**\nAdd the Frameworks to see the Demo\n\nOpenCV:http://opencv.org/<br />\nTesseract OCR:https://github.com/gali8/Tesseract-OCR-iOS\n\n**Thanks to Stackoverflow for solving queries related to OpenCV**<br />\n\n**Credits**<br />\n\nExcellent Square Detection Code Ref:[Here](http://stackoverflow.com/questions/8667818/opencv-c-obj-c-detecting-a-sheet-of-paper-square-detection) and [Here](https://github.com/Itseez/opencv/blob/master/samples/cpp/squares.cpp)<br />\n\nOpenCV Tutorials:[Here](http://opencv.org/\">http://opencv.org)<br />\n\nCustom Camera Project Ref:[Here](https://github.com/LoganWright/SimpleCam)<br />  (LoganWright)\n\nCrop Rect Validation Vector Ref:[Here](https://github.com/mysterioustrousers/MTGeometry) (Mysterious Trousers)\n\n\n**Also Thanks to this Project from where it is Inspired From**\n\nMaximilian Mackh :[Here](https://github.com/mmackh/MAImagePickerController-of-InstaPDF)\n\n\n**This Project Contains**\n\n1.Custom Camera(Support Potrait Only Now) with Zoom Slider,Pinch Zoom,Tap to Focus features also.<br />\n2.Custom Ripple View Controller Animation.<br />\n3.OpenCV square Detection in Image for detecting objects.<br />\n4.OpenCV perpespective and warptransform for CROP feature.<br />\n5.Crop Validation for checking INVALID RECT.<br />\n6.Memory Efficient Camera(Thanks to SimpleCam Project).<br />\n7.Tesseract OCR.<br />\n\n\n**My Other Repositories**\n\n**MMPaper:**<br />\nhttps://github.com/mukyasa/MMPaper<br />\n\n**MMTextFieldEffects:**<br />\nhttps://github.com/mukyasa/MMTextFieldEffects<br />\n\n**MMGooglePlayNewsStand:**<br />\nhttps://github.com/mukyasa/MMGooglePlayNewsStand\n\n**MMPaperPanFlip:**<br /> \nhttps://github.com/mukyasa/MMPaperPanFlip<br />\n\n**MMTransitionEffect:**<br />\nhttps://github.com/mukyasa/MMTransitionEffect<br />\n\n\nContact Me\n==========\nMukesh Mandora\n\nContact: mandoramuku07@gmail.com\n\nTwitter: http://twitter.com/mandymuku\n\nLinkedIn: https://in.linkedin.com/in/mukeshmandora\n\nGithub:https://github.com/mukyasa\n\n\n## License\nMMCamScanner is available under the Apache license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "uvipen/QuickDraw",
  "language": "Python",
  "readme_contents": "# [PYTHON] QuickDraw\n\n## Introduction\n\nHere is my python source code for QuickDraw - an online game developed by google. with my code, you could: \n* **Run an app which you could draw in front of a camera (If you use laptop, your webcam will be used by default)**\n* **Run an app which you could draw on a canvas**\n\n## Camera app\nIn order to use this app, you need a pen (or any object) with blue, red or green color. When the pen (object) appears in front of camera, it will be catched and highlighted by an yellow circle. When you are ready for drawing, you need to press **space** button. When you want to stop drawing, press **space** again\nBelow is the demo by running the sript **camera_app.py**:\n<p align=\"center\">\n  <img src=\"demo/quickdraw.gif\" width=600><br/>\n  <i>Camera app demo</i>\n</p>\n\n## Drawing app\nThe script and demo will be released soon\n\n## Dataset\nThe dataset used for training my model could be found at [Quick Draw dataset] https://console.cloud.google.com/storage/browser/quickdraw_dataset/sketchrnn. Here I only picked up 20 files for 20 categories\n\n## Categories:\nThe table below shows 20 categories my model used:\n\n|           |           |           |           |\n|-----------|:-----------:|:-----------:|:-----------:|\n|   apple   |   book    |   bowtie  |   candle  |\n|   cloud   |    cup    |   door    | envelope  |\n|eyeglasses |  guitar   |   hammer  |    hat    |\n| ice cream |   leaf    | scissors  |   star    |\n|  t-shirt  |   pants   | lightning |    tree   |\n\n## Trained models\n\nYou could find my trained model at **trained_models/whole_model_quickdraw**\n\n## Training\n\nYou need to download npz files corresponding to 20 classes my model used and store them in folder **data**. If you want to train your model with different list of categories, you only need to change the constant **CLASSES** at **src/config.py** and download necessary npz files. Then you could simply run **python3 train.py**\n\n## Experiments:\n\nFor each class, I take the first 10000 images, and then split them to training and test sets with ratio 8:2. The training/test loss/accuracy curves for the experiment are shown below:\n\n<img src=\"demo/loss_accuracy_curves.png\" width=\"800\"> \n\n## Requirements\n\n* **python 3.6**\n* **cv2**\n* **pytorch** \n* **numpy**\n"
 },
 {
  "repo": "vipul-sharma20/document-scanner",
  "language": "Python",
  "readme_contents": "document-scanner\n================\n\nA document scanner built using OpenCV + Python.\nI highly recommend to see my blog post for better understanding: [http://vipulsharma20.blogspot.on](http://vipulsharma20.blogspot.in)\n\nMy sincere thanks to the article and the author here: [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/) which has some really good set of articles on OpenCV and way more informative.\n\n* Original Image\n![Alt](http://1.bp.blogspot.com/-gbFBHQKFU7w/VpGzzVfxmLI/AAAAAAAAEks/jtX2fikHs5o/s1600/Original.jpg \"original\")\n\n* Grayscaled Image\n![Alt](http://2.bp.blogspot.com/--LzOU44-dhM/VpG2B6DJbxI/AAAAAAAAEk4/4BGXnfhvsk4/s1600/Original%2BGray.jpg \"gray\")\n\n* Gaussian Blur\n![Alt](http://2.bp.blogspot.com/-KfEWWzIXxBg/VpG2RY0upjI/AAAAAAAAElA/psuYvv1rnm0/s1600/Original%2BBlurred.jpg \"blurred\")\n\n* Edge Detection (Canny Edge Detection)\n![Alt](http://3.bp.blogspot.com/-5TVP2UFeGXk/VpG5-bIYNqI/AAAAAAAAElM/zmyNrbvnh8Q/s1600/Original%2BEdged.jpg \"edge\")\n\n* Contour Detection\n![Alt](http://1.bp.blogspot.com/-Es0PkMvJJxU/VpHcQEXzXaI/AAAAAAAAElc/NuCZmuW1K6o/s1600/Outline_all.jpg \"contour\")\n\n* Approximated Contour\n![Alt](http://4.bp.blogspot.com/-DL7XWsLvWg8/VpHeN6bA3gI/AAAAAAAAElo/1TMug5_tCeQ/s1600/Outline.jpg \"approx\")\n\n* Perspective Transform\n![Alt](http://4.bp.blogspot.com/-1dhSo9PrD6o/VpHjhgH0viI/AAAAAAAAEl4/AzYqjzLiNbI/s1600/dst.jpg \"transform\")\n\n"
 },
 {
  "repo": "alyssaq/face_morpher",
  "language": "Python",
  "readme_contents": "Face Morpher\n============\n\n| Warp, average and morph human faces!\n| Scripts will automatically detect frontal faces and skip images if\n  none is detected.\n\nBuilt with Python, `dlib`_, Numpy, Scipy, dlib.\n\n| Supported on Python 2.7, Python 3.6+\n| Tested on macOS Mojave and 64bit Linux (dockerized).\n\nRequirements\n--------------\n-  ``pip install -r requirements.txt``\n- Download `http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2` and extract file.\n- Export environment variable ``DLIB_DATA_DIR`` to the folder where ``shape_predictor_68_face_landmarks.dat`` is located. Default ``data``. E.g ``export DLIB_DATA_DIR=/Downloads/data``\n\nEither:\n\n-  `Use as local command-line utility`_\n-  `Use as pip library`_\n-  `Try out in a docker container`_\n\n.. _`Use as local command-line utility`:\n\nUse as local command-line utility\n---------------------------------\n::\n\n    $ git clone https://github.com/alyssaq/face_morpher\n\nMorphing Faces\n--------------\n\nMorph from a source to destination image:\n\n::\n\n    python facemorpher/morpher.py --src=<src_imgpath> --dest=<dest_imgpath> --plot\n\nMorph through a series of images in a folder:\n\n::\n\n    python facemorpher/morpher.py --images=<folder> --out_video=out.avi\n\nAll options listed in ``morpher.py`` (pasted below):\n\n::\n\n    Morph from source to destination face or\n    Morph through all images in a folder\n\n    Usage:\n        morpher.py (--src=<src_path> --dest=<dest_path> | --images=<folder>)\n                [--width=<width>] [--height=<height>]\n                [--num=<num_frames>] [--fps=<frames_per_second>]\n                [--out_frames=<folder>] [--out_video=<filename>]\n                [--plot] [--background=(black|transparent|average)]\n\n    Options:\n        -h, --help              Show this screen.\n        --src=<src_imgpath>     Filepath to source image (.jpg, .jpeg, .png)\n        --dest=<dest_imgpath>   Filepath to destination image (.jpg, .jpeg, .png)\n        --images=<folder>       Folderpath to images\n        --width=<width>         Custom width of the images/video [default: 500]\n        --height=<height>       Custom height of the images/video [default: 600]\n        --num=<num_frames>      Number of morph frames [default: 20]\n        --fps=<fps>             Number frames per second for the video [default: 10]\n        --out_frames=<folder>   Folder path to save all image frames\n        --out_video=<filename>  Filename to save a video\n        --plot                  Flag to plot images to result.png [default: False]\n        --background=<bg>       Background of images to be one of (black|transparent|average) [default: black]\n        --version               Show version.\n\nAveraging Faces\n---------------\n\nAverage faces from all images in a folder:\n\n::\n\n    python facemorpher/averager.py --images=<images_folder> --out=average.png\n\nAll options listed in ``averager.py`` (pasted below):\n\n::\n\n    Face averager\n\n    Usage:\n        averager.py --images=<images_folder> [--blur] [--plot]\n                [--background=(black|transparent|average)]\n                [--width=<width>] [--height=<height>]\n                [--out=<filename>] [--destimg=<filename>]\n\n    Options:\n        -h, --help             Show this screen.\n        --images=<folder>      Folder to images (.jpg, .jpeg, .png)\n        --blur                 Flag to blur edges of image [default: False]\n        --width=<width>        Custom width of the images/video [default: 500]\n        --height=<height>      Custom height of the images/video [default: 600]\n        --out=<filename>       Filename to save the average face [default: result.png]\n        --destimg=<filename>   Destination face image to overlay average face\n        --plot                 Flag to display the average face [default: False]\n        --background=<bg>      Background of image to be one of (black|transparent|average) [default: black]\n        --version              Show version.\n\nSteps (facemorpher folder)\n--------------------------\n\n1. Locator\n^^^^^^^^^^\n\n-  Locates face points\n-  For a different locator, return an array of (x, y) control face\n   points\n\n2. Aligner\n^^^^^^^^^^\n\n-  Align faces by resizing, centering and cropping to given size\n\n3. Warper\n^^^^^^^^^\n\n-  Given 2 images and its face points, warp one image to the other\n-  Triangulates face points\n-  Affine transforms each triangle with bilinear interpolation\n\n4a. Morpher\n^^^^^^^^^^^\n\n-  Morph between 2 or more images\n\n4b. Averager\n^^^^^^^^^^^^\n\n-  Average faces from 2 or more images\n\nBlender\n^^^^^^^\n\nOptional blending of warped image:\n\n-  Weighted average\n-  Alpha feathering\n-  Poisson blend\n\nExamples - `Being John Malkovich`_\n----------------------------------\n\nCreate a morphing video between the 2 images:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_video=out.avi``\n\n(out.avi played and recorded as gif)\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/being_john_malvokich.gif\n   :alt: gif\n\nSave the frames to a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--out_frames=out_folder --num=30``\n\nPlot the frames:\n^^^^^^^^^^^^^^^^\n\n| ``> python facemorpher/morpher.py --src=alyssa.jpg --dest=john_malkovich.jpg``\n| ``--num=12 --plot``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/plot.png\n   :alt: plot\n\nAverage all face images in a folder:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n85 images used\n\n| ``> python facemorpher/averager.py --images=images --blur --background=transparent``\n| ``--width=220 --height=250``\n\n.. figure:: https://raw.github.com/alyssaq/face_morpher/master/examples/average_faces.png\n   :alt: average\\_faces\n\n.. _`Use as pip library`:\n\nUse as pip library\n---------------------------------\n::\n\n    $ pip install facemorpher\n\nExamples\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAdditional options are exactly the same as the command line\n\n::\n\n    import facemorpher\n\n    # Get a list of image paths in a folder\n    imgpaths = facemorpher.list_imgpaths('imagefolder')\n\n    # To morph, supply an array of face images:\n    facemorpher.morpher(imgpaths, plot=True)\n\n    # To average, supply an array of face images:\n    facemorpher.averager(['image1.png', 'image2.png'], plot=True)\n\n\nOnce pip installed, 2 binaries are also available as a command line utility:\n\n::\n\n    $ facemorpher --src=<src_imgpath> --dest=<dest_imgpath> --plot\n    $ faceaverager --images=<images_folder> --plot\n\nTry out in a docker container\n---------------------------------\nMount local folder to `/images` in docker container, run it and enter a bash session.\n--rm removes the container when you close it.\n::\n\n    $ docker run -v  /Users/alyssa/Desktop/images:/images --name py3 --rm -it jjanzic/docker-python3-opencv bash\n\nOnce you're in the container, install ``facemorpher`` and try the examples listed above\n::\n\n    root@0dad0912ebbe:/# pip install facemorpher\n    root@0dad0912ebbe:/# facemorpher --src=<img1> --dest=<img2> --plot\n\nDocumentation\n-------------\n\nhttp://alyssaq.github.io/face_morpher\n\nBuild & publish Docs\n^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    ./scripts/publish_ghpages.sh\n\nLicense\n-------\n`MIT`_\n\n.. _Being John Malkovich: http://www.rottentomatoes.com/m/being_john_malkovich\n.. _Mac installation steps: https://gist.github.com/alyssaq/f60393545173379e0f3f#file-4-opencv3-with-python3-md\n.. _MIT: http://alyssaq.github.io/mit-license\n.. _OpenCV: http://opencv.org\n.. _Homebrew: https://brew.sh\n.. _source: https://github.com/opencv/opencv\n.. _dlib: http://dlib.net\n"
 },
 {
  "repo": "JimmyHHua/opencv_tutorials",
  "language": "Python",
  "readme_contents": "# OpenCV 4.0 Tutorial\n[![](https://img.shields.io/badge/opencv-v4.0.0-orange.svg)](https://opencv.org/)       [![](https://img.shields.io/badge/opencv-tutorial-brightgreen.svg)](https://docs.opencv.org/4.0.0/d9/df8/tutorial_root.html)\n\n\u2712\ufe0f [\u4e2d\u6587\u7248\u672c](./README_CN.md)\n## Introduction\n\nThis repository contains source code of OpenCV Tutorial application, the environment is python3.0 and opencv4.0.\n\n## Sample\n- **Image load**\n```python\nimport cv2\n\nsrc = cv2.imread(\"test.png\")\ncv2.namedWindow(\"input\", cv2.WINDOW_AUTOSIZE)\ncv2.imshow(\"input\", src)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n<div align=center><img src=\"https://i.loli.net/2019/05/22/5ce4b40258c9155103.jpg\" width=200></div>\n\n- **Gray Image**\n```python\ngray = cv2.cvtColor(src, cv.COLOR_BGR2GRAY)\n```\n\n<div align=center><img src=https://i.loli.net/2019/05/22/5ce4b2ae1e7ce86434.png width=120>       <img src=https://i.loli.net/2019/05/22/5ce4b2ae220a248459.png width=120></div>\n\n\n\n***More opencv4.0 tutorials plese follow the learning road as below*** \ud83d\udc47\ud83d\udc47\ud83d\udc47\n\n## Learning Road \u26f3\ufe0f\n\n***Annotation:***\n- \u2714\ufe0f  **: Basic**\n- \u270f\ufe0f  **: Attention**\n- \u2763\ufe0f  **: Important**\n\nNo    | Description   | Annotation\n:--------: | :--------: | :--------:\ncode_001 | [Load Image](python/code_001/opencv_001.py)   | \u2714\ufe0f\ncode_002 | [Gray Image](python/code_002/opencv_002.py)   | \u2714\ufe0f\ncode_003 | [Image Create](python/code_003/opencv_003.py)   | \u2714\ufe0f\ncode_004 | [Pixel Read and Write](python/code_004/opencv_004.py)   | \u2714\ufe0f\ncode_005 | [Image Pixel Arithmetic Operations](python/code_005/opencv_005.py)   | \u2714\ufe0f\ncode_006 | [Image Pseudo-Color Enhancement](python/code_006/opencv_006.py)   | \u2714\ufe0f\ncode_007 | [Image Pixel Operation (Logical Operation)](python/code_007/opencv_007.py)   | \u2714\ufe0f\ncode_008 | [Image Channel Separation and Merging](python/code_008/opencv_008.py)   | \u2714\ufe0f\ncode_009 | [Color Space Conversion](python/code_009/opencv_009.py)   | \u270f\ufe0f\ncode_010 | [Image Pixel Value Statistics](python/code_010/opencv_010.py)   | \u2714\ufe0f\ncode_011 | [Image Pixel Normalization](python/code_011/opencv_011.py)   | \u2714\ufe0f\ncode_012 | [Video Read and Write](python/code_012/opencv_012.py)   | \u2714\ufe0f\ncode_013 | [Image Flip](python/code_013/opencv_013.py)   | \u2714\ufe0f\ncode_014 | [Image Interpolation](python/code_014/opencv_014.py)   | \u2714\ufe0f\ncode_015 | [Draw Geometry](python/code_015/opencv_015.py)   | \u2714\ufe0f\ncode_016 | [ROI of Image](python/code_016/opencv_016.py)   | \u2714\ufe0f\ncode_017 | [Image Histogram](python/code_017/opencv_017.py)   | \u2714\ufe0f\ncode_018 | [Histogram Dqualization](python/code_018/opencv_018.py)   | \u270f\ufe0f\ncode_019 | [Histogram Comparison](python/code_019/opencv_019.py)   | \u2714\ufe0f\ncode_020 | [Histogram Backprojection](python/code_020/opencv_020.py)   | \u2714\ufe0f\ncode_021 | [Image Convolution](python/code_021/opencv_021.py)   | \u2714\ufe0f\ncode_022 | [Averaging and Gaussian Blur](python/code_022/opencv_022.py)   | \u2763\ufe0f\ncode_023 | [Median Blur](python/code_023/opencv_023.py)   | \u2714\ufe0f\ncode_024 | [Image Noise](python/code_024/opencv_024.py)   | \u2714\ufe0f\ncode_025 | [Smoothing Images](python/code_025/opencv_025.py)   | \u2714\ufe0f\ncode_026 | [Gaussian Bilateral Blur](python/code_026/opencv_026.py)   | \u2714\ufe0f\ncode_027 | [Mean-shift Blur)](python/code_027/opencv_027.py)   | \u2714\ufe0f\ncode_028 | [Image Integral Algorithm](python/code_028/opencv_028.py)   | \u2714\ufe0f\ncode_029 | [Fast Image Edge Filtering Algorithm](python/code_029/opencv_029.py)   | \u2714\ufe0f\ncode_030 | [Custom Filter](python/code_030/opencv_030.py)   | \u2714\ufe0f\ncode_031 | [Sobel Operator](python/code_031/opencv_031.py)   | \u2714\ufe0f\ncode_032 | [More Gradient Operators](python/code_032/opencv_032.py)   | \u2714\ufe0f\ncode_033 | [Laplace Operator](python/code_033/opencv_033.py)   | \u2714\ufe0f\ncode_034 | [Image Sharpening ](python/code_034/opencv_034.py)   | \u2714\ufe0f\ncode_035 | [USM Sharpen Algorithm](python/code_035/opencv_035.py)   | \u2714\ufe0f\ncode_036 | [Canny Edge Detection](python/code_036/opencv_036.py)   | \u2763\ufe0f\ncode_037 | [Image Pyramid](python/code_037/opencv_037.py)   | \u2714\ufe0f\ncode_038 | [Laplace Pyramid](python/code_038/opencv_038.py)   | \u2714\ufe0f\ncode_039 | [Image Template Matching](python/code_039/opencv_039.py)   | \u2714\ufe0f\ncode_040 | [Binary introduction](python/code_040/opencv_040.py)   | \u2714\ufe0f\ncode_041 | [Basic Thresholding](python/code_041/opencv_041.py)   | \u2714\ufe0f\ncode_042 | [OTSU Thresholding](python/code_042/opencv_042.py)   | \u270f\ufe0f\ncode_043 | [TRIANGLE Thresholding](python/code_043/opencv_043.py)   | \u2714\ufe0f\ncode_044 | [Adaptive Thresholding](python/code_044/opencv_044.py)   | \u270f\ufe0f\ncode_045 | [Binary and Smoothing](python/code_045/opencv_045.py)   | \u270f\ufe0f\ncode_046 | [Image Connectivity component](python/code_046/opencv_046.py)   | \u2714\ufe0f\ncode_047 | [Image Connected component state statistics](python/code_047/opencv_047.py)   | \u2714\ufe0f\ncode_048 | [Image Contours](python/code_048/opencv_048.py)   | \u2763\ufe0f\ncode_049 | [Bounding Rectangle](python/code_049/opencv_049.py)   | \u2763\ufe0f\ncode_050 | [Contour Area and Perimeter](python/code_050/opencv_050.py)   | \u270f\ufe0f\ncode_051 | [Contour Approximation](python/code_051/opencv_051.py)   | \u2714\ufe0f\ncode_052 | [Contour Centroid Calculate](python/code_052/opencv_052.py)   | \u2714\ufe0f\ncode_053 | [HuMoment for Contour Matching](python/code_053/opencv_053.py)   | \u2714\ufe0f\ncode_054 | [Contour Cricle and Ellipse fitting](python/code_054/opencv_054.py)   | \u2714\ufe0f\ncode_055 | [Convex Hull](python/code_055/opencv_055.py)   | \u270f\ufe0f\ncode_056 | [Fitting a Line](python/code_056/opencv_056.py)   | \u2714\ufe0f\ncode_057 | [Point Polygon Test](python/code_057/opencv_057.py)   | \u2714\ufe0f\ncode_058 | [The Largest Inner Circle](python/code_058/opencv_058.py)   | \u2714\ufe0f\ncode_059 | [Hoffman Line Detection](python/code_059/opencv_059.py)   | \u2714\ufe0f\ncode_060 | [Probability Hoffman Line Detection](python/code_060/opencv_060.py)   | \u2763\ufe0f\ncode_061 | [Hoffman Cricle Detection](python/code_061/opencv_061.py)   | \u2763\ufe0f\ncode_062 | [Dilation and Erosion](python/code_062/opencv_062.py)   | \u2763\ufe0f\ncode_063 | [Structuring Element](python/code_063/opencv_063.py)   | \u2714\ufe0f\ncode_064 | [Opening Transformation](python/code_064/opencv_064.py)   | \u270f\ufe0f\ncode_065 | [Closing Transformation](python/code_065/opencv_065.py)   | \u270f\ufe0f\ncode_066 | [Application of Opening and Closing Operations](python/code_066/opencv_066.py)   | \u270f\ufe0f\ncode_067 | [Top Hat](python/code_067/opencv_067.py)   | \u2714\ufe0f\ncode_068 | [Black Hat](python/code_068/opencv_068.py)   | \u2714\ufe0f\ncode_069 | [Morph Gradient](python/code_069/opencv_069.py)   | \u2714\ufe0f\ncode_070 | [Contour based on Morph Gradient](python/code_070/opencv_070.py)   | \u270f\ufe0f\ncode_071 | [Hit and Miss](python/code_071/opencv_071.py)   | \u2714\ufe0f\ncode_072 | [Defect Detecting-1](python/code_072)   | \u2714\ufe0f\ncode_073 | [Defect Detecting-2](python/code_073/opencv_073.py)   | \u2714\ufe0f\ncode_074 | [Extract the Maximum Contour and Coding Key Points](python/code_074)   | \u2714\ufe0f\ncode_075 | [Image Inpainting](python/code_075/opencv_075.py)   | \u2714\ufe0f\ncode_076 | [Perspective Transformation](python/code_076/opencv_076.py)   | \u270f\ufe0f\ncode_077 | [Video Read, Write and Process](python/code_077/opencv_077.py)   | \u270f\ufe0f\ncode_078 | [Identify and Track Specific Color Objects in Video](python/code_078)   | \u2714\ufe0f\ncode_079 | [Video Analysis-Background/Foreground Extraction](python/code_079/opencv_079.py)   | \u2714\ufe0f\ncode_080 | [Video Analysis\u2013Background Subtraction and ROI Extraction of the Foreground](python/code_080)   | \u2714\ufe0f\ncode_081 | [Corner Detection-Harris](python/code_081)   | \u2714\ufe0f\ncode_082 | [Corner Detection-Shi-Tomas](python/code_082)   | \u270f\ufe0f\ncode_083 | [Corner Detection-Sub-Pixel ](python/code_083)   | \u2714\ufe0f\ncode_084 | [Video Analysis-KLT Optical Flow-1](python/code_084)   | \u270f\ufe0f\ncode_085 | [Video Analysis-KLT Optical Flow-2](python/code_085)   | \u270f\ufe0f\ncode_086 | [Video Analysis-Dense Optical Flow](python/code_086)   | \u270f\ufe0f\ncode_087 | [Video Analysis-Frame Difference Moving Object Analysis](python/code_087/opencv_087.py)   | \u2714\ufe0f\ncode_088 | [Video Analysis-Meanshift](python/code_088)   | \u270f\ufe0f\ncode_089 | [Video Analysis-CamShift](python/code_089)   | \u270f\ufe0f\ncode_090 | [Video Analysis-Object Movement Trajectory Drawing](python/code_090)   | \u2714\ufe0f\ncode_091 | [Object Detection-HAAR Cascade Classification ](python/code_091)   | \u2714\ufe0f\ncode_092 | [Object Detection-HAAR Feature Analysis](python/code_092)   | \u2714\ufe0f\ncode_093 | [Object Detection-LBP Feature Analysis](python/code_093/opencv_093.py)   | \u2714\ufe0f\ncode_094 | [ORB Feature Critical Point Detection](python/code_094)   | \u270f\ufe0f\ncode_095 | [ORB Feature Descriptor Matching](python/code_095)   | \u2714\ufe0f\ncode_096 | [Multiple  Descriptor Matching Methods](python/code_096)   | \u270f\ufe0f\ncode_097 | [Location of Known Objects Based on Descriptor Matches](python/code_097)   | \u270f\ufe0f\ncode_098 | [SIFT Feature Critical Point Detection](python/code_097)   | \u2714\ufe0f\ncode_099 | [SIFT Feature Descriptor Matching](python/code_097)   | \u2714\ufe0f\ncode_100 | [HOG Pedestrian Detection](python/code_100/opencv_100.py)   | \u2714\ufe0f\ncode_101 | [HOG Multiscale Detection](python/code_101/opencv_101.py)   | \u270f\ufe0f\ncode_102 | [HOG Extract Descriptor](python/code_102/opencv_102.py)   | \u2714\ufe0f\ncode_103 | [HOG Use Descriptors to Generate Sample Data](python/code_103/opencv_103.py)   | \u2714\ufe0f\ncode_104 | [(Detection Case)-HOG+SVM Train](python/code_104/opencv_104.py)   | \u2714\ufe0f\ncode_105 | [(Detection Case)-HOG+SVM Predict](python/code_105/opencv_105.py)   | \u2714\ufe0f\ncode_106 | [AKAZE Features and Descriptors](python/code_106)   | \u2714\ufe0f\ncode_107 | [Brisk Features and Descriptors](python/code_107)   | \u2714\ufe0f\ncode_108 | [GFTT Detector](python/code_108)   | \u2714\ufe0f\ncode_109 | [BLOB Feature Analysis](python/code_109)   | \u2714\ufe0f\ncode_110 | [KMeans Data Classification](python/code_110)   | \u2714\ufe0f\ncode_111 | [KMeans Image Segmentation](python/code_111)   | \u2714\ufe0f\ncode_112 | [KMeans Background Change](python/code_112)   | \u2714\ufe0f\ncode_113 | [KMeans Extract Image Color Card](python/code_113)   | \u2714\ufe0f\ncode_114 | [KNN Classification](python/code_114)   | \u2714\ufe0f\ncode_115 | [KNN-Train Data Save and Load](python/code_115)   | \u2714\ufe0f\ncode_116 | [Decision Tree Algorithm](python/code_116)   | \u2714\ufe0f\ncode_117 | [Image Mean-shift Segmentation](python/code_117)   | \u2714\ufe0f\ncode_118 | [Grabcut-Image Segmentation](python/code_118)   | \u2714\ufe0f\ncode_119 | [Grabcut-Background Change](python/code_119)   | \u270f\ufe0f\ncode_120 | [Qrcode detect and decode](python/code_120)   | \u270f\ufe0f\ncode_121 | [DNN- Read the information of each layer of the model](python/code_121)   | \u2714\ufe0f\ncode_122 | [DNN- Realize image classification](python/code_122)   | \u2714\ufe0f\ncode_123 | [DNN- Model runs to set the target device and compute the background](python/code_123)   | \u2714\ufe0f\ncode_124 | [DNN- SSD Single Image Detection](python/code_124)   | \u2714\ufe0f\ncode_125 | [DNN- SSD Real-time Video Detection](python/code_125)   | \u2714\ufe0f\ncode_126 | [DNN- Face Detection based on Residual Network](python/code_126)   | \u2714\ufe0f\ncode_127 | [DNN- Video Face Detection based on Residual Network](python/code_127)   | \u2714\ufe0f\ncode_128 | [DNN- Call the Detection Model of Tensorflow](python/code_128)   | \u2714\ufe0f\ncode_129 | [DNN- Call the Openpose Implementation Attitude Assessment](python/code_129)   | \u2714\ufe0f\ncode_130 | [DNN- Call YOLO Object Detection Network](python/code_130)   | \u2714\ufe0f\ncode_131 | [DNN- YOLOv3-tiny Real-time Object Detection](python/code_131)   | \u2714\ufe0f\ncode_132 | [DNN- Single and Multiple Image Detection](python/code_132)   | \u2714\ufe0f\ncode_133 | [DNN- Colorful Image Colorization ](python/code_133)   | \u2714\ufe0f\ncode_134 | [DNN- ENet Image Segmentation](python/code_134)   | \u2714\ufe0f\ncode_135 | [DNN- Real-time Fast Image Style Transfer](python/code_135)   | \u2714\ufe0f\n\n---\n\n### Appendix\n\n\u26f3\ufe0f The weight can be download from Google Driver\uff1a\n\n\ud83c\udf31 [Weight for DNN](https://drive.google.com/drive/folders/1mg6VXpkvEmyL1scaelX5FnW8uw1gk9iq?usp=sharing)"
 },
 {
  "repo": "kylemcdonald/ofxCv",
  "language": "C++",
  "readme_contents": "# Introduction\n\nofxCv represents an alternative approach to wrapping OpenCV for openFrameworks.\n\n# Installation\n\nFirst, pick the branch that matches your version of openFrameworks:\n\n* OF [stable](https://github.com/openframeworks/openFrameworks/tree/stable) (0.9.8): use [ofxCv/stable](https://github.com/kylemcdonald/ofxCv/tree/stable)\n* OF [master](https://github.com/openframeworks/openFrameworks) (0.10.0): use [ofxCv/master](https://github.com/kylemcdonald/ofxCv/)\n\nEither clone out the source code using git:\n\n\t> cd openFrameworks/addons/\n\t> git clone https://github.com/kylemcdonald/ofxCv.git\n\nOr download the source from GitHub [here](https://github.com/kylemcdonald/ofxCv/archive/master.zip), unzip the folder, rename it from `ofxCv-master` to `ofxCv` and place it in your `openFrameworks/addons` folder.\n\nTo run the examples, import them into the project generator, create a new project, and open the project file in your IDE.\n\n# Goals\n\nofxCv has a few goals driving its development.\n\n### Wrap complex things in a helpful way\n\nSometimes this means: providing wrapper functions that require fewer arguments than the real CV functions, providing a smart interface that handles dynamic memory allocation to make things faster for you, or providing in place and out of place alternatives.\n\n### Present the power of OpenCv clearly\n\nThis means naming things in an intuitive way, and, more importantly, providing classes that have methods that transform the data represented by that class. It also means providing demos of CV functions, and generally being more useful than ofxOpenCv.\n\n### Interoperability of openFrameworks and OpenCv\n\nMaking it easy to work directly with CV by providing lightweight conversion functions, and providing wrappers for CV functions that do the conversions for you.\n\n### Elegant internal OpenCv code\n\nProvide clean implementations of all functions in order to provide a stepping stone to direct OpenCV use. This means using function names and variable names that follow the OpenCV documentation, and spending the time to learn proper CV usage so I can explain it clearly to others through code. Sometimes there will be heavy templating in order to make OF interoperable with OpenCV, but this should be avoided in favor of using straight OpenCV as often as possible.\n\n# Usage\n\nSometimes this readme will fall out of date. Please refer to the examples as the primary reference in that case.\n\n## Project setup\n\nUsing ofxCv requires:\n\n* ofxCv/libs/ofxCv/include/ Which contains all the ofxCv headers.\n* ofxCv/libs/ofxCv/src/ Which contains all the ofxCv source.\n* ofxCv/src/ Which ties together all of ofxCv into a single include.\n* opencv/include/ The OpenCv headers, located in addons/ofxOpenCv/\n* opencv/lib/ The precompiled static OpenCv libraries, located in addons/ofxOpenCv/\n\nYour linker will also need to know where the OpenCv headers are. In XCode this means modifying one line in Project.xconfig:\n\n\tHEADER_SEARCH_PATHS = $(OF_CORE_HEADERS) \"../../../addons/ofxOpenCv/libs/opencv/include/\" \"../../../addons/ofxCv/libs/ofxCv/include/\"\n\nAlternatively, I recommend using [OFXCodeMenu](https://github.com/openframeworks/OFXcodeMenu) to add ofxCv to your project.\n\n## Including ofxCv\n\nInside your ofApp.h you will need one include:\n\n\t#include \"ofxCv.h\"\n\nOpenCv uses the `cv` namespace, and ofxCv uses the `ofxCv` namespace. You can automatically import them by writing this in your `.cpp` files:\n\n\tusing namespace cv;\n\tusing namespace ofxCv;\n\nIf you look inside the ofxCv source, you'll find lots of cases of `ofxCv::` and `cv::`. In some rare cases, you'll need to write `cv::` in your code. For example, on OSX `Rect` and `Point` are defined by OpenCv, but also `MacTypes.h`. So if you're using an OpenCv `Rect` or `Point` you'll need to say so explicitly with `cv::Rect` or `cv::Point` to disambiguate.\n\nofxCv takes advantage of namespaces by using overloaded function names. This means that the ofxCv wrapper for `cv::Canny()` is also called `ofxCv::Canny()`. If you write simply `Canny()`, the correct function will be chosen based on the arguments you pass.\n\n## Working with ofxCv\n\nUnlike ofxOpenCv, ofxCv encourages you to use either native openFrameworks types or native OpenCv types, rather than introducing a third type like `ofxCvImage`. To work with OF and OpenCv types in a fluid way, ofxCv includes the `toCv()` and `toOf()` functions. They provide the ability to convert openFrameworks data to OpenCv data and vice versa. For large data, like images, this is done by wrapping the data rather than copying it. For small data, like vectors, this is done by copying the data.\n\nThe rest of ofxCv is mostly helper functions (for example, `threshold()`) and wrapper classes (for example, `Calibration`).\n\n### toCv() and copy()\n\n`toCv()` is used to convert openFrameworks data to OpenCv data. For example:\n\n\tofImage img;\n\timg.load(\"image.png\");\n\tMat imgMat = toCv(img);\n\nThis creates a wrapper for `img` called `imgMat`. To create a deep copy, use `clone()`:\n\n\tMat imgMatClone = toCv(img).clone();\n\nOr `copy()`, which works with any type supported by `toCv()`:\n\n\tMat imgCopy;\n\tcopy(img, imgCopy);\n\n`toCv()` is similar to ofxOpenCv's `ofxCvImage::getCvImage()` method, which returns an `IplImage*`. The biggest difference is that you can't always use `toCv()` \"in place\" when calling OpenCv code directly. In other words, you can always write this:\n\n\tMat imgMat = toCv(img);\n\tcv::someFunction(imgMat, ...);\n\nBut you should avoid using `toCv()` like this:\n\n\tcv::someFunction(toCv(img), ...);\n\nBecause there are cases where in place usage will cause a compile error. More specifically, calling `toCv()` in place will fail if the function requires a non-const reference for that parameter.\n\n### imitate()\n\n`imitate()` is primarily used internally by ofxCv. When doing CV, you regularly want to allocate multiple buffers of similar dimensions and channels. `imitate()` follows a kind of prototype pattern, where you pass a prototype image `original` and the image to be allocated `mirror` to `imitate(mirror, original)`. `imitate()` has two big advantages:\n\n* It works with `Mat`, `ofImage`, `ofPixels`, `ofVideoGrabber`, and anything else that extends `ofBaseHasPixels`.\n* It will only reallocate memory if necessary. This means it can be used liberally.\n\nIf you are writing a function that returns data, the ofxCv style is to call `imitate()` on the data to be returned from inside the function, allocating it as necessary.\n\n### drawMat() vs. toOf()\n\nSometimes you want to draw a `Mat` to the screen directly, as quickly and easily as possible, and `drawMat()` will do this for you. `drawMat()` is not the most optimal way of drawing images to the screen, because it creates a texture every time it draws. If you want to draw things efficiently, you should allocate a texture using `ofImage img;` *once* and draw it using `img.draw()`.\n\n1. Either use `Mat mat = toCv(img);` to treat the `ofImage` as a `Mat`, modify the `mat`, then `img.update()` to upload the modified pixels to the GPU.\n2. Alternatively; call `toOf(mat, img)` each time after modifying the `Mat`. This will only reallocate the texture if necessary, e.g. when the size has changed.\n\n\n# Working with OpenCv 2\n\nOpenCv 2 is an incredibly well designed API, and ofxCv encourages you to use it directly. Here are some hints on using OpenCv.\n\n### OpenCv Types\n\nOpenCv 2 uses the `Mat` class in place of the old `IplImage`. Memory allocation, copying, and deallocation are all handled automatically. `operator=` is a shallow, reference-counted copy. A `Mat` contains a collection of `Scalar` objects. A `Scalar` contains a collection of basic types (unsigned char, bool, double, etc.). `Scalar` is a short vector for representing color or other multidimensional information. The hierarchy is: `Mat` contains `Scalar`, `Scalar` contains basic types.\n\nDifferent functions accept `Mat` in different ways:\n\n* `Mat` will create a lightweight copy of the underlying data. It's easy to write, and it allows you to use `toCv()` \"in-place\" when passing arguments to the function.\n* `Mat&` allows the function to modify the header passed in. This means the function can allocate if necessary.\n* `const Mat&` means that the function isn't going to modify the underlying data. This should be used instead of `Mat` when possible. It also allows \"in-place\" `toCv()` usage.\n\n### Mat creation\n\nIf you're working with `Mat` directly, it's important to remember that OpenCv talks about `rows` and `cols` rather than `width` and `height`. This means that the arguments are \"backwards\" when they appear in the `Mat` constructor. Here's an example of creating a `Mat` wrapper for some grayscale `unsigned char* pixels` for which we know the `width` and `height`:\n\n\tMat mat = Mat(height, width, CV_8UC1, pixels, 0);\n\n### Mat operations\n\nBasic mathematical operations on `Mat` objects of the same size and type can be accomplished with matrix expressions. Matrix expressions are a collection of overloaded operators that accept `Mat`, `Scalar`, and basic types. A normal mathematical operation might look like:\n\n\tfloat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nA matrix operation looks similar:\n\n\tMat x, a, b;\n\t...\n\tx = (a + b) * 10;\n\nThis will add every element of `a` and `b`, then multiply the results by 10, and finally assign the result to `x`.\n\nAvailable matrix expressions include mathematical operators `+`, `-`, `/` (per element division), `*` (matrix multiplication), `.mul()` (per-element multiplication). As well as comparison operators `!=`, `==`, `<`, `>`, `>=`, `<=` (useful for thresholding). Binary operators `&`, `|`, `^`, `~`. And a few others like `abs()`, `min()`, and `max()`. For the complete listing see the OpenCv documention or `mat.hpp`.\n\n# Code Style\n\nofxCv tries to have a consistent code style. It's most similar to the K&R variant used for Java, and the indentation is primarily determined by XCode's auto-indent feature.\n\nMultiline comments are used for anything beyond two lines.\n\nCase statements have a `default:` fall-through with the last case.\n\nWhen two or three similar variables are initialized, commas are used instead of multiple lines. For example `Mat srcMat = toCv(src), dstMat = toCv(dst);`. This style was inherited from reading Jason Saragih's FaceTracker.\n\n- - --\n\n*ofxCv was developed with support from [Yamaguchi Center for Arts and Media](http://ycam.jp/).*\n"
 },
 {
  "repo": "DingProg/Makeup",
  "language": "Java",
  "readme_contents": "# \u9879\u76ee\u4ecb\u7ecd  \n\n\u672c\u9879\u76ee\u662f\u4e00\u4e2aAndroid Project\uff0c\u7528Canvas\u7ed9\u4eba\u8138\u5316\u5986(\u753b\u5986)\u7684APP\u6f14\u793a\u9879\u76ee  \n\n\u4e3b\u8981\u5185\u5bb9\u5305\u62ec\uff1a\n- \u5507\u5f69\uff0c\u7f8e\u77b3\uff0c\u7c89\u5e95\uff0c\u773c\u5f71\uff0c\u816e\u7ea2\uff0c\u773c\u7ebf\uff0c\u53cc\u773c\u76ae\uff0c\u7709\u6bdb\u7b49\uff0c\u80fd\u753b\u7684\u5986\uff0c\u90fd\u753b\u4e86\n- \u5229\u7528\u56fe\u5f62\u5c40\u90e8\u53d8\u5f62\u7b97\u6cd5\u8fdb\u884c \u5927\u773c\uff0c\u7626\u8138\uff0c\u4e30\u80f8\uff0c\u5927\u957f\u817f\u7b49\n- \u78e8\u5e73/\u7f8e\u767d\n\n# \u90e8\u5206\u6548\u679c\u5c55\u793a\n\u7f8e\u5986  \n![](https://github.com/DingProg/Makeup/blob/master/doc/3.png)\n![](https://github.com/DingProg/Makeup/blob/master/doc/5.png)      \n\u5927\u773c  \n![](https://github.com/DingProg/Makeup/blob/master/doc/1.png)  \n\u7626\u8138  \n![](https://github.com/DingProg/Makeup/blob/master/doc/2.png)  \n\u5927\u957f\u817f  \n![](https://github.com/DingProg/Makeup/blob/master/doc/4.png)   \n\n\n![](https://github.com/DingProg/Makeup/blob/master/doc/smallface.gif)\n\n\u66f4\u591a\u6f14\u793a\u6548\u679c\u8bf7\u76f4\u63a5\u67e5\u770b\u4e0b\u65b9\u539f\u7406\u6587\u7ae0\uff0c\u6216\u8005\u76f4\u63a5\u4e0b\u8f7d [\u6f14\u793aAPP Release V1.0.0\u7248\u672c](https://github.com/DingProg/Makeup/releases)   \n\n\u5982\u679c\u4f60\u8981\u770bOpenCV\u76f8\u5173\u7684(\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d)\uff0c\u53ef\u4ee5\u5207\u6362\u5230\u5206\u652f[with-photo-changecolor](https://github.com/DingProg/Makeup/tree/with-photo-changecolor)   \n\u76f8\u5173\u7684\u6f14\u793aAPP\u4e3a [\u5e26\u66ff\u6362\u8bc1\u4ef6\u7167\u80cc\u666f/\u6c61\u70b9\u4fee\u590d\u7248\u672c](https://github.com/DingProg/Makeup/releases)\n\n# \u6f14\u793aAPP \u4e3b\u8981\u5b9e\u73b0\u4e86\u7684\u90e8\u5206\u4e3a\n```java\npublic enum Region {\n\n    FOUNDATION(\"\u7c89\u5e95\"),\n    BLUSH(\"\u816e\u7ea2\"),\n    LIP(\"\u5507\u5f69\"),\n    BROW(\"\u7709\u6bdb\"),\n\n    EYE_LASH(\"\u776b\u6bdb\"),\n    EYE_CONTACT(\"\u7f8e\u77b3\"),\n    EYE_DOUBLE(\"\u53cc\u773c\u76ae\"),\n    EYE_LINE(\"\u773c\u7ebf\"),\n    EYE_SHADOW(\"\u773c\u5f71\");\n\n    private String name;\n    Region(String name) {\n        this.name = name;\n    }\n}\n\npublic enum BeautyType {\n\n    SMALLFACE(2,\"\u7626\u8138\"),\n    LONGLEG(3,\"\u5927\u957f\u817f\u589e\u9ad8\"),\n    EYE(4,\"\u773c\u775b\u653e\u5927\"),\n    BREST(5,\"\u4e30\u80f8\"),\n    WHITE(7,\"\u7f8e\u767d\"),\n    SMALLBODY(9,\"\u7626\u8138\u7626\u8eab\");\n\n    private int type;\n    private String name;\n\n    BeautyType(int type, String name) {\n        this.type = type;\n        this.name = name;\n    }\n}\n```\n\n# \u539f\u7406\n\n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986\uff08\u753b\u5986)](https://github.com/DingProg/Makeup/blob/master/doc/doc1.md)  \n[Android\uff1a\u8ba9\u4f60\u7684\u201c\u5973\u795e\u201d\u9006\u88ad\uff0c\u4ee3\u7801\u64b8\u5f69\u5986 2\uff08\u5927\u773c\uff0c\u7626\u8138\uff0c\u5927\u957f\u817f\uff09](https://github.com/DingProg/Makeup/blob/master/doc/doc2.md)\n\n# \u58f0\u660e  \n\u672c\u9879\u76ee\u662f\u6f14\u793a\u6027\u53ca\u5b66\u4e60\u6027\u9879\u76ee\uff0c\u9879\u76ee\u4e2d\u6240\u7528\u7d20\u6750\u5bf9\u4e8e\u76f4\u63a5\u62ff\u53bb\u5546\u7528\u6240\u9020\u6210\u7684\u4fb5\u6743\uff0c\u6982\u4e0d\u8d1f\u8d23."
 },
 {
  "repo": "nomacs/nomacs",
  "language": "C++",
  "readme_contents": "# nomacs - Image Lounge \ud83c\udf78\n\nnomacs is a free, open source image viewer, which supports multiple platforms. You can use it for viewing all common image formats including RAW and psd images. nomacs is licensed under the GNU General Public License v3 and available for Windows, Linux, FreeBSD, Mac, and OS/2.\n\n[![Build Status](https://travis-ci.org/nomacs/nomacs.svg?branch=master)](https://travis-ci.org/nomacs/nomacs)\n[![Build status](https://ci.appveyor.com/api/projects/status/0lw27jchw3ymaqd4?svg=true)](https://ci.appveyor.com/project/diemmarkus/nomacs)\n[![Downloads](https://img.shields.io/github/downloads/nomacs/nomacs/total.svg)](https://github.com/nomacs/nomacs/releases/latest)\n[![Crowdin](https://badges.crowdin.net/nomacs/localized.svg)](http://translate.nomacs.org/project/nomacs)\n\n## Build nomacs (Windows)\n\nWe assume you have an IDE (i.e. Visual Studio), python, git, and [Qt](https://www.qt.io/download-open-source) installed.  \n\nGet all dependencies:\n```bash\ngit submodule init\ngit submodule update\n```\nProject folders in ``3rd-party`` will not be empty anymore. Now call:\n```bash\npython scripts/make.py \"qtpath/bin\"\n```\n\nThis will build nomacs into `build/nomacs`. If you are using Visual Studio, you can then double-click `build/nomacs/nomacs.sln`. Right-click the nomacs project and choose `Set as StartUp Project`.\n\nBuild individual projects using:\n```bash\npython scripts/make.py \"qt/bin\" --project quazip,libraw --force\n```\n\n### Developer Build\nI like having a separate developer build (without submodules) that uses 3rd party libs already compiled. To do so you need to: \n```bash\ngit submodule update --init --remote scripts \n\n# python scripts/make.py \"C:\\Qt\\Qt-5.14.1-installer\\5.14.2\\msvc2017_64\\bin\" --lib-path C:\\coding\\nomacs\\nomacs\\3rd-party\\build\npython scripts/make.py \"qt/bin\" --lib-path \"nomacs/3rd-party/build\"\n```\n\n### If anything did not work\n\n- check if you have setup opencv (otherwise uncheck ENABLE_OPENCV)\n- check if your Qt is set correctly (otherwise set the path to `qt_install_dir/qtbase/bin/qmake.exe`)\n- check if your builds proceeded correctly\n\n## Build nomacs (Ubuntu)\n\nGet the required packages:\n\n``` console\nsudo apt-get install debhelper cdbs qt5-qmake qttools5-dev-tools qt5-default qttools5-dev libqt5svg5-dev qt5-image-formats-plugins libexiv2-dev libraw-dev libopencv-dev cmake libtiff-dev libquazip5-dev libwebp-dev git build-essential lcov libzip-dev\n```\n\nGet the nomacs sources from github:\n``` console\ngit clone https://github.com/nomacs/nomacs.git\n```\n\nThis will by default place the source into ~/nomacs\nGo to the nomacs/ImageLounge directory and run `cmake` to get the Makefiles:\n``` console\nmkdir build\ncd build\ncmake ../ImageLounge/.\n```\n\nCompile nomacs:\n``` console\nmake\n```\n\nYou will now have a binary (~/nomacs/build/nomacs), which you can test (or use directly). To install it to /usr/local/bin, use:\n``` console\nsudo make install\n```\n\nnote that you have to execute\n``` console\nsudo ldconfig\n```\nafter a successful install.\n\nInstall the [heif plugin](https://github.com/jakar/qt-heif-image-plugin) for HEIF support.\n\n### For Package Maintainers\n\n- Set `ENABLE_TRANSLATIONS` to `true` (default: `false`)\n- Build all officially supported [plugins](https://github.com/nomacs/nomacs-plugins/)\n\n## Build nomacs (MacOS)\n\nInstall [Homebrew](http://brew.sh/) for easier installation of dependencies.\nInstall required dependencies:\n\n``` console\n$ brew install qt5 exiv2 opencv libraw quazip cmake pkg-config\n```\n\nGo to the `nomacs` directory and run cmake to get the Makefiles:\n\n``` console\n$ mkdir build\n$ cd build\n$ Qt5_DIR=/usr/local/opt/qt5/ cmake -DQT_QMAKE_EXECUTABLE=/usr/local/opt/qt5/bin ../ImageLounge/.\n```\n\nRun make:\n\n```console\n$ make\n```\n\nYou will now have a binary (`nomacs.app`), which you can test (or use directly). To install it to `/usr/local/bin`, use\n\n```console\n$ sudo make install\n```\n\n## Build in Docker\nWe have created a docker image that best simulates the travis system (currently it's ubuntu xenial 16.04). To build nomacs in a docker, you have to create the image:\n````bash\ndocker build --rm -f \"Dockerfile\" -t nomacs:latest empty-docker-dir\n`````\nTo deploy nomacs in a docker on your system, you can mount this directory using:\n````bash\ndocker run --rm -it -v C:\\\\coding\\\\nomacs:/usr/nomacs nomacs:latest\n````\nIf needed, you can upload the image:\n````bash\ndocker login\ndocker tag nomacs diemmarkus/nomacs\ndocker push diemmarkus/nomacs:latest\n````\n\n## Links\n\n- [nomacs.org](https://nomacs.org)\n- [GitHub](https://github.com/nomacs)\n\n[![nomacs-icon](https://nomacs.org/startpage/nomacs.svg)](https://nomacs.org)\n"
 },
 {
  "repo": "Cartucho/OpenLabeling",
  "language": "Python",
  "readme_contents": "# OpenLabeling: open-source image and video labeler\n\n[![GitHub stars](https://img.shields.io/github/stars/Cartucho/OpenLabeling.svg?style=social&label=Stars)](https://github.com/Cartucho/OpenLabeling)\n\nImage labeling in multiple annotation formats:\n- PASCAL VOC (= [darkflow](https://github.com/thtrieu/darkflow))\n- [YOLO darknet](https://github.com/pjreddie/darknet)\n- ask for more (create a new issue)...\n\n<img src=\"https://media.giphy.com/media/l49JDgDSygJN369vW/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc1csRs9PoDgCeuk/giphy.gif\" width=\"40%\">\n<img src=\"https://media.giphy.com/media/3o752fXKwTJJkhXP32/giphy.gif\" width=\"40%\"><img src=\"https://media.giphy.com/media/3ohc11t9auzSo6fwLS/giphy.gif\" width=\"40%\">\n\n## Citation\n\nThis project was developed for the following paper, please consider citing it:\n\n```bibtex\n@INPROCEEDINGS{8594067,\n  author={J. {Cartucho} and R. {Ventura} and M. {Veloso}},\n  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, \n  title={Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots}, \n  year={2018},\n  pages={2336-2341},\n}\n```\n\n## Latest Features\n\n- Jun 2019: Deep Learning Object Detection Model\n- May 2019: [ECCV2018] Distractor-aware Siamese Networks for Visual Object Tracking\n- Jan 2019: easy and quick bounding-boxe's resizing!\n- Jan 2019: video object tracking with OpenCV trackers!\n- TODO: Label photos via Google drive to allow \"team online labeling\".\n[New Features Discussion](https://github.com/Cartucho/OpenLabeling/issues/3)\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Prerequisites](#prerequisites)\n- [Run project](#run-project)\n- [GUI usage](#gui-usage)\n- [Authors](#authors)\n\n## Quick start\n\nTo start using the YOLO Bounding Box Tool you need to [download the latest release](https://github.com/Cartucho/OpenLabeling/archive/v1.3.zip) or clone the repo:\n\n```\ngit clone --recurse-submodules git@github.com:Cartucho/OpenLabeling.git\n```\n\n### Prerequisites\n\nYou need to install:\n\n- [Python](https://www.python.org/downloads/)\n- [OpenCV](https://opencv.org/) version >= 3.0\n    1. `python -mpip install -U pip`\n    1. `python -mpip install -U opencv-python`\n    1. `python -mpip install -U opencv-contrib-python`\n- numpy, tqdm and lxml:\n    1. `python -mpip install -U numpy`\n    1. `python -mpip install -U tqdm`\n    1. `python -mpip install -U lxml`\n\nAlternatively, you can install everything at once by simply running:\n\n```\npython -mpip install -U pip\npython -mpip install -U -r requirements.txt\n```\n- [PyTorch](https://pytorch.org/get-started/locally/) \n    Visit the link for a configurator for your setup.\n    \n### Run project\n\nStep by step:\n\n  1. Open the `main/` directory\n  2. Insert the input images and videos in the folder **input/**\n  3. Insert the classes in the file **class_list.txt** (one class name per line)\n  4. Run the code:\n  5. You can find the annotations in the folder **output/**\n\n         python main.py [-h] [-i] [-o] [-t] [--tracker TRACKER_TYPE] [-n N_FRAMES]\n\n         optional arguments:\n          -h, --help                Show this help message and exit\n          -i, --input               Path to images and videos input folder | Default: input/\n          -o, --output              Path to output folder (if using the PASCAL VOC format it's important to set this path correctly) | Default: output/\n          -t, --thickness           Bounding box and cross line thickness (int) | Default: -t 1\n          --tracker tracker_type    tracker_type being used: ['CSRT', 'KCF','MOSSE', 'MIL', 'BOOSTING', 'MEDIANFLOW', 'TLD', 'GOTURN', 'DASIAMRPN']\n          -n N_FRAMES               number of frames to track object for\n  To use DASIAMRPN Tracker:\n  1. Install the [DaSiamRPN](https://github.com/foolwood/DaSiamRPN) submodule and download the model (VOT) from [google drive](https://drive.google.com/drive/folders/1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H)\n  2. copy it into 'DaSiamRPN/code/'\n  3. set default tracker in main.py or run it with --tracker DASIAMRPN\n\n\n#### How to use the deep learning feature\n\n- Download one or some deep learning models from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n  and put it into `object_detection/models` directory (you need to create the `models` folder by yourself). The outline of `object_detection` looks like that:\n  + `tf_object_detection.py`\n  + `utils.py`\n  + `models/ssdlite_mobilenet_v2_coco_2018_05_09`\n\nDownload the pre-trained model by clicking this link http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz and put it into `object_detection/models`. Create the `models` folder if necessary. Make sure to extract the model.\n\n  **Note**: Default model used in `main_auto.py` is `ssdlite_mobilenet_v2_coco_2018_05_09`. We can\n  set `graph_model_path` in file `main_auto.py` to change the pretrain model\n- Using `main_auto.py` to automatically label data first\n\n  TODO: explain how the user can \n\n### GUI usage\n\nKeyboard, press: \n\n<img src=\"https://github.com/Cartucho/OpenLabeling/blob/master/keyboard_usage.jpg\">\n\n| Key | Description |\n| --- | --- |\n| a/d | previous/next image |\n| s/w | previous/next class |\n| e | edges |\n| h | help |\n| q | quit |\n\nVideo:\n\n| Key | Description |\n| --- | --- |\n| p | predict the next frames' labels |\n\nMouse:\n  - Use two separate left clicks to do each bounding box\n  - **Right-click** -> **quick delete**!\n  - Use the middle mouse to zoom in and out\n  - Use double click to select a bounding box\n\n## Authors\n\n* **Jo\u00e3o Cartucho**\n\n    Feel free to contribute\n\n    [![GitHub contributors](https://img.shields.io/github/contributors/Cartucho/OpenLabeling.svg)](https://github.com/Cartucho/OpenLabeling/graphs/contributors)\n"
 },
 {
  "repo": "opencv/opencv_for_ios_book_samples",
  "language": "Objective-C",
  "readme_contents": "OpenCV for iOS (samples for the book)\n=====================================\n\n - Authors: Alexander Shishkov and Kirill Kornyakov\n - Book: <http://bit.ly/OpenCV_for_iOS_book>\n - Copyright: Packt Publishing 2013\n - License: see the `LICENSE.txt` file\n\nBuild & run sample projects\n---------------------------\n\nAlmost every chapter of the book describes a separate project. There are 15\nXcode projects for 17 chapters in the book. Chapter 16 extends the project of\nChapter 14, and for Chapter 17 you need only OpenCV source code.\n\n- First of all you will need a computer with Mac OSX and Xcode. An iOS device\n  is also helpful, since not all samples can be executed on Simulator.\n- Install OpenCV v2.4.6 or newer. You can go to <http://opencv.org>, click on\n  _Downloads_, and download the latest OpenCV framework for iOS. Copy it to the\n  folder with this code.\n- Now you can import sample projects to Xcode and run them on Simulator or a\n  real device.\n\nFor detailed instructions and explanations please refer to the book.\n\nChapters\n--------\n\n  1. _Getting Started with iOS_ helps you to setup your development environment\n     and run your first \"Hello World\" iOS application.\n  2. _Displaying Image from Resources_ introduces you to basic GUI concepts on\n     iOS, and covers loading of an image from resources and displaying it on the\n     display.\n  3. _Linking OpenCV to iOS Project_ explains how to link OpenCV library and\n     call any function from it.\n  4. _Detecting Faces with Cascade Classifier_ shows how to detect faces using\n     OpenCV.\n  5. _Printing Postcard_ demonstrates how a simple photo effect can be\n     implemented.\n  6. _Working with Images in Gallery_ explains how to load and save images\n     from/to Gallery.\n  7. _Applying Retro Effect_ demonstrates another interesting photo effect,\n     which makes photos look old.\n  8. _Taking Photos From Camera_ shows how to capture static images with camera.\n  9. _Creating Static Library_ explains how to create a Static Library project\n     in Xcode.\n  10. _Capturing Video from Camera_ shows how to capture video stream from\n      camera.\n  11. _Control Advanced Camera Settings_ explains how to control advanced camera\n      settings, like exposure, focus and white balance.\n  12. _Applying Effects to Live Video_ shows how to process captured video\n      frames on the fly.\n  13. _Saving Video from Camera_ explains how to save video stream to the device\n      with hardware encoding.\n  14. _Optimizing Performance with ARM NEON_ explains how to use SIMD\n      instructions to vectorize you code and improve performance.\n  15. _Detecting Facial Features_ presents a simple facial feature detection\n      demo.\n  16. _Using Accelerate Framework_ explains how to link the framework, and how\n      to use it for performance optimization.\n  17. _Building OpenCV for iOS from sources_ explains where to get and how to\n      build the latest OpenCV sources.\n\nScreenshots\n-----------\n\n### Chapter 5. \"Printing Postcard\"\n![](./Chapter05_PrintingPostcard/screenshot.png)\n\n### Chapter 6. \"Working with Images in Gallery\"\n![](./Chapter06_WorkingWithGallery/screenshot.png)\n\n### Chapter 7. \"Applying Retro Effect\"\n![](./Chapter07_ApplyingRetroEffect/screenshot.png)\n\n### Chapter 12. \"Applying Effects to Live Video\"\n![](./Chapter12_ProcessingVideo/screenshot.png)\n\n### Chapter 15. \"Detecting Facial Features\"\n![](./Chapter15_DetectingFacialFeatures/screenshot.png)"
 },
 {
  "repo": "mbeyeler/opencv-machine-learning",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine Learning for OpenCV\n\n[![Google group](https://img.shields.io/badge/Google-Discussion%20group-lightgrey.svg)](https://groups.google.com/d/forum/machine-learning-for-opencv)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.833523.svg)](https://doi.org/10.5281/zenodo.833523)\n\nThis is the Jupyter notebook version of the following book:\n\n<img src=\"https://images-na.ssl-images-amazon.com/images/I/41CKBKW8y4L.jpg\" width=\"200\" align=\"left\" style=\"padding: 1px; border: 1px solid black; margin-right: 5px\"/> <br/>\nMichael Beyeler <br/>\n<a href=\"https://www.amazon.com/Machine-Learning-OpenCV-Michael-Beyeler/dp/1783980281\" target=\"_blank\"><b>Machine Learning for OpenCV</b></a> <br/>\nIntelligent Image Processing with Python\n<br/><br/>\n14 July 2017 <br/>\nPackt Publishing Ltd., London, England <br/>\nPaperback: 382 pages <br/>\nISBN 978-178398028-4\n<br clear=\"both\"/><br/>\n\nThe content is available on [GitHub](https://github.com/mbeyeler/opencv-machine-learning).\nThe code is released under the [MIT license](https://opensource.org/licenses/MIT).\n\nThe book is also available as a two-part video course:\n- [Part I: Supervised Learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-supervised-learning-video) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789347357/59952/59953/the-course-overview))\n- [Part II: Advanced methods and deep learning](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-opencv-%E2%80%93-advanced-methods-and-deep-learning-vide) ([Free sample](https://www.packtpub.com/mapt/video/big_data_and_business_intelligence/9781789340525/62127/62128/the-course-overview))\n\nFor questions, discussions, and more detailed help please refer to the [Google group](https://groups.google.com/d/forum/machine-learning-for-opencv).\n\nIf you use either book or code in a scholarly publication, please cite as:\n\n> M. Beyeler, (2017). Machine Learning for OpenCV. Packt Publishing Ltd., London, England, 380 pages, ISBN 978-178398028-4.\n\nOr use the following bibtex:\n\n```\n@book{MachineLearningOpenCV,\n\ttitle = {{Machine Learning for OpenCV}},\n\tsubtitle = {{Intelligent image processing with Python}},\n\tauthor = {Michael Beyeler},\n\tyear = {2017},\n\tpages = {380},\n\tpublisher = {Packt Publishing Ltd.},\n\tisbn = {978-178398028-4}\n}\n```\n\nScholarly work referencing this book:\n- S Lynch (2018). Image Processing with Python. *Dynamical Systems with Applications using Python*, Springer.\n- MQG Quiroz (2018). Inductive Machine Learning with Image Processing for Objects Detection of a Robotic Arm with Raspberry PI. *International Conference on Technology Trends*.\n- A Konate (2018). Un aper\u00e7u sur quelques m\u00e9thodes en apprentissage automatique supervis\u00e9. *HAL* 01946237.\n\n\n## Table of Contents\n\n[Preface](notebooks/00.00-Preface.ipynb)\n\n[Foreword by Ariel Rokem](notebooks/00.01-Foreword-by-Ariel-Rokem.ipynb)\n\n1. [A Taste of Machine Learning](notebooks/01.00-A-Taste-of-Machine-Learning.ipynb)\n\n2. [Working with Data in OpenCV](notebooks/02.00-Working-with-Data-in-OpenCV.ipynb)\n   - [Dealing with Data Using Python's NumPy Package](notebooks/02.01-Dealing-with-Data-Using-Python-NumPy.ipynb)\n   - [Loading External Datasets in Python](notebooks/02.02-Loading-External-Datasets-in-Python.ipynb)\n   - [Visualizing Data Using Matplotlib](notebooks/02.03-Visualizing-Data-Using-Matplotlib.ipynb)\n   - [Dealing with Data Using OpenCV's TrainData container](notebooks/02.05-Dealing-with-Data-Using-the-OpenCV-TrainData-Container-in-C%2B%2B.ipynb)\n\n3. [First Steps in Supervised Learning](notebooks/03.00-First-Steps-in-Supervised-Learning.ipynb)\n   - [Measuring Model Performance with Scoring Functions](notebooks/03.01-Measuring-Model-Performance-with-Scoring-Functions.ipynb)\n   - [Understanding the k-NN Algorithm](notebooks/03.02-Understanding-the-k-NN-Algorithm.ipynb)\n   - [Using Regression Models to Predict Continuous Outcomes](notebooks/03.03-Using-Regression-Models-to-Predict-Continuous-Outcomes.ipynb)\n   - [Applying Lasso and Ridge Regression](notebooks/03.04-Applying-Lasso-and-Ridge-Regression.ipynb)\n   - [Classifying Iris Species Using Logistic Regression](notebooks/03.05-Classifying-Iris-Species-Using-Logistic-Regression.ipynb)\n\n4. [Representing Data and Engineering Features](notebooks/04.00-Representing-Data-and-Engineering-Features.ipynb)\n   - [Preprocessing Data](notebooks/04.01-Preprocessing-Data.ipynb)\n   - [Reducing the Dimensionality of the Data](notebooks/04.02-Reducing-the-Dimensionality-of-the-Data.ipynb)\n   - [Representing Categorical Variables](notebooks/04.03-Representing-Categorical-Variables.ipynb)\n   - [Representing Text Features](notebooks/04.04-Represening-Text-Features.ipynb)\n   - [Representing Images](notebooks/04.05-Representing-Images.ipynb)\n\n5. [Using Decision Trees to Make a Medical Diagnosis](notebooks/05.00-Using-Decision-Trees-to-Make-a-Medical-Diagnosis.ipynb)\n   - [Building Our First Decision Tree](notebooks/05.01-Building-Our-First-Decision-Tree.ipynb)\n   - [Using Decision Trees to Diagnose Breast Cancer](notebooks/05.02-Using-Decision-Trees-to-Diagnose-Breast-Cancer.ipynb)\n   - [Using Decision Trees for Regression](notebooks/05.03-Using-Decision-Trees-for-Regression.ipynb)\n\n6. [Detecting Pedestrians with Support Vector Machines](notebooks/06.00-Detecting-Pedestrians-with-Support-Vector-Machines.ipynb)\n   - [Implementing Your First Support Vector Machine](notebooks/06.01-Implementing-Your-First-Support-Vector-Machine.ipynb)\n   - [Detecting Pedestrians in the Wild](notebooks/06.02-Detecting-Pedestrians-in-the-Wild.ipynb)\n   - [Additional SVM Exercises](notebooks/06.03-Additional-SVM-Exercises.ipynb)\n\n7. [Implementing a Spam Filter with Bayesian Learning](notebooks/07.00-Implementing-a-Spam-Filter-with-Bayesian-Learning.ipynb)\n   - [Implementing Our First Bayesian Classifier](notebooks/07.01-Implementing-Our-First-Bayesian-Classifier.ipynb)\n   - [Classifying E-Mails Using Naive Bayes](notebooks/07.02-Classifying-Emails-Using-Naive-Bayes.ipynb)\n\n8. [Discovering Hidden Structures with Unsupervised Learning](notebooks/08.00-Discovering-Hidden-Structures-with-Unsupervised-Learning.ipynb)\n   - [Understanding k-Means Clustering](notebooks/08.01-Understanding-k-Means-Clustering.ipynb)\n   - [Compressing Color Images Using k-Means](notebooks/08.02-Compressing-Color-Images-Using-k-Means.ipynb)\n   - [Classifying Handwritten Digits Using k-Means](notebooks/08.03-Classifying-Handwritten-Digits-Using-k-Means.ipynb)\n   - [Implementing Agglomerative Hierarchical Clustering](notebooks/08.04-Implementing-Agglomerative-Hierarchical-Clustering.ipynb)\n\n9. [Using Deep Learning to Classify Handwritten Digits](notebooks/09.00-Using-Deep-Learning-to-Classify-Handwritten-Digits.ipynb)\n   - [Understanding Perceptrons](notebooks/09.01-Understanding-Perceptrons.ipynb)\n   - [Implementing a Multi-Layer Perceptron in OpenCV](notebooks/09.02-Implementing-a-Multi-Layer-Perceptron-in-OpenCV.ipynb)\n   - [Getting Acquainted with Deep Learning](notebooks/09.03-Getting-Acquainted-with-Deep-Learning.ipynb)\n   - [Training an MLP in OpenCV to Classify Handwritten Digits](notebooks/09.04-Training-an-MLP-in-OpenCV-to-Classify-Handwritten-Digits.ipynb)\n   - [Training a Deep Neural Net to Classify Handwritten Digits Using Keras](notebooks/09.05-Training-a-Deep-Neural-Net-to-Classify-Handwritten-Digits-Using-Keras.ipynb)\n\n10. [Combining Different Algorithms Into an Ensemble](notebooks/10.00-Combining-Different-Algorithms-Into-an-Ensemble.ipynb)\n    - [Understanding Ensemble Methods](notebooks/10.01-Understanding-Ensemble-Methods.ipynb)\n    - [Combining Decision Trees Into a Random Forest](notebooks/10.02-Combining-Decision-Trees-Into-a-Random-Forest.ipynb)\n    - [Using Random Forests for Face Recognition](notebooks/10.03-Using-Random-Forests-for-Face-Recognition.ipynb)\n    - [Implementing AdaBoost](notebooks/10.04-Implementing-AdaBoost.ipynb)\n    - [Combining Different Models Into a Voting Classifier](notebooks/10.05-Combining-Different-Models-Into-a-Voting-Classifier.ipynb)\n\n11. [Selecting the Right Model with Hyper-Parameter Tuning](notebooks/11.00-Selecting-the-Right-Model-with-Hyper-Parameter-Tuning.ipynb)\n    - [Evaluating a Model](notebooks/11.01-Evaluating-a-Model.ipynb)\n    - [Understanding Cross-Validation, Bootstrapping, and McNemar's Test](notebooks/11.02-Understanding-Cross-Validation-Bootstrapping-and-McNemar's-Test.ipynb)\n    - [Tuning Hyperparameters with Grid Search](notebooks/11.03-Tuning-Hyperparameters-with-Grid-Search.ipynb)\n    - [Chaining Algorithms Together to Form a Pipeline](notebooks/11.04-Chaining-Algorithms-Together-to-Form-a-Pipeline.ipynb)\n\n12. [Wrapping Up](notebooks/12.00-Wrapping-Up.ipynb)\n\n\n\n## Running the Code\n\nThere are at least two ways you can run the code:\n- Using [Binder](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master) (no installation required).\n- Using Jupyter Notebook on your local machine.\n\nThe code in this book was tested with Python 3.5, although Python 3.6 and 2.7 should work as well. \n\n\n### Using Binder\n\n[Binder](http://www.mybinder.org) allows you to run Jupyter notebooks in an interactive Docker container.\nNo installation required!\n\nLaunch the project: [mbeyeler/opencv-machine-learning](https://mybinder.org/v2/gh/mbeyeler/opencv-machine-learning/master)\n\n\n\n### Using Jupyter Notebook\n\nYou basically want to follow the installation instructions in Chapter 1 of the book.\n\nIn short:\n\n1. Download and install [Python Anaconda](https://www.continuum.io/downloads).\n   On Unix, when asked if the Anaconda path should be added to your `PATH` variable, choose yes. Then either open a new terminal or run `$ source ~/.bashrc`.\n\n2. Fork and clone the GitHub repo:\n   - Click the\n     [`Fork`](https://github.com/mbeyeler/opencv-machine-learning#fork-destination-box)\n     button in the top-right corner of this page.\n   - Clone the repo, where `YourUsername` is your actual GitHub user name:\n\n   ```\n   $ git clone https://github.com/YourUsername/opencv-machine-learning\n   $ cd opencv-machine-learning\n   ```\n   \n   - Add the following to your remotes:\n   ```\n   $ git remote add upstream https://github.com/mbeyeler/opencv-machine-learning\n   ```\n   \n3. Add Conda-Forge to your trusted channels (to simplify installation of OpenCV on Windows platforms):\n\n   ```\n   $ conda config --add channels conda-forge\n   ```\n\n4. Create a conda environment for Python 3 with all required packages:\n\n   ```\n   $ conda create -n Python3 python=3.6 --file requirements.txt\n   ```\n\n5. Activate the conda environment.\n   On Linux / Mac OS X:\n\n   ```\n   $ source activate Python3\n   ```\n\n   On Windows:\n\n   ```\n   $ activate Python3\n   ```\n\n   You can learn more about conda environments in the\n   [Managing Environments](http://conda.pydata.org/docs/using/envs.html)\n   section of the conda documentation.\n\n6. Launch Jupyter notebook:\n\n   ```\n   $ jupyter notebook\n   ```\n\n   This will open up a browser window in your current directory.\n   Navigate to the folder `opencv-machine-learning`.\n   The README file has a table of contents.\n   Else navigate to the `notebooks` folder, click on the notebook of your choice,\n   and select `Kernel > Restart & Run All` from the top menu.\n   \n   \n## Getting the latest code\n\nIf you followed the instructions above and:\n- forked the repo,\n- cloned the repo,\n- added the `upstream` remote repository,\n\nthen you can always grab the latest changes by running a git pull:\n\n```\n$ cd opencv-machine-learning\n$ git pull upstream master\n```\n\n## Errata\n\nThe following errata have been reported that apply to the print version of the book. Some of these are typos, others are bugs in the code. Please note that all known bugs have been fixed in the code of this repository.\n- p.32: `Out[15]` should read '3' instead of 'int_arr[3]'.\n- p.32: `Out[22]` should read `array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])` instead of `array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])`.\n- p.33: In the sentence: \"Here, the first dimension defines the color channel...\", the order of color channels should read \"blue, green, and red in OpenCV\" instead of \"red, green, blue, green, and red\".\n- p.36: The range of x values should read \"0 <= x <= 10\" instead of \"0 <= x < 10\", since `np.linspace` by default includes the endpoint.\n- p.51: `In [15]` shoud read `precision = true_positive / (true_positive + false_positive)` instead of `precision = true_positive / (true_positive + true_negative)`.\n- p.51: `Out[15]` should read 0.2 instead of 1.0.\n- p.72: `In [6]` should read `ridgereg = linear_model.Ridge()` instead of `ridgereg = linear_model.RidgeRegression()`.\n- p.85: The first line of `In [8]` should read `min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-10,10))` instead of `min_max_scaler = preprocessing.MinMaxScaler(feature_range (-10,10))`.\n- p.91:  The last paragraph should read `We also specify an empty array, np.array([]), for the mean argument, which tells OpenCV to  compute the mean from the data:` instead of `We also specify an empty array, np.array([]), for the mask argument, which tells OpenCV to use all data points in the feature matrix:`.\n- p.112: `In [3]` should read `vec.get_feature_names()[:5]` instead of `function:vec.get_feature_names()[:5]`.\n- p.120: `In [16]` should read `dtree = cv2.ml.DTrees_create()` instead of `dtree = cv2.ml.dtree_create()`.\n- p.122: `In [26]` should read `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(dtc, out_file=f, feature_names=vec.get_feature_names(), class_names=['A', 'B', 'C', 'D'])` instead of `with open(\"tree.dot\", 'w'): f = tree.export_graphviz(clf, out_file=f)`. Also, the second line should be indented.\n- p.147: The first occurrences of `X_hypo = np.c_[xx.ravel().astype(np.float32), yy.ravel().astype(np.float32)]` and `_, zz = svm.predict(X_hypo)` should be removed, as they mistakenly appear twice.\n- p.193: `In [28]` is missing `from sklearn import metrics`.\n- p.197: The sentence right below `In [3]` should read \"Then we can pass the preceding data matrix (`X`) to `cv2.kmeans`\", not `cv2.means`.\n- p.201: Indentation in bullet points 2-4 are wrong. Please refer to the Jupyter notebook for the correct indentation.\n- p.228: The last sentence in the middle paragraph should read \"[...] thus hopefully classifying the sample as y_{hat}=+1\" instead of \"[...] thus hopefully classifying the sample as y_{hat}=-1\".\n- p.230: `In [2]` has wrong indentation: `class Perceptron(object)` correctly has indentation level 1, but `def __init__` should have indentation level 2, and the two commands `self.lr = lr; self.n_iter = n_iter` should have indentation level 3.\n- p.260: `In [5]` should read `from keras.models import Sequential` instead of `from keras.model import Sequential`.\n- p.260: `In [6]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1]), padding='valid', input_shape=input_shape))` instead of `model.add(Convolution2D(n_filters, kernel_size[0], kernel_size[1], border_mode='valid', input_shape=input_shape))`.\n- p.260: `In [8]` should read `model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1])))` instead of `model.add(Convolution2D(n_filters, (kernel_size[0], kernel_size[1])))`.\n- p.261: `In [12]` should read `model.fit(X_train, Y_train, batch_size=128, epochs=12, verbose=1, validation_data=(X_test, Y_test))` instead of `model.fit(X_train, Y_train, batch_size=128, nb_epoch=12, verbose=1, validation_data=(X_test, Y_test))`.\n- p.275, in bullet point 2 it should say `ret = classifier.predict(X_hypo)` instead of `zz = classifier.predict(X_hypo); zz = zz.reshape(xx.shape)`.\n- p.285: `plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')` should be indented so that it is aligned with the previous line.\n- p.288: `In [14]` should read `_, y_hat = rtree.predict(X_test)` instead of `_, y_hat = tree.predict(X_test)`.\n- p.305: The first paragraph should read \"...and the remaining folds (1, 2, and 4) for training\" instead of \"...and the remaining folds (1, 2, and 4) for testing\".\n- p.306: `In [2]` should read `from sklearn.model_selection import train_test_split` instead of `from sklearn.model_selection import model_selection`.\n- p.310: `In [18]` should read `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_train, cv2.ml.ROW_SAMPLE, y_boot)`.\n- p.311: `In [20]` should have a line `model.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)` instead of `knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)`, as well as `_, y_hat = model.predict(X_oob)` instead of `_, y_hat = knn.predict(X_oob)`.\n- p.328: `In [5]` is missing the statement `from sklearn.preprocessing import MinMaxScaler`.\n- p.328: `In [5]` should have a line `pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])` instead of `pipe = Pipeline([\"scaler\", MinMaxScaler(), (\"svm\", SVC())])`.\n\n\n## Acknowledgment\n\nThis book was inspired in many ways by the following authors and their corresponding publications:\n- Jake VanderPlas, Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly, ISBN 978-149191205-8, 2016, https://github.com/jakevdp/PythonDataScienceHandbook\n- Andreas Muller and Sarah Guido, Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly, ISBN\n978-144936941-5, 2016, https://github.com/amueller/introduction_to_ml_with_python\n- Sebastian Raschka, Python Machine Learning. Packt, ISBN 978-178355513-0, 2015, https://github.com/rasbt/python-machine-learning-book\n\nThese books all come with their own open-source code - check them out when you get a chance!\n"
 },
 {
  "repo": "yinguobing/head-pose-estimation",
  "language": "Python",
  "readme_contents": "# Head pose estimation\n\nThis repo shows how to estimate human head pose from videos using TensorFlow and OpenCV.\n\n![demo](doc/demo.gif)\n![demo](doc/demo1.gif)\n\n## Getting Started\n\n:information_source:  *Checkout branch `tf2` if you are using TensorFlow 2*\n\nThe following packages are required:\n\n- TensorFlow 1.14.\n- OpenCV 3.3 or higher.\n- Python 3.5\n\nThe code is tested on Ubuntu 16.04.\n\n## Installing\n\nThis repository comes with a pre-trained model for facial landmark detection. Just git clone then you are good to go.\n\n```bash\n# From your favorite development directory:\ngit clone https://github.com/yinguobing/head-pose-estimation.git\n```\n\n## Running\n\nA video file or a webcam index should be assigned through arguments. If no source provided, the default webcam will be used.\n\n### For video file\n\nFor any video format that OpenCV supported (`mp4`, `avi` etc.):\n\n```bash\npython3 estimate_head_pose.py --video /path/to/video.mp4\n```\n\n### For webcam\n\nThe webcam index should be assigned:\n\n```bash\npython3 estimate_head_pose.py --cam 0\n``` \n\n## How it works\n\nThere are three major steps:\n\n1. Face detection. A face detector is adopted to provide a face box containing a human face. Then the face box is expanded and transformed to a square to suit the needs of later steps.\n\n2. Facial landmark detection. A custom trained facial landmark detector based on TensorFlow is responsible for output 68 facial landmarks.\n\n3. Pose estimation. Once we got the 68 facial landmarks, a mutual PnP algorithms is adopted to calculate the pose.\n\nThe marks is detected frame by frame, which result in small variance between adjacent frames. This makes the pose unstable. A Kalman filter is used to solve this problem, you can draw the original pose to observe the difference.\n\n## Retrain the model\n\nTo reproduce the facial landmark detection model, you can refer to this [series](https://yinguobing.com/deeplearning/) of posts(in Chinese only). And the training code is also open sourced: https://github.com/yinguobing/cnn-facial-landmark\n\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Authors\nYin Guobing (\u5c39\u56fd\u51b0) - [yinguobing](https://yinguobing.com)\n\n![](doc/wechat_logo.png)\n\n## Acknowledgments\nThe pre-trained TensorFlow model file is trained with various public data sets which have their own licenses. Please refer to them before using this code.\n\n- 300-W: https://ibug.doc.ic.ac.uk/resources/300-W/\n- 300-VW: https://ibug.doc.ic.ac.uk/resources/300-VW/\n- LFPW: https://neerajkumar.org/databases/lfpw/\n- HELEN: http://www.ifp.illinois.edu/~vuongle2/helen/\n- AFW: https://www.ics.uci.edu/~xzhu/face/\n- IBUG: https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/\n\nThe 3D model of face comes from OpenFace, you can find the original file [here](https://github.com/TadasBaltrusaitis/OpenFace/blob/master/lib/local/LandmarkDetector/model/pdms/In-the-wild_aligned_PDM_68.txt).\n\nThe build in face detector comes from OpenCV. \nhttps://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n"
 },
 {
  "repo": "bytefish/opencv",
  "language": "C++",
  "readme_contents": "# bytefish/opencv #\n\nThis repository contains OpenCV code and documents.\n\nMore (maybe) here: [https://www.bytefish.de](https://www.bytefish.de).\n\n## colormaps ##\n\nAn implementation of various colormaps for OpenCV2 C++ in order to enhance visualizations. Feel free to fork and add your own colormaps.\n\n### Related posts ###\n\n* https://bytefish.de/blog/colormaps_in_opencv\n  \n## misc ##\n\nSample code that doesn't belong to a specific project. \n\n* Skin Color detection\n* PCA\n* TanTriggs Preprocessing\n\n## machinelearning ##\n\nDocument and sourcecode about OpenCV C++ machine learning API including:\n\n* Support Vector Machines\n* Multi Layer Perceptron\n* Normal Bayes\n* k-Nearest-Neighbor\n* Decision Tree\n\n### Related posts ###\n  \n* https://www.bytefish.de/blog/machine_learning_opencv\n\n## eigenfaces ##\n\nEigenfaces implementation using the OpenCV2 C++ API. There's a very basic function for loading the dataset, you probably want to make this a bit more sophisticated. The dataset is available at [http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html).\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/pca_in_opencv\n* https://www.bytefish.de/blog/eigenfaces\n* https://www.bytefish.de/blog/fisherfaces\n  \n## lbp ##\n\nImplements various Local Binary Patterns with the OpenCV2 C++ API:\n  \n* Original LBP\n* Circular LBP (also known as Extended LBP)\n* Variance-based LBP\n\nBasic code for spatial histograms and histogram matching with a chi-square distance is included, but it's not finished right now. There's a tiny demo application you can experiment with.\n\n### Related posts ###\n\n* https://www.bytefish.de/blog/local_binary_patterns\n* https://www.bytefish.de/blog/numpy_performance/\n  \n## lda ##\n\nFisherfaces implementation with the OpenCV2 C++ API. \n\n### Related posts ###\n\n* https://www.bytefish.de/blog/fisherfaces\n* https://www.bytefish.de/blog/lda_in_opencv\n* https://www.bytefish.de/blog/fisherfaces_in_opencv\n"
 },
 {
  "repo": "nladuo/captcha-break",
  "language": "C++",
  "readme_contents": "# captcha-break\ncaptcha break based on opencv2, tesseract-ocr and some machine learning algorithm.\n\n## Types\n### Basic[[cpp](./basic/cpp)][[python](./basic/python)]\n![](./basic/basic.jpg)  \nThe simplest captcha breaking.\n\n### CSDN[[cpp](./csdn/cpp)][[python](./csdn/python)]\n![](./csdn/csdn.png)  \nCAPTCHA from http://download.csdn.net/\n\n### SubMail[[cpp](./submail/cpp)]\n![](./submail/submail.png)   \nCAPTCHA from http://submail.cn/sms\n\n### Weibo.cn[[cpp](./weibo.cn/cpp)][[python](./weibo.cn/python)]\n![](./weibo.cn/weibo.cn.png)  \nCAPTCHA from http://login.weibo.cn/login/.  \n(Note: This website has changed now, and the captcha is not available!)\n\n### JiKeXueYuan[[python](./jikexueyuan/python)]\n![](./jikexueyuan/jikexueyuan.png)   \nCAPTCHA of http://passport.jikexueyuan.com/sso/verify\n\n### Weibo.com[[python3](./weibo.com)]\n![](./weibo.com/weibo.com.png)  \nCAPTCHA of [http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0](http://login.sina.com.cn/cgi/pin.php?r=8787878&s=0)\n\n\n## License\nMIT\n"
 },
 {
  "repo": "Mjrovai/OpenCV-Face-Recognition",
  "language": "Python",
  "readme_contents": "# OpenCV-Face-Recognition\nReal-time face recognition project with OpenCV and Python\n<br><br>\nLinks for complete Tutorial:\n<br>\nhttps://www.hackster.io/mjrobot/real-time-face-recognition-an-end-to-end-project-a10826\nhttps://www.instructables.com/id/Real-time-Face-Recognition-an-End-to-end-Project/\n<br>\n<p><img src=\"https://github.com/Mjrovai/OpenCV-Face-Recognition/blob/master/FaceRecogBlock.png?raw=true\"></p>\n"
 },
 {
  "repo": "Dovyski/cvui",
  "language": "C++",
  "readme_contents": "cvui\n=====\nA (very) simple UI lib built on top of OpenCV drawing primitives. Other UI libs, such as [imgui](https://github.com/ocornut/imgui), require a graphical backend (e.g. OpenGL) to work, so if you want to use imgui in a OpenCV app, you must make it OpenGL enabled, for instance. It is not the case with cvui, which uses *only* OpenCV drawing primitives to do all the rendering (no OpenGL or Qt required).\n\n![image](https://raw.githubusercontent.com/Dovyski/depository/master/cvui.png?20180627)\n\nFeatures\n--------\n- Lightweight and simple to use user interface;\n- Header-only with no external dependencies (except OpenCV);\n- Based on OpenCV drawing primitives only (OpenGL or Qt are not required);\n- Friendly and C-like API (no classes/objects, etc);\n- Easily render components without worrying about their position (using rows/columns);\n- Simple (yet powerful) mouse API;\n- Modest number of UI components (11 in total);\n- Available in C++ and Python (pure implementation, no bindings).\n\nBuild\n-----\ncvui is a header-only lib that does not require a build. Just add `cvui.h` (or `cvui.py`) to your project and you are ready to go. The only dependency is OpenCV (version `2.x` or `3.x`), which you are probably using already.\n\nUsage\n-----\nCheck the [online documentation](https://dovyski.github.io/cvui) or the [examples](https://github.com/Dovyski/cvui/tree/master/example) folder to learn how to use cvui. The general usage in C++ and Python is shown below.\n\nUsage in C++:\n```cpp\n#include <opencv2/opencv.hpp>\n\n// One (and only one) of your C++ files must define CVUI_IMPLEMENTATION\n// before the inclusion of cvui.h to ensure its implementaiton is compiled.\n#define CVUI_IMPLEMENTATION\n#include \"cvui.h\"\n\n#define WINDOW_NAME \"CVUI Hello World!\"\n\nint main(int argc, const char *argv[])\n{\n\t// Create a frame where components will be rendered to.\n\tcv::Mat frame = cv::Mat(200, 500, CV_8UC3);\n\n\t// Init cvui and tell it to create a OpenCV window, i.e. cv::namedWindow(WINDOW_NAME).\n\tcvui::init(WINDOW_NAME);\n\n\twhile (true) {\n\t\t// Fill the frame with a nice color\n\t\tframe = cv::Scalar(49, 52, 49);\n\n\t\t// Render UI components to the frame\n\t\tcvui::text(frame, 110, 80, \"Hello, world!\");\n\t\tcvui::text(frame, 110, 120, \"cvui is awesome!\");\n\n\t\t// Update cvui stuff and show everything on the screen\n\t\tcvui::imshow(WINDOW_NAME, frame);\n\n\t\tif (cv::waitKey(20) == 27) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n```\n\nUsage in Python:\n```python\nimport numpy as np\nimport cv2\nimport cvui\n\nWINDOW_NAME = 'CVUI Hello World!'\n\n# Create a frame where components will be rendered to.\nframe = np.zeros((200, 500, 3), np.uint8)\n\n# Init cvui and tell it to create a OpenCV window, i.e. cv2.namedWindow(WINDOW_NAME).\ncvui.init(WINDOW_NAME)\n\nwhile True:\n\t# Fill the frame with a nice color\n\tframe[:] = (49, 52, 49)\n\n\t# Render UI components to the frame\n\tcvui.text(frame, 110, 80, 'Hello, world!')\n\tcvui.text(frame, 110, 120, 'cvui is awesome!')\n\n\t# Update cvui stuff and show everything on the screen\n\tcvui.imshow(WINDOW_NAME, frame)\n\n\tif cv2.waitKey(20) == 27:\n\t\tbreak\n```\n\nLicense\n-----\nCopyright (c) 2016 Fernando Bevilacqua. Licensed under the [MIT license](LICENSE.md).\n\nChange log\n-----\nSee all changes in the [CHANGELOG](CHANGELOG.md) file.\n"
 },
 {
  "repo": "antoinelame/GazeTracking",
  "language": "Python",
  "readme_contents": "# Gaze Tracking\n\n![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)\n![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![GitHub stars](https://img.shields.io/github/stars/antoinelame/GazeTracking.svg?style=social)](https://github.com/antoinelame/GazeTracking/stargazers)\n\nThis is a Python (2 and 3) library that provides a **webcam-based eye tracking system**. It gives you the exact position of the pupils and the gaze direction, in real time.\n\n[![Demo](https://i.imgur.com/WNqgQkO.gif)](https://youtu.be/YEZMk1P0-yw)\n\n_\ud83d\ude80 Quick note: I'm looking for job opportunities as a software developer, for exciting projects in ambitious companies. Anywhere in the world. Send me an email!_\n\n## Installation\n\nClone this project:\n\n```\ngit clone https://github.com/antoinelame/GazeTracking.git\n```\n\nInstall these dependencies (NumPy, OpenCV, Dlib):\n\n```\npip install -r requirements.txt\n```\n\n> The Dlib library has four primary prerequisites: Boost, Boost.Python, CMake and X11/XQuartx. If you doesn't have them, you can [read this article](https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/) to know how to easily install them.\n\nRun the demo:\n\n```\npython example.py\n```\n\n## Simple Demo\n\n```python\nimport cv2\nfrom gaze_tracking import GazeTracking\n\ngaze = GazeTracking()\nwebcam = cv2.VideoCapture(0)\n\nwhile True:\n    _, frame = webcam.read()\n    gaze.refresh(frame)\n\n    new_frame = gaze.annotated_frame()\n    text = \"\"\n\n    if gaze.is_right():\n        text = \"Looking right\"\n    elif gaze.is_left():\n        text = \"Looking left\"\n    elif gaze.is_center():\n        text = \"Looking center\"\n\n    cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n    cv2.imshow(\"Demo\", new_frame)\n\n    if cv2.waitKey(1) == 27:\n        break\n```\n\n## Documentation\n\nIn the following examples, `gaze` refers to an instance of the `GazeTracking` class.\n\n### Refresh the frame\n\n```python\ngaze.refresh(frame)\n```\n\nPass the frame to analyze (numpy.ndarray). If you want to work with a video stream, you need to put this instruction in a loop, like the example above.\n\n### Position of the left pupil\n\n```python\ngaze.pupil_left_coords()\n```\n\nReturns the coordinates (x,y) of the left pupil.\n\n### Position of the right pupil\n\n```python\ngaze.pupil_right_coords()\n```\n\nReturns the coordinates (x,y) of the right pupil.\n\n### Looking to the left\n\n```python\ngaze.is_left()\n```\n\nReturns `True` if the user is looking to the left.\n\n### Looking to the right\n\n```python\ngaze.is_right()\n```\n\nReturns `True` if the user is looking to the right.\n\n### Looking at the center\n\n```python\ngaze.is_center()\n```\n\nReturns `True` if the user is looking at the center.\n\n### Horizontal direction of the gaze\n\n```python\nratio = gaze.horizontal_ratio()\n```\n\nReturns a number between 0.0 and 1.0 that indicates the horizontal direction of the gaze. The extreme right is 0.0, the center is 0.5 and the extreme left is 1.0.\n\n### Vertical direction of the gaze\n\n```python\nratio = gaze.vertical_ratio()\n```\n\nReturns a number between 0.0 and 1.0 that indicates the vertical direction of the gaze. The extreme top is 0.0, the center is 0.5 and the extreme bottom is 1.0.\n\n### Blinking\n\n```python\ngaze.is_blinking()\n```\n\nReturns `True` if the user's eyes are closed.\n\n### Webcam frame\n\n```python\nframe = gaze.annotated_frame()\n```\n\nReturns the main frame with pupils highlighted.\n\n## You want to help?\n\nYour suggestions, bugs reports and pull requests are welcome and appreciated. You can also starring \u2b50\ufe0f the project!\n\nIf the detection of your pupils is not completely optimal, you can send me a video sample of you looking in different directions. I would use it to improve the algorithm.\n\n## Licensing\n\nThis project is released by Antoine Lam\u00e9 under the terms of the MIT Open Source License. View LICENSE for more information.\n"
 },
 {
  "repo": "ahmetozlu/vehicle_counting_tensorflow",
  "language": "Python",
  "readme_contents": "# VEHICLE DETECTION, TRACKING AND COUNTING\nThis sample project focuses on \"Vechicle Detection, Tracking and Counting\" using [**TensorFlow Object Counting API**](https://github.com/ahmetozlu/tensorflow_object_counting_api). ***Please contact if you need professional vehicle detection & tracking & counting project with the super high accuracy.***\n\n---\n\n***The [TensorFlow Object Counting API](https://github.com/ahmetozlu/tensorflow_object_counting_api) is used as a base for object counting on this project, more info can be found on this [repo](https://github.com/ahmetozlu/tensorflow_object_counting_api).***\n\n---\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/36344830-095cc4ec-1431-11e8-8e57-976c40d87cf9.gif\">\n</p>\n\n---\n\n***The developing is on progress! This sample project will be updated soon, the more talented traffic analyzer app will be available in this repo!***\n\n---\n\n## General Capabilities of This Sample Project\n\nThis sample project has more than just counting vehicles, here are the additional capabilities of it:\n\n- Detection and classification of the vehicles (car, truck, bicycle, motorcycle, bus)\n- Recognition of approximate vehicle color\n- Detection of vehicle direction of travel\n- Prediction the speed of the vehicle\n- Prediction of approximate vehicle size\n- **The images of detected vehicles are cropped from video frame and they are saved as new images under \"[detected_vehicles](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/detected_vehicles)\" folder path**\n- **The program gives a .csv file as an output ([traffic_measurement.csv](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/traffic_measurement.csv)) which includes \"Vehicle Type/Size\", \" Vehicle Color\", \" Vehicle Movement Direction\", \" Vehicle Speed (km/h)\" rows, after the end of the process for the source video file.**\n\nToDos:\n\n- More powerful detection models will be shared.\n- Sample codes will be developed to process different types of input videos (for different types of road traffics such as two way lane road).\n- Code cleanup will be performed.\n- UI will be developed. \n\nThe input video can be accessible by this [link](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/sub-1504614469486.mp4).\n\n## Theory\n\n### System Architecture\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/35445395-8dba4406-02c2-11e8-84bf-b480edbe9472.jpg\">\n</p>\n\n- Vehicle detection and classification have been developed using TensorFlow Object Detection API, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/blob/master/vehicle_detection_main.py) for more info.\n- Vehicle speed prediction has been developed using OpenCV via image pixel manipulation and calculation, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/utils/speed_and_direction_prediction_module) for more info.\n- Vehicle color prediction has been developed using OpenCV via K-Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features, [see](https://github.com/ahmetozlu/vehicle_counting_tensorflow/tree/master/utils/color_recognition_module) for more info.\n\n[TensorFlow\u2122](https://www.tensorflow.org/) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n[OpenCV (Open Source Computer Vision Library)](https://opencv.org/about.html) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n\n### Tracker\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/41812993-a4b5a172-7735-11e8-89f6-083ec0625f21.png\" | width=700>\n</p>\n\nSource video is read frame by frame with OpenCV. Each frames is processed by [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) is developed on TensorFlow. This is a loop that continue working till reaching end of the video. The main pipeline of the tracker is given at the above Figure.\n\n### Model\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/22610163/48481757-b1d5a900-e81f-11e8-824b-4317115fe5b4.png\">\n</p>\n\nBy default I use an [\"SSD with Mobilenet\" model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17) in this project. You can find more information about SSD in [here](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab). See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n\n*The minimum vehicle detection threshold can be set [in this line](https://github.com/ahmetozlu/tensorflow_object_counting_api/blob/master/utils/visualization_utils.py#L443) in terms of percentage. The default minimum vehicle detecion threshold is 0.5!*\n\n## Project Demo\n\nDemo video of the project is available on [My YouTube Channel](https://www.youtube.com/watch?v=PrqnhHf6fhM).\n\n## Installation\n\n**Docker setup with Nvidia GPU:** Run the demo in the GPU without installing anything, just nvidia-docker. The command to set up this docker:\n\n    docker-compose up\n    \nAlternative for nvidia-docker, you can follow the installation steps are given below!\n\n**1.) Python and pip**\n\nPython is automatically installed on Ubuntu. Take a moment to confirm (by issuing a python -V command) that one of the following Python versions is already installed on your system:\n\n- Python 3.3+\n\nThe pip or pip3 package manager is usually installed on Ubuntu. Take a moment to confirm (by issuing a *pip -V* or *pip3 -V* command) that pip or pip3 is installed. We strongly recommend version 8.1 or higher of pip or pip3. If Version 8.1 or later is not installed, issue the following command, which will either install or upgrade to the latest pip version:\n\n    $ sudo apt-get install python3-pip python3-dev # for Python 3.n\n    \n**2.) OpenCV**\n\nSee required commands to install OpenCV on Ubuntu in [here](https://gist.github.com/dynamicguy/3d1fce8dae65e765f7c4).\n\n**3.) TensorFlow**\n\nInstall TensorFlow by invoking one of the following commands:\n\n    $ pip3 install tensorflow     # Python 3.n; CPU support (no GPU support)\n    $ pip3 install tensorflow-gpu # Python 3.n; GPU support\n\n**4.) TensorFlow Object Detection API**\n\nSee required commands to install TensorFlow Object Detection API on Ubuntu in [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).\n  \nIf you are still getting problem about installation after completed the installation of the packet that are given above, please check that [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) out to get detailed info about installation.\n\n---\n- After completing these 4 installation steps that are given at above, you can test the project by this command:\n\n      python3 vehicle_detection_main.py\n---\n\n## Citation\nIf you use this code for your publications, please cite it as:\n\n    @ONLINE{vdtct,\n        author = \"Ahmet \u00d6zl\u00fc\",\n        title  = \"Vehicle Detection, Tracking and Counting by TensorFlow\",\n        year   = \"2018\",\n        url    = \"https://github.com/ahmetozlu/vehicle_counting_tensorflow\"\n    }\n\n## Author\nAhmet \u00d6zl\u00fc\n\n## License\nThis system is available under the MIT license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "kyamagu/mexopencv",
  "language": "Matlab",
  "readme_contents": "mexopencv\n=========\n[![Travis](https://img.shields.io/travis/kyamagu/mexopencv/master.svg)][1]\n[![AppVeyor](https://img.shields.io/appveyor/ci/kyamagu/mexopencv/master.svg)][2]\n[![License](https://img.shields.io/badge/license-BSD%203--Clause-blue.svg)](LICENSE)\n\nCollection and development kit of MATLAB MEX functions for OpenCV library.\n\nThe package provides MATLAB MEX functions that interface with hundreds of\nOpenCV APIs. Also the package contains a C++ class that converts between\nMATLAB's native data type and OpenCV data types. The package is suitable for\nfast prototyping of OpenCV application in MATLAB, use of OpenCV as an external\ntoolbox in MATLAB, and development of custom MEX functions.\n\nThe current version of mexopencv is compatible with OpenCV 3.4.1.\n\nFor previous OpenCV 3.x versions, checkout the corresponding tags:\n\n- [v3.4.0][24]\n- [v3.3.1][23]\n- [v3.3.0][22]\n- [v3.2.0][21]\n- [v3.1.0][20]\n- [v3.0.0][19]\n\nFor OpenCV 2.x, checkout these older branches:\n\n- [v2.4][18] (last tested with OpenCV v2.4.11)\n- [v2.3][17]\n- [v2.1][16]\n\nConsult the [wiki][3] for help.\n\nTable of Contents\n=================\n\n- [Structure](#structure)\n- [Build](#build)\n    - [Linux](#linux)\n    - [OS X](#os-x)\n    - [Windows](#windows)\n- [Usage](#usage)\n    - [Documentation](#documentation)\n    - [Unit Testing](#unit-testing)\n- [License](#license)\n\nStructure\n=========\n\nThe project tree is organized as follows:\n\n    +cv/             OpenCV or custom API directory\n    +mexopencv/      mexopencv utility API directory\n    doc/             directory for documentation\n    include/         header files\n    lib/             directory for compiled C++ library files\n    samples/         directory for sample application codes\n    src/             directory for C++ source files\n    src/+cv/         directory for MEX source files\n    src/+cv/private/ directory for private MEX source files\n    test/            directory for test scripts and resources\n    opencv_contrib/  directory for sources/samples/tests of additional modules\n    utils/           directory for utilities\n    Doxyfile         config file for doxygen\n    Makefile         make script\n    README.markdown  this file\n\nBuild\n=====\n\nPrerequisite\n\n- [MATLAB][4] or [Octave][5] (>= 4.0.0)\n- [OpenCV][6] (3.4.1)\n\nDepending on your platform, you also need the required build tools:\n\n- Linux: g++, make, pkg-config\n- OS X: Xcode Command Line Tools, pkg-config\n- Windows: Visual Studio\n\nRefer to the `Makefile` and `make.m` scripts for a complete list of\noptions accepted for building mexopencv across supported platforms.\n\nRefer to the [wiki][3] for detailed build instructions.\n\nOpenCV\n------\n\nCurrently, mexopencv targets the final **3.4.1** stable version of OpenCV. You\nmust build it against this exact version, rather than using the bleeding-edge\ndev-version of `opencv` and `opencv_contrib`. UNIX users should consider using\na package manager to install OpenCV if available.\n\n- [OpenCV][7]\n- [OpenCV contributed modules][8]\n\n**DO NOT use the \"master\" branch of `opencv` and `opencv_contrib`!**\n**Only the 3.4.1 release is supported by mexopencv.**\n\nLinux\n-----\n\nFirst make sure you have OpenCV 3.4.1 installed in the system:\n\n- if applicable, install OpenCV 3 package available in your package manager\n  (e.g., `libopencv-dev` in Debian/Ubuntu, `opencv-devel` in Fedora).\n  Note that these packages are not always up-to-date, so you might need to use\n  older mexopencv versions to match their [opencv package][9] version.\n- otherwise, you can always build and install OpenCV from [source][7]:\n\n        $ cd <opencv_build_dir>\n        $ cmake <options> <opencv_src_dir>\n        $ make\n        $ sudo make install\n\nAt this point, you should make sure that the [`pkg-config`][10] command can\nidentify and locate OpenCV libraries (if needed, set the `PKG_CONFIG_PATH`\nenvironment variable to help it find the `opencv.pc` file):\n\n    $ pkg-config --cflags --libs opencv\n\nIf you have all the prerequisites, go to the mexopencv directory and type:\n\n    $ make\n\nThis will build and place all MEX functions inside `+cv/`. Specify your MATLAB\ndirectory if you installed MATLAB to a non-default location:\n\n    $ make MATLABDIR=/opt/local/MATLAB/R2017a\n\nYou can also work with [Octave][5] instead of MATLAB by specifying:\n\n    $ make WITH_OCTAVE=true\n\nTo enable support for contributed modules, you must build OpenCV from both\n[`opencv`][7] and [`opencv_contrib`][8] sources. You can then compile\nmexopencv as:\n\n    $ make all contrib\n\nFinally you can test mexopencv functionality:\n\n    $ make test\n\nDeveloper documentation can be generated with Doxygen if installed:\n\n    $ make doc\n\nThis will create HTML files under `doc/`.\n\nRefer to the wiki for detailed instructions on how to compile OpenCV for both\n[MATLAB][14] and [Octave][15].\n\nOS X\n----\n\nCurrently, the recommended approach to install OpenCV in OS X is\n[Homebrew][11]. Install Homebrew first, and do the following to install\nOpenCV 3:\n\n    $ brew install pkg-config homebrew/science/opencv3\n    $ brew link opencv3\n\nOtherwise, you can build OpenCV from [source][7], similar to the Linux case.\n\nIf you have all the prerequisites, go to the mexopencv directory and run\n(modifying the options as needed):\n\n    $ make MATLABDIR=/Applications/MATLAB_R2016a.app PKG_CONFIG_MATLAB=opencv3 LDFLAGS=-L/usr/local/share/OpenCV/3rdparty/lib -j2\n\nReplace the path to MATLAB with your version. This will build and place all\nMEX functions inside `+cv/`.\n\nWindows\n-------\n\nRefer to [the wiki][13] for detailed instructions on how to compile OpenCV\non Windows, and build mexopencv against it.\n\nIn a nutshell, execute the following in MATLAB to compile mexopencv:\n\n    >> addpath('C:\\path\\to\\mexopencv')\n    >> mexopencv.make('opencv_path','C:\\OpenCV\\build')\n\nReplace the path above with the location where OpenCV binaries are installed\n(i.e location specified in `CMAKE_INSTALL_PREFIX` while building OpenCV).\n\nContrib modules are enabled as:\n\n    >> addpath('C:\\path\\to\\mexopencv')\n    >> addpath('C:\\path\\to\\mexopencv\\opencv_contrib')\n    >> mexopencv.make('opencv_path','C:\\OpenCV\\build', 'opencv_contrib',true)\n\nIf you have previously compiled mexopencv with a different configuration,\ndon't forget to clean old artifacts before building:\n\n    >> mexopencv.make('clean',true, 'opencv_contrib',true)\n\nUsage\n=====\n\nOnce MEX functions are compiled, you can add path to the project directory and\ncall MEX functions within MATLAB using package name `cv`.\n\n``` matlab\naddpath('/path/to/mexopencv');\naddpath('/path/to/mexopencv/opencv_contrib');\n\n% recommended\nout = cv.filter2D(img, kern);  % with package name 'cv'\n\n% not recommended\nimport cv.*;\nout = filter2D(img, kern);     % no need to specify 'cv' after imported\n```\n\nNote that some functions such as `cv.imread` will overload MATLAB's built-in\n`imread` function when imported. Use the scoped name when you need to avoid\nname collision. It is also possible to import individual functions. Check\n`help import` in MATLAB.\n\nCheck a list of functions available by `help` command in MATLAB.\n\n``` matlab\n>> help cv;              % shows list of functions in package 'cv'\n\n>> help cv.VideoCapture; % shows documentation of VideoCapture\n```\n\nLook at the `samples/` directory for examples.\n\nDocumentation\n-------------\n\nmexopencv includes a simple documentation utility that generates HTML help\nfiles for MATLAB. The following command creates HTML user documentation\nunder `doc/matlab/` directory.\n\n``` matlab\naddpath('/path/to/mexopencv/utils');\nMDoc;\n```\n\nOn-line documentation is [available][12].\n\nUnit Testing\n------------\n\nYou can test the functionality of compiled files by `UnitTest` class located\ninside `test` directory.\n\n``` matlab\naddpath('/path/to/mexopencv/test');\nUnitTest;\n```\n\nLook at the `test/unit_tests/` directory for all unit-tests.\n\nLicense\n=======\n\nThe code may be redistributed under the [BSD 3-Clause license](LICENSE).\n\n\n[1]: https://travis-ci.org/kyamagu/mexopencv\n[2]: https://ci.appveyor.com/project/kyamagu/mexopencv\n[3]: https://github.com/kyamagu/mexopencv/wiki\n[4]: https://www.mathworks.com/products/matlab.html\n[5]: https://www.gnu.org/software/octave/\n[6]: https://opencv.org/\n[7]: https://github.com/opencv/opencv/releases/tag/3.4.1\n[8]: https://github.com/opencv/opencv_contrib/releases/tag/3.4.1\n[9]: https://packages.ubuntu.com/bionic/libopencv-dev\n[10]: https://people.freedesktop.org/~dbn/pkg-config-guide.html\n[11]: https://brew.sh/\n[12]: http://kyamagu.github.io/mexopencv/matlab\n[13]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Windows%2C-MATLAB%2C-OpenCV-3%29\n[14]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Linux%2C-MATLAB%2C-OpenCV-3%29\n[15]: https://github.com/kyamagu/mexopencv/wiki/Installation-%28Linux%2C-Octave%2C-OpenCV-3%29\n[16]: https://github.com/kyamagu/mexopencv/tree/v2.1\n[17]: https://github.com/kyamagu/mexopencv/tree/v2.3\n[18]: https://github.com/kyamagu/mexopencv/tree/v2.4\n[19]: https://github.com/kyamagu/mexopencv/tree/v3.0.0\n[20]: https://github.com/kyamagu/mexopencv/tree/v3.1.0.1\n[21]: https://github.com/kyamagu/mexopencv/tree/v3.2.0\n[22]: https://github.com/kyamagu/mexopencv/tree/v3.3.0\n[23]: https://github.com/kyamagu/mexopencv/tree/v3.3.1\n[24]: https://github.com/kyamagu/mexopencv/tree/v3.4.0\n"
 },
 {
  "repo": "hrastnik/FaceSwap",
  "language": "C++",
  "readme_contents": "# [Youtube video](https://youtu.be/32i1ca8pcTg)\n\n# How to run?\n\nDownload [OpenCV](http://opencv.org/downloads.html) and [dlib](http://dlib.net/)\n\n- Setup OpenCV\n    - Run the downloaded OpenCV executable file and select a directory to extract the OpenCV library (for example D:\\opencv\\)\n- Setup dlib\n    - Extract the downloaded zip file to a directory (for example D:\\dlib)\n- Download and install Visual Studio 2015 or later versions\n- Run Visual Studio\n- Create new empty project and name it something (for example MyProject)\n- Make sure the \"Debug\" solution configuration is selected\n- In Visual Studio open the Solution Explorer window\n- Right click MyProject and choose Properties\n- Click the \"Configuration Manager...\" button in the top left corner\n- Setup the configuration for the debug\n  - In the active solution platform select x64\n  - Close the Configuration Manager window\n  - In the property window make sure the selected Configuration in the top left is \"Debug\" and Platform is \"x64\"\n  - In the panel on the left choose C/C++\n  - In the Additional Include Directories field add two directories:\n      - \"D:\\opencv\\opencv\\build\\include\"\n      - \"D:\\dlib\\dlib-19.2\"\n      * Note the path might be different if you have different dlib version\n  - In the panel on the left choose Linker>General\n  - In the Additional Library Directories add \"D:\\opencv\\opencv\\build\\x64\\vc14\\lib\"\n      * Note the path might be different if you have different architecture or VS version\n  - In the panel on the left choose Linker>Input\n  - In the Additional Dependencies add \"opencv_world320d.lib\"\n  - Click Apply\n  \n- Change the Configuration in the top left to \"Release\" and repeat \n- Setup the configuration for the release\n  - In the panel on the left choose C/C++\n  - In the Additional Include Directories field add two directories:\n      - \"D:\\opencv\\opencv\\build\\include\"\n      - \"D:\\dlib\\dlib-19.2\"\n      * Note the path might be different if you have different dlib version\n  - In the panel on the left choose Linker>General\n  - In the Additional Library Directories add \"D:\\opencv\\opencv\\build\\x64\\vc14\\lib\"\n      * Note the path might be different if you have different architecture or VS version\n  - In the panel on the left choose Linker>Input\n  - In the Additional Dependencies add \"opencv_world320.lib\"\n\n- Close the property window\n- Right click Source Files in the Solution Explorer\n- Select \"Add Existing Item...\" and add the .cpp files from this project\n- Right click Header Files in the Solution Explorer\n- Select \"Add Existing Item...\" and add the .h files from this project\n- Copy haarcascade_frontalface_default.xml from OpenCV sources/data/haarcascades directory to project directory\n- Download shape_predictor_68_face_landmarks.dat from http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2 and place in project directory \n\nAfter that FaceSwap should work. \n\n# Building on GNU/Linux\n\nIf you want to run this on Ubuntu 16.04 run this set of commands:\n\n    sudo apt install libopencv-dev liblapack-dev libdlib-dev\n    wget http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2\n    bunzip2 *.bz2\n    ln -s /usr/share/opencv/haarcascades/haarcascade_frontalface_default.xml .\n\n    g++ -std=c++1y *.cpp $(pkg-config --libs opencv lapack) -ldlib \n    ./a.out\n    \nSpecial thanks to https://github.com/nqzero for providing the build commands.\n\n# Building on MacOS\n\nSpecial thanks to https://github.com/shaunharker for providing the build commands.\n\n    brew install lapack\n    brew install openblas\n    brew install opencv\n    brew install dlib --with-openblas\n    git clone https://github.com/hrastnik/FaceSwap.git\n    cd FaceSwap\n    wget http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2\n    bunzip2 *.bz2\n    ln -s /usr/local/share/opencv/haarcascades/haarcascade_frontalface_default.xml .\n    export PKG_CONFIG_PATH=/usr/local/opt/lapack/lib/pkgconfig:/usr/local/opt/openblas/lib/pkgconfig:$PKG_CONFIG_PATH\n    g++ -std=c++1y *.cpp $(pkg-config --libs opencv lapack openblas) -ldlib\n    mkdir bin\n    mv a.out bin\n    cd bin\n    ./a.out\n\n# How does it work?\n\nThe algorithm searches until it finds two faces in the frame. Then it estimates facial landmarks using dlib face landmarks. Facial landmarks are used to \"cut\" the faces out of the frame and to estimate the transformation matrix used to move one face over the other.\n\nThe faces are then color corrected using histogram matching and in the end the edges of the faces are feathered and blended in the original frame.\n\n# Result\nBefore...\n\n[![Before](./images/before.jpg)](https://youtu.be/32i1ca8pcTg)\n\nAfter...\n\n[![After](./images/after.jpg)](https://youtu.be/32i1ca8pcTg)\n"
 },
 {
  "repo": "techfort/pycv",
  "language": "Python",
  "readme_contents": "# pycv\n\n---\n\n*&#9758; **This repository is no longer actively maintained.** Please consider switching to the [next edition's repository](https://github.com/PacktPublishing/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition), which officially supports OpenCV 4.x and should generally work with OpenCV 3.4 as well. Over there, you will find many bugfixes, improvements, and updates.*\n\n---\n\n## Learning OpenCV 3 with Python - Second Edition\n\nThis is the repository and reference website for [Learning OpenCV 3 Computer Vision with Python](https://www.packtpub.com/application-development/learning-opencv-3-computer-vision-python-second-edition), a book authored by [Joe Minichino](https://github.com/techfort) and [Joe Howse](https://github.com/JoeHowse), and published by Packt Publishing.\n\nThe book also has a Simplified Chinese edition, [OpenCV 3\u8ba1\u7b97\u673a\u89c6\u89c9:Python\u8bed\u8a00\u5b9e\u73b0](http://hzbook.com/Books/9290.html), translated by Liu Bo, Miao Beibei, and Shi Bin, and published by HZ Books / China Machine Press. This repository might also help readers of OpenCV 3\u8ba1\u7b97\u673a\u89c6\u89c9:Python\u8bed\u8a00\u5b9e\u73b0, but for Chinese-language support please contact HZ Books.\n\n### Code\nCode is divided in chapters reflecting the samples contained in the book. Feel free to report errors either by:\n* opening an issue on github; or\n* contacting the authors directly (if you don't have a GitHub account).\n\nGithub is the preferred way as it grants visibility to all issues reported.\n\n### Virtual Machine\nI (Joe Minichino) created a VM for VirtualBox which allows you to skip installation steps and jump straight into action. However, I am yet to find a free storage service where to upload the 12GB file to make it freely available. This is currently being addressed so check to this page constantly.\n"
 },
 {
  "repo": "avisingh599/mono-vo",
  "language": "C++",
  "readme_contents": "This is an OpenCV 3.0 based implementation of a monocular visual odometry algorithm.\n\n## Algorithm\nUses Nister's Five Point Algorithm for Essential Matrix estimation, and FAST features, with a KLT tracker.\nMore details are available [here as a report](http://avisingh599.github.io/assets/ugp2-report.pdf), and\n[here as a blog post](http://avisingh599.github.io/vision/monocular-vo/). \n\nNote that this project is not yet capable of doing reliable relative scale estimation, \nso the scale informaion is extracted from the KITTI dataset ground truth files.\n\n## Demo Video\n\n[![Demo video](http://share.gifyoutube.com/Ke1ope.gif)](http://www.youtube.com/watch?v=homos4vd_Zs)\n\n\n## Requirements\nOpenCV 3.0\n\n## How to compile?\nProvided with this repo is a CMakeLists.txt file, which you can use to directly compile the code as follows:\n```bash\nmkdir build\ncd build\ncmake ..\nmake\n```\n\n## How to run? \nAfter compilation, in the build directly, type the following:\n```bash\n./vo\n```\n## Before you run\nIn order to run this algorithm, you need to have either your own data, \nor else the sequences from [KITTI's Visual Odometry Dataset](http://www.cvlibs.net/datasets/kitti/eval_odometry.php).\nIn order to run this algorithm on your own data, you must modify the intrinsic calibration parameters in the code.\n\n## Performance\n![Results on the KITTI VO Benchmark](http://avisingh599.github.io/images/visodo/2K.png)\n\n## Contact\nFor any queries, contact: avisingh599@gmail.com\n\n## License\nMIT"
 },
 {
  "repo": "twistedfall/opencv-rust",
  "language": "Rust",
  "readme_contents": "# Rust OpenCV bindings\n\n[![Github Actions](https://github.com/twistedfall/opencv-rust/workflows/opencv-rust/badge.svg)](https://github.com/twistedfall/opencv-rust/actions?query=workflow%3Aopencv-rust)\n[![Documentation](https://docs.rs/opencv/badge.svg)](https://docs.rs/opencv)\n[![Package](https://img.shields.io/crates/v/opencv.svg)](https://crates.io/crates/opencv)\n\nExperimental Rust bindings for OpenCV 3 and 4.\n\nThe API is usable, but unstable and not very battle-tested; use at your own risk.\n\n## Quickstart\n\nMake sure the supported OpenCV version (3.2, 3.4 or 4.x) is installed in your system.\n\nUpdate your Cargo.toml\n```toml\nopencv = \"0.46\"\n```\n\nSelect OpenCV version if different from default (opencv-4) in Cargo.toml:\n```toml\nopencv = {version = \"0.46\", default-features = false, features = [\"opencv-34\", \"buildtime-bindgen\"]}\n```\n\nOr enable usage of `contrib` modules:\n```toml\nopencv = {version = \"0.46\", features = [\"contrib\"]}\n```\n\nImport prelude\n```rust\nuse opencv::prelude::*;\n```\n\n## Getting OpenCV\n\n### Linux\n\nYou have several options of getting the OpenCV library:\n\n* install it from the repository, make sure to install `-dev` packages because they contain headers necessary\n  for the crate build (also check that your package contains `pkg_config` or `cmake` files).\n\n* build OpenCV manually and set up the following environment variables prior to building the project with\n  `opencv` crate:\n  * `PKG_CONFIG_PATH` for the location of `*.pc` files or `OpenCV_DIR` for the location of `*.cmake` files\n  * `LD_LIBRARY_PATH` for where to look for the installed `*.so` files during runtime\n\n### Windows package\n\nInstalling OpenCV is easy through the following sources:\n\n* from [chocolatey](https://chocolatey.org), also install `llvm` package, it's required for building:\n  ```shell script\n  choco install llvm opencv\n  ```\n  also set `OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS` environment variables (see below\n  for details).\n  \n  Also, check the user guides [here](https://github.com/twistedfall/opencv-rust/issues/118#issuecomment-619608278)\n  and [here](https://github.com/twistedfall/opencv-rust/issues/113#issue-596076777).\n\n* from [vcpkg](https://docs.microsoft.com/en-us/cpp/build/vcpkg), also install `llvm` package,\n  necessary for building:\n  ```shell script\n  vcpkg install llvm opencv4[contrib,nonfree]\n  ```\n  You most probably want to set environment variable `VCPKGRS_DYNAMIC` to \"1\" unless you're specifically\n  targeting a static build.\n\n### macOS package\n\nGet OpenCV from homebrew:\n\n* [homebrew](https://brew.sh):\n  ```shell script\n  brew install opencv\n  ```\n  You will also need a working C++ compiler and libclang, you can use the ones from XCode or install `llvm`\n  from brew. You most probably need to also check the item 6 of the troubleshooting below.\n\n### Manual build\n\nYou can of course always compile OpenCV of the version you prefer manually. This is also supported, but it\nrequires some additional configuration.\n\nYou need to set up the following environment variables to point to the installed files of your OpenCV build: \n`OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS` (see below for details).\n\n## Troubleshooting\n\n1. One of the common problems is link errors in the end of the build.\n\n   Make sure you're building with `buildtime-bindgen` feature enabled (requires installed clang/llvm), it will\n   recreate rust and cpp files to match the version you have installed. Please be sure to also set up the\n   relevant environment variables that will allow the linker to find the libraries it needs (see below).\n\n2. You're getting runtime errors like:\n   ```\n   thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { code: -215, message: \"OpenCV(4.2.0) /build/opencv/src/opencv-4.2.0/modules/highgui/src/window.cpp:384: error: (-215:Assertion failed) size.width>0 && size.height>0 in function \\'imshow\\'\\n\" }', src/libcore/result.rs:1188:5\n   ```\n   ```\n   thread 'extraction::tests::test_contour_matching' panicked at 'called `Result::unwrap()` on an `Err` value: Error { code: -215, message: \"OpenCV(4.1.1) /tmp/opencv-20190908-41416-17rm3ol/opencv-4.1.1/modules/core/src/matrix_wrap.cpp:79: error: (-215:Assertion failed) 0 <= i && i < (int)vv.size() in function \\'getMat_\\'\\n\" }', src/libcore/result.rs:1165:5\n   ```\n\n   These errors (note the .cpp source file and `Error` return value) are coming from OpenCV itself, not from\n   the crate. It means that you're using the OpenCV API incorrectly, e.g. passing incompatible or unexpected\n   arguments. Please refer to the OpenCV documentation for details.\n\n3. You're getting errors that methods don't exist or not implemented for specific `struct`s, but you can see\n   them in the documentation and in the crate source.\n\n   Be sure to import ```use opencv::prelude::*;```. The crate contains a lot of traits that need to be imported\n   first.\n\n   Also check that if you're using a contrib module that the `contrib` feature is enabled for the crate. \n\n4. On Windows, you're getting the `(exit code: 0xc0000135, STATUS_DLL_NOT_FOUND)` error when running the\n   compiled binary.\n\n   That often means that Windows can't find the OpenCV library dll. Be sure to set up `PATH` environment\n   variable correctly or copy the dll next to the binary you're trying to run. Check\n   [that](https://github.com/twistedfall/opencv-rust/issues/118#issuecomment-619608278) guide too.\n\n5. On Windows with VCPKG you're getting a lot of linking errors in multiple files like in\n   [this issue](https://github.com/twistedfall/opencv-rust/issues/161).\n   \n   Unless you're doing a very specific build, you want to have environment variable `VCPKGRS_DYNAMIC` set to\n   \"1\".\n\n6. On macOS you're getting the `dyld: Library not loaded: @rpath/libclang.dylib` error during the build process.\n\n   OS can't find `libclang.dylib` dynamic library because it resides in a non-standard path, set up \n   the `DYLD_FALLBACK_LIBRARY_PATH` environment variable to point to the path where libclang.dylib can be\n   found, e.g. for Xcode:\n   \n   ```  \n   export DYLD_FALLBACK_LIBRARY_PATH=\"$(xcode-select --print-path)/Toolchains/XcodeDefault.xctoolchain/usr/lib/\"\n   ```\n\n## Reporting issues\n\nIf you still have trouble using the crate after going through the Troubleshooting steps please fill free to\nreport it to the [bugtracker](https://github.com/twistedfall/opencv-rust/issues).\n\nWhen reporting an issue please state:\n1. Operating system\n2. The way you installed OpenCV: package, official binary distribution, manual compilation, etc.\n3. OpenCV version\n4. Attach the full output of the following command from your project directory:\n   ```shell script\n   RUST_BACKTRACE=full cargo build -vv \n   ```\n\n## Environment variables\n\nThe following variables must be set when building without `pkg_config`, `cmake` or `vcpkg`. You can set them\non any platform, the specified values will override those automatically discovered.\n\n* `OPENCV_LINK_LIBS`\n  Comma separated list of library names to link to. `.lib`, `.so` or `.dylib` extension is optional. If you\n  specify the \".framework\" extension then build script will link a macOS framework instead of plain shared\n  library.\n  E.g. \"opencv_world411\".\n  \n  If this list starts with '+' (plus sign) then the specified items will be appended to whatever the system\n  probe returned. E.g. a value of \"+dc1394\" will do a system discovery of the OpenCV library and its linked\n  libraries and then will additionally link `dc1394` library at the end. Can be useful if the system probe\n  produces a mostly working setup, but has incomplete link list, or the order is wrong (especially important\n  during static linking).\n\n* `OPENCV_LINK_PATHS`\n  Comma separated list of paths to search for libraries to link. E.g. \"C:\\tools\\opencv\\build\\x64\\vc15\\lib\".\n  The path list can start with '+', see `OPENCV_LINK_LIBS` for a detailed explanation (e.g.\n  \"+/usr/local/lib\").\n\n* `OPENCV_INCLUDE_PATHS`\n  Comma separated list of paths to search for system include files during compilation. E.g.\n  \"C:\\tools\\opencv\\build\\include\". One of the directories specified therein must contain\n  \"opencv2/core/version.hpp\" or \"core/version.hpp\" file, it's used to detect the version of the headers.\n  The path list can start with '+', see `OPENCV_LINK_LIBS` for a detailed explanation (e.g.\n  \"+/opt/cuda/targets/x86_64-linux/include/\").\n\nThe following variables are rarely used, but you might need them under some circumstances:\n\n* `OPENCV_HEADER_DIR`\n  During crate build it uses OpenCV headers bundled with the crate. If you want to use your own (system)\n  headers supply `OPENCV_HEADER_DIR` environment variable.\n  The directory in that environment variable should contain `opencv2` dir, e.g. set it `/usr/include` for\n  OpenCV-3.4.x or `/usr/include/opencv4` for OpenCV-4.x.\n\n* `OPENCV_PACKAGE_NAME`\n  In some cases you might want to override the pkg-config, cmake or vcpkg package name, you can use this\n  environment variable for that. If you set it pkg-config will expect to find the file with that name and `.pc`\n  extension in the package directory. Cmake will look for that file with `.cmake` extension. And vcpkg will use\n  that name to try to find package in `packages` directory under `VCPKG_ROOT`. You can also use separate\n  environment variables to set different package names for different package systems:\n    * `OPENCV_PKGCONFIG_NAME`\n    * `OPENCV_CMAKE_NAME`\n    * `OPENCV_VCPKG_NAME`\n\n* `OPENCV_CMAKE_BIN`\n  Path to cmake binary (used in OpenCV discovery process using cmake). If not set then just \"cmake\" will be\n  used. For example, you can set something like \"/usr/local/bin/cmake\" here.\n\n* `OPENCV_DISABLE_PROBES`\n  Comma separated list of OpenCV package auto-discovery systems to exclude from running. Might be useful if\n  one of the higher priority systems is producing incorrect results. Can contain the following values:\n    * environment - reads data only from the `OPENCV_LINK_LIBS`, `OPENCV_LINK_PATHS` and `OPENCV_INCLUDE_PATHS`\n      environment variables\n    * pkg_config\n    * cmake\n    * vcpkg_cmake - like vcpkg, but only uses vcpkg for path discovery, the actual OpenCV probe is done using\n      cmake (cmake related environment variables are applicable with this probe)\n    * vcpkg\n\n* `OPENCV_CLANG_STDLIB_PATH`\n  Path that contains the stdlib headers for parsing with libclang. Should be used only as a workaround for\n  the rare cases where it doesn't get picked up automatically. Should help with issues like\n  [this](https://github.com/twistedfall/opencv-rust/issues/125).\n\n* `OPENCV_MODULE_WHITELIST` and `OPENCV_MODULE_BLACKLIST`\n  Comma separated lists that affect modules that get their bindings generated. Setting whitelist will only\n  generate the specified modules, setting blacklist will exclude the specified modules from generation. If the\n  same module is specified in both list it will be excluded (i.e. blacklist has precedence). E.g.\n  \"core,dnn,features2d\" .\n\nThe following variables affect the building the of the `opencv` crate, but belong to external components:\n\n* `PKG_CONFIG_PATH`\n  Where to look for `*.pc` files see the [man pkg-config](https://linux.die.net/man/1/pkg-config)\n  Path specified here must contain `opencv.pc` (pre OpenCV 4) or `opencv4.pc` (OpenCV 4 and later).\n\n* `VCPKG_ROOT` and `VCPKGRS_DYNAMIC`\n  The root of `vcpkg` installation and flag allowing use of `*.dll` libraries, see the\n  [documentation for `vcpkg` crate](https://docs.rs/vcpkg)\n\n* `OpenCV_DIR`\n  The directory that contains OpenCV package cmake files. Usually there are `OpenCVConfig.cmake`,\n  `OpenCVConfig-version.cmake` and `OpenCVModules.cmake` in it.\n\n* `LD_LIBRARY_PATH`\n  On Linux it sets the list of directories to look for the installed `*.so` files during runtime.\n  [Linux documentation](https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html) has more info.\n  Path specified here must contain `libopencv_*.so` files.\n\n* `DYLD_LIBRARY_PATH` and `DYLD_FALLBACK_LIBRARY_PATH`\n  Similar to `LD_LIBRARY_PATH`, but for loading `*.dylib` files on macOS, see [man dyld](https://man.cx/dyld(1))\n  and [this SO answer](https://stackoverflow.com/a/3172515) for more info. Path specified here must contain\n  `*.dylib` files.\n\n* `PATH`\n  Windows searches for `*.dll`s in `PATH` among other places, be sure to set it up, or copy required OpenCV\n  `*.dll`s next to your binary. Be sure to specify paths in UNIX style (/C/Program Files/Dir) because colon\n   in `PATH` might be interpreted as the entry separator. Summary [here](https://stackoverflow.com/a/6546427).\n\n* clang crate environment variables\n  See crate's [README](https://github.com/KyleMayes/clang-sys/blob/master/README.md#environment-variables)\n\n## Cargo features\n* `opencv-32` - build against OpenCV 3.2.0, this feature is aimed primarily on stable Debian and\n  Ubuntu users who can install OpenCV from the repository without having to compile it from the\n  source\n* `opencv-34` - build against OpenCV 3.4.x\n* `opencv-4` (default) - build against OpenCV 4.x\n* `contrib` - enable the usage of OpenCV contrib modules for corresponding OpenCV version\n* `buildtime-bindgen` (default) - regenerate all bindings, requires installed clang/llvm (minimum supported\n  version is 6.0), with this feature enabled the bundled headers are no longer used for the code generation,\n  the ones from the installed OpenCV are used instead\n* `clang-runtime` - only useful with the combination with `buildtime-bindgen`, enables the runtime detection\n  of libclang (`runtime` feature of `clang-sys`). This makes the build slower because it impairs the parallel\n  generation of OpenCV modules. Useful as a workaround for when your dependencies (like `bindgen`) pull in\n  `clang-sys` with hard `runtime` feature. See also this [issue](https://github.com/twistedfall/opencv-rust/issues/129).\n* `docs-only` - internal usage, for building docs on [docs.rs](https://docs.rs/opencv)\n\n## API details\n\n[API Documentation](https://docs.rs/opencv) is automatically translated from OpenCV's doxygen docs. Most\nlikely you'll still want to refer to the official [OpenCV C++ documentation](https://docs.opencv.org/master)\nas well.\n\n### OpenCV version support\n\nThe following OpenCV versions are supported at the moment:\n* 3.2 - enabled by `opencv-32` feature\n* 3.4 - enabled by `opencv-34` feature\n* 4.3 - enabled by the default `opencv-4` feature\n\nIf you need support for `contrib` modules, also enable `contrib` feature.\n\n### Minimum rustc version\n\nGenerally you should use the latest stable rustc to compile this crate.\n\n### Platform support\n\nCurrently, the main development and testing of the crate is performed on Linux, but other major platforms are\nalso supported: macOS and Windows.\n\nFor some more details please refer to the CI build scripts:\n[Linux OpenCV install](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-bionic.sh),\n[macOS OpenCV install as framework](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-macos-framework.sh),\n[macOS OpenCV install via brew](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-macos-brew.sh),\n[Windows OpenCV install via Chocolatey](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-windows-chocolatey.sh),\n[Windows OpenCV install via vcpkg](https://github.com/twistedfall/opencv-rust/blob/master/ci/install-windows-vcpkg.sh),\n[Test runner script](https://github.com/twistedfall/opencv-rust/blob/master/ci/script.sh).\n\n### Functionality\n\nGenerally the crate tries to only wrap OpenCV API and provide some convenience functions\nto be able to use it in Rust easier. We try to avoid adding any functionality besides\nthat.\n\n### Errors\n\nMost functions return a `Result` to expose a potential C++ exception. Although some methods like property reads\nor functions that are marked CV_NOEXCEPT in the OpenCV headers are infallible and return a naked value.\n\n### Properties\n\nProperties of OpenCV classes are accessible through setters and getters. Those functions are infallible, they\nreturn the value directly instead of `Result`.\n\n### Infallible functions\n\nFor infallible functions (like setters) that accept `&str` values the following logic applies: if a Rust\nstring passed as argument contains null byte then this string will be truncated up to that null byte. So if\nfor example you pass \"123\\0456\" to the setter, the property will be set to \"123\". \n\n### Callbacks\n\nSome API functions accept callbacks, e.g. `set_mouse_callback`. While currently it's possible to successfully\nuse those functions there are some limitations to keep in mind. Current implementation of callback handling\nleaks the passed callback argument. That means that the closure used as a callback will never be freed during\nthe lifetime of a program and moreover Drop will not be called for it. There is a plan to implement possibility\nto be able to free at least some of the closures.\n\n### Unsafety\n\nAlthough the crate tries to provide an ergonomic Rust interface for OpenCV, don't expect\nRust safety guarantees at this stage. It's especially true for the borrow-checking and the\nshared mutable ownership. Notable example would be `Mat` which is a reference counted\nobject in its essence. You can own a seemingly separate `Mat` in Rust terms, but\nit's going to be a mutable reference to the other `Mat` under the hood. Treat safety\nof the crate's API as you would treat one of C++, use `clone()` when needed.\n\n## Contrib modules\n\nTo be able to use some modules you need to have [`opencv_contrib`](https://github.com/opencv/opencv_contrib)\ninstalled. You can find the full list of contrib modules [here](https://github.com/opencv/opencv_contrib/tree/master/modules) with the exception that `dnn` module is also considered contrib for OpenCV 3.2.\n\n## Missing modules and functions\n\nWhile most of the API is covered, for various reasons (that might no longer hold) there are modules and\nfunctions that are not yet implemented. If a missing module/function is near and dear to you, please file an\nissue (or better, open a pull request!).\n\n## The binding strategy\n\nThis crate works similar to the model of python and java's OpenCV wrappers - it uses libclang to parse the\nOpenCV C++ headers, generates a C interface to the C++ API, and wraps the C interface in Rust.\n\nAll the major modules in the C++ API are merged together in a huge `cv::` namespace. We instead made one rust\nmodule for each major OpenCV module. So, for example, C++ `cv::Mat` is `opencv::core::Mat` in this crate.\n\nThe methods and field names have been snake_cased. Methods arguments with default value lose these default\nvalues, but they are reported in the API documentation.\n\nOverloaded methods have been mostly manually given different names or automatically renamed to *_1, *_2, etc.\n\n## OpenCV 2 support\n\nIf you can't use OpenCV 3.x or higher, the (no longer maintained) `0.2.4` version of this crate is known to\nwork with OpenCV `2.4.7.13` (and probably other 2.4 versions). Please refer to the README.md file for that\nversion because the crate has gone through the considerable rewrite since.\n\n## Contributor's Guide\n\nThe binding generator code lives in a separate crate under [binding-generator](binding-generator). During the\nbuild phase (with `buildtime-bindgen` feature enabled) it creates bindings from the header files and puts them\ninto [bindings](bindings) directory. Those are then transferred to [src](src) for the consumption by the\ncrate users. \n\nThe crate itself, as imported by users, consists of generated rust code in [src](src) committed to the repo.\nThis way, users don't have to handle the code generation overhead in their builds. When developing this crate,\nyou can test changes to the binding generation using `cargo build -vv`. When changing the `binding-generator`,\nbe sure to push changes to the generated code!\n\nIf you're looking for things to improve be sure to search for `todo` and `fixme` labels in the project\nsource, those usually carry the comment of what exactly needs to be fixed.\n\nThe license for the original work is [MIT](https://opensource.org/licenses/MIT).\n\nSpecial thanks to [ttacon](https://github.com/ttacon) for yielding the crate name.\n"
 },
 {
  "repo": "jeffbass/imagezmq",
  "language": "Python",
  "readme_contents": "====================================\nimageZMQ: Transporting OpenCV images\n====================================\n\nIntroduction\n============\n\n**imageZMQ** is a set of Python classes that transport OpenCV images from one\ncomputer to another using PyZMQ messaging. For example, here is a screen on a\nMac computer showing simultaneous video streams from 8 Raspberry Pi cameras:\n\n.. image:: docs/images/screenshottest.png\n\nUsing **imageZMQ**, this is possible with 11 lines of Python on each Raspberry\nPi and with 8 lines of Python on the Mac.\n\nFirst, run this code on the Mac (or other display computer):\n\n.. code-block:: python\n  :number-lines:\n\n    # run this program on the Mac to display image streams from multiple RPis\n    import cv2\n    import imagezmq\n    image_hub = imagezmq.ImageHub()\n    while True:  # show streamed images until Ctrl-C\n        rpi_name, image = image_hub.recv_image()\n        cv2.imshow(rpi_name, image) # 1 window for each RPi\n        cv2.waitKey(1)\n        image_hub.send_reply(b'OK')\n\n\nThen, on each Raspberry Pi, run:\n\n.. code-block:: python\n  :number-lines:\n\n    # run this program on each RPi to send a labelled image stream\n    import socket\n    import time\n    from imutils.video import VideoStream\n    import imagezmq\n\n    sender = imagezmq.ImageSender(connect_to='tcp://jeff-macbook:5555')\n\n    rpi_name = socket.gethostname() # send RPi hostname with each image\n    picam = VideoStream(usePiCamera=True).start()\n    time.sleep(2.0)  # allow camera sensor to warm up\n    while True:  # send images as stream until Ctrl-C\n        image = picam.read()\n        sender.send_image(rpi_name, image)\n\n\nWow! A video surveillance system with 8 (or more!) Raspberry Pi cameras in\n19 lines of Python.\n\nSee `About the multiple RPi video streaming examples <docs/more-details.rst>`_\nfor more details about this example.\n\n.. contents::\n\nWhy use imageZMQ?\n=================\n\n**imageZMQ** is an easy to use image transport mechanism for a distributed image\nprocessing network. For example, a network of a dozen Raspberry Pis with cameras\ncan send images to a more powerful central computer. The Raspberry Pis perform\nimage capture and simple image processing like flipping, blurring and motion\ndetection. Then the images are passed via **imageZMQ** to the central computer for\nmore complex image processing like image tagging, text extraction, feature\nrecognition, etc. An example of using **imageZMQ** can be found\nat `Using imageZMQ in distributed computer vision projects. <docs/imagezmq-uses.rst>`_\n\nFeatures\n========\n\n- Sends OpenCV images from one computer to another using ZMQ.\n- Can send jpeg compressed OpenCV images, to lighten network loads.\n- Uses the powerful ZMQ messaging library through PyZMQ bindings.\n- Allows a choice of 2 different ZMQ messaging patterns (REQ/REP or PUB/SUB).\n- Enables the image hub to receive and process images from multiple image senders\n  simultaneously.\n\nWhy ZMQ? Why not some other messaging protocol?\n===============================================\n\nThere are a number of high quality and well maintained messaging protocols for\npassing messages between computers. I looked at MQTT, RabbitMQ, AMQP and ROS as\nalternatives. I chose ZMQ and its Python PyZMQ bindings for several reasons:\n\n- ZMQ does not require a message broker. It is a peer to peer protocol that does\n  not need to pass an image first to a message broker and then to the imagehub.\n  This means fewer running processes and less \u201cdouble handling\u201d of images.\n  OpenCV images are large compared to simple text messages, so the absence of a\n  message broker is important.\n- ZMQ is very fast for passing OpenCV images. It enables high throughput between\n  image senders and image hubs.\n- ZMQ and its PyZMQ bindings are easy to install.\n\n**imageZMQ** has been transporting images from a dozen Raspberry Pi computers\nscattered around my farm to 2 linux image hub servers for over 2\nyears. The RPi's capture and send dozens to thousands of frames frames a day.\n**imageZMQ** has worked very reliably and is very fast. You can learn more about\nmy \"science experiment urban permaculture farm\" project at\n`Yin Yang Ranch project overview. <https://github.com/jeffbass/yin-yang-ranch>`_\n\n\nMessaging Patterns: REQ/REP versus PUB/SUB\n==========================================\n\nZMQ allows many different messaging patterns. Two are implemented in **imageZMQ**:\n\n- REQ/REP: Each RPi sends an image and waits for a REPLY from the central image\n  hub. The RPi sends a new image only when the REPLY is received. In the REQ/REP\n  messaging pattern, each image sender must await a REPLY before continuing. It is a\n  \"blocking\" pattern for the sender.\n- PUB/SUB: Each RPi sends an image, but does not expect a REPLY from the central\n  image hub. It can continue sending images without awaiting any acknowledgement\n  from the image hub. The image hub provides no REPLY. It is a \"non-blocking\"\n  pattern for the sender.\n\nThere are advantages and disadvantages for each pattern. For further details,\nsee: `REQ/REP versus PUB/SUB Messaging Patterns <docs/req-vs-pub.rst>`_.\n**REQ/REP is the default.**\n\n\nDependencies and Installation\n=============================\n\n+--------------+--------+---------------+-----------+-------+\n| |pyversions| | |pypi| | |releasedate| | |license| | |doi| |\n+--------------+--------+---------------+-----------+-------+\n\n.. |pyversions| image:: /docs/images/python_versions.svg\n   :target: https://pypi.org/project/imagezmq/\n\n.. |pypi| image:: /docs/images/pypi_version.svg\n   :target: https://pypi.org/project/imagezmq/\n\n.. |releasedate| image:: /docs/images/release_date.svg\n   :target: https://github.com/jeffbass/imagezmq/releases/tag/v1.1.1\n\n.. |license| image::  /docs/images/license.svg\n   :target: LICENSE.txt\n\n.. |doi| image::  /docs/images/doi.svg\n   :target: https://doi.org/10.5281/zenodo.3840855\n\n**imageZMQ** has been tested with:\n\n- Python 3.5, 3.6, 3.7 and 3.8\n- PyZMQ 16.0, 17.1 and 19.0\n- Numpy 1.13, 1.16 and 1.18\n- OpenCV 3.3, 4.0 and 4.1\n- Raspbian Buster, Raspbian Stretch and Raspbian Jessie\n- picamera 1.13 (used to capture images for the tests)\n- imutils 0.4.6, 0.5.2 and 0.5.3 (used to capture images from PiCamera)\n\nOpenCV is challenging to install. I recommend using the installation\ninstructions at `PyImageSearch <https://www.pyimagesearch.com/>`_.\nAdrian Rosebrock's PyImageSearch blog and books are great resources for\nlearning about and installing OpenCV on Raspberry Pi's, Macs and\nLinux computers.\n\n- `Raspbian Stretch: Install OpenCV 3 + Python on your Raspberry Pi\n  <https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/>`_\n- `macOS: Install OpenCV 3 and Python 3.5\n  <https://www.pyimagesearch.com/2016/12/05/macos-install-opencv-3-and-python-3-5/>`_\n- `Ubuntu 16.04: How to install OpenCV\n  <https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/>`_\n\nBe sure to install OpenCV, including Numpy, into a Python Virtual Environment,\nas shown in the above tutorials. Be sure to install **imageZMQ**\ninto the **same** virtual environment. For example, my virtual\nenvironment is named **py3cv3**.\n\nInstall **imageZMQ** using pip:\n\n.. code-block:: bash\n\n    workon py3cv3  # use your virtual environment name\n    pip install imagezmq\n\n**imageZMQ** has a directory of tests organized into sender and receiver pairs.\nYou will get the \"tests\" directory containing all the test programs by\ncloning the GitHub repository:\n\n.. code-block:: bash\n\n    git clone https://github.com/jeffbass/imagezmq.git\n\nOnce you have cloned the imagezmq directory to a directory on your local machine,\nyou can run the tests per the instructions below. You can use imageZMQ in your\nown code by importing it (``import imagezmq``).\n\n**imageZMQ** and all of the software dependencies must be installed on the\ndisplay computer that will be receiving the images AND it must all be installed\non every Raspberry Pi that will be sending images. If you will be using multiple\nRaspberry Pis to capture and send images it is best to install the software on a\nsingle Raspberry Pi and test it using the tests below. Once all the tests\nhave run successfully, clone the SD card as needed to use the software on\nmultiple Raspberry Pis.\n\nRunning the Tests\n=================\n\nWhen running the tests, use multiple terminal windows on the computer that will\nbe displaying the images (I used a Mac for these examples; in my descriptions\nI use the term \"Mac\" to refer to any Mac or Linux computer, including a\nRaspbery Pi). One terminal window is used to launch the programs that run on the\nMac to receive the images. Another terminal window on the Mac is used to ssh\ninto the Raspberry Pi and run the image sending program. If sending from multiple\nRaspberry Pis, ssh to each Raspberry Pi in a new terminal window. **imageZMQ**\nand its dependencies must be installed on the Mac and on each Raspberry Pi that\nwill be sending images.\n\nThere are 3 tests. Each of the tests uses 2 programs in matched pairs. One\nprogram sends images and the other program displays images. Because of the\nREQ/REP pattern that is being used, it is important that the receiving program\nbe started before the sending program.\n\n**imageZMQ** is in early development as part of a larger system. There are\ncurrently separate methods for sending and receiving images vs. jpg compressed\nimages. Further development will refactor these into single methods for sending\nand receiving. ::\n\nTest 1: Simple generated images sent and displayed on Mac\n---------------------------------------------------------\n**The first test** runs both the sending program and the receiving program on\nthe Mac. This confirms that all the software is installed correctly and that\n``cv2.imshow()`` works on the Mac. No Raspberry Pi or camera is involved. The\nsending program generates test images and sends them to the receiving program.\nFirst, in one terminal window, activate your virtual environment, then change to\nthe tests directory and run the receiving program, which will receive and\ndisplay images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_1_receive_images.py\n\nThen, in a second terminal window on the same display computer (Mac), change to\nthe tests directory and run the sending program, which will generate and send\nimages::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_1_send_images.py\n\nAfter a few seconds, a ``cv2.imshow()`` window should open and display a green\nsquare on a black background. There will be a yellow number in the green square\nthat will increase (1, 2, ...) once per second until you stop both\nprograms by pressing Ctrl-C. It is normal to get a cascade of error messages\nwhen stopping the program with Ctrl-C. This simple test program has no\ntry / except error trapping.\n\nTest 2: Sending stream of OpenCV images from RPi(s) to Mac\n----------------------------------------------------------\n**The second test** runs the sending program on a Raspberry Pi, capturing\nimages from the PiCamera at up to 32 frames a second and sending them via\n**imageZMQ** to the Mac. The receiving program on the Mac displays a continuous\nvideo stream of the images captured by the Raspberry Pi. First, in one terminal\nwindow, activate your virtual environment, change to the tests directory and\nrun the receiving program which will display the images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_2_receive_images.py\n\nThen, in a second terminal window on the Mac, ssh into the Raspberry Pi that\nwill be sending images. Activate your Python virtual environment, change to the\ntests directory and **edit the test_2_send_images.py program to specify the tcp\naddress of your display computer.** There are 2 lines in the program that show\ndifferent ways of specifying the tcp address: by hostname or by tcp numeric address.\nPick one method, change the tcp address to that of your display computer and\ncomment out the method you are not using. Finally, run the program, which will\ncapture and send images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_2_send_images.py\n\nIn about 5 seconds, a ``cv2.imshow()`` window will appear on the Mac and display\nthe video stream being sent by the Raspberry Pi.  You can repeat this step in\nadditional terminal windows, to ssh into additional Raspberry Pi computers and\nstart additional video streams. The receiving program can receive and display\nimages from multiple Raspberry Pis, with each Raspberry Pi's image stream\nshowing in a separate window. For this to work, each Raspberry Pi must have a\nunique hostname because the images are sorted into different\n``cv2.imshow()`` windows based on the hostname. The ``cv2.imshow()`` windows\nfor multiple Raspberry Pi streams will be stacked on top of each other until you\ndrag them and arrange them on your desktop. The example picture at the start of\nthis documentation shows 8 simultaneous video streams for 8 Raspberry Pi\ncomputers with different hostnames. Each program must be stopped by pressing\nCtrl-C in its terminal window. It is normal to get a cascade of error messages\nwhen stopping these programs with Ctrl-C. This simple test program has no try /\nexcept error trapping.\n\nTest 3: Sending stream of jpgs from RPi(s) to Mac\n-------------------------------------------------\n**The third test** runs a different pair of sending / receiving programs. The\nprogram on the Raspberry Pi captures images from the PiCamera at up to 32\nframes a second and **compresses them to jpeg form** before sending them via\n**imageZMQ** to the Mac. The receiving program on the Mac converts the jpg\ncompressed frames back to OpenCV images and displays them as a continuous video\nstream. This jpeg compression can greatly reduce the network load of sending many\nimages from multiple sources.\n\nThe programs that send and receive the images using jpg compression are run in\nthe same way as the above pair of programs that send uncompressed images. Use\nthe instructions above as a guide. The programs for Test 3 are::\n\n    test_3_receive_jpg.py  # run on the Mac to receive & decompress images\n    test_3_send_jpg.py     # ron on each Raspberry Pi to compress & send images\n\nAs with the previous Test 2 program pair, you will need to edit the \"connect_to\"\naddress in the sending program to the tcp address of your Mac (or other display\ncomputer).  You will also need to remember to start the *receive* program on the\nMac before you start the sending program on the Raspberry Pi. As before, each\nprogram must be stopped by pressing Ctrl-C in its terminal window. It is normal\nto get a cascade of error messages when stopping these programs with Ctrl-C.\nThis simple test program has no try / except error trapping. Be sure to activate\nyour virtual environment as you did for Test 2 (see above) before running these\ntests.\n\nTest 4: Using PUB/SUB to send simple generated images and display them on Mac\n-----------------------------------------------------------------------------\n**The fourth test** is a repeat of Test 1, but uses the PUB/SUB messaging\npattern instead of the REQ/REP messaging pattern. It shows the differences\nin running PUB/SUB versus REQ/REP in the simplest possible test program.\n\nTest 4 runs both the sending program and the receiving program on\nthe Mac. No Raspberry Pi or camera is involved. This test shows the start / stop\nflexibility of the PUB/SUB pattern. All 3 of the above REQ/REP tests require\nthat the receiving program be started first, then the sending program. And they\nrequire that the sending program be restarted if the receiving program is\nrestarted. This is standard behavior for the REQ/REP messaging pattern. But\nthis test shows that either PUB/SUB program can be started first and that\nmessage sending will resume if either program is restarted. That is a feature\nof the PUB/SUB messaging pattern. See other documentation listed below for\nfurther differences, advantages and disadvantages of the REQ/REP versus PUB/SUB\nmessaging patterns.\n\nThe sending program generates test images and sends them to the receiving program.\nFirst, in one terminal window, activate your virtual environment, then change to\nthe tests directory and run the receiving program, which will receive and\ndisplay images::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_4_pub.py\n\nThen, in a second terminal window on the same display computer (Mac), change to\nthe tests directory and run the sending program, which will generate and send\nimages::\n\n    workon py3cv3  # use your virtual environment name\n    cd imagezmq/tests\n    python test_4_sub.py\n\nAfter a few seconds, a ``cv2.imshow()`` window should open and display a green\nsquare on a black background. There will be a yellow number in the green square\nthat will increase (1, 2, ...) once per second. Now you can stop either\nprogram and restart it and see that the sending of numbers continues and picks\nup where it left off (though some transmitted images may have been skipped\nduring restart). It is normal to get a cascade of error messages\nwhen starting and stopping the program with Ctrl-C. These simple test program\nhave no try / except error trapping, since their only purpose is this simple\ndemonstration.\n\nTiming tests: Complete imageZMQ usage examples\n==============================================\nThe test programs above are short and simple. They test that the software and\ndependencies are installed correctly and that images transfer successfully between\na Raspberry Pi computer and a display computer such as a Mac.  The tests\ndirectory contains 2 more send / receive program pairs that provide a more\ncomplete example of imageZMQ usage. Each of these programs includes\ntry / except blocks that enable ending the programs by typing Ctrl-C\nwithout starting a cascade of error messages. They also perform frames per\nsecond (FPS) timing tests that measure the speeds of image transfer using the\ncompressed versus the non-compressed transfer methods. They also show how to\ncapture the hub response in the sending program, which wasn't needed in the\nsimple tests.\n\nOne pair of programs transmits and receives **OpenCV images** and measures FPS::\n\n    timing_receive_images.py  # run on Mac to display images\n    timing_send_images.py     # run on Raspberry Pi to send images\n\nAnother pair of programs transmits and receives **jpg compressed images** and\nmeasures FPS::\n\n    timing_send_jpg_buf.py     # run on Raspberry Pi to send images\n    timing_receive_jpg_buf.py  # run on Mac to display images\n\nAs with the other test program pairs, you will need to edit the \"connect_to\"\naddress in the sending program to the tcp address of your Mac (or other display\ncomputer).  You will also need to remember to start the *receive* program on the\nMac before you start the sending program on the Raspberry Pi. With these programs,\nthe try / except blocks will end the programs cleanly with no errors when you\npress Ctrl-C. Be sure to activate your virtual environment before running these\ntests.\n\nAdditional Documentation and Examples\n=====================================\n- `API and Two Simple Example Programs <docs/api-examples.rst>`_\n- `More details about the multiple RPi video streaming example <docs/more-details.rst>`_\n- `REQ/REP versus PUB/SUB Messaging Patterns <docs/req-vs-pub.rst>`_\n- `Examples showing different techniques for using imageZMQ <docs/examples.rst>`_\n- `Using imageZMQ in distributed computer vision projects <docs/imagezmq-uses.rst>`_\n- `FAQ: Frequently Asked Questions <docs/FAQ.rst>`_\n- How **imageZMQ** is used in my own projects connecting multiple\n  Raspberry Pi **imagenodes** to an **imagehub**:\n\n  - My Yin Yang Ranch project to manage a small urban permaculture farm:\n    `Yin Yang Ranch project overview <https://github.com/jeffbass/yin-yang-ranch>`_\n  - `imagenode: Capture and Send Images and Sensor Data <https://github.com/jeffbass/imagenode>`_\n  - `imagehub: Receive and Store Images and Event Logs <https://github.com/jeffbass/imagehub>`_\n\n\nI gave a talk about imageZMQ and its use in my Yin Yang Ranch project at\nPyCon 2020:\n**Jeff Bass - Yin Yang Ranch: Building a Distributed Computer\nVision Pipeline using Python, OpenCV and ZMQ**\n\n`PyCon 2020 Talk Video about the project  <https://youtu.be/76GGZGneJZ4?t=2>`_\n\n`PyCon 2020 Talk Presentation slides  <https://speakerdeck.com/jeffbass/yin-yang-ranch-building-a-distributed-computer-vision-pipeline-using-python-opencv-and-zmq-17024000-4389-4bae-9e4d-16302d20a5b6>`_\n\nContributing\n============\n**imageZMQ** is still in active development. I welcome open issues and\npull requests, but because the programs are still evolving, it is best to\nopen an issue for some discussion before submitting pull requests. We can\nexchange ideas about your potential pull request and open a development branch\nwhere you can develop your code and get feedback and testing help from myself\nand others. **imageZMQ** is used in my own long running projects and the\nprojects of others, so backwards compatibility with the existing API is\nimportant.\n\nContributors\n============\nThanks for all contributions big and small. Some significant ones:\n\n+------------------------+-----------------+----------------------------------------------------------+\n| **Contribution**       | **Name**        | **GitHub**                                               |\n+------------------------+-----------------+----------------------------------------------------------+\n| Initial code & docs    | Jeff Bass       | `@jeffbass <https://github.com/jeffbass>`_               |\n+------------------------+-----------------+----------------------------------------------------------+\n| Added PUB / SUB option | Maksym          | `@bigdaddymax <https://github.com/bigdaddymax>`_         |\n+------------------------+-----------------+----------------------------------------------------------+\n| HTTP Streaming example | Maksym          | `@bigdaddymax <https://github.com/bigdaddymax>`_         |\n+------------------------+-----------------+----------------------------------------------------------+\n| Fast PUB / SUB example | Philipp Schmidt | `@philipp-schmidt <https://github.com/philipp-schmidt>`_ |\n+------------------------+-----------------+----------------------------------------------------------+\n\nHelpful Forks of imageZMQ\n=========================\nSome users have come up with Forks of **imageZMQ** that I think will be helpful\nto others, either by using their code or reading their changed code. If\nyou have developed a fork of **imageZMQ** that demonstrates a concept that\nwould be helpful to others, please open an issue describing your fork so we\ncan have a discussion first rather than opening a pull request. Thanks!\n\n+----------------------------+------------+----------------------------------------------------------------------+\n| **Helpful Fork**           | **Name**   | **GitHub repository of fork**                                        |\n+----------------------------+------------+----------------------------------------------------------------------+\n| Add timeouts to image      | Pat Ryan   | `@youngsoul <https://github.com/youngsoul/imagezmq>`_ See CHANGES.md |\n| sender to fix restarts or  |            |                                                                      |\n| non-response of ImageHub   |            |                                                                      |\n+----------------------------+------------+----------------------------------------------------------------------+\n\nAcknowledgements and Thank Yous\n===============================\n- **ZeroMQ** is a great messaging library with great documentation\n  at `ZeroMQ.org <http://zeromq.org/>`_.\n- **PyZMQ** serialization examples provided a starting point for **imageZMQ**. See the\n  `PyZMQ documentation <https://pyzmq.readthedocs.io/en/latest/index.html>`_.\n- **OpenCV** and its Python bindings provide great scaffolding for computer\n  vision projects large or small: `OpenCV.org <https://opencv.org/>`_.\n- **PyImageSearch.com** is the best resource for installing OpenCV and its Python\n  bindings. Adrian Rosebrock provides many practical OpenCV techniques with\n  tutorials, code examples, blogs\n  and books at `PyImageSearch.com <https://www.pyimagesearch.com/>`_. Installing\n  OpenCV on my Raspberry Pi computers, Macs and Linux boxes went from\n  frustrating to easy thanks to his tutorials. I also learned a **LOT** about\n  computer vision methods and techniques by taking his PyImageSearch Gurus\n  course. Highly recommended.\n- **imutils** is a collection of Python classes and methods that allows computer\n  vision programs using OpenCV to be cleaner and more compact. It has a very\n  helpful threaded image reader for Raspberry PiCamera modules or webcams. It\n  allowed me to shorten my camera reading programs on the Raspberry Pi by half:\n  `imutils on GitHub <https://github.com/jrosebr1/imutils>`_. **imutils** is an\n  open source project authored by Adrian Rosebrock.\n"
 },
 {
  "repo": "fendouai/OpenCVTutorials",
  "language": "Python",
  "readme_contents": "# \u5199\u5728\u524d\u9762\u7684\u8bdd\n\n**OpenCV**\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7ecf\u5178\u7684\u4e13\u7528\u5e93\uff0c\u5176\u652f\u6301\u591a\u8bed\u8a00\u3001\u8de8\u5e73\u53f0\uff0c\u529f\u80fd\u5f3a\u5927\u3002**OpenCV-Python**\u4e3aOpenCV\u63d0\u4f9b\u4e86Python\u63a5\u53e3\uff0c\u4f7f\u5f97\u4f7f\u7528\u8005\u5728Python\u4e2d\u80fd\u591f\u8c03\u7528C/C++\uff0c\u5728\u4fdd\u8bc1\u6613\u8bfb\u6027\u548c\u8fd0\u884c\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u6240\u9700\u7684\u529f\u80fd\u3002\n\n**OpenCV-Python Tutorials**\u662f\u5b98\u65b9\u63d0\u4f9b\u7684\u6587\u6863\uff0c\u5176\u5185\u5bb9\u5168\u9762\u3001\u7b80\u5355\u6613\u61c2\uff0c\u4f7f\u5f97\u521d\u5b66\u8005\u80fd\u591f\u5feb\u901f\u4e0a\u624b\u4f7f\u7528\u30022014\u5e74\u6bb5\u529b\u8f89\u5728\u5f53\u65f6\u5df2\u7ffb\u8bd1\u8fc7OpenCV3.0\uff0c\u4f46\u65f6\u9694\u4e94\u5e74\uff0c\u5982\u4eca\u7684**OpenCV4.1**\u4e2d\u8bb8\u591a\u51fd\u6570\u548c\u5185\u5bb9\u5df2\u7ecf\u6709\u6240\u66f4\u65b0\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u8be5\u5b98\u65b9\u6587\u6863\u518d\u8fdb\u884c\u4e00\u6b21\u7ffb\u8bd1\u3002\n\n\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u96be\u514d\u6709\u6240\u758f\u6f0f\uff0c\u5982\u53d1\u73b0\u9519\u8bef\uff0c\u5e0c\u671b\u5927\u5bb6\u6307\u51fa\uff0c\u8c22\u8c22\u652f\u6301\u3002\n\nOpenCV-Python Tutorials\u5b98\u65b9\u6587\u6863\uff1ahttps://docs.opencv.org/4.1.2/d6/d00/tutorial_py_root.html\n\n# \u76ee\u5f55\n\n### [OpenCV\u4e2d\u6587\u5b98\u65b9\u6587\u6863](http://www.woshicver.com/.)\n\n*   <span class=\"caption-text\">OpenCV\u7b80\u4ecb</span>\n\n    * [0_OpenCV-Python Tutorials](http://www.woshicver.com/FirstSection/0_OpenCV-Python%20Tutorials/)\n    \n*   <span class=\"caption-text\">OpenCV\u5b89\u88c5</span>\n\n    *   [1_1_OpenCV-Python\u6559\u7a0b\u7b80\u4ecb](http://www.woshicver.com/SecondSection/1_1_OpenCV-Python\u6559\u7a0b\u7b80\u4ecb/)\n    *   [1_2_\u5728Windows\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_2_\u5728Windows\u4e2d\u5b89\u88c5OpenCV-Python/)\n    *   [1_3_\u5728Fedora\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_3_\u5728Fedora\u4e2d\u5b89\u88c5OpenCV-Python/)\n    *   [1_4_\u5728Ubuntu\u4e2d\u5b89\u88c5OpenCV-Python](http://www.woshicver.com/SecondSection/1_4_\u5728Ubuntu\u4e2d\u5b89\u88c5OpenCV-Python/)\n    \n*   <span class=\"caption-text\">OpenCV\u4e2d\u7684GUI\u7279\u6027</span>\n\n    *   [2_1_\u56fe\u50cf\u5165\u95e8](http://www.woshicver.com/ThirdSection/2_1_\u56fe\u50cf\u5165\u95e8/)\n    *   [2_2_\u89c6\u9891\u5165\u95e8](http://www.woshicver.com/ThirdSection/2_2_\u89c6\u9891\u5165\u95e8/)\n    *   [2_3_OpenCV\u4e2d\u7684\u7ed8\u56fe\u529f\u80fd](http://www.woshicver.com/ThirdSection/2_3_OpenCV\u4e2d\u7684\u7ed8\u56fe\u529f\u80fd/)\n    *   [2_4_\u9f20\u6807\u4f5c\u4e3a\u753b\u7b14](http://www.woshicver.com/ThirdSection/2_4_\u9f20\u6807\u4f5c\u4e3a\u753b\u7b14/)\n    *   [2_5_\u8f68\u8ff9\u680f\u4f5c\u4e3a\u8c03\u8272\u677f](http://www.woshicver.com/ThirdSection/2_5_\u8f68\u8ff9\u680f\u4f5c\u4e3a\u8c03\u8272\u677f/)\n    \n*   <span class=\"caption-text\">\u6838\u5fc3\u64cd\u4f5c</span>\n\n    *   [3_1_\u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c](http://www.woshicver.com/FourthSection/3_1_\u56fe\u50cf\u7684\u57fa\u672c\u64cd\u4f5c/)\n    *   [3_2_\u56fe\u50cf\u4e0a\u7684\u7b97\u6cd5\u8fd0\u7b97](http://www.woshicver.com/FourthSection/3_2_\u56fe\u50cf\u4e0a\u7684\u7b97\u6cd5\u8fd0\u7b97/)\n    *   [3_3_\u6027\u80fd\u8861\u91cf\u548c\u63d0\u5347\u6280\u672f](http://www.woshicver.com/FourthSection/3_3_\u6027\u80fd\u8861\u91cf\u548c\u63d0\u5347\u6280\u672f/)\n    \n*   <span class=\"caption-text\">OpenCV\u4e2d\u7684\u56fe\u50cf\u5904\u7406</span>\n\n    *   [4_1_\u6539\u53d8\u989c\u8272\u7a7a\u95f4](http://www.woshicver.com/FifthSection/4_1_\u6539\u53d8\u989c\u8272\u7a7a\u95f4/)\n    *   [4_2_\u56fe\u50cf\u51e0\u4f55\u53d8\u6362](http://www.woshicver.com/FifthSection/4_2_\u56fe\u50cf\u51e0\u4f55\u53d8\u6362/)\n    *   [4_3_\u56fe\u50cf\u9608\u503c](http://www.woshicver.com/FifthSection/4_3_\u56fe\u50cf\u9608\u503c/)\n    *   [4_4_\u56fe\u50cf\u5e73\u6ed1](http://www.woshicver.com/FifthSection/4_4_\u56fe\u50cf\u5e73\u6ed1/)\n    *   [4_5_\u5f62\u6001\u8f6c\u6362](http://www.woshicver.com/FifthSection/4_5_\u5f62\u6001\u8f6c\u6362/)\n    *   [4_6_\u56fe\u50cf\u68af\u5ea6](http://www.woshicver.com/FifthSection/4_6_\u56fe\u50cf\u68af\u5ea6/)\n    *   [4_7_Canny\u8fb9\u7f18\u68c0\u6d4b](http://www.woshicver.com/FifthSection/4_7_Canny\u8fb9\u7f18\u68c0\u6d4b/)\n    *   [4_8_\u56fe\u50cf\u91d1\u5b57\u5854](http://www.woshicver.com/FifthSection/4_8_\u56fe\u50cf\u91d1\u5b57\u5854/)\n    *   [4_9_1_OpenCV\u4e2d\u7684\u8f6e\u5ed3](http://www.woshicver.com/FifthSection/4_9_1_OpenCV\u4e2d\u7684\u8f6e\u5ed3/)\n    *   [4_9_2_\u8f6e\u5ed3\u7279\u5f81](http://www.woshicver.com/FifthSection/4_9_2_\u8f6e\u5ed3\u7279\u5f81/)\n    *   [4_9_3_\u8f6e\u5ed3\u5c5e\u6027](http://www.woshicver.com/FifthSection/4_9_3_\u8f6e\u5ed3\u5c5e\u6027/)\n    *   [4_9_4_\u8f6e\u5ed3\uff1a\u66f4\u591a\u5c5e\u6027](http://www.woshicver.com/FifthSection/4_9_4_\u8f6e\u5ed3\uff1a\u66f4\u591a\u5c5e\u6027/)\n    *   [4_9_5_\u8f6e\u5ed3\u5206\u5c42](http://www.woshicver.com/FifthSection/4_9_5_\u8f6e\u5ed3\u5206\u5c42/)\n    *   [4_10_1_\u76f4\u65b9\u56fe-1\uff1a\u67e5\u627e\uff0c\u7ed8\u5236\uff0c\u5206\u6790](http://www.woshicver.com/FifthSection/4_10_1_\u76f4\u65b9\u56fe-1\uff1a\u67e5\u627e\uff0c\u7ed8\u5236\uff0c\u5206\u6790/)\n    *   [4_10_2_\u76f4\u65b9\u56fe-2\uff1a\u76f4\u65b9\u56fe\u5747\u8861](http://www.woshicver.com/FifthSection/4_10_2_\u76f4\u65b9\u56fe-2\uff1a\u76f4\u65b9\u56fe\u5747\u8861/)\n    *   [4_10_3_\u76f4\u65b9\u56fe3\uff1a\u4e8c\u7ef4\u76f4\u65b9\u56fe](http://www.woshicver.com/FifthSection/4_10_3_\u76f4\u65b9\u56fe3\uff1a\u4e8c\u7ef4\u76f4\u65b9\u56fe/)\n    *   [4_10_4_\u76f4\u65b9\u56fe-4\uff1a\u76f4\u65b9\u56fe\u53cd\u6295\u5f71](http://www.woshicver.com/FifthSection/4_10_4_\u76f4\u65b9\u56fe-4\uff1a\u76f4\u65b9\u56fe\u53cd\u6295\u5f71/)\n    *   [4_11_\u5085\u91cc\u53f6\u53d8\u6362](http://www.woshicver.com/FifthSection/4_11_\u5085\u91cc\u53f6\u53d8\u6362/)\n    *   [4_12_\u6a21\u677f\u5339\u914d](http://www.woshicver.com/FifthSection/4_12_\u6a21\u677f\u5339\u914d/)\n    *   [4_13_\u970d\u592b\u7ebf\u53d8\u6362](http://www.woshicver.com/FifthSection/4_13_\u970d\u592b\u7ebf\u53d8\u6362/)\n    *   [4_14_\u970d\u592b\u5708\u53d8\u6362](http://www.woshicver.com/FifthSection/4_14_\u970d\u592b\u5708\u53d8\u6362/)\n    *   [4_15_\u56fe\u50cf\u5206\u5272\u4e0e\u5206\u6c34\u5cad\u7b97\u6cd5](http://www.woshicver.com/FifthSection/4_15_\u56fe\u50cf\u5206\u5272\u4e0e\u5206\u6c34\u5cad\u7b97\u6cd5/)\n    *   [4_16_\u4ea4\u4e92\u5f0f\u524d\u666f\u63d0\u53d6\u4f7f\u7528GrabCut\u7b97\u6cd5](http://www.woshicver.com/FifthSection/4_16_\u4ea4\u4e92\u5f0f\u524d\u666f\u63d0\u53d6\u4f7f\u7528GrabCut\u7b97\u6cd5/)\n    \n*   <span class=\"caption-text\">\u7279\u5f81\u68c0\u6d4b\u4e0e\u63cf\u8ff0</span>\n\n    *   [5_1_\u7406\u89e3\u7279\u5f81](http://www.woshicver.com/Sixth/5_1_\u7406\u89e3\u7279\u5f81/)\n    *   [5_2_\u54c8\u91cc\u65af\u89d2\u68c0\u6d4b](http://www.woshicver.com/Sixth/5_2_\u54c8\u91cc\u65af\u89d2\u68c0\u6d4b/)\n    *   [5_3_Shi-Tomasi\u62d0\u89d2\u63a2\u6d4b\u5668\u548c\u826f\u597d\u7684\u8ddf\u8e2a\u529f\u80fd](http://www.woshicver.com/Sixth/5_3_Shi-Tomasi\u62d0\u89d2\u63a2\u6d4b\u5668\u548c\u826f\u597d\u7684\u8ddf\u8e2a\u529f\u80fd/)\n    *   [5_4_SIFT\uff08\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362\uff09\u7b80\u4ecb](http://www.woshicver.com/Sixth/5_4_SIFT\uff08\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362\uff09\u7b80\u4ecb/)\n    *   [5_5_SURF\u7b80\u4ecb\uff08\u52a0\u901f\u7684\u5f3a\u5927\u529f\u80fd\uff09](http://www.woshicver.com/Sixth/5_5_SURF\u7b80\u4ecb\uff08\u52a0\u901f\u7684\u5f3a\u5927\u529f\u80fd\uff09/)\n    *   [5_6_\u7528\u4e8e\u89d2\u70b9\u68c0\u6d4b\u7684FAST\u7b97\u6cd5](http://www.woshicver.com/Sixth/5_6_\u7528\u4e8e\u89d2\u70b9\u68c0\u6d4b\u7684FAST\u7b97\u6cd5/)\n    *   [5_7_BRIEF\uff08\u4e8c\u8fdb\u5236\u7684\u9c81\u68d2\u72ec\u7acb\u57fa\u672c\u7279\u5f81\uff09](http://www.woshicver.com/Sixth/5_7_BRIEF\uff08\u4e8c\u8fdb\u5236\u7684\u9c81\u68d2\u72ec\u7acb\u57fa\u672c\u7279\u5f81\uff09/)\n    *   [5_8_ORB\uff08\u5b9a\u5411\u5feb\u901f\u548c\u65cb\u8f6c\u7b80\u8981\uff09](http://www.woshicver.com/Sixth/5_8_ORB\uff08\u5b9a\u5411\u5feb\u901f\u548c\u65cb\u8f6c\u7b80\u8981\uff09/)\n    *   [5_9_\u7279\u5f81\u5339\u914d](http://www.woshicver.com/Sixth/5_9_\u7279\u5f81\u5339\u914d/)\n    *   [5_10_\u7279\u5f81\u5339\u914d+\u5355\u5e94\u6027\u67e5\u627e\u5bf9\u8c61](http://www.woshicver.com/Sixth/5_10_\u7279\u5f81\u5339\u914d+\u5355\u5e94\u6027\u67e5\u627e\u5bf9\u8c61/)\n    \n*   <span class=\"caption-text\">\u89c6\u9891\u5206\u6790</span>\n\n    *   [6_1_\u5982\u4f55\u4f7f\u7528\u80cc\u666f\u5206\u79bb\u65b9\u6cd5](http://www.woshicver.com/Seventh/6_1_\u5982\u4f55\u4f7f\u7528\u80cc\u666f\u5206\u79bb\u65b9\u6cd5/)\n    *   [6_2_Meanshift\u548cCamshift](http://www.woshicver.com/Seventh/6_2_Meanshift\u548cCamshift/)\n    *   [6_3_\u5149\u6d41](http://www.woshicver.com/Seventh/6_3_\u5149\u6d41/)\n    \n*   <span class=\"caption-text\">\u76f8\u673a\u6821\u51c6\u548c3D\u91cd\u5efa</span>\n\n    *   [7_1_\u76f8\u673a\u6821\u51c6](http://www.woshicver.com/Eighth/7_1_\u76f8\u673a\u6821\u51c6/)\n    *   [7_2_\u59ff\u6001\u4f30\u8ba1](http://www.woshicver.com/Eighth/7_2_\u59ff\u6001\u4f30\u8ba1/)\n    *   [7_3_\u5bf9\u6781\u51e0\u4f55](http://www.woshicver.com/Eighth/7_3_\u5bf9\u6781\u51e0\u4f55/)\n    *   [7_4_\u7acb\u4f53\u56fe\u50cf\u7684\u6df1\u5ea6\u56fe](http://www.woshicver.com/Eighth/7_4_\u7acb\u4f53\u56fe\u50cf\u7684\u6df1\u5ea6\u56fe/)\n    \n*   <span class=\"caption-text\">\u673a\u5668\u5b66\u4e60</span>\n\n    *   [8_1_\u7406\u89e3KNN](http://www.woshicver.com/Ninth/8_1_\u7406\u89e3KNN/)\n    *   [8_2_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cKNN](http://www.woshicver.com/Ninth/8_2_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cKNN/)\n    *   [8_3_\u7406\u89e3SVM](http://www.woshicver.com/Ninth/8_3_\u7406\u89e3SVM/)\n    *   [8_4_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cSVM](http://www.woshicver.com/Ninth/8_4_\u4f7f\u7528OCR\u624b\u5199\u6570\u636e\u96c6\u8fd0\u884cSVM/)\n    *   [8_5_\u7406\u89e3K\u5747\u503c\u805a\u7c7b](http://www.woshicver.com/Ninth/8_5_\u7406\u89e3K\u5747\u503c\u805a\u7c7b/)\n    *   [8_6_OpenCV\u4e2d\u7684K\u5747\u503c](http://www.woshicver.com/Ninth/8_6_OpenCV\u4e2d\u7684K\u5747\u503c/)\n    \n*   <span class=\"caption-text\">\u8ba1\u7b97\u6444\u5f71\u5b66</span>\n\n    *   [9_1_\u56fe\u50cf\u53bb\u566a](http://www.woshicver.com/Tenth/9_1_\u56fe\u50cf\u53bb\u566a/)\n    *   [9_2_\u56fe\u50cf\u4fee\u8865](http://www.woshicver.com/Tenth/9_2_\u56fe\u50cf\u4fee\u8865/)\n    *   [9_3_\u9ad8\u52a8\u6001\u8303\u56f4](http://www.woshicver.com/Tenth/9_3_\u9ad8\u52a8\u6001\u8303\u56f4/)\n    \n*   <span class=\"caption-text\">\u76ee\u6807\u68c0\u6d4b</span>\n\n    *   [10_1_\u7ea7\u8054\u5206\u7c7b\u5668](http://www.woshicver.com/Eleventh/10_1_\u7ea7\u8054\u5206\u7c7b\u5668/)\n    *   [10_2_\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3](http://www.woshicver.com/Eleventh/10_2_\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3/)\n    \n*   <span class=\"caption-text\">OpenCV-Python Binding</span>\n\n    *   [11_1_OpenCV-Python Bindings](http://www.woshicver.com/Twelfth/11_1_OpenCV-Python%20Bindings/)\n\n# \u5173\u4e8e\n\nOpenCV \u4e2d\u6587\u5b98\u65b9\u6587\u6863\n\n[http://woshicver.com/](http://woshicver.com/)\n"
 },
 {
  "repo": "FaceAR/OpenFaceIOS",
  "language": "C++",
  "readme_contents": "\u6bd5\u4e1a\u5b63\u4e86\uff0c\u6700\u8fd1\u6bd4\u8f83\u5fd9\uff0c\u5148\u653e\u5728\u8fd9\u91cc\uff0c\u540e\u9762\u6709\u673a\u4f1a\u5728\u66f4\u65b0\uff0c\u62b1\u6b49\u4e86\u5404\u4f4d\u5b9d\u5b9d\u3002\n\n#OpenFaceIOS\n\nOpenFace is get from https://github.com/TadasBaltrusaitis/OpenFace, a state-of-the art open source tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. And I create an ios project. Delete Dlib,TBB, Only use OpenCV . And add Face Swap function, It can swap face to face or swap model to face. \n\nMore details about the project - http://www.cl.cam.ac.uk/research/rainbow/projects/openface/\nCode - https://github.com/TadasBaltrusaitis/OpenFace\n\n## Functionality\n\nThe system is capable of performing a number of facial analysis tasks:\n\n- Facial Landmark Detection\n\n![Sample facial landmark detection image](https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/multi_face_img.png)\n\n- Facial Landmark and head pose tracking (links to YouTube videos)\n\n<a href=\"https://www.youtube.com/watch?v=V7rV0uy7heQ\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/V7rV0uy7heQ/0.jpg\" alt=\"Multiple Face Tracking\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/watch?v=vYOa8Pif5lY\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/vYOa8Pif5lY/0.jpg\" alt=\"Multiple Face Tracking\" width=\"240\" height=\"180\" border=\"10\" /></a>\n\n- Gaze tracking (image of it in action)\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/gaze_ex.png\" height=\"378\" width=\"567\" >\n\n- Facial Feature Extraction (aligned faces and HOG features)\n\n![Sample aligned face and HOG image](https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/appearance.png)\n\n- IOS Facial Landmark Detection & head pose tracking & Gaze tracking\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen1.png\" height=\"382\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen2.png\" height=\"382\" width=\"222\" >\n\n- Face Swap\n\nUser\uff1aDooonut https://github.com/Dooonut\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen3.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen4.png\" height=\"282\" width=\"222\" >\n\n\n<img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen5.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen6.png\" height=\"282\" width=\"222\" ><img src=\"https://github.com/FaceAR/OpenFaceIOS/blob/master/imgs/KeeganRen7.png\" height=\"282\" width=\"222\" >\n"
 },
 {
  "repo": "sunglok/3dv_tutorial",
  "language": "C++",
  "readme_contents": "## An Invitation to 3D Vision: A Tutorial for Everyone\r\n_An Invitation to 3D Vision_ is an introductory tutorial on 3D vision (a.k.a. geometric vision or visual geometry or multi-view geometry).\r\nIt aims to make beginners understand basic theory of 3D vision and implement their own applications using [OpenCV][].\r\nIn addition to tutorial slides, example codes are provided in the purpose of education. They include simple but interesting and practical applications. The example codes are written as short as possible (mostly __less than 100 lines__) to be clear and easy to understand.\r\n\r\n* Download [tutorial slides](https://github.com/sunglok/3dv_tutorial/releases/download/misc/3dv_slides.pdf)\r\n* Download [example codes in a ZIP file](https://github.com/sunglok/3dv_tutorial/archive/master.zip)\r\n* Read [how to run example codes](https://github.com/sunglok/3dv_tutorial/blob/master/HOWTO_RUN.md)\r\n\r\n### What does its name come from?\r\n* The main title, _An Invitation to 3D Vision_, came from [a legendary book by Yi Ma, Stefano Soatto, Jana Kosecka, and Shankar S. Sastry](http://vision.ucla.edu/MASKS/). We wish that our tutorial will be the first gentle invitation card for beginners to 3D vision and its applications.\r\n* The subtitle, _for everyone_, was inspired from [Prof. Kim's online lecture](https://hunkim.github.io/ml/) (in Korean). Our tutorial is also intended not only for students and researchers in academia, but also for hobbyists and developers in industries. We tried to describe important and typical problems and their solutions in [OpenCV][]. We hope readers understand it easily without serious mathematical background.\r\n\r\n### Examples\r\n* __Single-view Geometry__\r\n  * Camera Projection Model\r\n    * Object Localization and Measurement: [object_localization.cpp][] (result: [image](https://drive.google.com/open?id=10Lche-1HHazDeohXEQK443ruDTAmIO4E))\r\n    * Image Formation: [image_formation.cpp][] (result: [image0](https://drive.google.com/file/d/0B_iOV9kV0whLY2luc05jZGlkZ2s/view), [image1](https://drive.google.com/file/d/0B_iOV9kV0whLS3M4S09ZZHpjTkU/view), [image2](https://drive.google.com/file/d/0B_iOV9kV0whLV2dLZHd0MmVkd28/view), [image3](https://drive.google.com/file/d/0B_iOV9kV0whLS1ZBR25WekpMYjA/view), [image4](https://drive.google.com/file/d/0B_iOV9kV0whLYVB0dm9Fc0dvRzQ/view))\r\n    * Geometric Distortion Correction: [distortion_correction.cpp][] (result: [video](https://www.youtube.com/watch?v=HKetupWh4V8))\r\n  * General 2D-3D Geometry\r\n    * Camera Calibration: [camera_calibration.cpp][] (result: [text](https://drive.google.com/file/d/0B_iOV9kV0whLZ0pDbWdXNWRrZ00/view))\r\n    * Camera Pose Estimation (Chessboard): [pose_estimation_chessboard.cpp][] (result: [video](https://www.youtube.com/watch?v=4nA1OQGL-ig))\r\n    * Camera Pose Estimation (Book): [pose_estimation_book1.cpp][]\r\n    * Camera Pose Estimation and Calibration: [pose_estimation_book2.cpp][]\r\n    * Camera Pose Estimation and Calibration w/o Initially Given Camera Parameters: [pose_estimation_book3.cpp][] (result: [video](https://www.youtube.com/watch?v=GYp4h0yyB3Y))\r\n* __Two-view Geometry__\r\n  * Planar 2D-2D Geometry (Projective Geometry)\r\n    * Perspective Distortion Correction: [perspective_correction.cpp][] (result: [original](https://drive.google.com/file/d/0B_iOV9kV0whLVlFpeFBzYWVadlk/view), [rectified](https://drive.google.com/file/d/0B_iOV9kV0whLMi1UTjN5QXhnWFk/view))\r\n    * Planar Image Stitching: [image_stitching.cpp][] (result: [image](https://drive.google.com/file/d/0B_iOV9kV0whLOEQzVmhGUGVEaW8/view))\r\n    * 2D Video Stabilization: [video_stabilization.cpp][] (result: [video](https://www.youtube.com/watch?v=be_dzYicEzI))\r\n  * General 2D-2D Geometry (Epipolar Geometry)\r\n    * Visual Odometry (Monocular, Epipolar Version): [vo_epipolar.cpp][]\r\n    * Triangulation (Two-view Reconstruction): [triangulation.cpp][]\r\n* __Multi-view Geometry__\r\n  * Bundle Adjustment\r\n    * Global Version: [bundle_adjustment_global.cpp][]\r\n    * Incremental Version: [bundle_adjustment_inc.cpp][]\r\n  * Structure-from-Motion\r\n    * Global SfM: [sfm_global.cpp][]\r\n    * Incremental SfM: [sfm_inc.cpp][]\r\n  * Feature-based Visual Odometry and SLAM\r\n    * Visual Odometry (Monocular, Epipolar Version): [vo_epipolar.cpp][]\r\n    * Visual Odometry (Stereo Version)\r\n    * Visual Odometry (Monocular, PnP and BA Version)\r\n    * Visual SLAM (Monocular Version)\r\n  * Direct Visual Odometry and SLAM\r\n    * Visual Odometry (Monocular, Direct Version)\r\n  * c.f. The above examples need [Ceres Solver][] for bundle adjustment.\r\n* __Correspondence Problem__\r\n  * Line Fitting with RANSAC: [line_fitting_ransac.cpp][]\r\n  * Line Fitting with M-estimators: [line_fitting_m_est.cpp][]\r\n* **Appendix**\r\n  * Line Fitting\r\n  * Planar Homograph Estimation\r\n  * Fundamental Matrix Estimation\r\n\r\n### Dependencies\r\n* [OpenCV][] (> 3.0.0, 3-clause BSD License)\r\n  * _OpenCV_ is a base of all example codes for basic computer vision algorithms, linear algebra, image/video manipulation, and GUI.\r\n* [Ceres Solver][] (3-clause BSD License): A numerical optimization library\r\n  * _Ceres Solver_ is additionally used by m-estimator, bundle adjustment, structure-from-motion, and visual odometry/SLAM.\r\n\r\n### License\r\n* [Beerware](http://en.wikipedia.org/wiki/Beerware)\r\n\r\n### Authors\r\n* [Sunglok Choi](http://sites.google.com/site/sunglok/) (sunglok AT hanmail DOT net)\r\n\r\n### Acknowledgement\r\nThe authors thank the following contributors and projects.\r\n\r\n* [Jae-Yeong Lee](https://sites.google.com/site/roricljy/): He motivated many examples.\r\n* [Giseop Kim](https://sites.google.com/view/giseopkim): He contributed the initial version of SfM codes with [Toy-SfM](https://github.com/royshil/SfM-Toy-Library) and [cvsba](https://www.uco.es/investiga/grupos/ava/node/39).\r\n* [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/): The KITTI odometry dataset #07 was used to demonstrate visual odometry and SLAM.\r\n* [Russell Hewett](https://courses.engr.illinois.edu/cs498dh3/fa2013/projects/stitching/ComputationalPhotograph_ProjectStitching.html): His two hill images were used to demonstrate image stitching.\r\n* [Kang Li](http://www.cs.cmu.edu/~kangli/code/Image_Stabilizer.html): His shaking CCTV video was used to demonstrate video stabilization.\r\n* [Richard Blais](http://www.richardblais.net/): His book cover and video in [the OpenCV tutorial](http://docs.opencv.org/3.1.0/dc/d16/tutorial_akaze_tracking.html) were used to demonstrate camera pose estimation and augmented reality.\r\n\r\n[OpenCV]: http://opencv.org/\r\n[Ceres Solver]: http://ceres-solver.org/\r\n\r\n[object_localization.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/object_localization.cpp\r\n[image_formation.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/image_formation.cpp\r\n[distortion_correction.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/distortion_correction.cpp\r\n[camera_calibration.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/camera_calibration.cpp\r\n[pose_estimation_chessboard.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_chessboard.cpp\r\n[pose_estimation_book1.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book1.cpp\r\n[pose_estimation_book2.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book2.cpp\r\n[pose_estimation_book3.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/pose_estimation_book3.cpp\r\n[perspective_correction.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/perspective_correction.cpp\r\n[image_stitching.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/image_stitching.cpp\r\n[video_stabilization.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/video_stabilization.cpp\r\n[vo_epipolar.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/vo_epipolar.cpp\r\n[triangulation.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/triangulation.cpp\r\n[bundle_adjustment_global.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/bundle_adjustment_global.cpp\r\n[bundle_adjustment_inc.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/bundle_adjustment_inc.cpp\r\n[sfm_global.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/sfm_global.cpp\r\n[sfm_inc.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/sfm_inc.cpp\r\n[line_fitting_ransac.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/line_fitting_ransac.cpp\r\n[line_fitting_m_est.cpp]: https://github.com/sunglok/3dv_tutorial/blob/master/src/line_fitting_m_est.cpp\r\n"
 },
 {
  "repo": "EdjeElectronics/OpenCV-Playing-Card-Detector",
  "language": "Python",
  "readme_contents": "# OpenCV-Playing-Card-Detector\nThis is a Python program that uses OpenCV to detect and identify playing cards from a PiCamera video feed on a Raspberry Pi. Check out the YouTube video that describes what it does and how it works:\n\nhttps://www.youtube.com/watch?v=m-QPjO-2IkA\n\n## Usage\nDownload this repository to a directory and run CardDetector.py from that directory. Cards need to be placed on a dark background for the detector to work. Press 'q' to end the program.\n\nThe program was originally designed to run on a Raspberry Pi with a Linux OS, but it can also be run on Windows 7/8/10. To run on Windows, download and install Anaconda (https://www.anaconda.com/download/, Python 3.6 version), launch Anaconda Prompt, and execute the program by launching IDLE (type \"idle\" and press ENTER in the prompt) and opening/running the CardDetector.py file in IDLE. The Anaconda environment comes with the opencv and numpy packages installed, so you don't need to install those yourself. If you are running this on Windows, you will also need to change the program to use a USB camera, as described below.\n\nThe program allows you to use either a PiCamera or a USB camera. If using a USB camera, change line 38 in CardDetector.py to:\n```\nvideostream = VideoStream.VideoStream((IM_WIDTH,IM_HEIGHT),FRAME_RATE,2,0).start()\n```\n\nThe card detector will work best if you use isolated rank and suit images generated from your own cards. To do this, run Rank_Suit_Isolator.py to take pictures of your cards. It will ask you to take a picture of an Ace, then a Two, and so on. Then, it will ask you to take a picture of one card from each of the suits (Spades, Diamonds, Clubs, Hearts). As you take pictures of the cards, the script will automatically isolate the rank or suit and save them in the Card_Imgs directory (overwriting the existing images).\n\n\n## Files\nCardDetector.py contains the main script\n\nCards.py has classes and functions that are used by CardDetector.py\n\nPiVideoStream.py creates a video stream from the PiCamera, and is used by CardDetector.py\n\nRank_Suit_Isolator.py is a standalone script that can be used to isolate the rank and suit from a set of cards to create train images\n\nCard_Imgs contains all the train images of the card ranks and suits\n\n## Dependencies\nPython 3.6\n\nOpenCV-Python 3.2.0 and numpy 1.8.2:\nSee https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/\nfor how to build and install OpenCV-Python on the Raspberry Pi\n\npicamera library:\n```\nsudo apt-get update\nsudo apt-get install python-picamera python3-picamera\n```\n\n\n"
 },
 {
  "repo": "chandrikadeb7/Face-Mask-Detection",
  "language": "Python",
  "readme_contents": "<h1 align=\"center\">Face Mask Detection</h1>\n\n<div align= \"center\">\n  <h4>Face Mask Detection system built with OpenCV, Keras/TensorFlow using Deep Learning and Computer Vision concepts in order to detect face masks in static images as well as in real-time video streams.</h4>\n</div>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n![Python](https://img.shields.io/badge/python-v3.6+-blue.svg)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/chandrikadeb7/Face-Mask-Detection/issues)\n[![Forks](https://img.shields.io/github/forks/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/network/members)\n[![Stargazers](https://img.shields.io/github/stars/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/stargazers)\n[![Issues](https://img.shields.io/github/issues/chandrikadeb7/Face-Mask-Detection.svg?logo=github)](https://github.com/chandrikadeb7/Face-Mask-Detection/issues)\n[![MIT License](https://img.shields.io/github/license/chandrikadeb7/Face-Mask-Detection.svg?style=flat-square)](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/LICENSE)\n[![LinkedIn](https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555)](https://www.linkedin.com/in/chandrika-deb/)\n\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n![Live Demo](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Demo.gif)\n\n\n\n## :innocent: Motivation\nIn the present scenario due to Covid-19, there is no efficient face mask detection applications which are now in high demand for transportation means, densely populated areas, residential districts, large-scale manufacturers and other enterprises to ensure safety. Also, the absence of large datasets of __\u2018with_mask\u2019__ images has made this task more cumbersome and challenging. \n\n \n## :hourglass: Project Demo\n:movie_camera: [YouTube Demo Link](https://www.youtube.com/watch?v=AAkNyZlUae0)\n\n:computer: [Dev Link](https://dev.to/chandrikadeb7/face-mask-detection-my-major-project-3fj3)\n\n[![Already deployed version](https://raw.githubusercontent.com/vasantvohra/TrashNet/master/hr.svg)](https://face-mask--detection-app.herokuapp.com/)\n\n\n\n<p align=\"center\"><img src=\"https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Screen%20Shot%202020-05-14%20at%208.49.06%20PM.png\" width=\"700\" height=\"400\"></p>\n\n\n## :warning: TechStack/framework used\n\n- [OpenCV](https://opencv.org/)\n- [Caffe-based face detector](https://caffe.berkeleyvision.org/)\n- [Keras](https://keras.io/)\n- [TensorFlow](https://www.tensorflow.org/)\n- [MobileNetV2](https://arxiv.org/abs/1801.04381)\n\n## :star: Features\nOur face mask detector didn't use any morphed masked images dataset. The model is accurate, and since we used the MobileNetV2 architecture, it\u2019s also\u00a0computationally efficient and thus making it easier to deploy the model to embedded systems (Raspberry Pi, Google Coral, etc.).\n\nThis system can therefore be used in real-time applications which require face-mask detection for safety purposes due to the outbreak of Covid-19. This project can be integrated with embedded systems for application in airports, railway stations, offices, schools, and public places to ensure that public safety guidelines are followed.\n\n## :file_folder: Dataset\nThe dataset used can be downloaded here - [Click to Download](https://drive.google.com/drive/folders/1XDte2DL2Mf_hw4NsmGst7QtYoU7sMBVG?usp=sharing)\n\nThis dataset consists of\u00a0__3835 images__\u00a0belonging to two classes:\n*\t__with_mask: 1916 images__\n*\t__without_mask: 1919 images__\n\nThe images used were real images of faces wearing masks. The images were collected from the following sources:\n\n* __Bing Search API__ ([See Python script](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/search.py))\n* __Kaggle datasets__ \n* __RMFD dataset__ ([See here](https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset))\n\n## :key: Prerequisites\n\nAll the dependencies and required libraries are included in the file <code>requirements.txt</code> [See here](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/requirements.txt)\n\n## \ud83d\ude80&nbsp; Installation\n1. Clone the repo\n```\n$ git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git\n```\n\n2. Change your directory to the cloned repo and create a Python virtual environment named 'test'\n```\n$ mkvirtualenv test\n```\n\n3. Now, run the following command in your Terminal/Command Prompt to install the libraries required\n```\n$ pip3 install -r requirements.txt\n```\n\n## :bulb: Working\n\n1. Open terminal. Go into the cloned project directory and type the following command:\n```\n$ python3 train_mask_detector.py --dataset dataset\n```\n\n2. To detect face masks in an image type the following command: \n```\n$ python3 detect_mask_image.py --image images/pic1.jpeg\n```\n\n3. To detect face masks in real-time video streams type the following command:\n```\n$ python3 detect_mask_video.py \n```\n## :key: Results\n\n#### Our model gave 93% accuracy for Face Mask Detection after training via <code>tensorflow-gpu==2.0.0</code>\n\n![](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/Readme_images/Screenshot%202020-06-01%20at%209.48.27%20PM.png)\n\n#### We got the following accuracy/loss training curve plot\n![](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/plot.png)\n\n## Streamlit app\n\nFace Mask Detector webapp using Tensorflow & Streamlit\n\ncommand\n```\n$ streamlit run app.py \n```\n## Images\n\n<p align=\"center\">\n  <img src=\"Readme_images/1.PNG\">\n</p>\n<p align=\"center\">Upload Images</p>\n\n<p align=\"center\">\n  <img src=\"Readme_images/2.PNG\">\n</p>\n<p align=\"center\">Results</p>\n\n## :clap: And it's done!\nFeel free to mail me for any doubts/query \n:email: chandrikadeb7@gmail.com\n\n## :handshake: Contribution\nFeel free to **file a new issue** with a respective title and description on the the [Face-Mask-Detection](https://github.com/chandrikadeb7/Face-Mask-Detection/issues) repository. If you already found a solution to your problem, **I would love to review your pull request**! \n\n## :heart: Owner\nMade with :heart:&nbsp;  by [Chandrika Deb](https://github.com/chandrikadeb7)\n\n## :+1: Credits\n* [https://www.pyimagesearch.com/](https://www.pyimagesearch.com/)\n* [https://www.tensorflow.org/tutorials/images/transfer_learning](https://www.tensorflow.org/tutorials/images/transfer_learning)\n\n## :eyes: License\nMIT \u00a9 [Chandrika Deb](https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/LICENSE)\n"
 },
 {
  "repo": "AlphaQi/MTCNN-light",
  "language": "C++",
  "readme_contents": "# MTCNN-light\n## Introduction\nthis repository is the implementation of MTCNN with no framework,  Just need opencv and openblas.  \n\"Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks\", implemented with C++\uff0cno framework  \nit is very easy for you to use.  \nit is can be a part of your project with no framework, like caffe and mxnet.  \nit is real time for VGA, and you can improve it's runtime.  \n\n## Time Cost\nThe average time cost is about 68ms per frame(640,480).The result is generated by testing a camera. mini_size is 40     \ncpu   i5-4590  \nos    windows10   64bit\n\n## Dependencies\nopencv  2.0+  \nopenblas  \n\n##ubuntu   \n### opencv    \nyou can find many tutorials.\n\n### openblas\nIt is very easy to install  \n1 download the source code from https://github.com/xianyi/OpenBLAS  \n2 Extract it and type \"cd xxx\", xxx means the directory  \n3 type \"make\"   \n4 type \"make install PREFIX=your_installation_directory\"   \n\n### if you don't have cmake \napt-get install cmake\n\n## usage\ncd root_directory   \nvim CMakeLists.txt   \nchange   include_directories(the_directory_of_openblas_include_of_yours)  \nchange   link_directories(the_directory_of_openblas_lib_of_yours)  \nsave and exit\n\ncmake .   \nmake   \n./main   \n\n## for windows    \n### opencv and openblas    \nthere is binary packages of openblas for windows, you just need download it   \nBut you should to be careful, if you download the 64bit ,you need configure    \nthe opencv and vs project environment with 64bit, don't choose x86\n"
 },
 {
  "repo": "hihozhou/php-opencv",
  "language": "C++",
  "readme_contents": "# PHP-OPENCV - PHP extension for Opencv\n\n\n\n[![Build Status](https://travis-ci.org/hihozhou/php-opencv.svg?branch=master)](https://travis-ci.org/hihozhou/php-opencv) [![Minimum PHP Version](https://img.shields.io/badge/php-%3E%3D%207.0-8892BF.svg)](https://php.net/)\n\n\n> **\u26a0\ufe0f\u26a0\ufe0f NO LONGER IN ACTIVE DEVELOPMENT | \u9879\u76ee\u4e0d\u518d\u7ef4\u62a4 \u26a0\ufe0f\u26a0\ufe0f** \n>Due to personal scheduling reasons, the project has been suspended for maintenance and it is hoped that the project code will provide useful value.\n\n\n## Document\n- [PHP OpenCV Document](https://doc.phpopencv.org)\n\n\n## Requirements\n\n- OpenCV 4.0.0+\n- PHP7.0+\n\n\n\n## Installation\n\n### Use OpenCV docker(commendatory)\n\nIf you don't know how to install OpenCV, you can use my OpenCV docker image(only 300M, including opencv_contrib).\n\n```bash\ndocker pull hihozhou/php-opencv\n```\n\n### Compile and install php-opencv extension\n\n```bash\ngit clone https://github.com/hihozhou/php-opencv.git\ncd php-opencv\nphpize\n./configure --with-php-config=your php-config path\nmake\nmake install\n```\n\n## Configure\n\nphp.ini\n\n```\nextension=\"your opencv.so path\"\n```\n## Example\n\n### LBPH face recognition\n\n```php\nuse CV\\Face\\LBPHFaceRecognizer;\n//use ...;\n\n$src = imread('facePic.jpg');\n$gray = cvtColor($src, COLOR_BGR2GRAY);\nequalizeHist($gray, $gray);\n$faceRecognizer = LBPHFaceRecognizer::create();\n/* ... */ //get $images and $labels for train\n$faceRecognizer->train($images, $labels);//How to get $image and $labels, see the document\n/* ... */ //Face detection using CascadeClassifier\n$faceLabel = $faceRecognizer->predict($gray);\n/* ... */ //draw face and name\n```\n\nresult:\n\n![predict](tests/face_recognizer.jpg)\n\n\n### Image Processing\n\n```php\n//Obama.php\nuse function CV\\{ imread, imshow, waitkey, namedWindow};\n\n$im = imread('Obama.png');//load image\nnamedWindow('This is Obama id card',WINDOW_FULLSCREEN);//create window\nimshow('This is Obama id card',$im);//show image on window\n\nwaitkey(0);\n\n```\n\nresult:\n\n![Obama](tests/Obama.png)\n\nLoad image by gray\n\n```php\n$gray = imread('Obama.png',IMREAD_GRAYSCALE);\n//or\nuse  function CV\\{ cvtColor};\n$gray = cvtColor($im, COLOR_BGR2GRAY);\n\n```\n\n![Obama_gray](tests/Obama_gray.png)\n\n\nYou can draw something.  \ne.g:  \n\n```php\nuse CV\\{Mat,Scalar, Point, Size};\nuse function CV\\{ellipse, imwrite, waitKey};\nuse const CV\\{CV_8UC3};\n\n$windowWidth = 600;\n$thickness = 2;\n$lineType = 8;\n$matScalar = new Scalar(0,0,0);\n$mat = new Mat($windowWidth, $windowWidth, CV_8UC3, $matScalar);\n$point=new Point($windowWidth/2, $windowWidth/2);\n$size=new Size($windowWidth/4, $windowWidth/16);\n$scalar=new Scalar(255, 129, 0);\nfor($i = 0; $i <= 360; $i += 45){\n    ellipse($mat,$point,$size,$i,0,360,$scalar,$thickness,$lineType);\n}\nimwrite('./tests/ellipse.png',$mat);\n\n```\n\nresult:\n\n![ellipse](tests/ellipse.png)\n\n\n\n## Features\n- [x] 1.[core](http://phpopencv.org/zh-cn/docs/mat.html)\n- [x] 2.[imgproc](http://phpopencv.org/zh-cn/docs/gausian_median_blur_bilateral_filter.html)\n- [x] 3.highgui\n- [ ] 4.contrib\n- [ ] 5.features2d\n- [ ] 6.flann\n- [ ] 7.gpu\n- [ ] 8.calib3d\n- [ ] 9.legacy\n- [x] 10.ml\n- [ ] 11.nonfree\n- [x] 12.objdetect\n- [ ] 13.ocl\n- [ ] 14.photo\n- [ ] 15.stitching\n- [ ] 16.superres\n- [ ] 17.ts\n- [x] 18.video\n- [ ] 19.Videostab\n\n\n\n## Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](https://github.com/hihozhou/php-opencv/graphs/contributors)].\n    \n## \u611f\u8c22\n\n\u611f\u8c22[\u97e9\u5929\u5cf0](https://github.com/matyhtf)\u8001\u5927\u7684\u6307\u5bfc\uff0c  \n\u611f\u8c22[\u97e9\u5929\u5cf0](https://github.com/matyhtf)\u8001\u5927\u7684\u6307\u5bfc\uff0c\n\u611f\u8c22[\u76d8\u53e4\u5927\u53d4](https://github.com/pangudashu)\u7684[php7-internal](https://github.com/pangudashu/php7-internal)\u9879\u76ee\u4ee5\u53ca\u5e73\u5e38\u7684\u6307\u5bfc\uff0c\n\u611f\u8c22`\u6728\u6876\u6280\u672f\u5347\u7ea7\u4ea4\u6d41\u7fa4`\u548c`\u9ed1\u591c\u8def\u4eba\u6280\u672f\u7fa4`\u3001\u4ee5\u53ca`PHP\u5185\u6838\u4ea4\u6d41`\u7684\u7fa4\u53cb\u5bf9\u6280\u672f\u7684\u5e2e\u52a9\u3002\n"
 },
 {
  "repo": "estherjk/face-detection-node-opencv",
  "language": "JavaScript",
  "readme_contents": "# face-detection-node-opencv\n\nReal-time face detection using OpenCV, Node.js, and WebSockets.\n\nClick [here](http://youtu.be/v2SY0naPBFw) to see it in action.\n\n## Requirements\n\n* [Node.js](http://nodejs.org/)\n* [OpenCV 3.4.0](http://opencv.org/)\n    * May also work with older versions of OpenCV, but most recently tested with OpenCV 3.4.0\n* A webcam, e.g. laptop-integrated webcam, USB webcam\n\n## Installing Node.js packages\n\n* Navigate to the `server` directory\n* To install the packages: `npm install`\n\n## Running the demo\n\n* Make sure you are still in the `server` directory\n* To run the server: `node server.js`\n* To run the demo locally, open a browser and go to `localhost:8080`\n\nThe app should be up and running!\n"
 },
 {
  "repo": "ucisysarch/opencvjs",
  "language": "C++",
  "readme_contents": "# OpenCV.js\n\nThis is a JavaScript binding that exposes OpenCV library to the web. This project is made possible by support of Intel corporation. Currently, this is based on OpenCV 3.1.0.\n\n### How to Build\n1. Get the source code\n\n  ```\n  git clone https://github.com/ucisysarch/opencvjs.git\n  cd opencvjs\n  git clone https://github.com/opencv/opencv\n  cd opencv\n  git checkout 3.1.0\n  ```\n2. Install emscripten. You can obtain emscripten by using [Emscripten SDK](https://kripken.github.io/emscripten-site/docs/getting_started/downloads.html).\n\n  ```\n  ./emsdk update\n  ./emsdk install sdk-master-64bit --shallow\n  ./emsdk activate sdk-master-64bit\n  source ./emsdk_env.sh\n  ```\n3. Patch Emscripten & Rebuild.\n\n  ```\n  patch -p1 < PATH/TO/patch_emscripten_master.diff\n  ```\n4. Rebuild emscripten\n  ```\n  ./emsdk install sdk-master-64bit --shallow\n  ```\n\n5. Compile OpenCV and generate bindings by executing make.py script.\n\n  ```\n    python make.py\n  ```\n\n### Tests\nTest suite contains several tests and examples demonstrating how the API can be used. Run the tests by launching test/tests.html file usig a browser.\n\n### Exported OpenCV Subset\nClasses and functions that are intended for binding generators (i.e. come with wrapping macros such as CV_EXPORTS_W and CV_WRAP) are exposed. Hence, supported OpenCV subset is comparable to OpenCV for Python. Also, enums with exception of anonymous enums are also exported.\n\nCurrently, the following modules are supported. You can modify the make script to exclude certain modules.\n\n1. Core\n2. Image processing\n3. Photo\n4. Shape\n5. Video\n6. Object detection\n7. Features framework\n8. Image codecs\n\n### At a glance\nThe following example demonstrates how to apply a gaussian blur filter on an image. Note that everything is wrapped in a JavaScript module ('cv').\n\n```Javascript\n  // Gaussian Blur\n  var mat1 = cv.Mat.ones(7, 7, cv.CV_8UC1),\n      mat2 = new cv.Mat();\n\n  cv.GaussianBlur(mat1, mat2, [3, 3], 0, 0, cv.BORDER_DEFAULT);\n\n  mat1.delete();\n  mat2.delete();\n```\nNext example shows how to calculate image keypoints and their descriptors using ORB (Oriented Brief) method.\n```Javascript\n  var numFeatures = 900,\n\t    scaleFactor = 1.2,\n\t    numLevels = 8,\n\t    edgeThreshold = 31,\n\t\t  firstLevel =0,\n\t\t  WTA_K= 2,\n\t\t  scoreType = 0, //ORB::HARRIS_SCORE\n\t\t  patchSize = 31,\n\t\t  fastThreshold=20,\n\t\t  keyPoints = new cv.KeyPointVector(),\n\t\t  descriptors = new cv.Mat();\n\n\tvar orb = new cv.ORB(numFeatures, scaleFactor, numLevels, edgeThreshold, firstLevel,\n\t\t\t\t\t\t\t\t\t     WTA_K, scoreType, patchSize, fastThreshold);\n\n  // image and mask are of type cv.Mat\n\torb.detect(image, keyPoints, mask);\n\torb.compute(image, keyPoints, descriptors);\n\n\tkeyPoints.delete();\n\tdescriptors.delete();\n\torb.delete();\n```\n\nFunctions work on cv::Mat and various vectors. The following vectors are registered and can be used.\n\n```cpp\n  register_vector<int>(\"IntVector\");\n  register_vector<unsigned char>(\"UCharVector\"););\n  register_vector<float>(\"FloatVector\");\n  register_vector<std::vector<Point>>(\"PointVectorVector\");\n  register_vector<cv::Point>(\"PointVector\");\n  register_vector<cv::Mat>(\"MatVector\");\n  register_vector<cv::KeyPoint>(\"KeyPointVector\");\n  register_vector<cv::Rect>(\"RectVector\");\n  register_vector<cv::Point2f>(\"Point2fVector\");\n```\n### Memory management\nAll the allocated objects should be freed manually by calling delete() method. To avoid manual memory management for basic types, the following data types are exported as JavaScript value arrays.\n\n```\ncv.Size\ncv.Point\n```\n\n### File System Access\nIf your OpenCV application needs to access a file, for instance a dataset or a previoulsy trained classifier, you can modify the make script and attach the files by using emscripten \"--preload-file\" flag.\n\n\n### Limitations\n1. MatExpr is not exported.\n2. No support for default parameters yet.\n2. Constructor overloading are implemented by number of paramteres and not their types. Hence, only following Mat constructors are exported.\n\n```cpp\n  cv::Mat()\n  cv::Mat(const std::vector<unsigned char>& data)\n  cv::Mat(Size size, int type)\n  cv::Mat(int rows, int cols, int type)\n  cv::Mat(Size size, int type, void* data, size_t step)\n```\n"
 },
 {
  "repo": "takmin/OpenCV-Marker-less-AR",
  "language": "C++",
  "readme_contents": "2012/01/10\nMarker less AR is using natural images as markers instead of monochromatic markers.   This program includes object recognition, tracking, and overlay of 3D model functions.\nYou can use this program source code under MIT license.\nhttp://www.opensource.org/licenses/mit-license.php\n\nThis program was written in C++ using OpenCV 2.3.1 and GLUT 3.7.6.  You should install these libraries before you compile this source code. \n\nGLUT\nhttp://www.opengl.org/resources/libraries/glut/\n\nOpenCV\nhttps://sourceforge.net/projects/opencvlibrary/\n\nThis program also includes \"GLMetaseq\" which had been developed by Sunao Hashimoto and Keisuke Konishi.\nGLMetaseq is 3D model loader of mqo format.\n\nGLMetaseq\nhttp://kougaku-navi.net/ARToolKit.html\n\nThe Windows demo program \"ARengine.exe\" includes a 3D models \"mikuX.mqo\", I obtained these files created by Zusa-san from:\nhttp://nanoha.kirara.st/3dcg/file/dlrank.php\n\nThis program consists of 3 parts: Object Recognition, Tracking, and Overlay.  You can use Object Recognition function apart from tracking and overlay.\n\nIf you are a Windows user, I strongly recommend to begin with demo program.  This demo program is build on Windows 7 (32bit).  You can find how to use it in \"HowToUse.pdf\" in WinDemo.zip.\nThis source code is still alpha version and I've not written enough documents yet.  \nThis code is still on the way of refactoring, so API interface design will be changed later.  I will write API documents after refactoring.\nAt the current version, the only document \"HowToUse.pdf\" may be helpful to understand this application.\n\nIf you have any question, please contact here:\nTakuya MINAGAWA (z.takmin@gmail.com)\n\n"
 },
 {
  "repo": "PINTO0309/OpenVINO-YoloV3",
  "language": "Python",
  "readme_contents": "# OpenVINO-YoloV3\nYoloV3 / tiny-YoloV3 + RaspberryPi3 / Ubuntu LaptopPC + NCS/NCS2 + USB Camera + Python\n  \nInspired from **https://github.com/mystic123/tensorflow-yolo-v3.git**\n  \n**Performance comparison as a mobile application (Based on sensory comparison)**  \n\u25ef=HIGH, \u25b3=MEDIUM, \u00d7=LOW  \n\n|No.|Model|Speed|Accuracy|Adaptive distance|\n|:-:|:-|:-:|:-:|:-|\n|1|SSD|\u00d7|\u25ef|ALL|\n|2|**[MobileNet-SSD](https://github.com/PINTO0309/MobileNet-SSD-RealSense.git)**|\u25b3|\u25b3|Short distance|\n|3|**[YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u00d7|\u25ef|ALL|\n|4|**[tiny-YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u25ef|\u25b3|Long distance|\n\n  \n![05](media/05.png)\n\n## My articles\n1. [[24 FPS] Boost RaspberryPi3 with four Neural Compute Stick 2 (NCS2) MobileNet-SSD / YoloV3 [48 FPS for Core i7]](https://qiita.com/PINTO/items/94d5557fca9911cc892d#24-fps-boost-raspberrypi3-with-four-neural-compute-stick-2-ncs2-mobilenet-ssd--yolov3-48-fps-for-core-i7)\n2. [[13 FPS] NCS2 x 4 + Full size YoloV3 performance has been tripled](https://qiita.com/PINTO/items/c766ac9614052f4d6304#13-fps-ncs2-x-4--full-size-yolov3-performance-has-been-tripled)\n3. [Support for local training and OpenVINO of One Class tiny-YoloV3 with a proprietary data set](https://qiita.com/PINTO/items/7dd7135085a7249bf17a#support-for-local-training-and-openvino-of-one-class-tiny-yolov3-with-a-proprietary-data-set)\n\n## Change history\n[Mar 01, 2019]\u3000Improve accuracy. Fixed preprocessing and postprocessing bug.  \n[Mar 17, 2019]\u3000Added a training procedure with your own data set.  \n[Apr 03, 2019]\u3000Work on OpenVINO 2019 R1 started.  \n[Apr 14, 2019]\u3000Compatible with 2019 R1.  \n[Apr 26, 2019]\u3000Compatible with 2019 R1.0.1.  \n\n## Operation sample\n**<CPP + YoloV3 - Intel Core i7-8750H, CPU Only, 4 FPS - 5 FPS>**  \n[<img src=\"media/01.gif\" width=80%>](https://youtu.be/vOcj_3ByK68)  \n  \n**<CPP + tiny-YoloV3 - Intel Core i7-8750H, CPU Only, 60 FPS>**  \n[<img src=\"media/02.gif\" width=80%>](https://youtu.be/md4udC4baZA)  \n\n**<Python + tiny-YoloV3 + USBCamera, Core i7-8750H, CPU Only, 30 FPS>**  \n[<img src=\"media/03.gif\" width=60%>](https://youtu.be/HTDzpFFpBbc)  \n\n**<Python + tiny-YoloV3 + Async + USBCamera, Core i7-8750H, NCS2, 30 FPS+>**  \n**To raise the detection rate, lower the threshold by yourself.**  \n**The default threshold is 40%.**  \n[<img src=\"media/11.gif\" width=60%>](https://youtu.be/7zkb413HCAs) \n\n**<Python + YoloV3 + MP4, Core i7-8750H, NCS2 x4, 13 FPS>  \n\u3010Note\u3011 Due to the performance difference of ARM <-> Core series, performance is degraded in RaspberryPi3.**  \n[<img src=\"media/06.gif\" width=60%>](https://youtu.be/AT75LBIOAck)  \n  \n## Python Version YoloV3 / tiny-YoloV3 (Dec 28, 2018 Operation confirmed)\n### YoloV3\n```bash\n$ python3 openvino_yolov3_test.py\n```\n### tiny-YoloV3 + NCS2 MultiStick\n```bash\n$ python3 openvino_tiny-yolov3_MultiStick_test.py -numncs 1\n```\n### YoloV3 + NCS2 MultiStick (Pretty slow)\n```bash\n$ python3 openvino_yolov3_MultiStick_test.py -numncs 4\n```\n\n\n## CPP Version YoloV3 / tiny-YoloV3 (Dec 16, 2018 Operation confirmed)\n**[cpp version is here](cpp)** \"cpp/object_detection_demo_yolov3_async\"\n  \n## Environment\n\n- LattePanda Alpha (Intel 7th Core m3-7y30) or LaptopPC (Intel 8th Core i7-8750H)\n- Ubuntu 16.04 x86_64\n- RaspberryPi3\n- Raspbian Stretch armv7l\n- OpenVINO toolkit 2019 R1.0.1 (2019.1.133)\n- Python 3.5\n- OpenCV 4.1.0-openvino\n- Tensorflow v1.12.0 or Tensorflow-GPU v1.12.0 (pip install)\n- YoloV3 (MS-COCO)\n- tiny-YoloV3 (MS-COCO)\n- USB Camera (PlaystationEye) / Movie file (mp4)\n- Intel Neural Compute Stick v1 / v2\n  \n## OpenVINO Supported Layers (As of Apr 14, 2019)\n\n- [Supported Framework Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html)\n- [Supported Caffe Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html#caffe-supported-layers)\n- [Supported TensorFlow Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#tensorflow-supported-layers)\n- [Supported MXNet Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html#mxnet-supported-layers)\n- [Supported ONNX Layers](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html#supported-onnx-layers)\n\n**Supported Devices (https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_layers)**\n<table class=\"doxtable\">\n<tr>\n<th align=\"left\">Layers </th><th align=\"center\">GPU </th><th align=\"center\">CPU </th><th align=\"center\">MYRIAD(VPU) </th><th align=\"center\">GNA </th><th align=\"center\">FPGA </th><th align=\"center\">ShapeInfer  </th></tr>\n<tr>\n<td align=\"left\">Activation-Clamp </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ELU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-Leaky ReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-PReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ReLU </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-ReLU6 </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-Sigmoid/Logistic </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Activation-TanH </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ArgMax </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">BatchNormalization </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Concat </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Const </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Dilated </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Dilated 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Grouped </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Grouped 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Ordinary </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Convolution-Ordinary 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Crop </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">CTCGreedyDecoder </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Deconvolution </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Deconvolution 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">DetectionOutput </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Max </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Mul </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Eltwise-Sum </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Flatten </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">FullyConnected (Inner Product) </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Gather </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Gemm </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">GRN </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Interp </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">LRN (Norm) </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">LSTMCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">GRUCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">RNNCell </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">LSTMSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">GRUSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">RNNSequence </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Memory </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">MVN </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Normalize </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pad </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Permute </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pooling(AVG,MAX) </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Pooling(AVG,MAX) 3D </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Power </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PriorBox </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PriorBoxClustered </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Proposal </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">PSROIPooling </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">RegionYolo </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ReorgYolo </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Resample </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Reshape </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">RNN </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">ROIPooling </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">ScaleShift </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SimplerNMS </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Slice </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SoftMax </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">SpatialTransformer </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Split </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">TensorIterator </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Tile </td><td align=\"center\">Supported </td><td align=\"center\">Supported</td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Supported </td></tr>\n<tr>\n<td align=\"left\">Unpooling </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n<tr>\n<td align=\"left\">Upsampling </td><td align=\"center\">Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td><td align=\"center\">Not Supported </td></tr>\n</table>\n  \n## OpenVINO - Python API\n**https://docs.openvinotoolkit.org/latest/_inference_engine_ie_bridges_python_docs_api_overview.html**\n  \n<br>\n<br>\n\n# Environment construction procedure\n### 1. Work with LaptopPC (Ubuntu 16.04)\n1.OpenVINO 2019R1.0.1 Full-Install. Execute the following command.\n```bash\n$ cd ~\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1ciX7cHqCh8lLFYI0HKkhC3r_fMirrlKk\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1ciX7cHqCh8lLFYI0HKkhC3r_fMirrlKk\" -o l_openvino_toolkit_p_2019.1.133.tgz\n$ tar -zxf l_openvino_toolkit_p_2019.1.133.tgz\n$ rm l_openvino_toolkit_p_2019.1.133.tgz\n$ cd l_openvino_toolkit_p_2019.1.133\n$ sudo -E ./install_openvino_dependencies.sh\n\n## GUI version installer\n$ sudo ./install_GUI.sh\n or\n## CUI version installer\n$ sudo ./install.sh\n```\n2.Configure the Model Optimizer. Execute the following command.\n```bash\n$ cd /opt/intel/openvino/install_dependencies/\n$ sudo -E ./install_openvino_dependencies.sh\n$ nano ~/.bashrc\nsource /opt/intel/openvino/bin/setupvars.sh\n\n$ source ~/.bashrc\n$ cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/\n$ sudo ./install_prerequisites.sh\n```\n3.\u3010Optional execution\u3011 Additional installation steps for the Intel\u00ae Movidius\u2122 Neural Compute Stick v1 and Intel\u00ae Neural Compute Stick v2\n```bash\n$ sudo usermod -a -G users \"$(whoami)\"\n$ cat <<EOF > 97-usbboot.rules\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2150\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2485\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"f63b\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\nEOF\n\n$ sudo cp 97-usbboot.rules /etc/udev/rules.d/\n$ sudo udevadm control --reload-rules\n$ sudo udevadm trigger\n$ sudo ldconfig\n$ rm 97-usbboot.rules\n```\n4.\u3010Optional execution\u3011 Additional installation steps for processor graphics (GPU, Intel HD Graphics series only)\n```bash\n$ cd /opt/intel/openvino/install_dependencies/\n$ sudo -E su\n$ uname -r\n4.15.0-42-generic #<--- display kernel version sample\n\n### Execute only when the kernel version is older than 4.14\n$ ./install_4_14_kernel.sh\n\n$ ./install_NEO_OCL_driver.sh\n$ sudo reboot\n```\n\n### 2. Work with RaspberryPi (Raspbian Stretch)\n**[Note] Only the execution environment is introduced.**  \n  \n1.Execute the following command.\n```bash\n$ sudo apt update\n$ sudo apt upgrade\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1NFt6g6ZkneHioU2P7rUJ8BFpQhIazbym\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1NFt6g6ZkneHioU2P7rUJ8BFpQhIazbym\" -o l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ tar -zxvf l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ rm l_openvino_toolkit_raspbi_p_2019.1.133.tgz\n$ sed -i \"s|<INSTALLDIR>|$(pwd)/inference_engine_vpu_arm|\" inference_engine_vpu_arm/bin/setupvars.sh\n```\n2.Execute the following command.\n```bash\n$ nano ~/.bashrc\n### Add 1 row below\nsource /home/pi/inference_engine_vpu_arm/bin/setupvars.sh\n\n$ source ~/.bashrc\n### Successful if displayed as below\n[setupvars.sh] OpenVINO environment initialized\n\n$ sudo usermod -a -G users \"$(whoami)\"\n$ sudo reboot\n```\n3.Update USB rule.\n```bash\n$ sh inference_engine_vpu_arm/install_dependencies/install_NCS_udev_rules.sh\n### It is displayed as follows\nUpdate udev rules so that the toolkit can communicate with your neural compute stick\n[install_NCS_udev_rules.sh] udev rules installed\n```\n**[Note] OpenCV 4.1.0 will be installed without permission when the work is finished.\nIf you do not want to affect other environments, please edit environment variables after installation is completed.**  \n\n# Training with your own data set  \nSee the article below.  \nA sample of one-class training with Darknet and tiny-YoloV3.  \n**https://qiita.com/PINTO/items/7dd7135085a7249bf17a#support-for-local-training-and-openvino-of-one-class-tiny-yolov3-with-a-proprietary-data-set**\n<br>\n<br>\n<br>\n<br>\n\n# How to install Bazel (version 0.17.2, x86_64 only)\n### 1. Bazel introduction command\n```bash\n$ cd ~\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1dvR3pdM6vtkTWqeR-DpgVUoDV0EYWil5\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1dvR3pdM6vtkTWqeR-DpgVUoDV0EYWil5\" -o bazel\n$ sudo cp ./bazel /usr/local/bin\n$ rm ./bazel\n```\n### 2. Supplementary information\n**https://github.com/PINTO0309/Bazel_bin.git**\n\n# How to check the graph structure of a \".pb\" file [Part.1]\nSimple structure analysis.\n### 1. Build and run graph structure analysis program\n```bash\n$ cd ~\n$ git clone -b v1.11.0 https://github.com/tensorflow/tensorflow.git\n$ cd tensorflow\n$ git checkout -b v1.11.0\n$ bazel build tensorflow/tools/graph_transforms:summarize_graph\n$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=xxxx.pb\n```\n### 2. Sample of display result\nYoloV3\n```bash\nFound 1 possible inputs: (name=inputs, type=float(1), shape=[?,416,416,3]) \nNo variables spotted.\nFound 1 possible outputs: (name=output_boxes, op=ConcatV2) \nFound 62002034 (62.00M) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 536 Const, 372 Identity, 87 Mul, 75 Conv2D, 72 FusedBatchNorm, 72 Maximum, 28 Add, \\\n24 Reshape, 14 ConcatV2, 9 Sigmoid, 6 Tile, 6 Range, 5 Pad, 4 SplitV, 3 Pack, 3 RealDiv, 3 Fill, \\\n3 Exp, 3 BiasAdd, 2 ResizeNearestNeighbor, 2 Sub, 1 Placeholder\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\nbazel run tensorflow/tools/benchmark:benchmark_model -- \\\n--graph=/home/b920405/git/OpenVINO-YoloV3/pbmodels/frozen_yolo_v3.pb \\\n--show_flops \\\n--input_layer=inputs \\\n--input_layer_type=float \\\n--input_layer_shape=-1,416,416,3 \\\n--output_layer=output_boxes\n```\ntiny-YoloV3\n```bash\nFound 1 possible inputs: (name=inputs, type=float(1), shape=[?,416,416,3]) \nNo variables spotted.\nFound 1 possible outputs: (name=output_boxes, op=ConcatV2) \nFound 8858858 (8.86M) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 134 Const, 63 Identity, 21 Mul, 16 Reshape, 13 Conv2D, 11 FusedBatchNorm, 11 Maximum, \\\n10 ConcatV2, 6 Sigmoid, 6 MaxPool, 4 Tile, 4 Add, 4 Range, 3 RealDiv, 3 SplitV, 2 Pack, 2 Fill, \\\n2 Exp, 2 Sub, 2 BiasAdd, 1 Placeholder, 1 ResizeNearestNeighbor\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\nbazel run tensorflow/tools/benchmark:benchmark_model -- \\\n--graph=/home/b920405/git/OpenVINO-YoloV3/pbmodels/frozen_tiny_yolo_v3.pb \\\n--show_flops \\\n--input_layer=inputs \\\n--input_layer_type=float \\\n--input_layer_shape=-1,416,416,3 \\\n--output_layer=output_boxes\n```\n\n# How to check the graph structure of a \".pb\" file [Part.2]\nConvert to text format.\n### 1. Run graph structure analysis program\n```bash\n$ python3 tfconverter.py\n### \".pbtxt\" in ProtocolBuffer format is output.\n### The size of the generated text file is huge.\n```\n\n# How to check the graph structure of a \".pb\" file [Part.3]\nUse Tensorboard.\n### 1. Run log output program for Tensorboard\n```python\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nwith tf.Session() as sess:\n    model_filename =\"xxxx.pb\"\n    with gfile.FastGFile(model_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        g_in = tf.import_graph_def(graph_def)\n\n    LOGDIR=\"path/to/logs\"\n    train_writer = tf.summary.FileWriter(LOGDIR)\n    train_writer.add_graph(sess.graph)\n```\n### 2. Starting Tensorboard\n```bash\n$ tensorboard --logdir=path/to/logs\n```\n### 3. Display of Tensorboard\nAccess `http://localhost:6006` from the browser.\n  \n# How to check the graph structure of a \".pb\" file [Part.4]\nUse **[netron](https://github.com/lutzroeder/netron.git)**.\n### 1. Install netron\n```bash\n$ sudo -H pip3 install netron\n```\n### 2. Starting netron\n```bash\n$ netron -b [MODEL_FILE]\n```\n### 3. Display of netron\nAccess `http://localhost:8080` from the browser.  \n![07](media/07.png)\n# Neural Compute Stick 2\n**https://ncsforum.movidius.com/discussion/1302/intel-neural-compute-stick-2-information**\n\n# Issue\n**[OpenVINO failing on YoloV3's YoloRegion, only one working on FP16, all working on FP32](https://software.intel.com/en-us/forums/computer-vision/topic/804019)**  \n**[Regarding YOLO family networks on NCS2. Possibly a work-around](https://software.intel.com/en-us/forums/computer-vision/topic/805425)**  \n**[Convert YOLOv3 Model to IR](https://software.intel.com/en-us/forums/computer-vision/topic/805370)**  \n\n# Reference\n**https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend**\n**https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#raspbian-stretch**\n"
 },
 {
  "repo": "MicrocontrollersAndMore/OpenCV_3_License_Plate_Recognition_Python",
  "language": "Python",
  "readme_contents": "The video pretty much explains it all:\nhttps://www.youtube.com/watch?v=fJcl6Gw1D8k\n"
 },
 {
  "repo": "Web-Sight/WebSight",
  "language": "JavaScript",
  "readme_contents": "WebSight demonstrates a comparison of performance between JavaScript, <a href=\"http://asmjs.org/\">asm.js</a>, and <a href=\"http://webassembly.org/\">WebAssembly</a>. A user uploaded static image or live video is displayed for each target. Performance is measured by the length of time it takes to detect face(s) or eyes in the image or video.\n\nEach target is run in its own <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API\">web worker</a>. The popular open source computer vision library, <a href=\"http://opencv.org/\">OpenCV</a>, was compiled using <a href=\"http://kripken.github.io/emscripten-site/\">Emscripten</a> to asm.js and wasm (WebAssembly module) to utilize the <a href=\"https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework\">Viola-Jones algorithm</a> for object detection. <a href=\"https://github.com/foo123/HAAR.js\">HAAR.js</a> was modified to remove the DOM manipulation, so it could be used in the JavaScript web worker.\n\nThanks to <a href=\"https://github.com/shamadee/web-dsp\">WebDSP</a> for the inspiration, and to <a href=\"https://github.com/ucisysarch/opencvjs\">uscisysarch</a>, <a href=\"https://github.com/njor/opencvjs\">njor</a>, and <a href=\"https://github.com/foo123/HAAR.js\">foo123</a> for giving us a starting point.\n\nContributors to this project are <a href=\"https://github.com/BrianJFeldman\">BrianJFeldman</a>, <a href=\"https://github.com/DebraD\">DebraD</a>, <a href=\"https://github.com/MarkGeeRomano\">MarkGeeRomano</a> and <a href=\"https://github.com/YervantB\">YervantB</a>.\n"
 },
 {
  "repo": "qupath/qupath",
  "language": "Java",
  "readme_contents": "[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fqupath.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/qupath)\n\nQuPath\n======\n\n**QuPath is open source software for bioimage analysis**.\n\nFeatures include:\n\n* Extensive tools to annotate and view images, including whole slide images\n* Workflows for both IHC and H&E analysis\n* New algorithms for common tasks, e.g. cell segmentation, tissue microarray dearraying\n* Interactive machine learning, e.g. for cell & texture classification\n* Customization, batch-processing & data interrogation by scripting\n* Easy integration with other tools, including ImageJ\n\n\nTo **download QuPath**, go to the [Latest Releases](https://github.com/qupath/qupath/releases/latest) page.\n\nFor **documentation**, see [https://qupath.readthedocs.io](https://qupath.readthedocs.io)\n\nFor **help & support**, try [image.sc](https://forum.image.sc/tag/qupath) or the [links here](https://qupath.readthedocs.io/en/latest/docs/starting/help.html)\n\nTo **build QuPath from source** see [here](https://qupath.readthedocs.io/en/latest/docs/reference/building.html).\n\n\n*QuPath is an academic project intended for research use only.*\n*The software has been made freely available under the terms of the [GPLv3](https://github.com/qupath/qupath/blob/master/LICENSE.md) in the hope it is useful for this purpose, and to make analysis methods open and transparent.*\n\nIf you find QuPath useful in work that you publish, please [cite the publication](https://qupath.readthedocs.io/en/latest/docs/intro/citing.html).\n\n\n\n## Developers\n\nQuPath is being actively developed at the University of Edinburgh by:\n\n* Pete Bankhead (creator)\n* Melvin Gelbard\n\nFor all contributors, see [here](https://github.com/qupath/qupath/graphs/contributors).\n\n\n----\n\n## Background\n\nQuPath was designed, implemented and documented by Pete Bankhead while at Queen's University Belfast, with additional code and testing by Jose Fernandez.\n\nVersions up to v0.1.2 are copyright 2014-2016 The Queen's University of Belfast, Northern Ireland.\nThese were written as part of projects that received funding from:\n\n* Invest Northern Ireland (RDO0712612)\n* Cancer Research UK Accelerator (C11512/A20256)\n\n\n![Image](https://raw.githubusercontent.com/wiki/qupath/qupath/images/qupath_demo.jpg)\n"
 },
 {
  "repo": "pageauc/speed-camera",
  "language": "Python",
  "readme_contents": "# SPEED CAMERA - Object Motion Tracker [![Mentioned in Awesome <INSERT LIST NAME>](https://awesome.re/mentioned-badge.svg)](https://github.com/thibmaek/awesome-raspberry-pi)\n### RPI, Unix and Windows Speed Camera Using python, openCV, USB Cam or RPI camera module\n## For Details See [Program Features](https://github.com/pageauc/speed-camera/wiki/Program-Description#program-features) and [Wiki Instructions](https://github.com/pageauc/speed-camera/wiki) and [YouTube Videos](https://github.com/pageauc/speed-camera#links)\n\n## RPI Quick Install or Upgrade   \n***IMPORTANT*** - A raspbian **sudo apt-get update** and **sudo apt-get upgrade** will\n**NOT** be performed as part of   \n**speed-install.sh** so it is recommended you run these prior to install\nto ensure your system is up-to-date.     \n\n***Step 1*** With mouse left button highlight curl command in code box below. Right click mouse in **highlighted** area and Copy.     \n***Step 2*** On RPI putty SSH or terminal session right click, select paste then Enter to download and run script.  \n\n    curl -L https://raw.github.com/pageauc/speed-camera/master/speed-install.sh | bash\n\nThis will download and run the **speed-install.sh** script. If running under python3 you will need opencv3 installed.\nSee my Github [menu driven compile opencv3 from source](https://github.com/pageauc/opencv3-setup) project\n\n## Program Description   \nThis is a raspberry pi, Windows, Unix Distro computer openCV object speed camera demo program.\nIt is written in python and uses openCV to detect and track the x,y coordinates of the \nlargest moving object in the camera view above a minimum pixel area.\nUser variables are stored in the [***config.py***](https://github.com/pageauc/speed-camera/blob/master/config.py) file.\nMotion detection is restricted between ***y_upper***, ***y_lower***, ***x_left***, ***x_right*** variables  (road or area of interest).\nIf a track is longer than ***track_len_trig*** variable then average speed will be \ncalculated based on ***cal_obj_px*** and ***cal_obj_mm*** variables and a speed photo will be\ntaken and saved in ***media/images*** dated subfolders per variable ***imageSubDirMaxFiles*** = ***1000*** \n(see config.py). \n\nIf ***log_data_to_CSV*** = ***True*** then a ***speed-cam.csv*** file will be created/updated with event data stored in\nCSV (Comma Separated Values) format. This can be imported into a spreadsheet, database, Etc program for further processing.\nRelease 8.9 adds a **sqlite3** database to store speed data. Default is ***data/speed_cam.db*** with data in the ***speed*** table .\nthere is a simple report ***sql_speed_gt.sh*** that can query for records with greater than a specified speed.\nI plan on doing more but this should be a good start. Take a look at the code for details.\n\nAlso included are \n  \n* [***menubox.sh***](https://github.com/pageauc/speed-camera/wiki/Admin-and-Settings#manage-settings-using-menuboxsh)\nscript is a whiptail menu system to allow easier management of program settings and operation. \n* [***webserver.py***](https://github.com/pageauc/speed-camera/wiki/How-to-View-Data#how-to-view-images-and-or-data-from-a-web-browser)\nAllows viewing images and/or data from a web browser (see config.py for webserver settings)\n* [***rclone***](https://github.com/pageauc/speed-camera/wiki/Manage-rclone-Remote-Storage-File-Transfer)\nfor optional remote file sync to a remote storage service like google drive, DropBox and many others. \n* [***watch-app.sh***](https://github.com/pageauc/speed-camera/wiki/watch-app.sh-Remote-Manage-Config)\nfor administration of settings from a remote storage service. Plus application monitoring.\n* [***sql_speed_gt.py***](https://github.com/pageauc/speed-camera/blob/master/sql_speed_gt.sh) Prompts for a speed value and runs a simple\nsqlite3 query to show all record that exceed the specified speed. Output can be found in media/reports folder and is available from browser. \n* [***sql_hour_count.py***](https://github.com/pageauc/speed-camera/blob/master/sql_hour_count.py) Run report for count by hour.\nalso produces a graph using gnuplot. Query output html report and .png graph can be found in media/reports folder and is available from browser.\n* [***alpr-speed.py***](https://github.com/pageauc/speed-camera/wiki/alpr-speed.py---Process-speed-images-with-OPENALPR-Automatic-License-Plate-Reader)\nProcess speed camera images with [OPENALPR](https://github.com/openalpr/openalpr) License plate reader\n* [***speed-search.py***](https://github.com/pageauc/rpi-speed-camera/wiki/How-to-Run-speed-search.py)\nallows searching for similar target object images using opencv template matching. \n* [***makehtml.py***](https://github.com/pageauc/speed-camera/wiki/How-to-View-Data#view-combined-imagedata-html-pages-on-a-web-browser)\ncreates html files that combine csv and image data for easier viewing from a web browser.\n(Does not work with ***secpicam480.py*** or ***secwebcam480.py*** plugins enabled.\n\n## Requirements\n[***Raspberry Pi computer***](https://www.raspberrypi.org/documentation/setup/) and a [***RPI camera module installed***](https://www.raspberrypi.org/documentation/usage/camera/)\nor USB Camera plugged in. Make sure hardware is tested and works. Most [RPI models](https://www.raspberrypi.org/products/) will work OK. \nA quad core RPI will greatly improve performance due to threading. A recent version of \n[Raspbian operating system](https://www.raspberrypi.org/downloads/raspbian/) is Recommended.   \nor  \n***MS Windows or Unix distro*** computer with a USB Web Camera plugged in and a\n[recent version of python installed](https://www.python.org/downloads/)\nFor Details See [***Wiki details***](https://github.com/pageauc/speed-camera/wiki/Prerequisites-and-Install#windows-or-non-rpi-unix-installs).\n\nIt is recommended you upgrade to OpenCV version 3.x.x  For Easy compile of opencv 3.4.2 from source \nSee https://github.com/pageauc/opencv3-setup\n\n## Windows or Non RPI Unix Installs\nFor Windows or Unix computer platforms (non RPI or Debian) ensure you have the most\nup-to-date python version. For Downloads visit https://www.python.org/downloads    \n\nThe latest python versions includes numpy and recent opencv version that is required to run this code. \nYou will also need a USB web cam installed and working. \nTo install this program access the GitHub project page at https://github.com/pageauc/speed-camera\nSelect the ***green Clone or download*** button. The files will be cloned or zipped\nto a speed-camera folder. You can run the code from python IDLE application (recommended), GUI desktop\nor command prompt terminal window. Note bash .sh shell scripts will not work with windows unless \nspecial support for bash is installed for windows Eg http://win-bash.sourceforge.net/  http://www.cygwin.com/\nNote I have Not tested these.   \n\n***IMPORTANT*** speed-cam.py ver 8.x or greater Requires Updated config.py and plugins.\n\n    cd ~/speed-camera\n    cp config.py config.py.bak\n    cp config.py.new config.py\n    \nTo replace plugins rename (or delete) plugins folder per below\n\n    cd ~/speed-camera\n    mv plugins pluginsold   # renames plugins folder\n    rm -r plugins           # deletes plugins folder\n\nThen run ***menubox.sh*** UPGRADE menu pick.\n \n## Manual Install or Upgrade   \nFrom logged in RPI SSH session or console terminal perform the following. Allows you to review install code before running\n\n    cd ~\n    wget https://raw.github.com/pageauc/speed-camera/master/speed-install.sh\n    more speed-install.sh       # You can review code if you wish\n    chmod +x speed-install.sh\n    ./speed-install.sh  # runs install script.\n    \n## Run to view verbose logging \n\n    cd ~/speed-camera    \n    ./speed-cam.py\n    \nSee [***How to Run***](https://github.com/pageauc/speed-camera/wiki/How-to-Run) speed-cam.py wiki section\n\n***IMPORTANT*** Speed Camera will start in ***calibrate*** = ***True*** Mode.    \nReview settings in ***config.py*** file and edit variables with nano as required.\nYou will need to perform a calibration to set the correct value for config.py ***cal_obj_px*** and ***cal_obj_mm*** \nvariables based on the distance from camera to objects being measured for speed.\nSee [***Calibration Procedure***](https://github.com/pageauc/speed-camera/wiki/Calibrate-Camera-for-Distance) for more details.     \n\nThe config.py motion tracking variable called track_counter = can be adjusted for your system and opencv version.\ndefault is 5 but a quad core RPI3 and latest opencv version eg 3.4.2 can be 10-15 or possibly greater. \n    \n## Run menubox.sh \n\n    cd ~/speed-camera\n    ./menubox.sh\n\nAdmin speed-cam Easier using menubox.sh (Once calibrated and/or testing complete)  \n![menubox main menu](https://github.com/pageauc/speed-camera/blob/master/menubox.png)     \n\nView speed-cam data and trends from web browser per sample screen shots\n\n![Speed Camera Web Recent View](https://github.com/pageauc/speed-camera/blob/master/speed_web_recent.png)   \n![Speed Camera Web html speed list Report](https://github.com/pageauc/speed-camera/blob/master/speed_web_sqlite.png)   \n![Speed Camera Web Recent View](https://github.com/pageauc/speed-camera/blob/master/speed_web_gnuplot.png)   \n\n## Links\n* YouTube Speed Lapse Video https://youtu.be/-xdB_x_CbC8\n* YouTube Speed Camera Video https://youtu.be/eRi50BbJUro\n* YouTube motion-track video https://youtu.be/09JS7twPBsQ\n* Speed Camera RPI Forum post https://www.raspberrypi.org/forums/viewtopic.php?p=1004150#p1004150\n* YouTube Channel https://www.youtube.com/user/pageaucp \n* Speed Camera GitHub Repo https://github.com/pageauc/speed-camera      \n\n## Credits  \nSome of this code is based on a YouTube tutorial by\nKyle Hounslow using C here https://www.youtube.com/watch?v=X6rPdRZzgjg\n\nThanks to Adrian Rosebrock jrosebr1 at http://www.pyimagesearch.com \nfor the PiVideoStream Class code available on github at\nhttps://github.com/jrosebr1/imutils/blob/master/imutils/video/pivideostream.py\n  \nHave Fun   \nClaude Pageau    \nYouTube Channel https://www.youtube.com/user/pageaucp   \nGitHub Repo https://github.com/pageauc\n"
 },
 {
  "repo": "lincolnhard/head-pose-estimation",
  "language": "C++",
  "readme_contents": "# head-pose-estimation\nReal-time head pose estimation built with OpenCV and dlib \n\n<b>2D:</b><br>Using dlib for facial features tracking, modified from http://dlib.net/webcam_face_pose_ex.cpp.html\n<br>The algorithm behind it is described in http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf\n<br>It applies cascaded regression trees to predict shape(feature locations) change in every frame.\n<br>Splitting nodes of trees are trained in random, greedy, maximizing variance reduction fashion.\n<br>The well trained model can be downloaded from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 \n<br>Training set is based on i-bug 300-W datasets. It's annotation is shown below:<br><br>\n![ibug](https://cloud.githubusercontent.com/assets/16308037/24229391/1910e9cc-0fb4-11e7-987b-0fecce2c829e.JPG)\n<br><br>\n<b>3D:</b><br>To match with 2D image points(facial features) we need their corresponding 3D model points. \n<br>http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp provides a similar 3D facial feature model.\n<br>It's annotation is shown below:<br><br>\n![gl](https://cloud.githubusercontent.com/assets/16308037/24229340/ea8bad94-0fb3-11e7-9e1d-0a2217588ba4.jpg)\n<br><br>\nFinally, with solvepnp function in OpenCV, we can achieve real-time head pose estimation.\n<br><br>\n\n"
 },
 {
  "repo": "goncalopp/simple-ocr-opencv",
  "language": "Python",
  "readme_contents": "# Simple Python OCR\n[![Build Status](https://travis-ci.org/goncalopp/simple-ocr-opencv.svg?branch=master)](https://travis-ci.org/goncalopp/simple-ocr-opencv)\n\nA simple pythonic OCR engine using opencv and numpy.\n\nOriginally inspired by [this stackoverflow question](http://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python)\n\n### Essential Concepts\n\n#### Segmentation\n\nIn order for OCR to be performed on a image, several steps must be \nperformed on the source image. Segmentation is the process of \nidentifying the regions of the image that represent characters. \n\nThis project uses rectangles to model segments. \n\n#### Supervised learning with a classification problem\n\nThe [classification problem][] consists in identifying to which class a \nobservation belongs to (i.e.: which particular character is contained \nin a segment).\n\n[Supervised learning][] is a way of \"teaching\" a machine. Basically, an \nalgorithm is *trained* through *examples* (i.e.: this particular \nsegment contains the character `f`). After training, the machine \nshould be able to apply its acquired knowledge to new data.\n\nThe [k-NN algorithm], used in this project, is one of the simplest  \nclassification algorithm.\n\n#### Grounding\n\nCreating a example image with already classified characters, for \ntraining purposes.\nSee [ground truth][].\n\n[classification problem]: https://en.wikipedia.org/wiki/Statistical_classification\n[Supervised learning]: https://en.wikipedia.org/wiki/Supervised_learning\n[k-NN algorithm]: https://en.wikipedia.org/wiki/K-nearest_neighbors_classification\n[ground truth]: https://en.wikipedia.org/wiki/Ground_truth\n\n#### How to understand this project\n\nUnfortunately, documentation is a bit sparse at the moment (I \ngladly accept contributions).\nThe project is well-structured, and most classes and functions have \ndocstrings, so that's probably a good way to start.\n\nIf you need any help, don't hesitate to contact me. You can find my \nemail on my github profile.\n\n\n#### How to use\n\nPlease check `example.py` for basic usage with the existing pre-grounded images.\n\nYou can use your own images, by placing them on the `data` directory. \nGrounding images interactively can be accomplished by using `grounding.UserGrounder`.\nFor more details check `example_grounding.py`\n\n#### Copyright and notices\n\nThis project is available under the [GNU AGPLv3 License](https://www.gnu.org/licenses/agpl-3.0.txt), a copy\nshould be available in LICENSE. If not, check out the link to learn more.\n \n    Copyright (C) 2012-2017 by the simple-ocr-opencv authors\n    All authors are the copyright owners of their respective additions\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU AGPLv3 License, as found in LICENSE.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.    \n  \n"
 },
 {
  "repo": "glassechidna/zxing-cpp",
  "language": "C++",
  "readme_contents": "# ZXing C++ Port\n\n[![Build Status](https://travis-ci.org/glassechidna/zxing-cpp.svg?branch=master)](https://travis-ci.org/glassechidna/zxing-cpp)\n\n[ZXing](https://github.com/zxing/zxing) is/was a Java library.\n\nAt some point a complete C++ port/rewrite was created and maintained in the official [ZXing](https://github.com/zxing/zxing) repo. However, at the time of writing the C++ port is no longer maintained and has been removed from the official ZXing repo.\n\nThis project was forked from the [last ZXing commit](https://github.com/zxing/zxing/commit/00f6340) to contain the C++ project, with the following exceptions\n\n * scons (Python) build system has been deleted.\n * Deleted black box tests, because they refer to a large test data in ZXing repo.\n * Added appropriate copyright/licensing details (based on those in the ZXing repo).\n * Updated README.md\n\nRemoval of build systems was done to minimise maintenance burden.\n\nIf tests and XCode projects (other than those produced automatically be CMake) are desired, then another repo should be created and this repo referenced as a submodule. \n\n# Building using CMake\n\nCMake is a tool, that generates native makefiles and workspaces. It integrates well with a number of IDEs including Qt Creator and Visual Studio.\n\nUsage with CLion or Qt Creator:\n\n  1. Simply open `CMakeLists.txt` as a new project\n  2. Additional command line arguments can be specified (see below)\n\nUsage with Makefiles, Visual Studio, etc. (see `cmake --help` for a complete list of generators):\n\n  1. `mkdir build`\n  2. `cd` to `build`\n  3. Unix: run `cmake -G \"Unix Makefiles\" ..`\n  3. Windows: run `cmake -G \"Visual Studio 10\" ..`\n  \nYou can switch between build modes by specifying:\n\n  - `-DCMAKE_BUILD_TYPE=Debug` or\n  - `-DCMAKE_BUILD_TYPE=Release`\n\n# OpenCV integration\n\nWhen build on a system where opencv is installed the open cv bridge classes and executable will be built too.\n\n# Development tips\n\nTo profile the code (very useful to optimize the code):\n\n  1. Install Valgrind\n  2. Run `valgrind --tool=callgrind build/zxing - path/to/test/data/*.jpg > report.html`\n  3. Analyze output using KCachegrind\n"
 },
 {
  "repo": "meiqua/shape_based_matching",
  "language": "C++",
  "readme_contents": "# shape_based_matching  \n\nupdate:   \n**[fusion implementation to run faster!](https://github.com/meiqua/shape_based_matching/issues/77)**  \n**[icp is also refined to be faster and easier to use](https://github.com/meiqua/shape_based_matching/issues/100)**  \n\n[Transforms in shape-based matching](./Transforms%20in%20shape-based%20matching.pdf)  \n[pose refine with icp branch](https://github.com/meiqua/shape_based_matching/tree/icp2D), 0.1-0.5 degree accuracy   \n[icp + subpixel branch](https://github.com/meiqua/shape_based_matching/tree/subpixel), < 0.1 degree accuracy  \n[icp + subpixel + sim3(previous is so3) branch](https://github.com/meiqua/shape_based_matching/tree/sim3), deal with scale error  \n\ntry to implement halcon shape based matching, refer to machine vision algorithms and applications, page 317 3.11.5, written by halcon engineers  \nWe find that shape based matching is the same as linemod. [linemod pdf](Gradient%20Response%20Maps%20for%20Real-TimeDetection%20of%20Textureless%20Objects.pdf)  \n\nhalcon match solution guide for how to select matching methods([halcon documentation](https://www.mvtec.com/products/halcon/documentation/#reference_manual)):  \n![match](./match.png)  \n\n## steps\n\n1. change test.cpp line 9 prefix to top level folder\n\n2. in cmakeList line 23, change /opt/ros/kinetic to somewhere opencv3 can be found(if opencv3 is installed in default env then don't need to)\n\n3. cmake make & run. To learn usage, see different tests in test.cpp. Particularly, scale_test are fully commented.\n\nNOTE: On windows, it's confirmed that visual studio 17 works fine, but there are some problems with MIPP in vs13. You may want old codes without [MIPP](https://github.com/aff3ct/MIPP): [old commit](https://github.com/meiqua/shape_based_matching/tree/fc3560a1a3bc7c6371eacecdb6822244baac17ba)  \n\n## thoughts about the method\n\nThe key of shape based matching, or linemod, is using gradient orientation only. Though both edge and orientation are resistant to disturbance,\nedge have only 1bit info(there is an edge or not), so it's hard to dig wanted shapes out if there are too many edges, but we have to have as many edges as possible if we want to find all the target shapes. It's quite a dilemma.  \n\nHowever, gradient orientation has much more info than edge, so we can easily match shape orientation in the overwhelming img orientation by template matching across the img.  \n\nSpeed is also important. Thanks to the speeding up magic in linemod, we can handle 1000 templates in 20ms or so.  \n\n[Chinese blog about the thoughts](https://www.zhihu.com/question/39513724/answer/441677905)  \n\n## improvment\n\nComparing to opencv linemod src, we improve from 6 aspects:  \n\n1. delete depth modality so we don't need virtual func, this may speed up  \n\n2. opencv linemod can't use more than 63 features. Now wo can have up to 8191  \n\n3. simple codes for rotating and scaling img for training. see test.cpp for examples  \n\n4. nms for accurate edge selection  \n\n5. one channel orientation extraction to save time, slightly faster for gray img\n\n6. use [MIPP](https://github.com/aff3ct/MIPP) for multiple platforms SIMD, for example, x86 SSE AVX, arm neon.\n   To have better performance, we have extended MIPP to uint8_t for some instructions.(Otherwise we can only use\n   half feature points to avoid int8_t overflow)  \n\n7. rotate features directly to speed up template extractions; selectScatteredFeatures more \nevenly; exautive select all features if not enough rather than abort templates(but features <= 4 will abort)\n\n## some test\n\n### Example for circle shape  \n\n#### You can imagine how many circles we will find if use edges  \n![circle1](test/case0/1.jpg)\n![circle1](test/case0/result/1.png)  \n\n#### Not that circular  \n![circle2](test/case0/2.jpg)\n![circle2](test/case0/result/2.png)  \n\n#### Blur  \n![circle3](test/case0/3.png)\n![circle3](test/case0/result/3.png)  \n\n### circle template before and after nms  \n\n#### before nms\n\n![before](test/case0/features/no_nms_templ.png)\n\n#### after nms\n\n![after](test/case0/features/nms_templ.png)  \n\n### Simple example for arbitary shape\n\nWell, the example is too simple to show the robustness  \nrunning time: 1024x1024, 60ms to construct response map, 7ms for 360 templates  \n\ntest img & templ features  \n![test](./test/case1/result.png)  \n![templ](test/case1/templ.png)  \n\n\n### noise test  \n\n![test2](test/case2/result/together.png)  \n\n## some issues you may want to know  \nWell, issues are not clearly classified and many questions are discussed in one issue sometimes. For better reference, some typical discussions are pasted here.  \n\n[object too small?](https://github.com/meiqua/shape_based_matching/issues/13#issuecomment-474780205)  \n[failure case?](https://github.com/meiqua/shape_based_matching/issues/19#issuecomment-481153907)  \n[how to run even faster?](https://github.com/meiqua/shape_based_matching/issues/21#issuecomment-489664586)  \n\n"
 },
 {
  "repo": "billmccord/OpenCV-Android",
  "language": "C++",
  "readme_contents": "= OpenCV-Android\n\nDedicated to providing an optimized port of OpenCV for the Google Android OS.\n\n== Requirements\n\nIn order to use OpenCV-Android, you will need to download and install both the Android SDK 1.6 and NDK r5.  It may or may not work with a higher / lower version. It has been confirmed that this doesn't work with older versions of Android SDK, so please use 1.6 or higher.\n\nIn addition to having the SDK or NDK you will also need to have one of the following:\n* An Android Phone Dev Phone (Might work with other phones, has not been tested)\n* The QuickTime Java Libraries (optional if you want to use the QTWebcamBroadcaster)\n\nFor those of you running on the emulator, I built a very simple Socket-based Camera server that will send images over a socket connection.  This uses the QuickTime libraries which are present by default on Mac OS X, but I haven't tested on other OSes.  If it doesn't work for you, you can always try the original WebcamBroadcaster I derived mine from which uses the JMF (which doesn't work with Mac OS X, hence the QTWebcamBroadcaster):\nhttp://www.tomgibara.com/android/camera-source\n\n== Build\n\nBuilding is now even easier with NDK r5.  I'm going to assume that you are familiar with Android if you are reading this and keep it rather short.\n\n* Make sure that ndk-build is in your path.\n\n* Go to the OpenCV-Android project directory and run 'ndk-build'\n  \nBy default, this will build the opencv library and push it into:\n[OPENCV_ANDROID_ROOT]/tests/VideoEmulation/libs\n\nYou can change where the lib is delivered by modifying the APP_PROJECT_PATH in:\n[OPENCV_ANDROID_ROOT]/Application.mk\n\nOnce you have built the OpenCV library, you can now build and [OPENCV_ANDROID_ROOT]/tests/VideoEmulation.\n\n<b>NOTE:</b> If you plan to use the Socket Camera, you will need to build and run the [OPENCV_ANDROID_ROOT]/tests/QTWebcamBroadcaster first.  Also, if you use Eclipse to develop Android, there are already projects defined for both of these applications.  You can simply import them into your workspace.\n\n<b>IMPORTANT NEW NOTE:</b> In order for the QTWebcamBroadcaster to work, you must ensure that you run it in a 32-bit VM.  Failure to run in 32-bit will result in an empty screen in your emulator (no image will appear.)  You can easily force the VM that runs this application to 32-bit by adding the VM flag:\n-d32\n\n\n== Setup\n\nIf you want to test face tracking, then you need to have a Haar Classifier Cascade XML.  I have provided one for use and it is stored in:\ntests/haarcascade_frontalface_alt.xml\n\nBefore attempting to run the VideoEmulator application, you must first copy this XML file into the emulator in the following location:\n/data/data/org.siprop.opencv/files/haarcascade_frontalface_alt.xml\n\nCurrently, this is a hard-coded path that we look up.  Hopefully, this can be remedied in a future version.\n\n== Run\n\nIn order to use the VideoEmulator, you have to use the emulator (hence the name.)  If you have a Dev Phone, you can play around with the old 'OpenCVSample' test or modify the VideoEmulator to support a real camera.  This is something we will work on resolving in the future.\n\nUsing the emulator there are two slightly different 'flavors' of running.  Both are socket based cameras, but one is written in C++ and emulates a real OpenCV Capture while the other (loosely) emulates a camera implementation in Java.  The C++ version is the default as it is slightly faster and takes a little less memory.  Also, the ultimate goal is to hook up with a real camera in C++ so that we don't have to pass huge amounts of data (images) back and forth through the JNI interface.\n\n<b>NOTE:</b> For all of these examples you cannot use localhost or 127.0.0.1 as your address for the socket camera.  The reason is because when the client is running on the Android emulator, both of these map to Android's localhost, not the machine you are running the emulator on.  This means you have to be connected to a network in order to use the socket camera, a limitation.\n\n=== C++\n\nSince this is the default, we have made it pretty easy...\n\n* Start the WebcamBroadcaster - this is a socket server that grabs images from your camera and serves them up\n* Start the VideoEmulator - this runs the Android application that allows you to try out the various pieces implemented thus far\n* Once the application comes up, you will have to configure for your machine address and port for the socket camera to work.\n* Leave Use C++ SocketCapture CHECKED!\n* Choose which test you want to run.\n\n=== Java\n\nTo use Java, you have to make a small code change.  Eventually we will make this a configurable option without having to make a code change.  The reason is because when we send data over a socket from Java to Java it is faster to send serialized buffered images.  However, when we send data to C++, we have to send a raw byte array.\n\n* Modify the WebcamBroadcaster by changing the default value assigned to RAW to false.\n* Start the WebcamBroadcaster - this is a socket server that grabs images from your camera and serves them up\n* Start the VideoEmulator - this runs the Android application that allows you to try out the various pieces implemented thus far\n* Once the application comes up, you will have to configure for your machine address and port for the socket camera to work.\n* UNCHECK Use C++ SocketCapture!\n* Choose which test you want to run.\n"
 },
 {
  "repo": "JunshengFu/vehicle-detection",
  "language": "Python",
  "readme_contents": "# **Vehicle Detection for Autonomous Driving** \n\n## Objective\n\n#### A demo of Vehicle Detection System: a monocular camera is used for detecting vehicles. \n\n\n#### [**(1) Highway Drive (with Lane Departure Warning)**](https://youtu.be/Brh9-uab7Qs) (Click to see the full video)\n\n[![gif_demo1][demo1_gif]](https://youtu.be/Brh9-uab7Qs)\n\n#### [**(2) City Drive (Vehicle Detection only)**](https://youtu.be/2wOxK86LcaM) (Click to see the full video)\n[![gif_demo2][demo2_gif]](https://youtu.be/2wOxK86LcaM)\n\n---\n\n### Code & Files\n\n#### 1. My project includes the following files\n\n* [main.py](main.py) is the main code for demos\n* [svm_pipeline.py](svm_pipeline.py) is the car detection pipeline with SVM\n* [yolo_pipeline.py](yolo_pipeline.py) is the car detection pipeline with a deep net [YOLO (You Only Look Once)](https://arxiv.org/pdf/1506.02640.pdf)\n* [visualization.py](visualizations.py) is the function for adding visalization\n\n---\nOthers are the same as in the repository of [Lane Departure Warning System](https://github.com/JunshengFu/autonomous-driving-lane-departure-warning):\n* [calibration.py](calibration.py) contains the script to calibrate camera and save the calibration results\n* [lane.py](model.h5) contains the lane class \n* [examples](examples) folder contains the sample images and videos\n\n\n#### 2. Dependencies & my environment\n\nAnaconda is used for managing my [**dependencies**](https://github.com/udacity/CarND-Term1-Starter-Kit).\n* You can use provided [environment-gpu.yml](environment-gpu.yml) to install the dependencies.\n* OpenCV3, Python3.5, tensorflow, CUDA8  \n* OS: Ubuntu 16.04\n\n#### 3. How to run the code\n\n(1) Download weights for YOLO\n\nYou can download the weight from [here](https://drive.google.com/open?id=0B5WIzrIVeL0WS3N2VklTVmstelE) and save it to\nthe [weights](weights) folder.\n\n(2) If you want to run the demo, you can simply run:\n```sh\npython main.py\n```\n\n#### 4. Release History\n\n* 0.1.1\n    * Fix two minor bugs and update the documents\n    * Date 18 April 2017\n\n* 0.1.0\n    * The first proper release\n    * Date 31 March 2017\n\n---\n\n### **Two approaches: Linear SVM vs Neural Network**\n\n### 1. Linear SVM Approach\n`svm_pipeline.py` contains the code for the svm pipeline.\n\n**Steps:**\n\n* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n* A color transform is applied to the image and append binned color features, as well as histograms of color, to HOG feature vector. \n* Normalize your features and randomize a selection for training and testing.\n* Implement a sliding-window technique and use SVM classifier to search for vehicles in images.\n* Run pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n* Estimate a bounding box for detected vehicles.\n\n[//]: # (Image References)\n[image1]: ./examples/car_not_car.png\n[image2]: ./examples/hog_1.png\n[image2-1]: ./examples/hog_2.png\n[image3]: ./examples/search_windows.png\n[image4]: ./examples/heat_map1.png\n[image5]: ./examples/heat_map2.png\n[image6]: ./examples/labels_map.png\n[image7]: ./examples/svn_1.png\n[image8]: ./examples/yolo_1.png\n[image_yolo1]: ./examples/yolo1.png\n[image_yolo2]: ./examples/yolo2.png\n[video1]: ./project_video.mp4\n[demo1_gif]: ./examples/demo1.gif\n[demo2_gif]: ./examples/demo2.gif\n\n#### 1.1 Extract Histogram of Oriented Gradients (HOG) from training images\nThe code for this step is contained in the function named `extract_features` and codes from line 464 to 552 in `svm_pipeline.py`. \n If the SVM classifier exist, load it directly. \n \n Otherwise, I started by reading in all the `vehicle` and `non-vehicle` images, around 8000 images in each category.  These datasets are comprised of \n images taken from the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html) and \n [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/).\n Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n\n![alt text][image1]\n\n\nI then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n\nHere is an example using the `RGB` color space and HOG parameters of `orientations=9`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n\n![alt text][image2]\n![alt text][image2-1]\n \nTo optimize the HoG extraction, I **extract the HoG feature for the entire image only once**. Then the entire HoG image\nis saved for further processing. (see line 319 to 321 in  `svm_pipeline.py`)\n\n#### 1.2 Final choices of HOG parameters, Spatial Features and Histogram of Color.\n\nI tried various combinations of parameters and choose the final combination as follows \n(see line 16-27 in `svm_pipeline.py`):\n* `YCrCb` color space\n* orient = 9  # HOG orientations\n* pix_per_cell = 8 # HOG pixels per cell\n* cell_per_block = 2 # HOG cells per block, which can handel e.g. shadows\n* hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n* spatial_size = (32, 32) # Spatial binning dimensions\n* hist_bins = 32    # Number of histogram bins\n* spatial_feat = True # Spatial features on or off\n* hist_feat = True # Histogram features on or off\n* hog_feat = True # HOG features on or off\n\nAll the features are **normalized** by line 511 to 513 in `svm_pipeline.py`, which is a critical step. Otherwise, classifier \nmay have some bias toward to the features with higher weights.\n#### 1.3. How to train a classifier\nI randomly select 20% of images for testing and others for training, and a linear SVM is used as classifier (see line\n520 to 531 in `svm_pipeline.py`)\n\n#### 1.4 Sliding Window Search\nFor this SVM-based approach, I use two scales of the search window (64x64 and 128x128, see line 41) and search only between \n[400, 656] in y axis (see line 32 in `svm_pipeline.py`). I choose 75% overlap for the search windows in each scale (see \nline 314 in `svm_pipeline.py`). \n\nFor every window, the SVM classifier is used to predict whether it contains a car nor not. If yes, save this window (see \nline 361 to 366 in `svm_pipeline.py`). In the end, a list of windows contains detected cars are obtianed.\n\n![alt text][image3]\n\n#### 1.5 Create a heat map of detected vehicles\nAfter obtained a list of windows which may contain cars, a function named `generate_heatmap` (in line 565 in \n`svm_pipeline.py`) is used to generate a heatmap. Then a threshold is used to filter out the false positives.\n\n![heatmap][image4]\n![heatmap][image5]\n\n#### 1.6 Image vs Video implementation\n**For image**, we could directly use the result from the filtered heatmap to create a bounding box of the detected \nvehicle. \n\n**For video**, we could further utilize neighbouring frames to filter out the false positives, as well as to smooth \nthe position of bounding box. \n* Accumulate the heatmap for N previous frame.  \n* Apply weights to N previous frames: smaller weights for older frames (line 398 to 399 in `svm_pipeline.py`).\n* I then apply threshold and use `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  \n* I then assume each blob corresponded to a vehicle and constructe bounding boxes to cover the area of each blob detected.  \n\n\n#### Example of test image\n\n![alt text][image7]\n\n---\n\n\n### 2. Neural Network Approach (YOLO)\n`yolo_pipeline.py` contains the code for the yolo pipeline. \n\n[YOLO](https://arxiv.org/pdf/1506.02640.pdf) is an object detection pipeline baesd on Neural Network. Contrast to prior work on object detection with classifiers \nto perform detection, YOLO frame object detection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n\n![alt text][image_yolo2]\n\nSteps to use the YOLO for detection:\n* resize input image to 448x448\n* run a single convolutional network on the image\n* threshold the resulting detections by the model\u2019s confidence\n\n![alt text][image_yolo1]\n\n`yolo_pipeline.py` is modified and integrated based on this [tensorflow implementation of YOLO](https://github.com/gliese581gg/YOLO_tensorflow).\nSince the \"car\" is known to YOLO, I use the precomputed weights directly and apply to the entire input frame.\n\n#### Example of test image\n![alt text][image8]\n\n---\n\n### Discussion\nFor the SVM based approach, the accuray is good, but the speed (2 fps) is an problem due to the fact of sliding window approach \nis time consuming! We could use image downsampling, multi-threads, or GPU processing to improve the speed. But, there are probably\na lot engineering work need to be done to make it running real-time. Also, in this application, I limit the vertical searching \nrange to control the number of searching windows, as well as avoid some false positives (e.g. cars on the tree).\n\nFor YOLO based approach, it achieves real-time and the accuracy are quite satisfactory. Only in some cases, it may failure to\n detect the small car thumbnail in distance. My intuition is that the original input image is in resolution of 1280x720, and it needs to be downscaled\n to 448x448, so the car in distance will be tiny and probably quite distorted in the downscaled image (448x448). In order to \n correctly identify the car in distance, we might need to either crop the image instead of directly downscaling it, or retrain \n the network.\n"
 },
 {
  "repo": "PatWie/tensorflow-cmake",
  "language": "CMake",
  "readme_contents": "# TensorFlow CMake/C++ Collection\n\nLooking at the official docs: What do you see? The usual fare?\nNow, guess what: This is a bazel-free zone. We use CMake here!\n\nThis collection contains **reliable** and **dead-simple** examples to use TensorFlow in C, C++, Go and Python: load a pre-trained model or compile a custom operation with or without CUDA. All builds are tested against the most recent stable TensorFlow version and rely on CMake with a custom [FindTensorFlow.cmake](https://github.com/PatWie/tensorflow-cmake/blob/master/cmake/modules/FindTensorFlow.cmake). This cmake file includes common work arounds for bugs in specific TF versions.\n\n| TensorFlow | Status |\n| ------ | ------ |\n|   1.14.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.14.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.13.1 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.13.1/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.12.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.12.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.11.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.11.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.10.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.10.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n|   1.9.0 | [![Build Status TensorFlow](https://ci.patwie.com/api/badges/PatWie/tensorflow-cmake/TensorFlow%201.9.0/status.svg)](http://ci.patwie.com/PatWie/tensorflow-cmake) |\n\n\n\nThe repository contains the following examples.\n\n| Example| Explanation |\n| ------ | ------ |\n| [custom operation](./custom_op)   | build a custom operation for TensorFLow in C++/CUDA (requires only pip) |\n| [inference  (C++)](./inference/cc) | run inference in C++ |\n| [inference  (C)](./inference/c) | run inference in C |\n| [inference  (Go)](./inference/go) | run inference in Go |\n| [event writer](./examples/event_writer)  | write event files for TensorBoard in C++ |\n| [keras cpp-inference example](./examples/keras)  | run a Keras-model in C++ |\n| [simple example](./examples/simple)  | create and run a TensorFlow graph in C++ |\n| [resize image example](./examples/resize)  | resize an image in TensorFlow with/without OpenCV |\n\n\n## Custom Operation\n\nThis example illustrates the process of creating a custom operation using C++/CUDA and CMake. It is *not* intended to show an implementation obtaining peak-performance. Instead, it is just a boilerplate-template.\n\n```console\nuser@host $ pip install tensorflow-gpu --user # solely the pip package is needed\nuser@host $ cd custom_op/user_ops\nuser@host $ cmake .\nuser@host $ make\nuser@host $ python test_matrix_add.py\nuser@host $ cd ..\nuser@host $ python example.py\n```\n\n## TensorFlow Graph within C++\n\nThis example illustrates the process of loading an image (using OpenCV or TensorFlow), resizing the image  saving the image as a JPG or PNG (using OpenCV or TensorFlow).\n\n```console\nuser@host $ cd examples/resize\nuser@host $ export TENSORFLOW_BUILD_DIR=...\nuser@host $ export TENSORFLOW_SOURCE_DIR=...\nuser@host $ cmake .\nuser@host $ make\n```\n\n\n## TensorFlow-Serving\n\nThere are two examples demonstrating the handling of TensorFlow-Serving: using a vector input and using an encoded image input.\n\n```console\nserver@host $ CHOOSE=basic # or image\nserver@host $ cd serving/${CHOOSE}/training\nserver@host $ python create.py # create some model\nserver@host $ cd serving/server/\nserver@host $ ./run.sh # start server\n\n# some some queries\n\nclient@host $ cd client/bash\nclient@host $ ./client.sh\nclient@host $ cd client/python\n# for the basic-example\nclient@host $ python client_rest.py\nclient@host $ python client_grpc.py\n# for the image-example\nclient@host $ python client_rest.py /path/to/img.[png,jpg]\nclient@host $ python client_grpc.py /path/to/img.[png,jpg]\n```\n\n## Inference\n\nCreate a model in Python, save the graph to disk and load it in C/C+/Go/Python to perform inference. As these examples are based on the TensorFlow C-API they require the `libtensorflow_cc.so` library which is *not* shipped in the pip-package (tensorfow-gpu). Hence, you will need to build TensorFlow from source beforehand, e.g.,\n\n```console\nuser@host $ ls ${TENSORFLOW_SOURCE_DIR}\n\nACKNOWLEDGMENTS     bazel-genfiles      configure          pip\nADOPTERS.md         bazel-out           configure.py       py.pynano\nANDROID_NDK_HOME    bazel-tensorflow    configure.py.bkp   README.md\n...\nuser@host $ cd ${TENSORFLOW_SOURCE_DIR}\nuser@host $  ./configure\nuser@host $  # ... or whatever options you used here\nuser@host $ bazel build -c opt --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow:libtensorflow.so\nuser@host $ bazel build -c opt --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow:libtensorflow_cc.so\n\nuser@host $ export TENSORFLOW_BUILD_DIR=/tensorflow_dist\nuser@host $ mkdir ${TENSORFLOW_BUILD_DIR}\nuser@host $ cp ${TENSORFLOW_SOURCE_DIR}/bazel-bin/tensorflow/*.so ${TENSORFLOW_BUILD_DIR}/\nuser@host $ cp ${TENSORFLOW_SOURCE_DIR}/bazel-genfiles/tensorflow/cc/ops/*.h ${TENSORFLOW_BUILD_DIR}/includes/tensorflow/cc/ops/\n```\n\n### 1. Save Model\n\nWe just run a very basic model\n\n```python\nx = tf.placeholder(tf.float32, shape=[1, 2], name='input')\noutput = tf.identity(tf.layers.dense(x, 1), name='output')\n```\n\nTherefore, save the model like you regularly do. This is done in `example.py` besides some outputs\n\n```console\nuser@host $ python example.py\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32_ref>]\ninput            [[1. 1.]]\noutput           [[2.1909506]]\ndense/kernel:0   [[0.9070684]\n [1.2838823]]\ndense/bias:0     [0.]\n```\n\n### 2. Run Inference\n\n#### Python\n\n```console\nuser@host $ python python/inference.py\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32_ref>]\ninput            [[1. 1.]]\noutput           [[2.1909506]]\ndense/kernel:0   [[0.9070684]\n [1.2838823]]\ndense/bias:0     [0.]\n```\n\n#### C++\n\n```console\nuser@host $ cd cc\nuser@host $ cmake .\nuser@host $ make\nuser@host $ cd ..\nuser@host $ ./cc/inference_cc\n\ninput           Tensor<type: float shape: [1,2] values: [1 1]>\noutput          Tensor<type: float shape: [1,1] values: [2.19095063]>\ndense/kernel:0  Tensor<type: float shape: [2,1] values: [0.907068372][1.28388226]>\ndense/bias:0    Tensor<type: float shape: [1] values: 0>\n```\n\n#### C\n\n```console\nuser@host $ cd c\nuser@host $ cmake .\nuser@host $ make\nuser@host $ cd ..\nuser@host $ ./c/inference_c\n\n2.190951\n\n```\n\n\n#### Go\n\n```console\nuser@host $ go get github.com/tensorflow/tensorflow/tensorflow/go\nuser@host $ cd go\nuser@host $ ./build.sh\nuser@host $ cd ../\nuser@host $ ./inference_go\n\ninput           [[1 1]]\noutput          [[2.1909506]]\ndense/kernel:0  [[0.9070684] [1.2838823]]\ndense/bias:0    [0]\n```\n"
 },
 {
  "repo": "pageauc/pi-timolo",
  "language": "Python",
  "readme_contents": "# PI-TIMOLO [![Mentioned in Awesome <INSERT LIST NAME>](https://awesome.re/mentioned-badge.svg)](https://github.com/thibmaek/awesome-raspberry-pi)\n### Raspberry (Pi)camera, (Ti)melapse, (Mo)tion, (Lo)wlight \n## For Details See [Program Features](https://github.com/pageauc/pi-timolo/wiki/Introduction#program-features) and [Wiki Instructions](https://github.com/pageauc/pi-timolo/wiki) and [YouTube Videos](https://www.youtube.com/playlist?list=PLLXJw_uJtQLa11A4qjVpn2D2T0pgfaSG0)\n\n***IMPORTANT:*** Raspbian Stretch and pi-timolo.py ver 11.11 and earlier has long exposure low light \ncamera freezing issue due to kernel panic that requires a reboot to gain\ncontrol of camera back per https://github.com/waveform80/picamera/issues/528 \npi-timolo.py ver 11.12 has a fix to resolve issue but\nrequires the latest Raspbian firmware. If you encounter camera freeze with latest Stretch image then\nyou will need to run ***sudo rpi-update*** to update Stretch to latest firmware.  Normal\nbackup precautions are advised before doing the firmware update.  See [wiki](https://github.com/pageauc/pi-timolo/wiki/Basic-Trouble-Shooting#raspbian-stretch-kernel-panic-and-camera-freeze)\n***Note:*** Raspbian Jessie works fine and does Not encounter freezing issue with long exposure low light operation.\n\n* ***Release 9.x*** New Features have been Added. See Wiki Details below    \n [plugins Setup and Operation](https://github.com/pageauc/pi-timolo/wiki/How-to-Use-Plugins)   \n [Rclone Setup and Media Sync](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-rclone) (Replaces gdrive)    \n [watch-app.sh Remote Configuration Management](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-config.py-Remote-Configuration)   \n [python3 Support Details](https://github.com/pageauc/pi-timolo/wiki/Prerequisites#python-3-support)   \n* ***Release 10.x*** Added Sched Start to Motion Track, Timelapse and VideoRepeat. See Wiki Details below    \n [How To Schedule Motion, Timelapse or VideoRepeat](https://github.com/pageauc/pi-timolo/wiki/How-to-Schedule-Motion,-Timelapse-or-VideoRepeat)  \n This release requires config.py be updated by the user with config.py.new since new variables have been added.\n* ***Release 11.12*** Added adhoc fix for Debian Stretch kernel panic and camera freeze issue when running\n under very low light conditions.  Note a ***sudo rpi-update*** may be required to update firmware if freezing\n still occurs under pi-timolo.py ver 11.12 or greater\n* ***Release 11.55*** Added config.py setting ***nightTwilightModeOn***  True is the normal twilight mode for outside conditions.\nFalse is used for indoors conditions where there is no twilight.  This setting will avoid overexposure when lights\nor sudden lighting changes are encountered.  If you have previously experienced overexposure when camera is indoors then\nthis setting should help. \n* ***Release 12.0*** Added [pantilthat panoramic image stitching Feature](https://github.com/pageauc/pi-timolo/wiki/Panoramic-Images-Stitching-Feature) \nwith support for Pimoroni and [Waveshare pantilthat](https://github.com/pageauc/waveshare.pantilthat) (and compatilble) hardware.\nThis release also renames all the [config.py](https://github.com/pageauc/pi-timolo/blob/master/source/config.py) variable Constants to snake_case and does some code cleanup.  You need to update to\nthe new config.py.  Built in instructions will prompt you if needed.  Please raise github issue if there are\nproblems. See config.py comments and revised wiki for details. \n \n## Requirements\nRequires a [***Raspberry Pi computer***](https://www.raspberrypi.org/documentation/setup/) and a \n[***RPI camera module installed***](https://www.raspberrypi.org/documentation/usage/camera/).\nMake sure hardware is tested and works. Most [RPI models](https://www.raspberrypi.org/products/) will work OK. \nA quad core RPI will greatly improve performance due to threading. A recent version of \n[Raspbian operating system](https://www.raspberrypi.org/downloads/raspbian/) is Recommended.\n \n## Quick Install or Upgrade\n**IMPORTANT** - It is suggested you do a Raspbian ***sudo apt-get update*** and ***sudo apt-get upgrade***\nbefore curl install, since it is **No longer** performed by the pi-timolo-install.sh script\n\n***Step 1*** With mouse left button highlight curl command in code box below. Right click mouse in **highlighted** area and Copy.     \n***Step 2*** On RPI putty SSH or terminal session right click, select paste then Enter to download and run script.     \n\n    curl -L https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo-install.sh | bash\n\nThe command above will download and Run the GitHub ***pi-timolo-install.sh*** script. \nAn upgrade will not overwrite configuration files.   \n\n* ***NOTICE*** gdrive is no longer installed with pi-timolo-install.sh, I have been testing\nrclone and it is Now the Default. Some ***rclone-*** samples are included. Make a copy of one, rename and edit for\nyour own needs.  See [Wiki - How to Setup Rclone](https://github.com/pageauc/pi-timolo/wiki/How-to-Setup-rclone).\nNote: If a ***/usr/local/bin/gdrive*** File Exists, It Will Remain. Older files are still available on this GitHub Repo.   \n\n## Test Install\nTo Test Run default config.py - motion track(HD image) plus timelapse(5 min interval). \n \n    cd ~/pi-timolo\n    ./pi-timolo.py\n\n### For More Details see [Basic Trouble Shooting](https://github.com/pageauc/pi-timolo/wiki/Basic-Trouble-Shooting) or [pi-timolo Wiki](https://github.com/pageauc/pi-timolo/wiki)\n\n## Description\nPI-TIMOLO is primarily designed for ***headless operation*** and includes rclone that\ncan securely synchronize specified media folders and files with a users remote storage service of choice. This works well for remote security and monitoring\ncameras. Camera config.py and conf settings can be easily administered remotely from a designated sync directory using ***watch-app.sh***\nscript using a crontab entry to periodically check for updates between the pi-timolo camera and a users remote storage rclone service name. \n\npi-timolo is python 2/3 compatible and can take timelapse and/or motion tracking images/videos, separately or together. Will take\nlong exposure Night (lowlight) images for Time Lapse and/or Motion. Has relatively smooth twilight transitions based on a threshold light\nsetting, so a real time clock is not required. Customization settings are saved in a ***config.py*** and conf files and optional special\npurpose plugin config files. Optional plugin feature allows overlaying config.py settings with custom settings for specific tasks.  \n\nIncludes ***makevideo.sh*** to create timelapse or motion lapse videos from images, ***convid.sh*** to convert/combine \nh264 to mp4 format, a simple minumum or no setup web server to view images or videos and ***menubox.sh*** \nto admin settings and stop start pi-timolo and webserver as background tasks. \n       \nFor more Details see [Github Wiki](https://github.com/pageauc/pi-timolo/wiki)   \n\n## Minimal Upgrade\nIf you are just interested in a minimal upgrade (must have pi-timolo previously installed)\nfrom a logged in ssh or terminal session execute the following commands.  \n\n    cd ~/pi-timolo\n    sudo apt-get install python-opencv\n    cp config.py config.py.old\n    cp pi-timolo.py pi-timolo.py.old\n    wget -O config.py https://raw.github.com/pageauc/pi-timolo/master/source/config.py\n    wget -O pi-timolo.py https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo.py    \n    \nEdit config.py to transfer any customized settings from config.py.old  \n    \n## Manual Install or Upgrade  \nFrom logged in RPI SSH session or console terminal perform the following. You can review\nthe pi-timolo-install.sh script code before executing.\n\n    cd ~\n    wget https://raw.github.com/pageauc/pi-timolo/master/source/pi-timolo-install.sh\n    more pi-timolo-install.sh    # Review code if required\n    chmod +x pi-timolo-install.sh\n    ./pi-timolo-install.sh\n    \n## Menubox\npi-timolo has a whiptail administration menu system. The menu's allow\nstart/stop of pi-timolo.py and/or webserver.py as background tasks, as well as\nediting configuration files, making timelapse videos from jpg images, converting or joining mp4 files Etc.    \n\nTo run menubox.sh from ssh console or terminal session execute commands below.\n\n    cd ~/pi-timolo\n    ./menubox.sh\n\n![menubox main menu](menubox.png)\n \n## Webserver\nI have also written a standalone LAN based webserver.py to allow easy access to pi-timolo image and video files\non the Raspberry from another LAN computer web browser.  There is no setup required but the display\nsettings can be customized via variables in the config.py file or via menubox admin menuing.     \n***NOTE:*** webserver.py is normally run in background using menubox.sh, webserver.sh or from /etc/rc.local     \nTo Test Run from ssh console or terminal session. \n    \n    cd ~/pi-timolo\n    ./webserver.py\n\n![webserver browser screen shot](webserver.jpg)\n \n## Reference Links  \nDetailed pi-timolo Wiki https://github.com/pageauc/pi-timolo/wiki  \n[my pi-timolo and other YouTube Videos playlist](https://www.youtube.com/playlist?list=PLLXJw_uJtQLa11A4qjVpn2D2T0pgfaSG0)    \n[MagPi Object Recognition using pi-timolo](https://magpi.raspberrypi.org/articles/wildlife-camera-object-recognition)    \n[makezine night vision project using pi-timolo](https://makezine.com/2016/05/26/spy-on-garden-critters-with-raspberry-pi-powered-night-vision/)    \n[hackster facial recognition using pi-timolo](https://www.hackster.io/gr1m/raspberry-pi-facial-recognition-16e34e)    \n[Neverending project timelapse using pi-timolo](https://www.theneverendingprojectslist.com/raspberrypiprojects/timelapse/)       \n[hedgehog camera using pi-timolo](http://www.sconemad.com/blog/hedgeycam/) and [step by step](https://oraclefrontovik.com/2016/08/28/a-step-by-step-guide-to-building-a-raspberry-pi-hedgehog-camera/)    \n[Museum Insect activity monitoring using pi-timolo](https://www.vam.ac.uk/blog/caring-for-our-collections/making-a-simple-insect-activity-monitor-using-a-raspberry-pi)    \n[Brett Beeson timelapse cloud project using pi-timolo](https://brettbeeson.com.au/timelapse-cloud/)    \n\t\nGood Luck\nClaude Pageau \n"
 },
 {
  "repo": "avidLearnerInProgress/pyCAIR",
  "language": "Python",
  "readme_contents": ".. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME937258 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME707802 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n\nREADME2888 (Click to Expand/Collapse)\n\n.. raw:: html\n\n   <h1 align=\"center\">\n\n.. raw:: html\n\n   </h1>\n\npyCAIR is a content-aware image resizing(CAIR)\n`library <https://pypi.org/project/pyCAIR/>`__ based on `Seam Carving\nfor Content-Aware Image\nResizing <http://http://graphics.cs.cmu.edu/courses/15-463/2012_fall/hw/proj3-seamcarving/imret.pdf>`__\npaper.\n\n|PyPI version| |License: GPL v3|\n\nTable of Contents\n=================\n\n1. `How CAIR works <#how-does-it-work>`__\n2. `Understanding the research\n   paper <#intutive-explanation-of-research-paper>`__\n3. `Project structure and\n   explanation <#project-structure-and-explanation>`__\n4. `Installation <#installation>`__\n5. `Usage <#usage>`__\n6. `Demo <#in-action>`__\n7. `Screenshots <#screenshots>`__\n8. `Todo <#todo>`__\n\nHow does it work\n================\n\n-  An energy map and a grayscale format of image is generated from the\n   provided image.\n\n-  Seam Carving algorithm tries to find the not so useful regions in\n   image by picking up the lowest energy values from energy map.\n\n-  With the help of Dynamic Programming coupled with backtracking, seam\n   carving algorithm generates individual seams over the image using\n   top-down approach or left-right approach.(depending on vertical or\n   horizontal resizing)\n\n-  By traversing the image matrix row-wise, the cumulative minimum\n   energy is computed for all possible connected seams for each entry.\n   The minimum energy level is calculated by summing up the current\n   pixel with the lowest value of the neighboring pixels from the\n   previous row.\n\n-  Find the lowest cost seam from the energy matrix starting from the\n   last row and remove it.\n\n-  Repeat the process iteratively until the image is resized depending\n   on user specified ratio.\n\n+-----------+----------------------------------+\n| |Result7| | |Result8|                        |\n+===========+==================================+\n| DP Matrix | Backtracking with minimum energy |\n+-----------+----------------------------------+\n\nIntutive explanation of research paper\n======================================\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes1.png\n       :alt: Notes1\n\n       Notes1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes2.png\n       :alt: Notes2\n\n       Notes2\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes3.png\n       :alt: Notes3\n\n       Notes3\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/06ce7c6e/notes/notes4.png\n       :alt: Notes4\n\n       Notes4\n\nProject structure and explanation\n=================================\n\n**Directory structure:**\n\n| **pyCAIR** (root directory)\n|   \\| - images/\n|   \\| - results /\n|   \\| - sequences/ (zipped in repository)\n|   \\| - videos/\n|   \\| -\n  `notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n|   \\| -\n  `imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n|   \\| -\n  `opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n|   \\| -\n  `seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n|   \\| -\n  `helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/helpers.py>`__\n\n**File:**\n`notdoneyet.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/notdoneyet.py>`__\n\n-  **user_input()** -\n   Parameters:\n\n   -  Alignment: Specify on which axis the resizing operation has to be\n      performed.\n   -  Scale Ratio: Floating point operation between 0 and 1 to scale the\n      output image.\n   -  Display Seam: If this option isn\u2019t selected, the image is only\n      seamed in background.\n   -  Input Image\n   -  Generate Sequences: Generate intermediate sequences to form a\n      video after all the operations are performed.\n\n**File:**\n`imgtovideos.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/imgtovideos.py>`__\n\n-  **generateVideo()** - pass each image path to **vid()** for video\n   generation.\n\n-  **vid()**- writes each input image to video buffer for creating a\n   complete video.\n\n**File:**\n`opencv_generators.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/opencv_generators.py>`__\n\n-  **generateEnergyMap()** - utilised OpenCV inbuilt functions for\n   obtaining energies and converting image to grayscale.\n\n-  **generateColorMap()** - utilised OpenCV inbuilt functions to\n   superimpose heatmaps on the given image.\n\n**File:**\n`seam_carve.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/pyCAIR/seam_carve.py>`__\n\n-  **getEnergy()** - generated energy map using sobel operators and\n   convolve function.\n\n-  **getMaps()** - implemented the function to get seams using Dynamic\n   Programming. Also, stored results of minimum seam in seperate list\n   for backtracking.\n\n-  **drawSeam()** - Plot seams(vertical and horizontal) using red color\n   on image.\n\n-  **carve()** - reshape and crop image.\n\n-  **cropByColumn()** - Implements cropping on both axes, i.e. vertical\n   and horizontal.\n\n-  **cropByRow()** - Rotate image to ignore repeated computations and\n   provide the rotated image as an input to *cropByColumn* function.\n\n**File:**\n`helpers.py <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/helpers.py>`__\n\n-  **writeImage()** - stores the images in results directory.\n\n-  **writeImageG()** - stores intermediate generated sequence of images\n   in sequences directory.\n\n-  **createFolder()** - self explanatory\n\n-  **getFileExtension()** - self explanatory\n\n**Other folders:**\n\n-  **images/** - stores the input images for testing.\n\n-  **videos/** - stores the videos generated from the intermediate\n   sequences.\n\n-  **results/** - stores the final results.\n\n-  **sequences/** - stores the intermediate sequences generated.\n\nInstallation\n============\n\n-  Simply run ``pip install pyCAIR``\n\n-  `Direct download\n   option <https://github.com/avidLearnerInProgress/pyCAIR/archive/0.1.tar.gz>`__\n\nUsage\n=====\n\n.. code:: python\n\n    '''\n    It runs the entire code and returns final results\n    '''\n    from pyCAIR import user_input\n    user_input(alignment, scale, seam, input_image, generate_sequences)\n\n    '''\n    It generates the energy map\n    '''\n    from pyCAIR import generateEnergyMap\n    generateEnergyMap(image_name, file_extension, file_name)\n\n    '''\n    It generates color maps\n    '''\n    from pyCAIR import generateColorMap\n    generateColorMap(image_name, file_extension, file_name)\n\n    '''\n    It converts sequence of images generated to video\n    '''\n    from pyCAIR import generateVideo\n    generateVideo()\n\n    '''\n    It returns all the paths where images are present for generating video\n    '''\n    from pyCAIR import getToProcessPaths\n    getToProcessPaths()\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByColumn\n    seam_img, crop_img = cropByColumn(image, display_seams, generate, lsit, scale_c, fromRow)\n\n    '''\n    It returns seams, cropped image for an image\n    '''\n    from pyCAIR import cropByRow\n    seam_img, crop_img = cropByRow(image, display_seams, generate, lsit, scale_c)\n\n    '''\n    It returns created folder\n    '''\n    from pyCAIR import createFolder\n    f = createFolder(folder_name)\n\n    '''\n    It returns extension of file\n    '''\n    from pyCAIR import getFileExtension\n    f = getFileExtension(file_name)\n\n    '''\n    It writes image to specified folder\n    '''\n    from pyCAIR import writeImage\n    f = writeImage(image, args)\n\nIn Action\n=========\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig13_col-wise_seamseq.gif\n       :alt: Gif1\n\n       Gif1\n\n..\n\n    .. figure:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/5eb764fd/others/fig4_col-wise_seamseq.gif\n       :alt: Gif2\n\n       Gif2\n\n..\n\n    `Video\n    Playlist <https://www.youtube.com/playlist?list=PL7k5xCepzh7o2kF_FMh4P9tZgALoAx48N>`__\n\nScreenshots\n===========\n\nResults for Image 1:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nResults for Image 2:\n--------------------\n\n+----------------+-----------+------------+\n| |Result0|      | |Result1| | |Result2|  |\n+================+===========+============+\n| Original Image | Grayscale | Energy Map |\n+----------------+-----------+------------+\n\n+------------------+---------------+\n| |Result3|        | |Result4|     |\n+==================+===============+\n| Color Map Winter | Color Map Hot |\n+------------------+---------------+\n\n+-------------------+-----------------+\n| |Result5|         | |Result6|       |\n+===================+=================+\n| Seams for Columns | Columns Cropped |\n+-------------------+-----------------+\n\n+----------------+--------------+\n| |Result7|      | |Result8|    |\n+================+==============+\n| Seams for Rows | Rows Cropped |\n+----------------+--------------+\n\nTodo\n====\n\n-  [x] Implement Seam Algorithm\n-  [x] Generate energy maps and color maps for image\n-  [x] Display Vertical Seams\n-  [x] Display Horizontal Seams\n-  [x] Crop Columns\n-  [x] Crop Rows\n-  [x] Use argparse for Command Line Application\n-  [x] Store subsamples in different directories for crop and seam\n   respectively\n-  [x] Generate video/gif from sub-samples\n-  [x] Provide a better Readme\n-  [x] Provide examples for usage\n-  [ ] Add badges\n-  [ ] Provide better project description on PyPI\n-  [ ] Documentation using Spinx\n-  [ ] Integrate object detection using YOLOv2\n-  [ ] Identify most important object (using probability of predicted\n   object)\n-  [ ] Invert energy values of most important object\n-  [ ] Re-apply Seam Carve and compare results\n\nLicense\n=======\n\nThis software is licensed under the `GNU General Public License\nv3.0 <https://github.com/avidLearnerInProgress/pyCAIR/blob/master/LICENSE>`__\n\u00a9 `Chirag Shah <https://github.com/avidLearnerInProgress>`__\n\n.. |PyPI version| image:: https://badge.fury.io/py/pyCAIR.svg\n   :target: https://badge.fury.io/py/pyCAIR\n.. |License: GPL v3| image:: https://img.shields.io/badge/License-GPL%20v3-blue.svg\n   :target: https://www.gnu.org/licenses/gpl-3.0\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/algorithm.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/c4692303/others/backtracking.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig4.png\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/gray.png\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/energy.png\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap1.png\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/colormap2.png\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_seams.png\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/column_cropped.png\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_seams.png\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig4/row_cropped.png\n.. |Result0| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/images/fig13.jpg\n.. |Result1| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/gray.jpg\n.. |Result2| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/energy.jpg\n.. |Result3| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap1.jpg\n.. |Result4| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/colormap2.jpg\n.. |Result5| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_seams.jpg\n.. |Result6| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/column_cropped.jpg\n.. |Result7| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_seams.jpg\n.. |Result8| image:: https://cdn.rawgit.com/avidLearnerInProgress/pyCAIR/0fc66d01/results/fig13/row_cropped.jpg\n"
 },
 {
  "repo": "adipandas/multi-object-tracker",
  "language": "Python",
  "readme_contents": "[cars-yolo-output]: examples/assets/cars.gif \"Sample Output with YOLO\"\n[cows-tf-ssd-output]: examples/assets/cows.gif \"Sample Output with SSD\"\n\n# Multi-object trackers in Python\nEasy to use implementation of various multi-object tracking algorithms.\n\n[![DOI](https://zenodo.org/badge/148338463.svg)](https://zenodo.org/badge/latestdoi/148338463)\n\n\n`YOLOv3 + CentroidTracker` |  `TF-MobileNetSSD + CentroidTracker`\n:-------------------------:|:-------------------------:\n![Cars with YOLO][cars-yolo-output]  |  ![Cows with tf-SSD][cows-tf-ssd-output]\nVideo source: [link](https://flic.kr/p/L6qyxj) | Video source: [link](https://flic.kr/p/26WeEWy)\n\n\n## Available Multi Object Trackers\n\n```\nCentroidTracker\nIOUTracker\nCentroidKF_Tracker\nSORT\n```\n\n## Available OpenCV-based object detectors:\n\n```\ndetector.TF_SSDMobileNetV2\ndetector.Caffe_SSDMobileNet\ndetector.YOLOv3\n```\n\n## Installation\n\nPip install for OpenCV (version 3.4.3 or later) is available [here](https://pypi.org/project/opencv-python/) and can be done with the following command:\n\n```\ngit clone https://github.com/adipandas/multi-object-tracker\ncd multi-object-tracker\npip install -r requirements.txt\npip install -e .\n```\n\n**Note - for using neural network models with GPU**  \nFor using the opencv `dnn`-based object detection modules provided in this repository with GPU, you may have to compile a CUDA enabled version of OpenCV from source.  \n* To build opencv from source, refer the following links:\n[[link-1](https://docs.opencv.org/master/df/d65/tutorial_table_of_content_introduction.html)],\n[[link-2](https://www.pyimagesearch.com/2020/02/03/how-to-use-opencvs-dnn-module-with-nvidia-gpus-cuda-and-cudnn/)]\n\n## How to use?: Examples\n\nThe interface for each tracker is simple and similar. Please refer the example template below.\n\n```\nfrom motrackers import CentroidTracker # or IOUTracker, CentroidKF_Tracker, SORT\n\ninput_data = ...\ndetector = ...\ntracker = CentroidTracker(...) # or IOUTracker(...), CentroidKF_Tracker(...), SORT(...)\n\nwhile True:\n    done, image = <read(input_data)>\n    if done:\n        break\n\n    detection_bboxes, detection_confidences, detection_class_ids = detector.detect(image)\n    # NOTE: \n    # * `detection_bboxes` are numpy.ndarray of shape (n, 4) with each row containing (bb_left, bb_top, bb_width, bb_height)\n    # * `detection_confidences` are numpy.ndarray of shape (n,);\n    # * `detection_class_ids` are numpy.ndarray of shape (n,).\n\n    output_tracks = tracker.track(detection_bboxes, detection_confidences, detection_class_ids)\n    \n    # `output_tracks` is a list with each element containing tuple of\n    # (<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>)\n    for track in output_tracks:\n        frame, id, bb_left, bb_top, bb_width, bb_height, confidence, x, y, z = track\n        assert len(track) == 10\n        print(track)\n```\n\nPlease refer [examples](https://github.com/adipandas/multi-object-tracker/tree/master/examples) folder of this repository for more details.\nYou can clone and run the examples as shown [here](examples/readme.md).\n\n## Pretrained object detection models\n\nYou will have to download the pretrained weights for the neural-network models. \nThe shell scripts for downloading these are provided [here](https://github.com/adipandas/multi-object-tracker/tree/master/examples/pretrained_models) below respective folders.\nPlease refer [DOWNLOAD_WEIGHTS.md](DOWNLOAD_WEIGHTS.md) for more details.\n\n### Notes\n* There are some variations in implementations as compared to what appeared in papers of `SORT` and `IoU Tracker`.\n* In case you find any bugs in the algorithm, I will be happy to accept your pull request or you can create an issue to point it out.\n\n## References, Credits and Contributions\n\nPlease see [REFERENCES.md](docs/readme/REFERENCES.md) and [CONTRIBUTING.md](docs/readme/CONTRIBUTING.md).\n\n## Citation\n\nIf you use this repository in your work, please consider citing it with:\n```\n@misc{multiobjtracker_amd2018,\n  author = {Deshpande, Aditya M.},\n  title = {Multi-object trackers in Python},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/adipandas/multi-object-tracker}},\n}\n```\n\n```\n@software{aditya_m_deshpande_2020_3951169,\n  author       = {Aditya M. Deshpande},\n  title        = {Multi-object trackers in Python},\n  month        = jul,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v1.0.0},\n  doi          = {10.5281/zenodo.3951169},\n  url          = {https://doi.org/10.5281/zenodo.3951169}\n}\n```\n"
 },
 {
  "repo": "royshil/SfM-Toy-Library",
  "language": "C++",
  "readme_contents": "# Toy Structure From Motion Library using OpenCV\n\nThis is a reference implementation of a Structure-from-Motion pipeline in OpenCV, following the work of Snavely et al. [2] and Hartley and Zisserman [1].\n\n*Note:* This is not a complete and robust SfM pipeline implementation. The purpose of this code is to serve as a tutorial and reference for OpenCV users and a soft intro to SfM in OpenCV. If you are looking for a more complete solution with many more options and parameters to tweak, check out the following:\n\n * OpenMVG http://openmvg.readthedocs.io/en/latest/#\n * libMV https://developer.blender.org/tag/libmv/\n * VisualSFM http://ccwu.me/vsfm/\n * Bundler http://www.cs.cornell.edu/~snavely/bundler/\n\nSfM-Toy-Library is now using OpenCV 3, which introduced many new convenience functions to Structure from Motion (see my [blog post](http://www.morethantechnical.com/2016/10/17/structure-from-motion-toy-lib-upgrades-to-opencv-3/) for details), making the implementation much cleaner and simpler. \n\nCeres solver was chosen to do bundle adjustment, for its simple API, straightforward modeling of the problem and long-term support.\n\nDoxygen-style documentation comments appear throughout.\n\n## Compile\n\nTo compile use CMake: http://www.cmake.org\n\n### Prerequisite\n- OpenCV 3.x: http://www.opencv.org\n- Ceres Solver (for bundle adjustment): http://ceres-solver.org/\n- Boost C++ libraries v1.54+: http://www.boost.org/\n\n### How to make\n\n#### On OSX Using XCode\n\nGet Boost and Ceres using homebrew: `brew install boost ceres-solver` (you will need to tap `homebrew/science` for Ceres)\n\n\tmkdir build\n\tcd build\n\tcmake -G \"Xcode\" ..\n\topen SfMToyExample.xcodeproj\n\t\n#### On Linux (or OSX) via a Makefile\n\nObtain Boost (with e.g. `apt-get install libboost-all-dev`) and Ceres (probably need to clone and compile), or on OSX view homebrew as mentioned before.\n\n\tmkdir build\n\tcd build\n\tcmake -G \"Unix Makefiles\" ..\n\tmake \n\n#### On Windows\n\nUse Cmake's GUI to create a MSVC solution, and build it.\n\n## Usage\n\n### Execute\n\n    USAGE ./build/SfMToyUI [options] <input-directory>\n      -h [ --help ]                   produce help message\n      -d [ --console-debug ] arg (=2) Debug output to console log level (0 = Trace,\n                                      4 = Error).\n      -v [ --visual-debug ] arg (=3)  Visual debug output to screen log level (0 = \n                                      All, 4 = None).\n      -s [ --downscale ] arg (=1)     Downscale factor for input images\n      -p [ --input-directory ] arg    Directory to find input images\n\n### Datasets\n\nHere's a place with some standard datasets for SfM: http://cvlabwww.epfl.ch/data/multiview/denseMVS.html\n\nAlso, you can use the \"Crazy Horse\" (A national memorial site in South Dakota) dataset, that I pictured myself, included in the repo.\n\n### Other\n\nSome relevant blog posts from over the years:\n- http://www.morethantechnical.com/2016/10/17/structure-from-motion-toy-lib-upgrades-to-opencv-3/\n- http://www.morethantechnical.com/2015/03/16/bootstrapping-planar-ar-tracking-without-markers-wcode/\n- http://www.morethantechnical.com/2013/11/04/moving-to-qt-on-the-sfm-toy-library-project/\n- http://www.morethantechnical.com/2012/08/09/checking-for-co-planarity-of-3d-points-in-opencv-wcode/\n- http://www.morethantechnical.com/2012/08/09/decomposing-the-essential-matrix-using-horn-and-eigen-wcode/\n- http://www.morethantechnical.com/2012/02/07/structure-from-motion-and-3d-reconstruction-on-the-easy-in-opencv-2-3-w-code/\n- http://www.morethantechnical.com/2012/01/04/simple-triangulation-with-opencv-from-harley-zisserman-w-code/\n\n## References\n\n1. Multiple View Geometry in Computer Vision, Hartley, R. I. and Zisserman, A., 2004, Cambridge University Press [http://www.robots.ox.ac.uk/~vgg/hzbook/]\n2. Modeling the World from Internet Photo Collections, N. Snavely, S. M. Seitz, R. Szeliski, IJCV 2007 [http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf]\n3. Triangulation, R.I. Hartley, P. Strum, 1997, Computer vision and image understanding [http://perception.inrialpes.fr/Publications/1997/HS97/HartleySturm-cviu97.pdf]\n4. Recovering baseline and orientation from essential matrix, B.K.P. Horn, 1990, J. Optical Society of America [http://people.csail.mit.edu/bkph/articles/Essential_Old.pdf]\n5. On benchmarking camera calibration and multi-view stereo for high resolution imagery. Strecha, Christoph, et al. IEEE Computer Vision and Pattern Recognition (CVPR) 2008. [http://infoscience.epfl.ch/record/126393/files/strecha_cvpr_2008.pdf]\n"
 },
 {
  "repo": "A9T9/RPA",
  "language": "JavaScript",
  "readme_contents": "# UI.Vision [RPA](https://ui.vision/rpa) (formerly Kantu)\n\n- Modern Robotic Process Automation plus Selenium IDE++\n\nQuestions? Suggestions? - Meet us in the UI.Vision [RPA user forum](https://forum.ui.vision).\n\nEvery user benefits from the questions and answers provided in the forum, that is why we would ask you to post the question [in the RPA forum](https://forum.ui.vision) first if a public forum is appropriate for your question. The forum is monitored by active users, tech support and the developers, so we would like to concentrate the discussion \"over there\" in one place\n\n\n# Are you interested in becoming a UI.Vision RPA beta tester?\n\nAs beta tester, we will email you (hidden) install links for new  UI.Vision RPA versions before they go live in the Chrome store. **Beta versions never overwrite a regular  UI.Vision RPA version**. The beta channel is technically a separate extension with its own toolbar icon. So beta-testing  UI.Vision RPA does not interfere with your regular RPA projects and macros. We will notify you by email once a new version is available so you can grab it. \n\n[If you are interested in helping out, please sign-up here](http://eepurl.com/dm0cTX).\n\nThe link goes to a Mailchimp signup form. \n\n\n# How to install  UI.Vision RPA:\n\n UI.Vision RPA for Chrome and Firefox is modern cross-platform RPA software for macOS, Linux and Windows. It includes a Selenium IDE and Web Macro Recorder. You find the latest version always in the Chrome and Firefox stores. You can use it _completely free for private and commercial purposes_: \n\n- [UI.Vision RPA in the Google Chrome Webstore](https://chrome.google.com/webstore/detail/uivision-rpa/gcbalfbdmfieckjlnblleoemohcganoc)\n\n- [UI.Vision RPA in the Firefox Webstore](https://addons.mozilla.org/en-US/firefox/addon/rpa/)\n\n- [UI.Vision RPA plus Selenium IDE Homepage](https://ui.vision/rpa)\n\n- List of supported [Selenium IDE commands](https://ui.vision/rpa/docs/selenium-ide/)\n\n\n# Building the Chrome and Firefox Extension\n\nWe use Node V8.11.1 and NPM V5.6.0.\n\nYou can [install UI.Vision RPA directly from the Chrome or Firefox stores](https://ui.vision/rpa), which is the easiest and the recommended way of using the UI.Vision RPA software. But of course developers can also build it directly from the source code with this command line:\n\n```\nnpm i\nnpm run build (or build-ff for Firefox)\n```\n\nOnce done, the ready-to-use extension code appears in the /dist directory (Chrome) or /dist_ff directory (Firefox).\n\n# Need the very latest source code version?\n\nPlease note that we use an internal source code repository for our daily development work. The very latest source code snapshot can always be requested directly from the development team. Please contact us at team AT ui.vision. We are looking forward talking to you. And of course, if you want to join the development you are welcome.\n"
 },
 {
  "repo": "andrewssobral/vehicle_detection_haarcascades",
  "language": "C++",
  "readme_contents": "# Vehicle Detection with Haar Cascades\n\nLast page update: **19/10/2016**\n\nLast version: **1.0.0** (see Release Notes for more info)\n\nHello everyone,\nAn easy way to perform vehicle detection is by using Haar Cascades. Currently, I don't have a detailed tutorial about it, but you can get some extra information in the OpenCV homepage, see [Cascade Classifier](http://docs.opencv.org/2.4/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html) page. See also [Cascade Classifier Training](http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html) for training your own cascade classifier.\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=c4LobbqeKZc\" target=\"_blank\">\n<img src=\"https://raw.githubusercontent.com/andrewssobral/vehicle_detection_haarcascades/master/doc/images/vehicle_detection_haarcascades.png\" border=\"0\" />\n</a>\n</p>\n\nThe haar-cascade **cars.xml** was trained using 526 images of cars from the rear (360 x 240 pixels, no scale).\nThe images were extracted from the Car dataset proposed by Brad Philip and Paul Updike taken of the freeways of southern California.\n\nFor more information, please see:\n\n* Train Your Own OpenCV Haar Classifier\n * http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html\n * https://github.com/mrnugget/opencv-haar-classifier-training\n\n* Related paper:\n * Oliveira, M.; Santos, V. Automatic Detection of Cars in Real Roads using Haar-like Features ([PDF](https://github.com/andrewssobral/vehicle_detection_haarcascades/raw/master/doc/Automatic_Detection_of_Cars_in_Real_Roads_using_Haar-like_Features.pdf))\n\n* Some additional resources:\n * http://lars.mec.ua.pt/public/Media/ResearchDevelopmentProjects/HaarFeatures_RoadFilms/HaarFeaturesTests/CarsRear/\n * http://lars.mec.ua.pt/public/Media/ResearchDevelopmentProjects/HaarFeatures_RoadFilms/HaarFeaturesTests/\n\nFor Windows users\n-----------------\n* Check if your OpenCV is installed at: C:\\OpenCV2.4.10\n* There is a Visual Studio 2013 template project in the **vs2013/** folder. Open it in the Visual Studio IDE and select [Release]-[Win32] or [Release]-[x64] mode. Next, click on **run_vehicle_detection.bat** and enjoy!\n\nFor Linux users\n-----------------\n* For Linux and Mac users, a Makefile is provided to compile the source code.\n* * Requirements: OpenCV 2.4.x (it only works with this version).\n* * Check out the latest project source code and compile it:\n```\n~/git clone https://github.com/andrewssobral/vehicle_detection_haarcascades.git\n~/cd vehicle_detection_haarcascades\n~/vehicle_detection_haarcascades/ chmod +x run_vehicle_detection_video1.sh\n~/vehicle_detection_haarcascades/ chmod +x run_vehicle_detection_video2.sh\n~/vehicle_detection_haarcascades/cd build\n~/vehicle_detection_haarcascades/build/ cmake ..\n~/vehicle_detection_haarcascades/build/ make\n```\n* * Run demos:\n```\n~/vehicle_detection_haarcascades/run_vehicle_detection_video1.sh\n~/vehicle_detection_haarcascades/run_vehicle_detection_video2.sh\n```\n\n\nDocker image\n----------------------------------------\n* Docker image is available at:\n* * **Ubuntu 16.04 + VNC + OpenCV 2.4.13 + Python 2.7 + Vehicle Detection, Tracking and Counting**\nhttps://hub.docker.com/r/andrewssobral/vehicle_detection_tracking_counting/\n\nRelease Notes:\n* Version 1.0.0:\nFirst version.\n"
 },
 {
  "repo": "andrewssobral/simple_vehicle_counting",
  "language": "C++",
  "readme_contents": "Vehicle Detection, Tracking and Counting\n========================================\n\nLast page update: **12/04/2017** (Added Python API & OpenCV 3.x support)\n\nLast version: **1.0.0** (see Release Notes for more info)\n\nHi everyone,\n\nThere are several ways to perform vehicle detection, tracking and counting.\nHere is a step-by-step of a simplest way to do this:\n\n1. First, you will need to detect the moving objects. An easy way to do vehicle detection is by using a Background Subtraction (BS) algorithm. You can try to use a background subtraction library like [BGSLibrary](https://github.com/andrewssobral/bgslibrary#bgslibrary).\n2. For vehicle tracking, you will need to use a tracking algorithm. A simplest way to do this is by using a blob tracker algorithm (see [cvBlob](https://code.google.com/p/cvblob/) or [OpenCVBlobsLib](http://opencvblobslib.github.io/opencvblobslib/)). So, send the foreground mask to **cvBlob** or **OpenCVBlobsLib**. For example, the **cvBlob** library provide some methods to get the **centroid**, the **track** and the **ID** of the moving objects. You can also set to draw a **bounding box**, the **centroid** and the **angle** of the tracked object.\n3. And then, check if the **centroid** of the moving object has crossed a **region of interest** (i.e. virtual line) in your video.\n4. Voil\u00e0! enjoy it :)\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/andrewssobral/simple_vehicle_counting/master/doc/images/vehicle_counting_screen.png\" /></p>\n\n\nCitation\n--------\nIf you use this code for your publications, please cite it as:\n```\n@ONLINE{vdtc,\n    author = \"Andrews Sobral\",\n    title  = \"Vehicle Detection, Tracking and Counting\",\n    year   = \"2014\",\n    url    = \"https://github.com/andrewssobral/simple_vehicle_counting\"\n}\n```\n\n\nFor Windows users\n-----------------\n* There is no Visual Studio 2013 template project anymore. Please, use CMAKE instead.\n\n#### Compiling with OpenCV 3.x and Visual Studio 2015 from CMAKE\n\n**Dependencies:**\n* OpenCV 3.x (tested with OpenCV 3.2.0)\n* GIT (tested with git version 2.7.2.windows.1).\n* CMAKE for Windows (tested with cmake version 3.1.1).\n* Microsoft Visual Studio (tested with VS2015).\n\n*Note: the procedure is similar for OpenCV 2.4.x and Visual Studio 2013.*\n\nPlease follow the instructions below:\n\n1) Go to Windows console.\n\n2) Clone git repository:\n```\ngit clone --recursive https://github.com/andrewssobral/simple_vehicle_counting.git\n```\n\n3) Go to **simple_vehicle_counting/build** folder.\n\n4) Set your OpenCV PATH:\n```\nset OpenCV_DIR=C:\\OpenCV3.2.0\\build\n```\n\n5) Launch CMAKE:\n```\ncmake -DOpenCV_DIR=%OpenCV_DIR% -G \"Visual Studio 14 Win64\" ..\n```\n\n6) Include OpenCV binaries in the system path:\n```\nset PATH=%PATH%;%OpenCV_DIR%\\x64\\vc14\\bin\n```\n\n7) Open the **bgs.sln** file in your Visual Studio and switch to **'RELEASE'** mode\n\n8) Click on **'ALL_BUILD'** project and build!\n\n9) If everything goes well, copy **simple_vehicle_counting.exe** to **simple_vehicle_counting/** and run!\n\n\nFor Linux users\n-----------------\n* For Linux and Mac users, a CMakefile is provided to compile the source code.\n\n* * Check out the latest project source code and compile it:\n```\n~/git clone --recursive https://github.com/andrewssobral/simple_vehicle_counting.git\n~/cd simple_vehicle_counting\n~/simple_vehicle_counting/cd build\n~/simple_vehicle_counting/build/ cmake ..\n~/simple_vehicle_counting/build/ make\n```\n* * Run demo:\n```\n~/simple_vehicle_counting/run_simple_vehicle_counting.sh\n```\n\n\nDocker image\n----------------------------------------\n* Docker image is available at:\n* * **Ubuntu 16.04 + VNC + OpenCV 2.4.13 + Python 2.7 + Vehicle Detection, Tracking and Counting**\nhttps://hub.docker.com/r/andrewssobral/vehicle_detection_tracking_counting/\n\nExample code\n------------\n```C++\n#include <iostream>\n#include <opencv2/opencv.hpp>\n\n#include \"package_bgs/PBAS/PixelBasedAdaptiveSegmenter.h\"\n#include \"package_tracking/BlobTracking.h\"\n#include \"package_analysis/VehicleCouting.h\"\n\nint main(int argc, char **argv)\n{\n  /* Open video file */\n  CvCapture *capture = 0;\n  capture = cvCaptureFromAVI(\"dataset/video.avi\");\n  if(!capture){\n    std::cerr << \"Cannot open video!\" << std::endl;\n    return 1;\n  }\n\n  /* Background Subtraction Algorithm */\n  IBGS *bgs;\n  bgs = new PixelBasedAdaptiveSegmenter;\n\n  /* Blob Tracking Algorithm */\n  cv::Mat img_blob;\n  BlobTracking* blobTracking;\n  blobTracking = new BlobTracking;\n\n  /* Vehicle Counting Algorithm */\n  VehicleCouting* vehicleCouting;\n  vehicleCouting = new VehicleCouting;\n\n  std::cout << \"Press 'q' to quit...\" << std::endl;\n  int key = 0;\n  IplImage *frame;\n  while(key != 'q')\n  {\n    frame = cvQueryFrame(capture);\n    if(!frame) break;\n\n    cv::Mat img_input = cv::cvarrToMat(frame);\n    cv::imshow(\"Input\", img_input);\n\n    // bgs->process(...) internally process and show the foreground mask image\n    cv::Mat img_mask;\n    bgs->process(img_input, img_mask);\n\n    if(!img_mask.empty())\n    {\n      // Perform blob tracking\n      blobTracking->process(img_input, img_mask, img_blob);\n\n      // Perform vehicle counting\n      vehicleCouting->setInput(img_blob);\n      vehicleCouting->setTracks(blobTracking->getTracks());\n      vehicleCouting->process();\n    }\n\n    key = cvWaitKey(1);\n  }\n\n  delete vehicleCouting;\n  delete blobTracking;\n  delete bgs;\n\n  cvDestroyAllWindows();\n  cvReleaseCapture(&capture);\n\n  return 0;\n}\n```\n\n\nPython API\n----------------------------------------\nA [python demo](python/demo.py) shows how to call the Python API.\nIt is similar as the [C++ demo](Demo.cpp).\n\nTo use the Python API, you should copy [\"python\" directory](python) to overwrite the generated one.\n\n```\n~/simple_vehicle_counting/cd build\n~/simple_vehicle_counting/build/cmake ..\n~/simple_vehicle_counting/build/make -j 8\n~/simple_vehicle_counting/build/cp -r ../python/* python/\n~/simple_vehicle_counting/build/../run_python_demo.sh\n```\n\nIf you have previously built the project at the project root,\nmake sure there are no previously generated libraries in the [\"python\" directory](python) by ```make clean```.\n\n\nRelease Notes:\n--------------\n* 12/04/2017: Added OpenCV 3.x support. Removed vs2013 template project (use CMAKE instead).\n\n* 07/04/2017: Added Python API, thanks to [@kyu-sz](https://github.com/kyu-sz).\n\n* Version 1.0.0: First version.\n"
 },
 {
  "repo": "SSARCandy/Coherent-Line-Drawing",
  "language": "C++",
  "readme_contents": "# Coherent Line Drawing\n\n[![C/C++ CI](https://github.com/SSARCandy/Coherent-Line-Drawing/workflows/C/C++%20CI/badge.svg)](https://github.com/SSARCandy/Coherent-Line-Drawing/actions)\n[![Build Status](https://travis-ci.org/SSARCandy/Coherent-Line-Drawing.svg?branch=master)](https://travis-ci.org/SSARCandy/Coherent-Line-Drawing)\n[![codecov](https://codecov.io/gh/SSARCandy/Coherent-Line-Drawing/branch/master/graph/badge.svg)](https://codecov.io/gh/SSARCandy/Coherent-Line-Drawing)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/SSARCandy/Coherent-Line-Drawing/blob/master/LICENSE)\n\n\nThis project implemented a non-photorealistic rendering technique presented by Kang et al, that can automatically generates a line drawing from a photograph. This project provide an easy-to-use, real-time interactive graphic user interface system.\n\n\n- [Original academic paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.559&rep=rep1&type=pdf)\n- [Introduction in Chinese](https://ssarcandy.tw/2017/06/26/Coherent-Line-Drawing/)\n\n![demo](./demo/4.jpg)\n\n## Workflow (Youtube)\n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=48fTXKUTM-8\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/48fTXKUTM-8/0.jpg\" \nalt=\"Coherent Line Drawing\" width=\"800\" border=\"10\" /></a>\n\n\n## Build\n\n### Requirement\n\n- C++ 17\n- CMake\n- OpenCV 3\n- WxWidget 3 (not required for cmd application)\n- Boost (not required for gui application)\n\n_If you prefer C++ 11 version, there is a branch called [cpp11](https://github.com/SSARCandy/Coherent-Line-Drawing/tree/cpp11), the function is equivalent._  \n\nIt should work fine in Windows/Linux/MacOS.  \nI provided some scripts that can use in Linux:\n\n\n```sh\n# Usage: build.sh [options]\n# Options:\n#   -c, --clean       Clean build\n#   -d, --debug       Build with debug mode\n#   -j, --jobs        Use N cores to build\n$ ./build.sh\n\n# Usage: linter.sh [options]\n# Check code style\n# Options:\n#   -i                In-place format\n$ ./linter.sh\n```\n\n### Command Line Version\n\nI provide another command line application that can directly use without graphic interface, the entry point is `src/cmd.cpp`. The options is list as following, also you can refer to `./test.sh` to see how to use:\n\n```sh\n# Coherent-Line-Drawing Options:\n#   -h [ --help ]            Help message\n#   -s [ --src ] arg         Source image path\n#   -o [ --output ] arg      Output image path\n#   --ETF_kernel arg (=5)    ETF kernel size\n#   --ETF_iter arg (=1)      Refining n times ETF\n#   --CLD_iter arg (=1)      Iterate n times FDoG\n#   --sigma_c arg (=1)       Line width\n#   --sigma_m arg (=3)       Degree of coherence\n#   --rho arg (=0.997)       Noise\n#   --tau arg (=0.8)         Thresholding\n```\n\n### Pre-Build GUI Version\n\nYou can download pre-build version at [here](https://github.com/SSARCandy/Coherent-Line-Drawing/releases).  \nIncluding Windows and macOS versions.\n\n**Screenshots**\n\n![demo](./demo/1.jpg)\n![demo](./demo/2.jpg)\n![demo](./demo/3.jpg)\n"
 },
 {
  "repo": "macmade/OpenCV-iOS",
  "language": "Makefile",
  "readme_contents": "OpenCV-iOS\n==========\n\n[![Build Status](https://img.shields.io/travis/macmade/OpenCV-iOS.svg?branch=master&style=flat)](https://travis-ci.org/macmade/OpenCV-iOS)\n[![Issues](http://img.shields.io/github/issues/macmade/OpenCV-iOS.svg?style=flat)](https://github.com/macmade/OpenCV-iOS/issues)\n![Status](https://img.shields.io/badge/status-inactive-lightgray.svg?style=flat)\n![License](https://img.shields.io/badge/license-bsd-brightgreen.svg?style=flat)\n[![Contact](https://img.shields.io/badge/contact-@macmade-blue.svg?style=flat)](https://twitter.com/macmade)  \n[![Donate-Patreon](https://img.shields.io/badge/donate-patreon-yellow.svg?style=flat)](https://patreon.com/macmade)\n[![Donate-Gratipay](https://img.shields.io/badge/donate-gratipay-yellow.svg?style=flat)](https://www.gratipay.com/macmade)\n[![Donate-Paypal](https://img.shields.io/badge/donate-paypal-yellow.svg?style=flat)](https://paypal.me/xslabs)\n\nOverview\n--------\n\nOpenCV (Open Source Computer Vision) is a library of programming functions for real time computer vision.\nThis project is a port of the OpenCV library for Apple iOS. It includes two XCode projects: one for iPhone, the other one for iPad.\n\nOpenCV is released under the BSD License, it is free for both academic and commercial use.\n\nThe official OpenCV documentation can be found at the following address:\nhttp://opencv.willowgarage.com/wiki/\n\nPlease read the [documentation][1] to learn how to start developping iPhone and iPad applications powered by OpenCV, or start developing your applications now by [downloading][2] the XCode project files.\n\nYou can use the XCode project files freely.\nNote that you still need to give credits to OpenCV, as stated by the BSD License.\nThat said, we'll be very happy if you drop us a line when releasing your app!\n\nProject Status\n--------------\n\nThis project is no longer maintained.  \nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\n\n[1]: http://www.eosgarden.com/en/opensource/opencv-ios/documentation/   \"Documentation\"\n[2]: http://www.eosgarden.com/en/opensource/opencv-ios/download/        \"Download\"\n\nRepository Infos\n----------------\n\n    Owner:\t\t\tJean-David Gadina - XS-Labs\n    Web:\t\t\twww.xs-labs.com\n    Blog:\t\t\twww.noxeos.com\n    Twitter:\t\t@macmade\n    GitHub:\t\t\tgithub.com/macmade\n    LinkedIn:\t\tch.linkedin.com/in/macmade/\n    StackOverflow:\tstackoverflow.com/users/182676/macmade\n"
 },
 {
  "repo": "surya-veer/movement-tracking",
  "language": "Python",
  "readme_contents": "# Realtime Face Movement Tracking ![](https://bit.ly/surya-veer-movement-tracking)\n90 Lines of code to convert your face movement into keyboard commands.\n\n# Description\nThis is a basic face movement tracking that can convert face movement into keyboard commands like **UP - DOWN - LEFT  - RIGHT**. I used facial landmarks to detect face and get the nose out of it for better referencing. I have created two versions of it, v1 is using a fixed reference boundary which not work expected properly because we need to come at the same position after each movement. To save this I created V2 which uses position change with respect to the previous position. This is more dynamic and easy to control the moves. No need to set position again and again.\n\n## movement-v1.py\nIn version1, I used a fixed reference boundary. If nose reference is out of boundary then I calculate the direction of movement. After getting direction I am converting it into keyboard commands using the keyboard library.\n\n## movement-v2.py\nIn version2, I am using reference change with respect to the previous position in a particular time window and then calculating the direction vector to get direction and converting it to keyboard command.\n\n## Dependencies\nThis is the list of dependencies for running this application. Use pip to install them.\n * **opencv**\n * **keyboard**\n\n ```\n $ pip install -r requirements.txt\n ```\n\n## How to use\n1. Download or clone this repository.\n2. Extract to some location.\n3. First, run **```movement-v1.py```** (for fix boundary) or run **```movement-v2.py```**(for dynamic movement) <br>\n  NOTE: If you are getting 215 assertion failed!! on line 81 check this (https://github.com/surya-veer/movement-tracking/issues/4#issuecomment-664018021)\n\n4. open any online atari game like Subway surfers or temple run.\n5. Start doing movements to play game. It will press up-down-left-right based on your movements.\n\n# Fun with face movements\nOpen any online game on the browser which needs UP-DOWN-LEFT-RIGHT movements following games, you can find many games if you search on google.\n1. Subway surfer https://www.kiloo.com/subway-surfers/\n2. Temple run https://m.plonga.com/adventure/Temple-Run-2-Online-Tablet\n\n### You can do a lot more things by the small code change.\n\n### ** SUPPORT OPEN SOURCE **\n"
 },
 {
  "repo": "shouzhong/Scanner",
  "language": "Java",
  "readme_contents": "# Scanner\n\n## \u8054\u7cfb\u6211\nQQ\u7fa4 777891894\uff08\u95ee\u9898\u4ea4\u6d41\uff0c\u7b54\u6848\uff1aandroid\uff09\n\n## \u8bf4\u660e\n\u8bc6\u522b\u5e93\uff0c\u8bc6\u522b\u5668\u53ef\u9009\u62e9\uff0c\u8fd9\u91cc\u6709\u4f60\u5e38\u7528\u7684\u4e8c\u7ef4\u7801/\u6761\u7801\u8bc6\u522b\uff0c\u8fd8\u6709\u4f60\u53ef\u80fd\u7528\u5230\u7684\u8eab\u4efd\u8bc1\u8bc6\u522b\u3001\u94f6\u884c\u5361\u8bc6\u522b\u3001\u8f66\u724c\u8bc6\u522b\u3001\u56fe\u7247\u6587\u5b57\u8bc6\u522b\u3001\u9ec4\u56fe\u8bc6\u522b\u3001\u9a7e\u9a76\u8bc1\u8bc6\u522b\uff0c\u5982\u679c\u6ca1\u6709\u4f60\u60f3\u8981\u7684\uff0c\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\u3002\u8be5\u5e93\u53ea\u8bc6\u522b\u626b\u63cf\u6846\u5185\u7684\u56fe\u50cf\uff0c\u8bc6\u522b\u901f\u7387\u4e0a\u5927\u5927\u63d0\u9ad8\uff0c\u800c\u4e14\u8fd9\u4e2a\u5e93\u6bd4\u8d77\u5176\u5b83\u7684\u5e93\u5c31\u662f\u89e3\u51b3\u4e86\u6444\u50cf\u5934\u9884\u89c8\u53d8\u5f62\uff0c\u9884\u89c8\u9875\u9762\u9ad8\u5ea6\u81ea\u5b9a\u4e49\uff0c\u4f60\u53ef\u4ee5\u50cf\u5e38\u89c4\u4e00\u6837\u6574\u4e2a\u9875\u9762\u90fd\u662f\u9884\u89c8\uff0c\u6216\u8005\u4f60\u53ef\u4ee5\u9009\u62e9\u5728\u4efb\u4f55\u4f4d\u7f6e\u5b9a\u4e49\u4efb\u4f55\u5c3a\u5bf8\u7684\u9884\u89c8\uff0c\u626b\u63cf\u6846\u4e5f\u9ad8\u5ea6\u81ea\u5b9a\u4e49\uff0c\u4f60\u53ef\u4ee5\u50cf\u5e38\u89c4\u4e00\u6837\u5c45\u4e2d\uff0c\u6216\u8005\u4f60\u4e5f\u53ef\u4ee5\u5728\u9884\u89c8\u7684\u4efb\u4f55\u4f4d\u7f6e\u5b9a\u4e49\u4efb\u4f55\u5c3a\u5bf8\u7684\u626b\u63cf\u6846\uff08\u5b9e\u9645\u8bc6\u522b\u7684\u626b\u63cf\u6846\u548c\u753b\u4e0a\u53bb\u7684\u626b\u63cf\u6846\u4e0d\u4e00\u5b9a\u662f\u4e00\u6837\u7684\uff0c\u7531\u4f60\u81ea\u5df1\u51b3\u5b9a\uff09\u3002\n## \u6548\u679c\u56fe\n\n<table>\n    <tr>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/1.jpg\"/></td>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/2.jpg\"/></td>\n    </tr>\n    <tr>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/3.jpg\"/></td>\n        <td><img src=\"https://github.com/shouzhong/Scanner/blob/master/img/4.jpg\"/></td>\n    </tr>\n</table>\n\n## [\u4e0b\u8f7d apk-demo](http://downgit.zhoudaxiaa.com/#/home?url=https://github.com/shouzhong/Scanner/blob/master/app/release/app-release.apk)\n\n## \u4f7f\u7528\n### \u4f9d\u8d56\n```\nimplementation 'com.shouzhong:Scanner:1.1.3'\n```\n\u4ee5\u4e0b\u9009\u62e9\u81ea\u5df1\u9700\u8981\u7684\n```\n// zxing\nimplementation 'com.google.zxing:core:3.3.3'\n// zbar\nimplementation 'com.shouzhong:ScannerZBarLib:1.0.0'\n// \u94f6\u884c\u5361\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerBankCardLib:1.0.3'\n// \u8eab\u4efd\u8bc1\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerIdCardLib:1.0.4'\n// \u8f66\u724c\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerLicensePlateLib:1.0.3'\n// \u56fe\u7247\u6587\u5b57\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerTextLib:1.0.0'\n// \u9ec4\u56fe\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerNsfwLib:1.0.0'\n// \u9a7e\u9a76\u8bc1\u8bc6\u522b\nimplementation 'com.shouzhong:ScannerDrivingLicenseLib:1.0.1'\n// \u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\nimplementation 'com.shouzhong:ScannerIdCard2Lib:1.0.0'\n```\n### \u4ee3\u7801\n\u57fa\u672c\u4f7f\u7528\n```\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n    <com.shouzhong.scanner.ScannerView\n        android:id=\"@+id/sv\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"1080px\"\n        android:background=\"#000000\"/>\n</RelativeLayout>\n```\n```\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_scanner);\n    scannerView = findViewById(R.id.sv);\n    scannerView.setViewFinder(new ViewFinder(this));\n    scannerView.setSaveBmp(true);\n    scannerView.setEnableZXing(true);\n    scannerView.setEnableZBar(true);\n    scannerView.setEnableBankCard(true);\n    scannerView.setEnableIdCard(true);\n    scannerView.setEnableLicensePlate(true);\n    scannerView.setCallback(new Callback() {\n        @Override\n        public void result(Result result) {\n            tvResult.setText(\"\u8bc6\u522b\u7ed3\u679c\uff1a\\n\" + result.toString());\n            scannerView.restartPreviewAfterDelay(2000);\n        }\n    });\n}\n\n@Override\nprotected void onResume() {\n    super.onResume();\n    scannerView.onResume();\n}\n\n@Override\nprotected void onPause() {\n    super.onPause();\n    scannerView.onPause();\n}\n```\n\u5f00\u542f\u6216\u8005\u5173\u95ed\u67d0\u4e2a\u8bc6\u522b\u5668\n```\n// \u542f\u7528zxing\u8bc6\u522b\u5668\nscannerView.setEnableZXing(true);\n// \u542f\u7528zbar\u8bc6\u522b\u5668\nscannerView.setEnableZBar(true);\n// \u542f\u7528\u94f6\u884c\u5361\u8bc6\u522b\u5668\nscannerView.setEnableBankCard(true);\n// \u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff08\u8fd9\u91cc\u53ea\u652f\u63012\u4ee3\u8eab\u4efd\u8bc1\uff09\nscannerView.setEnableIdCard(true);\n// \u542f\u7528\u8f66\u724c\u8bc6\u522b\nscannerView.setEnableLicensePlate(true);\n// \u542f\u7528\u9a7e\u9a76\u8bc1\u8bc6\u522b\nscannerView.setEnableDrivingLicense(true);\n// \u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\nscannerView.setEnableIdCard2(true);\n```\n\u5982\u679c\u4f60\u60f3\u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\n```\nscannerView.setScanner(new IScanner() {\n    /**\n     * \u8fd9\u91cc\u5b9e\u73b0\u81ea\u5df1\u7684\u8bc6\u522b\u5668\uff0c\u5e76\u628a\u8bc6\u522b\u7ed3\u679c\u8fd4\u56de\n     *\n     * @param data \u77e9\u5f62\u6846\u5185nv21\u56fe\u50cf\u6570\u636e\n     * @param width \u56fe\u50cf\u5bbd\u5ea6\n     * @param height \u56fe\u50cf\u9ad8\u5ea6\n     * @return\n     * @throws Exception\n     */\n    @Override\n    public Result scan(byte[] data, int width, int height) throws Exception {\n        // \u5982\u679c\u4f60\u60f3\u8f6c\u4e3aBitmap\uff0c\u8bf7\u4f7f\u7528NV21.nv21ToBitmap(byte[] nv21, int width, int height)\n        return null;\n    }\n});\n```\n\u8fd9\u91cc\u6ca1\u7ed9\u9ed8\u8ba4\u7684\u9884\u89c8\u9875\u9762\uff0c\u9700\u8981\u81ea\u5df1\u81ea\u5b9a\u4e49\uff0c\u8bf7\u53c2\u8003[demo](https://github.com/shouzhong/Scanner/blob/master/app/src/main/java/com/shouzhong/scanner/demo/TestViewFinder.java)\n\n## \u56de\u8c03\u8bf4\u660e\n\nResult\n\n\u5c5e\u6027 | \u8bf4\u660e\n------------ | -------------\nTYPE_CODE | \u7c7b\u578b\uff1a\u4e8c\u7ef4\u7801/\u6761\u7801\nTYPE_ID_CARD_FRONT | \u7c7b\u578b\uff1a\u8eab\u4efd\u8bc1\u4eba\u5934\u9762\nTYPE_ID_CARD_BACK | \u7c7b\u578b\uff1a\u8eab\u4efd\u8bc1\u56fd\u5fbd\u9762\nTYPE_BANK_CARD | \u7c7b\u578b\uff1a\u94f6\u884c\u5361\nTYPE_LICENSE_PLATE | \u7c7b\u578b\uff1a\u8f66\u724c\nTYPE_DRIVING_LICENSE | \u7c7b\u578b\uff1a\u9a7e\u9a76\u8bc1\ntype | \u7ed3\u679c\u7c7b\u578b\npath | \u4fdd\u5b58\u7684\u56fe\u7247\u8def\u5f84\ndata | \u6570\u636e\n```\n// \u4ee5\u4e0b\u662f\u5bf9data\u7684\u8bf4\u660e\n// \u5f53type\u4e3aTYPE_CODE\uff0cTYPE_BANK_CARD\uff0cTYPE_LICENSE_PLATE\u65f6\uff0cdata\u4e3a\u5b57\u7b26\u4e32\n// \u5f53type\u4e3aTYPE_ID_CARD_FRONT\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"cardNumber\": \"21412412421\",// \u8eab\u4efd\u8bc1\u53f7\n\t\"name\": \"\u5f20\u4e09\",// \u59d3\u540d\n\t\"sex\": \"\u7537\",// \u6027\u522b\n\t\"nation\": \"\u6c49\",// \u6c11\u65cf\n\t\"birth\": \"1999-01-01\",// \u51fa\u751f\n\t\"address\": \"\u5730\u5740\"// \u5730\u5740\n}\n// \u5f53type\u4e3aTYPE_ID_CARD_BACK\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"organization\": \"\u7b7e\u53d1\u673a\u5173\",// \u7b7e\u53d1\u673a\u5173\n\t\"validPeriod\": \"20180101-20380101\"// \u6709\u6548\u671f\u9650\n}\n// \u5f53type\u4e3aTYPE_DRIVING_LICENSE\u65f6\uff0cdata\u4e3ajson\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u5982\u4e0b\n{\n\t\"cardNumber\": \"43623446432\",// \u8bc1\u53f7\n\t\"name\": \"\u5f20\u4e09\",// \u59d3\u540d\n\t\"sex\": \"\u7537\",// \u6027\u522b\n\t\"nationality\": \"\u4e2d\u56fd\",// \u56fd\u7c4d\n\t\"address\": \"\u5730\u5740\",// \u5730\u5740\n\t\"birth\": \"1999-01-01\",// \u51fa\u751f\u65e5\u671f\n\t\"firstIssue\": \"2018-01-01\",// \u521d\u6b21\u9886\u8bc1\u65e5\u671f\n\t\"_class\": \"C1\",// \u51c6\u9a7e\u8f66\u578b\n\t\"validPeriod\": \"20180101-20240101\"// \u6709\u6548\u671f\u9650\n}\n```\n\nBankCardInfoBean\n\n\u5c5e\u6027\u6216\u65b9\u6cd5 | \u8bf4\u660e\n------------ | -------------\ncardNumber | \u94f6\u884c\u5361\u53f7\ncardType | \u94f6\u884c\u5361\uff08\u82f1\u6587\uff09\u7c7b\u578b\nbank | \u94f6\u884c\uff08\u82f1\u6587\uff09\u540d\u79f0\ngetCNBankName | \u83b7\u53d6\u94f6\u884c\uff08\u4e2d\u6587\uff09\u540d\u79f0\ngetBankId | \u83b7\u53d6\u94f6\u884c\u7f16\u53f7\ngetCNCardType | \u83b7\u53d6\u94f6\u884c\u5361\uff08\u4e2d\u6587\uff09\u7c7b\u578b\n\n## \u65b9\u6cd5\u8bf4\u660e\n\nScannerView\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\nsetViewFinder | \u626b\u63cf\u533a\u57df\nsetCallback | \u626b\u7801\u6210\u529f\u540e\u7684\u56de\u8c03\nsetCameraDirection | \u6444\u50cf\u5934\u65b9\u5411\uff0c\u540e\u7f6e\u4e3aCamera.CameraInfo.CAMERA_FACING_BACK\uff0c\u524d\u7f6e\u4e3aCamera.CameraInfo.CAMERA_FACING_FRONT\nsetEnableZXing | \u662f\u5426\u542f\u7528zxing\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableZBar | \u662f\u5426\u542f\u7528zbar\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableQrcode | \u662f\u5426\u542f\u52a8\u4e8c\u7ef4\u7801\u8bc6\u522b\uff0c\u9ed8\u8ba4true\uff0c\u53ea\u6709\u5728zxing\u6216\u8005zbar\u5f00\u542f\u65f6\u6709\u6548\nsetEnableBarcode | \u662f\u5426\u542f\u52a8\u6761\u7801\u8bc6\u522b\uff0c\u9ed8\u8ba4true\uff0c\u53ea\u6709\u5728zxing\u6216\u8005zbar\u5f00\u542f\u65f6\u6709\u6548\nsetEnableBankCard | \u662f\u5426\u542f\u7528\u94f6\u884c\u5361\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableIdCard | \u662f\u5426\u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableIdCard2 | \u662f\u5426\u542f\u7528\u8eab\u4efd\u8bc1\u8bc6\u522b\u5668\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\uff0c\u9ed8\u8ba4false\nsetEnableDrivingLicense | \u662f\u5426\u542f\u7528\u9a7e\u9a76\u8bc1\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetEnableLicensePlate | \u662f\u5426\u542f\u7528\u8f66\u724c\u8bc6\u522b\u5668\uff0c\u9ed8\u8ba4false\nsetScanner | \u81ea\u5b9a\u4e49\u8bc6\u522b\u5668\nonResume | \u5f00\u542f\u626b\u63cf\nonPause | \u505c\u6b62\u626b\u63cf\nrestartPreviewAfterDelay | \u8bbe\u7f6e\u591a\u5c11\u6beb\u79d2\u540e\u91cd\u542f\u626b\u63cf\nsetFlash | \u5f00\u542f/\u5173\u95ed\u95ea\u5149\u706f\ntoggleFlash | \u5207\u6362\u95ea\u5149\u706f\u7684\u70b9\u4eae\u72b6\u6001\nisFlashOn | \u95ea\u5149\u706f\u662f\u5426\u88ab\u70b9\u4eae\nsetShouldAdjustFocusArea | \u8bbe\u7f6e\u662f\u5426\u8981\u6839\u636e\u626b\u7801\u6846\u7684\u4f4d\u7f6e\u53bb\u8c03\u6574\u5bf9\u7126\u533a\u57df\u7684\u4f4d\u7f6e\uff0c\u90e8\u5206\u624b\u673a\u4e0d\u652f\u6301\uff0c\u9ed8\u8ba4false\nsetSaveBmp | \u8bbe\u7f6e\u662f\u5426\u4fdd\u5b58\u8bc6\u522b\u7684\u56fe\u7247\uff0c\u9ed8\u8ba4false\nsetRotateDegree90Recognition | \u662f\u5426\u5728\u539f\u6765\u8bc6\u522b\u7684\u56fe\u50cf\u57fa\u7840\u4e0a\u65cb\u8f6c90\u5ea6\u7ee7\u7eed\u8bc6\u522b\uff0c\u9ed8\u8ba4false\n\nScannerUtils\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\ndecodeCode | \u4e8c\u7ef4\u7801/\u6761\u7801\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeBank | \u94f6\u884c\u5361\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ngetBankCardInfo | \u83b7\u53d6\u94f6\u884c\u5361\u4fe1\u606f\uff0c\u8bf7\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeIdCard | \u8eab\u4efd\u8bc1\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeId2Card | \u8eab\u4efd\u8bc1\u8bc6\u522b\uff08\u7b2c\u4e8c\u79cd\u65b9\u5f0f\uff09\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeDrivingLicense | \u9a7e\u9a76\u8bc1\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeLicensePlate | \u8f66\u724c\u8bc6\u522b\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeText | \u56fe\u7247\u6587\u5b57\u8bc6\u522b\uff0c\u8bf7\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ndecodeNsfw | \u9ec4\u56fe\u8bc6\u522b\uff0c\u5927\u4e8e0.3\u53ef\u4ee5\u8bf4\u56fe\u7247\u6d89\u9ec4\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ncreateBarcode | \u6761\u7801\u751f\u6210\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\ncreateQRCode | \u4e8c\u7ef4\u7801\u751f\u6210\uff0c\u5efa\u8bae\u5728\u5b50\u7ebf\u7a0b\u8fd0\u884c\naddLogo | \u5f80\u56fe\u7247\u4e2d\u95f4\u52a0logo\n\nNV21\n\n\u65b9\u6cd5\u540d | \u8bf4\u660e\n------------ | -------------\nnv21ToBitmap | nv21\u8f6cbitmap\nbitmapToNv21 | bitmap\u8f6cnv21\n\n## \u600e\u4e48\u628a\u6211\u7684\u6574\u4e2a\u9879\u76ee\u5bfc\u8fdb\u53bb\n1. \u8be5\u9879\u76ee\u4f7f\u7528opencv-3.4.6\uff0c[\u70b9\u51fb\u4e0b\u8f7d](https://nchc.dl.sourceforge.net/project/opencvlibrary/3.4.6/opencv-3.4.6-android-sdk.zip)\n2. NDK\u7248\u672cr16\n3. \u628alicennseplate\u7684CMakeLists.txt\u7684\u7b2c12\u884c\u66ff\u6362\u6210\u81ea\u5df1\u7684opencv-android-sdk\u7684JNI\u8def\u5f84\n4. \u5220\u9664\u6240\u6709gradle\u91cc\u7684 apply from: 'bintray.gradle'\n5. \u5220\u9664bankcard\u7684build.gradle\u91cc\u7684android->externalNativeBuild\u4ee5\u53caandroid->defaultConfig->ndk\u548cexternalNativeBuild\u6807\u7b7e\n6. \u5220\u9664text\u7684build.gradle\u91cc\u7684android->externalNativeBuild\u4ee5\u53caandroid->defaultConfig->ndk\u548cexternalNativeBuild\u6807\u7b7e\n7. \u5982\u679c\u662flinux\u7528\u6237\uff0c\u8bf7\u5728licennseplate\u7684build.gradle\u6dfb\u52a0\u4ee5\u4e0b\n```\nandroid {\n...\n  defaultConfig {\n      ...\n      externalNativeBuild {\n          cmake {\n              cppFlags \"-std=c++11\"\n              // linux\u8bf7\u6dfb\u52a0\u4ee5\u4e0b\n              arguments \"-DANDROID_TOOLCHAIN=gcc\", \"-DANDROID_ARM_NEON=TRUE\", \"-DANDROID_STL_FORCE_FEATURES=OFF\"\n          }\n      }\n  }\n}\n```\n\n## \u6ce8\u610f\u4e8b\u9879\n1. so\u8d44\u6e90\u53ea\u6709arm\u683c\u5f0f\u7684\uff0cScannerDrivingLicenseLib\u548cScannerIdCard2Lib\u65e0arm64-v8a\u683c\u5f0f\n\n## \u6c42star\n#### [BaseLib](https://github.com/shouzhong/BaseLib)\uff0cui\u5f00\u53d1\u57fa\u7840\u5305\n#### [Bridge](https://github.com/shouzhong/Bridge)\uff0c\u8de8\u8fdb\u7a0b\u7ba1\u7406\u5e93\n#### [ScreenHelper](https://github.com/shouzhong/ScreenHelper)\uff0c\u5c4f\u5e55\u9002\u914d\u5e93\n"
 },
 {
  "repo": "JetsonHacksNano/CSI-Camera",
  "language": "Python",
  "readme_contents": "# CSI-Camera\nSimple example of using a MIPI-CSI(2) Camera (like the Raspberry Pi Version 2 camera) with the NVIDIA Jetson Nano Developer Kit. This is support code for the article on JetsonHacks: https://wp.me/p7ZgI9-19v\n\nThe camera should be installed in the MIPI-CSI Camera Connector on the carrier board. The pins on the camera ribbon should face the Jetson Nano module, the stripe faces outward.\n\nThe new Jetson Nano B01 developer kit has two CSI camera slots. You can use the sensor_mode attribute with nvarguscamerasrc to specify the camera. Valid values are 0 or 1 (the default is 0 if not specified), i.e.\n\n```\nnvarguscamerasrc sensor_id=0\n```\n\nTo test the camera:\n\n```\n# Simple Test\n#  Ctrl^C to exit\n# sensor_id selects the camera: 0 or 1 on Jetson Nano B01\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! nvoverlaysink\n\n# More specific - width, height and framerate are from supported video modes\n# Example also shows sensor_mode parameter to nvarguscamerasrc\n# See table below for example video modes of example sensor\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! \\\n   'video/x-raw(memory:NVMM),width=3280, height=2464, framerate=21/1, format=NV12' ! \\\n   nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=720' ! \\\n   nvvidconv ! nvegltransform ! nveglglessink -e\n\nNote: The cameras appear to report differently than show below on some Jetsons. You can use the simple gst-launch example above to determine the camera modes that are reported by the sensor you are using. As an example the same camera from below may report differently on a Jetson Nano B01:\n\nGST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000 \n\nYou should adjust accordingly. As an example, for 3264x2464 @ 21 fps on sensor_id 1 of a Jetson Nano B01:\n$ gst-launch-1.0 nvarguscamerasrc sensor_id=1 ! \\\n   'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! \\\n   nvvidconv flip-method=0 ! 'video/x-raw, width=816, height=616' ! \\\n   nvvidconv ! nvegltransform ! nveglglessink -e\n\nAlso, it's been noticed that the display transform is sensitive to width and height (in the above example, width=816, height=616). If you experience issues, check to see if your display width and height is the same ratio as the camera frame size selected (In the above example, 816x616 is 1/4 the size of 3264x2464).\n```\n\nThere are several examples:\n\nNote: You may need to install numpy for the Python examples to work, ie $ pip3 install numpy\n\nsimple_camera.py is a Python script which reads from the camera and displays to a window on the screen using OpenCV:\n\n$ python simple_camera.py\n\nface_detect.py is a python script which reads from the camera and uses  Haar Cascades to detect faces and eyes:\n\n$ python face_detect.py\n\nHaar Cascades is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. The function is then used to detect objects in other images. \n\nSee: https://docs.opencv.org/3.3.1/d7/d8b/tutorial_py_face_detection.html \n\nThe third example is a simple C++ program which reads from the camera and displays to a window on the screen using OpenCV:\n\n```\n$ g++ -std=c++11 -Wall -I/usr/lib/opencv simple_camera.cpp -L/usr/lib -lopencv_core -lopencv_highgui -lopencv_videoio -o simple_camera\n\n$ ./simple_camera\n```\n\nThe final example is dual_camera.py. This example is for the newer rev B01 of the Jetson Nano board, identifiable by two CSI-MIPI camera ports. This is a simple Python program which reads both CSI cameras and displays them in a window. The window is 960x1080. For performance, the script uses a separate thread for reading each camera image stream. To run the script:\n\n```\n$ python3 dual_camera.py\n```\n\nThe directory 'instrumented' contains instrumented code which can help adjust performance and frame rates.\n\n<h2>Notes</h2>\n\n<h3>Camera Image Formats</h3>\nYou can use v4l2-ctl to determine the camera capabilities. v4l2-ctl is in the v4l-utils:\n\n$ sudo apt-get install v4l-utils\n\nFor the Raspberry Pi V2 camera, typically the output is (assuming the camera is /dev/video0):\n\n```\n$ v4l2-ctl --list-formats-ext\nioctl: VIDIOC_ENUM_FMT\n\tIndex       : 0\n\tType        : Video Capture\n\tPixel Format: 'RG10'\n\tName        : 10-bit Bayer RGRG/GBGB\n\t\tSize: Discrete 3280x2464\n\t\t\tInterval: Discrete 0.048s (21.000 fps)\n\t\tSize: Discrete 3280x1848\n\t\t\tInterval: Discrete 0.036s (28.000 fps)\n\t\tSize: Discrete 1920x1080\n\t\t\tInterval: Discrete 0.033s (30.000 fps)\n\t\tSize: Discrete 1280x720\n\t\t\tInterval: Discrete 0.017s (60.000 fps)\n\t\tSize: Discrete 1280x720\n\t\t\tInterval: Discrete 0.017s (60.000 fps)\n```\n\n<h3>GStreamer Parameter</h3>\nFor the GStreamer pipeline, the nvvidconv flip-method parameter can rotate/flip the image. This is useful when the mounting of the camera is of a different orientation than the default.\n\n```\n\nflip-method         : video flip methods\n                        flags: readable, writable, controllable\n                        Enum \"GstNvVideoFlipMethod\" Default: 0, \"none\"\n                           (0): none             - Identity (no rotation)\n                           (1): counterclockwise - Rotate counter-clockwise 90 degrees\n                           (2): rotate-180       - Rotate 180 degrees\n                           (3): clockwise        - Rotate clockwise 90 degrees\n                           (4): horizontal-flip  - Flip horizontally\n                           (5): upper-right-diagonal - Flip across upper right/lower left diagonal\n                           (6): vertical-flip    - Flip vertically\n                           (7): upper-left-diagonal - Flip across upper left/low\n```\n\n<h2>OpenCV and Python</h2>\nStarting with L4T 32.2.1 / JetPack 4.2.2, GStreamer support is built in to OpenCV.\nThe OpenCV version is 3.3.1 for those versions. Please note that if you are using\nearlier versions of OpenCV (most likely installed from the Ubuntu repository), you\nwill get 'Unable to open camera' errors.\n<br>\nIf you can open the camera in GStreamer from the command line, and have issues opening the camera in Python, check the OpenCV version. \n\n```\n>>>cv2.__version__\n```\n\n<h2>Release Notes</h2>\n\nv3.1 Release March, 2020\n* L4T 32.3.1 (JetPack 4.3)\n* OpenCV 4.1.1\n* Tested on Jetson Nano B01\n* Tested with Raspberry Pi v2 cameras\n\nv3.0 December 2019\n* L4T 32.3.1\n* OpenCV 4.1.1.\n* Tested with Raspberry Pi v2 camera\n\nv2.0 Release September, 2019\n* L4T 32.2.1 (JetPack 4.2.2)\n* OpenCV 3.3.1\n* Tested on Jetson Nano\n\nInitial Release (v1.0) March, 2019\n* L4T 32.1.0 (JetPack 4.2)\n* Tested on Jetson Nano\n\n\n"
 },
 {
  "repo": "pibooth/pibooth",
  "language": "Python",
  "readme_contents": "|Pibooth| |BeerPay|\n\n|PythonVersions| |PypiPackage| |Downloads|\n\nThe ``pibooth`` project provides a photobooth application *out-of-the-box* in pure Python\nfor Raspberry Pi. Have a look to the `wiki <https://github.com/pibooth/pibooth/wiki>`_\nto discover some realizations from GitHub users, and don't hesitate to send us photos of your version.\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/background_samples.png\n   :align: center\n   :alt: Settings\n\n.. note:: Even if designed for a Raspberry Pi, this software may be installed on any Unix/Linux\n          based OS (tested on Ubuntu 16 and Mac OSX 10.14.6).\n\n.. contents::\n\nFeatures\n--------\n\n* Interface available in Danish, Dutch, English, French, German, Hungarian, Norwegian and Spanish (customizable)\n* Capture from 1 to 4 photos and concatenate them in a final picture\n* Support all cameras compatible with gPhoto2, OpenCV and Raspberry Pi\n* Support for hardware buttons and lamps on Raspberry Pi GPIO\n* Fully driven from hardware buttons / keyboard / mouse / touchscreen\n* Auto-start at the Raspberry Pi startup\n* Animate captures from the last sequence during idle time\n* Store final pictures and the individual captures\n* Printing final pictures using CUPS server (printing queue indication)\n* Custom texts can be added on the final picture (customizable fonts, colors, alignments)\n* Custom background(s) and overlay(s) can be added on the final picture\n* All settings available in a configuration file (most common options in a graphical interface)\n* Highly customizable thanks to its plugin system, you can develop your own plugin\n\nGallery\n-------\n\nYou can see some examples of the output picture formats you can get with ``pibooth`` on the following page.\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/gallery.png\n   :align: center\n   :alt: gallery\n   :target: https://github.com/pibooth/pibooth/blob/master/docs/examples.rst\n   :height: 200px\n\nRequirements\n------------\n\nThe requirements listed below are the ones used for the development of ``pibooth``, but\nother configuration may work fine. **All hardware buttons, lights and printer are optional**,\nthe application can be entirely controlled using a keyboard, a mouse or a touchscreen.\n\n.. warning:: Using a Pi Camera, the preview is visible only on a screen connected to the HDMI or\n             DSI connectors (the preview is an overlay managed at GPU low level).\n\nHardware\n^^^^^^^^\n\n* 1 Raspberry Pi 3 Model B (or higher)\n* 1 Camera (Raspberry Pi Camera v2.1 8 MP 1080p\n  or any DSLR camera `compatible with gPhoto2 <http://www.gphoto.org/proj/libgphoto2/support.php>`_\n  or any webcam `compatible with OpenCV <https://opencv.org>`_ )\n* 2 push buttons\n* 2 LEDs\n* 2 resistors of 100 Ohm\n* 1 printer\n\nSoftware\n^^^^^^^^\n\n* Raspbian ``Buster with desktop and recommended software``\n* Python ``3.5.3``\n* libgphoto2 ``2.5.23``\n* libcups ``2.2.1``\n\nInstall\n-------\n\nA brief description on how to set-up a Raspberry Pi to use this software.\n\n1. Download the Raspbian image and set-up an SD-card. You can follow\n   `these instructions <https://www.raspberrypi.org/documentation/installation/installing-images/README.md>`_.\n\n2. Insert the SD-card into the Raspberry Pi and fire it up. Use the ``raspi-config`` tool\n   to configure your system (e.g., expand partition, change hostname, password, enable SSH,\n   configure to boot into GUI, etc.).\n\n   .. hint:: Don't forget to enable the camera in raspi-config.\n\n3. Upgrade all installed software:\n\n   ::\n\n        $ sudo apt-get update\n        $ sudo apt-get upgrade\n\n4. Optionally install the last stable ``gPhoto2`` version (required only for DSLR camera):\n\n   ::\n\n        $ sudo wget raw.github.com/gonzalo/gphoto2-updater/master/gphoto2-updater.sh\n        $ sudo chmod 755 gphoto2-updater.sh\n        $ sudo ./gphoto2-updater.sh\n\n5. Optionally install ``CUPS`` to handle printers (more instructions to add a new printer can be found\n   `here <https://www.howtogeek.com/169679/how-to-add-a-printer-to-your-raspberry-pi-or-other-linux-computer>`_):\n\n   ::\n\n        $ sudo apt-get install cups libcups2-dev\n\n6. Optionally install ``OpenCV`` to improve images generation efficiency or if a Webcam is used:\n\n   ::\n\n        $ sudo apt-get install python3-opencv\n\n7. Install ``pibooth`` from the `pypi repository <https://pypi.org/project/pibooth/>`_:\n\n   ::\n\n        $ sudo pip3 install pibooth[dslr,printer]\n\n   .. hint:: If you don't have ``gPhoto2`` and/or ``CUPS`` installed (steps 5. and/or 6. skipped), remove\n             printer or dslr under the ``[]``\n\n.. note:: An editable/customizable version of ``pibooth`` can be installed by following\n          these `instructions <https://github.com/pibooth/pibooth/blob/master/docs/dev.rst>`_ .\n          Be aware that the code on the `master` branch may be unstable.\n\nRun\n---\n\nStart the photobooth application using the command::\n\n    $ pibooth\n\nAll pictures taken are stored in the folder defined in ``[GENERAL][directory]``. They are named\n**YYYY-mm-dd-hh-mm-ss_pibooth.jpg** which is the time when first capture of the sequence was taken.\nA subfolder **raw/YYYY-mm-dd-hh-mm-ss** is created to store the single raw captures.\n\n.. note:: if you have both ``Pi`` and ``DSLR`` cameras connected to the Raspberry Pi, **both are used**,\n          this is called the **Hybrid** mode. The preview is taken using the ``Pi`` one for a better\n          video rendering and the capture is taken using the ``DSLR`` one for better picture rendering.\n\nYou can display a basic help on application options by using the command::\n\n    $ pibooth --help\n\nStates and lights management\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe application follows the states sequence defined in the simplified diagram\nbelow:\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/state_sequence.png\n   :align: center\n   :alt: State sequence\n\nThe states of the **LED 1** and **LED 2** are modified depending on the actions available\nfor the user.\n\nDetailed state diagram can be found `on this page <https://github.com/pibooth/pibooth/blob/master/docs/plugin.rst>`_.\n\nCommands\n^^^^^^^^\n\nAfter the graphical interface is started, the following actions are available:\n\n======================= ================ =====================\nAction                  Keyboard key     Physical button\n======================= ================ =====================\nToggle Full screen      Ctrl + F         \\-\nChoose layout           LEFT or RIGHT    Button 1 or Button 2\nTake pictures           P                Button 1\nExport Printer/Cloud    Ctrl + E         Button 2\nOpen/close settings     ESC              Button 1 + Button 2\nSelect option           UP or DOWN       Button 1\nChange option value     LEFT or RIGHT    Button 2\n======================= ================ =====================\n\nFinal picture rendering\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``pibooth`` application handle the rendering of the final picture using 2 variables defined in\nthe configuration (see `Configuration`_ below):\n\n* ``[CAMERA][resolution] = (width, height)`` is the resolution of the captured picture in pixels.\n  As explained in the configuration file, the preview size is directly dependent from this parameter.\n* ``[PICTURE][orientation] = auto/landscape/portrait`` is the orientation of the final picture\n  (after concatenation of all captures). If the value is **auto**, the orientation is automatically\n  chosen depending on the resolution.\n\n.. note:: The resolution is an important parameter, it is responsible for the quality of the final\n          picture. Have a look to `picamera possible resolutions <http://picamera.readthedocs.io/en/latest/fov.html#sensor-modes>`_ .\n\nImage effects can be applied on the capture using the ``[PICTURE][effect]`` variable defined in the\nconfiguration.\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Effect applied on all captures\n    captures_effects = film\n\nInstead of one effect name, a list of names can be provided. In this case, the effects are applied\nsequentially on the captures sequence.\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Define a rolling sequence of effects. For each capture the corresponding effect is applied.\n    captures_effects = ('film', 'cartoon', 'washedout', 'film')\n\nHave a look to the predefined effects available depending on the camera used:\n\n* `picamera effects <https://picamera.readthedocs.io/en/latest/api_camera.html#picamera.PiCamera.image_effect>`_\n* `gPhoto2 effects (PIL based) <https://pillow.readthedocs.io/en/latest/reference/ImageFilter.html>`_\n\nTexts can be defined by setting the option ``[PICTURE][footer_text1]`` and ``[PICTURE][footer_text2]``\n(lets them empty to hide any text). For each one, the font, the color and the alignment can be chosen.\nFor instance:\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # Same font applied on footer_text1 and footer_text2\n    text_fonts = Amatic-Bold\n\nThis key can also take two names or TTF file paths:\n\n.. code-block:: ini\n\n    [PICTURE]\n\n    # 'arial' font applied on footer_text1, 'Roboto-BoldItalic' font on footer_text2\n    text_fonts = ('arial', 'Roboto-BoldItalic')\n\nThe available fonts can be listed using the following the command::\n\n \u00a0  $ pibooth --fonts\n\nTo regenerate the final pictures afterwards, from the originals captures present in the\n``raw`` folder, use the command::\n\n    $ pibooth-regen\n\nIt permits to adjust the configuration to enhance the previous pictures with better\nparameters (title, more effects, etc...)\n\nConfiguration\n-------------\n\nAt the first run, a configuration file is generated in ``~/.config/pibooth/pibooth.cfg``\nwhich permits to configure the behavior of the application.\n\nA quick configuration GUI menu (see `Commands`_ ) gives access to the most common options:\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/settings.png\n   :align: center\n   :alt: Settings\n\nMore options are available by editing the configuration file which is easily\ndone using the command::\n\n \u00a0  $ pibooth --config\n\nThe default configuration can be restored with the command (strongly recommended when\nupgrading ``pibooth``)::\n\n \u00a0 \u00a0$ pibooth --reset\n\nSee the `default configuration file <https://github.com/pibooth/pibooth/blob/master/docs/config.rst>`_\nfor further details.\n\nCustomize using plugins\n^^^^^^^^^^^^^^^^^^^^^^^\n\nSeveral plugins maintained by the community are available. They add extra features to\n``pibooth``. Have a look to the `plugins on PyPI  <https://pypi.org/search/?q=pibooth>`_.\n\nYou can also easily develop your own plugin, and declare it in the ``[GENERAL][plugins]``\nkey of the configuration. See guidelines to\n`develop custom plugin <https://github.com/pibooth/pibooth/blob/master/docs/plugin.rst>`_.\n\nGUI translations\n^^^^^^^^^^^^^^^^\n\nThe graphical interface texts are available in 8 languages by default: Danish, Dutch, English, \nFrench, German, Hungarian, Norwegian and Spanish. The default translations can be easily edited using the command::\n\n \u00a0  $ pibooth --translate\n\nA new language can be added by adding a new section (``[alpha-2-code]``).\nIf you want to have ``pibooth`` in your language feel free to send us the corresponding keywords via a GitHub issue.\n\nPrinter\n^^^^^^^\n\nThe print button (see `Commands`_) and print states are automatically activated/shown if:\n\n* `pycups <https://pypi.python.org/pypi/pycups>`_ and `pycups-notify <https://github.com/anxuae/pycups-notify>`_ are installed\n* at least one printer is configured in ``CUPS``\n* the key ``[PRINTER][printer_name]`` is equal to ``default`` or an existing printer name\n\nTo avoid paper waste, set the option ``[PRINTER][max_duplicates]`` to the maximum\nof identical pictures that can be sent to the printer.\n\nSet the option ``[PRINTER][max_pages]`` to the number of paper sheets available on the\nprinter. When this number is reached, the print function will be disabled and an icon\nindicates the printer failure. To reset the counter, open then close the settings\ngraphical interface (see `Commands`_).\n\nHere is the default configuration used for this project in CUPS, it may depend on\nthe printer used:\n\n================ =============================\nOptions          Value\n================ =============================\nMedia Size       10cm x 15cm\nColor Model      CMYK\nMedia Type       Glossy Photo Paper\nResolution       Automatic\n2-Sided Printing Off\nShrink page ...  Shrink (print the whole page)\n================ =============================\n\nCircuit diagram\n---------------\n\nHere is the diagram for hardware connections. Please refer to the\n`default configuration file <https://github.com/pibooth/pibooth/blob/master/docs/config.rst>`_\nto know the default pins used (`physical pin numbering <https://pinout.xyz>`_).\n\n.. image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/sketch.png\n   :align: center\n   :alt: Electronic sketch\n\nAn extra button can be added to start and shutdown properly the Raspberry Pi.\nEdit the file ``/boot/config.txt`` and set the line::\n\n    dtoverlay=gpio-shutdown\n\nThen connect a push button between physical *pin 5* and *pin 6*.\n\nTerms and conditions\n--------------------\n\nSee the LICENSE file to have details on the terms and coniditions.\n\nGDPR advices\n^^^^^^^^^^^^\n\n``pibooth`` was developed for a private usage with no connection to a professional or commercial activity,\nas a consequence GDPR does not apply.\nHowever if you are using photobooth in Europe, it is your responsability to check that your usage and\nmore particularly the usage of the pictures generated by ``pibooth`` follows the GDPR rules, especially make\nsure that the people that will use the ``pibooth`` are aware that the image will be stored on the device.\n\nCredits\n^^^^^^^\n\nPibooth icon from `Artcore Illustrations <https://www.iconspedia.com/icon/photobooth-icon-29464.html>`_\n\nIcons from the Noun Project (https://thenounproject.com/)\n\n- Polaroid by icon 54\n- Up hand drawn arrow by Kid A\n- Cameraman and Friends Posing For Camera by Gan Khoon Lay\n- Camera by Alfa Design\n- Print Photo by Kmg Design\n- Pointer hand by Peter van Driel\n\nSupport us on Beerpay\n---------------------\n\nIf you want to help us you can by clicking on the following links!\n\n|BeerPay| |BeerPay2|\n\n.. |BeerPay| image:: https://beerpay.io/werdeil/pibooth/badge.svg?style=beer-square\n   :align: middle\n   :target: https://beerpay.io/werdeil/pibooth\n\n.. |BeerPay2| image:: https://beerpay.io/werdeil/pibooth/make-wish.svg?style=flat-square\n   :align: middle\n   :target: https://beerpay.io/werdeil/pibooth?focus=wish\n\n.. |Pibooth| image:: https://raw.githubusercontent.com/pibooth/pibooth/master/templates/pibooth.png\n   :align: middle\n\n.. |PythonVersions| image:: https://img.shields.io/badge/python-2.7+ / 3.6+-red.svg\n   :target: https://www.python.org/downloads\n   :alt: Python 2.7+/3.6+\n\n.. |PypiPackage| image:: https://badge.fury.io/py/pibooth.svg\n   :target: https://pypi.org/project/pibooth\n   :alt: PyPi package\n\n.. |Downloads| image:: https://img.shields.io/pypi/dm/pibooth?color=purple\n   :target: https://pypi.org/project/pibooth\n   :alt: PyPi downloads\n"
 },
 {
  "repo": "RobertSasak/react-native-openalpr",
  "language": "C++",
  "readme_contents": "[![Build Status](https://travis-ci.com/RobertSasak/react-native-openalpr.svg?branch=master)](https://travis-ci.com/RobertSasak/react-native-openalpr)\n[![Gitter](https://badges.gitter.im/react-native-openalpr/community.svg)](https://gitter.im/react-native-openalpr/community)\n\n# react-native-openalpr\n\n[OpenALPR](https://github.com/openalpr/openalpr) integration for React Native. Provides a camera component that recognizes license plates in real-time. Supports both iOS and Android.\n\n<img alt=\"OpenALPR iOS Demo Video\" src=\"https://cdn-images-1.medium.com/max/800/1*u1nTJMFc34aDLTPCIr0-cQ.gif\" width=200 height=350 /> <img alt=\"OpenALPR Android Demo Video\" src=\"https://user-images.githubusercontent.com/10334791/27850595-62dc852e-615e-11e7-875c-57a017dbb28c.gif\" width=200 height=350 />\n\n## Requirements\n\n- iOS 9+\n- Android 5.0+\n- RN 0.60+\n\n## Installation\n\n### Installation with React Native\n\nStart by adding the package and linking it.\n\n```sh\n$ yarn add react-native-openalpr\n```\n\n### iOS Specific Setup\n\n#### Install react-native-permissions\n\nIt is a good practice to check and request CAMERA permission. Check full implementation in example folder.\n\n```sh\nyarn add react-native-permissions\n```\n\nAdd camera permission into your podfile.\n\n```\npod 'Permission-Camera', :path => \"../node_modules/react-native-permissions/ios/Camera.podspec\"\n```\n\n#### Install pods\n\n```sh\n$ cd ios && pod install && cd ...\n```\n\n#### Camera Permissions\n\n- Add an entry for `NSCameraUsageDescription` in your `info.plist` explaining why your app will use the camera. If you forget to add this, your app will crash!\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  ...\n \t<key>NSCameraUsageDescription</key>\n \t<string>We use your camera for license plate recognition to make it easier for you to add your vehicle.</string>\n</dict>\n```\n\n#### Bitcode\n\nBecause the OpenCV binary framework release is compiled without bitcode, the other frameworks built by this script are also built without it, which ultimately means your Xcode project also cannot be built with bitcode enabled. [Per this message](http://stackoverflow.com/a/32728516/868173), it sounds like we want this feature disabled for OpenCV anyway.\n\nTo disable bitcode in your project:\n\n- In `Build Settings` \u2192 `Build Options`, search for `Enable Bitcode` and set it to `No`.\n\n### Android-specific Setup\n\n#### Camera Permissions\n\n- Add permissions for `CAMERA` and `FLASHLIGHT` and the related features (below) to `AndroidManifest.xml`. If you forget to add these permissions, your app will crash!\n\n```xml\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  <!-- Camera Permissions -->\n  <uses-permission android:name=\"android.permission.CAMERA\" />\n\n  <uses-feature\n      android:name=\"android.hardware.camera\"\n      android:required=\"false\" />\n  <uses-feature\n      android:name=\"android.hardware.camera.autofocus\"\n      android:required=\"false\" />\n\n  <uses-permission android:name=\"android.permission.FLASHLIGHT\" />\n```\n\n#### Add to Gradle\n\n###### Your `android/settings.gradle` file should have following lines:\n\n```gradle\n\nrootProject.name = 'RNOpenALPRExample'\napply from: file(\"../node_modules/@react-native-community/cli-platform-android/native_modules.gradle\"); applyNativeModulesSettingsGradle(settings)\ninclude ':app'\n\n// Add these lines\ninclude ':openalpr'\nproject(':openalpr').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-openalpr/android/libraries/openalpr')\ninclude ':opencv'\nproject(':opencv').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-openalpr/android/libraries/opencv')\n```\n\n#### Linking\n\nThe library is linked automatically with leptonica, opencv, tesseract, and openalpr ([openalpr](https://github.com/SandroMachado/openalpr-android)).\nTo make it work, copy and paste the directory with the runtime needed data to your project at path `android/app/src/main/assets/runtime_data`.\n\nThe `runtime_data` file can be found in `/example/android/app/src/main/assets/` in this repo. Open `runtime_data/openalpr.conf` file and replace `com.rnopenalprexample` with your package name\n\n```\n[common]\n\n; Specify the path to the runtime data directory\nruntime_dir = /data/data/com.rnopenalprexample/runtime_data\n\n\nocr_img_size_percent = 1.33333333\nstate_id_img_size_percent = 2.0\n...\n```\n\n## Usage\n\nOpenALPR exposes a camera component (based on [react-native-camera](https://github.com/lwansbrough/react-native-camera)) that is optimized to run OpenALPR image processing on a live camera stream. Among other parameters, the camera accepts a callback, `onPlateRecognized`, for when a plate is recognized.\n\n```js\nimport React, { Component } from 'react'\nimport { StyleSheet, Text, View } from 'react-native'\n\nimport Camera, {\n  Aspect,\n  CaptureQuality,\n  TorchMode,\n} from 'react-native-openalpr'\n\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n  },\n  textContainer: {\n    position: 'absolute',\n    top: 100,\n    left: 50,\n  },\n  text: {\n    textAlign: 'center',\n    fontSize: 20,\n  },\n})\n\nexport default class PlateRecognizer extends React.Component {\n  state = {\n    plate: 'Scan a plate',\n  }\n\n  onPlateRecognized = ({ plate, confidence }) => {\n    this.setState({\n      plate,\n    })\n  }\n\n  render() {\n    return (\n      <View style={styles.container}>\n        <Camera\n          style={styles.preview}\n          aspect={Aspect.fill}\n          captureQuality={CaptureQuality.medium}\n          country=\"us\"\n          onPlateRecognized={this.onPlateRecognized}\n          plateOutlineColor=\"#ff0000\"\n          showPlateOutline\n          zoom={0}\n          torchMode={TorchMode.off}\n          touchToFocus\n        />\n        <View style={styles.textContainer}>\n          <Text style={styles.text}>{this.state.plate}</Text>\n        </View>\n      </View>\n    )\n  }\n}\n```\n\n### Options\n\n#### `zoom`\n\nThe zoon of the camera (Android only). Can be :\n\n0 to 99\n\n#### `aspect`\n\nThe aspect ratio of the camera. Can be one of:\n\n- `Aspect.stretch`\n- `Aspect.fit`\n- `Aspect.fill`\n\n#### `captureQuality`\n\nThe resolution at which video frames are captured and analyzed. For completeness, several options are provided. However, it is strongly recommended that you stick with one of the following for the best frame rates and accuracy:\n\n- `CaptureQuality.medium` (480x360)\n- `CaptureQuality.480p` (640x480)\n\n#### `country`\n\nSpecifies which OpenALPR config file to load, corresponding to the country whose plates you wish to recognize. Currently supported values are:\n\n- `au`\n- `br`\n- `eu`\n- `fr`\n- `gb`\n- `kr`\n- `mx`\n- `sg`\n- `us`\n- `vn2`\n\n#### `onPlateRecognized`\n\nThis callback receives a hash with keys:\n\n- `plate`, representing the recognized license plate string\n- `confidence`, OpenALPR's confidence(%) in the result\n\n#### `plateOutlineColor`\n\nHex string specifying the color of the border to draw around the recognized plate. Example: `#ff0000` for red.\n\n#### `showPlateOutline`\n\nIf true, this draws an outline over the recognized plate\n\n#### `torchMode`\n\nTurns the flashlight on or off. Can be one of:\n\n- `TorchMode.on`\n- `TorchMode.off`\n- `TorchMode.auto`\n\n#### `touchToFocus`\n\nIf true, this focuses the camera where the user taps\n\n## Examples\n\n- [Example Project](https://github.com/RobertSasak/react-native-openalpr/tree/master/example)\n\n## Development\n\n- This project works with iOS and Android. It may have some bugs depending on how the underlying native components are updated\n\n### Running the Example project on Android While Developing\n\n1. Clone the repo and enter the `example` directory\n\n```ssh\ngit clone https://github.com/RobertSasak/react-native-openalpr.git\ncd react-native-openalpr\ncd example\n```\n\n2. From the `example` directory, run `yarn`\n\n3. Copy the `android` folder from `/react-native-openalpr/android` to `/react-native-openalpr/example/node_modules/react-native-openalpr/`\n\n4. Open Android Studio and import the project `react-native-openalpr/example/android` and wait until Android Studio indexes and links.\n\n5. Run `npm start` from dir /react-native-openalpr/example/\n\n6. Open the path in your browser `http://localhost:8081/index.android.bundle?platform=android&dev=true&hot=false&minify=false`\n\n7. Create file the `/react-native-openalpr/example/android/app/src/main/assets/index.android.bundle`. Copy and paste the data from browser window to the file you just created and save.\n\n8. Return to Android Studio and run project on your development device.\n\nNote: If you are getting errors, double check that you have completed all of the steps above. If you are having issues running `npm start` on Mac OSX and are using homebrew, [this issue might help](https://github.com/facebook/react-native/issues/910).\n\n## Credits\n\n- OpenALPR built from [OpenALPR-iOS](https://github.com/twelve17/openalpr-ios)\n- Project scaffold based on [react-native-camera](https://github.com/lwansbrough/react-native-camera)\n"
 },
 {
  "repo": "gloomyfish1998/opencv_tutorial",
  "language": "C++",
  "readme_contents": "## OpenCV 4.0 C++/python SDK tutorial\n- include dnn module code example\n- very useful case study\n- not only for beginer but also for expericence developer\n- include most opencv modules and API usages.\n- data \u5305\u542b\u6f14\u793a\u56fe\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\n- \u65b0\u589e\u76ee\u5f55Python\uff0c\u6dfb\u52a0\u4e86OpenCV Python\u8bed\u8a00\u6559\u7a0b\n\n## B\u7ad9\u514d\u8d39\u770b\uff01OpenCV C++\u4e0ePython \u5f00\u53d1\u73af\u5883\u642d\u5efa\u89c6\u9891\nhttp://space.bilibili.com/365916694/#/\n\n## \u8d3e\u5fd7\u521a\n2004\u6bd5\u4e1a\u4e8e\u5c71\u4e1c\u5927\u5b66\u9f50\u9c81\u8f6f\u4ef6\u5b66\u9662\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u4e13\u4e1a\u3002\u4e3b\u8981\u4e13\u6ce8\u4e8e\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u5b66\u4e60\u4e0e\u7814\u7a76\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5e94\u7528\u5f00\u53d1\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5e94\u7528\u3002\u4e24\u672c\u4e66\u7c4d\u300aJava\u6570\u5b57\u56fe\u50cf\u5904\u7406-\u7f16\u7a0b\u6280\u5de7\u4e0e\u5e94\u7528\u5b9e\u8df5\u300b\u3001\u300aOpenCV  Android\u5f00\u53d1\u5b9e\u6218\u300b\u4f5c\u8005\u300151CTO\u5b66\u9662\u91d1\u724c\u8bb2\u5e08\u3001CSDN\u535a\u5ba2\u4e13\u5bb6\u3001\u4e13\u6ce8\u56fe\u50cf\u5904\u7406\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7814\u7a76\u4e0e\u5f00\u53d1\u591a\u5e74\u3001\u4e13\u4e1a\u6280\u672f\u535a\u5ba2\u8bbf\u95ee\u91cf\u8d85\u8fc7320\u4e07\u6b21\u3001\u7cbe\u901aOpenCV\u3001ImageJ\u5f00\u6e90\u6846\u67b6\u3001\u56fe\u50cf\u5bf9\u8c61\u68c0\u6d4b\u4e0e\u8bc6\u522b\u7b49\u5e94\u7528\u5f00\u53d1\u6280\u672f\u3002\u5728\u5bf9\u8c61\u68c0\u6d4b\u3001\u533b\u5b66\u7ec6\u80de\u68c0\u6d4b\u4e0e\u8bc6\u522b\u3001\u6587\u672c\u5904\u7406\u3001\u56fe\u50cf\u641c\u7d22\u3001\u4eba\u8138\u7f8e\u5316\u7b97\u6cd5\u65b9\u9762\u6709\u6df1\u5165\u7814\u7a76\u3001\u5f00\u53d1\u8fc7\u591a\u4e2a\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u6a21\u5757\u5e76\u6210\u529f\u5e94\u7528\u5728\u533b\u5b66\u68c0\u6d4b\u4e0e\u5de5\u4e1a\u9886\u57df\u3002\n\n\n### \u6b22\u8fce\u5927\u5bb6\u52a0\u5165\u4eba\u5de5\u667a\u80fd\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\n### OpenCV\u7814\u4e60\u793e\n\u52a0\u5165\u5373\u9001400+\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8bba\u6587\u3002\n- \u6bcf\u5468\u4e00\u5230\u6bcf\u5468\u4e94\u5206\u4eabOpenCV/tensorflow\u77e5\u8bc6\u70b9\u5b66\u4e60\uff08\u97f3\u9891+\u6587\u5b57+\u6e90\u7801\uff09\n- OpenCV+tensorflow\u771f\u5b9e\u6848\u4f8b\u4ee3\u7801\u5206\u4eab\n- \u76f4\u63a5\u5411\u8001\u5e08\u63d0\u95ee\u3001\u6bcf\u5929\u7b54\u7591\u8f85\u5bfc\n- \u7cfb\u7edf\u5316\u5b66\u4e60\u7ea6300\u4e2a\u77e5\u8bc6\u70b9\uff0c\u4ece\u6613\u5230\u96be\u3001\u7531\u6d45\u5165\u6df1\n- \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u77e5\u8bc6\u957f\u6587\u5206\u4eab\n- \u6bcf\u5929\u4e0d\u52303\u6bdb\u94b1\uff0c\u62e5\u62b1\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\n- Python\u4e0eC++\u53cc\u8bed\u77e5\u8bc6\u70b9\u5206\u4eab\n- \u5efa\u7acb\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u4eba\u8109\uff0c\u8ba4\u8bc6\u66f4\u591a\u540c\u884c\uff0c\u4e00\u8d77\u5b66\u4e60\u4ea4\u6d41\uff01\n- \u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587PDF\u4e0e\u5b66\u4e60\u8d44\u6599PPT\u5206\u4eab\n\u5c11\u770b\u4e00\u573a\u7535\u5f71\uff0c\u5c11\u5403\u4e00\u987f\u5927\u9910\uff0c\u5c31\u53ef\u4ee5\u52a0\u6301\u672a\u6765\uff0c99\u5143\u5b66\u4e60\u5c31\u53ef\u4ee5\u505a\u5230\uff01\n![](OpenCVRD.png)\n\n\n## \u89c6\u9891\u8bfe\u7a0b\n\n| \u8bfe\u7a0b\u540d\u79f0\u00a0 \u00a0 \u00a0 \u00a0 | \u8bed\u8a00 \u00a0 \u00a0 \u00a0 \u00a0  | \u5730\u5740\u00a0 \u00a0 \u00a0 \u00a0  |\n| ------------- |:-------------:| :-------------:|\n|Tensorflow+OpenCV\u5b9e\u6218\u884c\u4eba\u68c0\u6d4b|Python|https://edu.51cto.com/course/16898.html|\n| Tensorflow Object Detection API\u5bf9\u8c61\u68c0\u6d4b\u6559\u7a0b| Python| http://edu.51cto.com/course/15208.html |\n| Tensorflow\u96f6\u57fa\u7840\u5165\u95e8\u8bfe\u7a0b| Python| http://edu.51cto.com/course/14584.html |\n| OpenCV Python\u56fe\u50cf\u4e0e\u89c6\u9891\u5206\u6790\u6559\u7a0b| Python| http://edu.51cto.com/course/14029.html |\n| OpenCV Python\u56fe\u50cf\u5904\u7406\u8fdb\u9636\u6559\u7a0b\u89c6\u9891\u8bfe\u7a0b| Python| http://edu.51cto.com/course/13789.html |\n| OpenCV Python\u96f6\u57fa\u7840\u5165\u95e8\u89c6\u9891\u6559\u7a0b| Python| http://edu.51cto.com/course/13680.html |\n| OpenCV4Android\u6444\u50cf\u5934\u5e94\u7528\u6848\u4f8b\u5b9e\u6218\u9891\u6559\u7a0b| Android| http://edu.51cto.com/course/12700.html |\n| Python+OpenCV3.3\u56fe\u50cf\u5904\u7406\u89c6\u9891\u6559\u7a0b| Python| http://edu.51cto.com/course/11324.html |\n| OpenCV3.3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc(DNN)\u6a21\u5757-\u5e94\u7528\u89c6\u9891\u6559\u7a0b| C++| http://edu.51cto.com/course/11516.html |\n| \u4eba\u5de5\u667a\u80fd\u4e4bOpenCV\u4eba\u8138\u8bc6\u522b\u6848\u4f8b\u5b9e\u6218\u89c6\u9891\u6559\u7a0b| C++| http://edu.51cto.com/course/10759.html |\n| OpenCV\u56fe\u50cf\u5206\u5272\u5b9e\u6218\u89c6\u9891\u6559\u7a0b|C++| http://edu.51cto.com/course/10166.html |\n| OpenCV\u89c6\u9891\u5206\u6790\u4e0e\u5bf9\u8c61\u8ddf\u8e2a\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8837.html |\n| OpenCV\u7ea7\u8054\u5206\u7c7b\u5668\u8bad\u7ec3\u4e0e\u4f7f\u7528\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8645.html |\n| OpenCV\u56fe\u50cf\u5904\u7406-\u5c0f\u6848\u4f8b\u5b9e\u6218\u6559\u7a0b|C++| http://edu.51cto.com/course/8354.html |\n| OpenCV \u7279\u5f81\u63d0\u53d6\u4e0e\u68c0\u6d4b\u5b9e\u6218\u89c6\u9891\u8bfe\u7a0b|C++| http://edu.51cto.com/course/8305.html |\n| OpenCV\u56fe\u50cf\u5904\u7406\u89c6\u9891\u8bfe\u7a0b|C++| http://edu.51cto.com/course/7521.html |\n| OpenCV For Android\u57fa\u7840\u5165\u95e8\u5b9e\u6218\u89c6\u9891\u8bfe\u7a0b|Android | http://edu.51cto.com/course/8012.html |\n| OpenCV3.2\u5e94\u7528\u5b9e\u6218\u7cfb\u5217\u89c6\u9891\u8bfe\u7a0b-Android\u5e73\u53f0\u94f6\u884c\u5361\u5361\u53f7\u8bc6\u522b|Android| http://edu.51cto.com/course/9170.html |\n| OpenCV3.2\u670d\u52a1\u5668\u7aef\u56fe\u50cf\u5904\u7406\u4e0eJava\u56fe\u50cf\u5904\u7406\u89c6\u9891\u6559\u7a0b|Java| http://edu.51cto.com/course/10475.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u4e8c\u503c\u56fe\u50cf\u5206\u6790\u89c6\u9891\u8bfe\u7a0b(Java\u8bed\u8a00\u63cf\u8ff0)|Java| http://edu.51cto.com/course/6901.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u7a7a\u95f4\u57df\u5377\u79ef(Java\u8bed\u8a00\u63cf\u8ff0)\u89c6\u9891\u8bfe\u7a0b|Java| http://edu.51cto.com/course/6464.html |\n| \u6570\u5b57\u56fe\u50cf\u5904\u7406-\u57fa\u7840\u5165\u95e8\u89c6\u9891\u8bfe\u7a0b(Java\u8bed\u8a00\u63cf\u8ff0)|Java| http://edu.51cto.com/course/6335.html |\n\n## \u8054\u7cfb\u65b9\u5f0f\nQQ:57558865 \u6ce8\u660e\uff1aopencv\u5373\u53ef\n\n\u90ae\u4ef6:57558865@qq.com\n\n\u5fae\u4fe1\uff1agloomy_fish \u6ce8\u660e \u5408\u4f5c\n\n<b>\u975e\u6280\u672f\u5408\u4f5c\u8bf7\u52ff\u52a0\u5fae\u4fe1</b>\n\n\u5fae\u535a \uff1a http://weibo.com/u/3181256271\n\nCSDN\u535a\u5ba2\u5730\u5740\uff1a http://blog.csdn.net/jia20003\n\n51CTO\u5b66\u9662\u8bfe\u7a0b\u4e3b\u9875\uff1ahttp://edu.51cto.com/lecturer/8837804.html \n\n### OpenCV\u5b66\u5802 \u5fae\u4fe1\u516c\u4f17\u53f7\n![](CVSCHOOL.jpg)\n\n\n\n\n"
 },
 {
  "repo": "cyrildiagne/screenpoint",
  "language": "Python",
  "readme_contents": "# ScreenPoint\n\nFinds the (x,y) coordinates of the centroid of an image (eg: a mobile phone camera image) pointing at another image (eg: a computer screen) using [OpenCV SIFT](https://docs.opencv.org/2.4/modules/nonfree/doc/feature_detection.html).\n\n![debug view](example/match_debug.png)\n\n## Installation\n\nThis library only supports Python 3.6 or Python 3.7. That until SIFT becomes available again in opencv-python-contrib.\n\n```bash\npip install screenpoint\n```\n\n## Usage\n\n```python\nimport screenpoint\nimport cv2\n\n# Load input images.\nscreen = cv2.imread('screen.png', 0)\nview = cv2.imread('view.jpg', 0)\n\n# Project view centroid to screen space.\n# x and y are the coordinate of the `view` centroid in `screen` space.\nx, y = screenpoint.project(view, screen)\n```\n\nSee [example.py](example.py) for more information.\n"
 },
 {
  "repo": "BloodAxe/OpenCV-Tutorial",
  "language": "C++",
  "readme_contents": "OpenCV Tutorial\n==========================\n\nThis repository contains source code of OpenCV Tutorial application.\n\nSamples list (done)\n==========================\n* Edge detection (Canny, Sobel, Schaar)\n* Image transformations (Sepia, negative, contrast and brightness adjustments)\n* Feature Detection (SURF, ORB, FREAK)\n* Video tracking (KLT, BRIEF, ORB)\n\nSamples list (plans)\n==========================\n* PTAM  \n\nStep by step tutorials\n==========================\n* [Part 1](http://computer-vision-talks.com/2012-06-23-opencv-tutorial-part-1/)\n* [Part 2](http://computer-vision-talks.com/2012-06-24-opencv-tutorial-part-2/)\n* [Part 3](http://computer-vision-talks.com/2012-06-27-opencv-tutorial-part-3/)\n* [Part 4](http://computer-vision-talks.com/2012-07-07-opencv-tutorial-part-4/)\n* [Part 5](http://computer-vision-talks.com/2012-07-14-opencv-tutorial-part-5/)\n* [Part 6](http://computer-vision-talks.com/2012-07-22-opencv-tutorial-part-6/)\n* [Part 7](http://computer-vision-talks.com/2012-10-22-opencv-tutorial-part-7/)\n\nRoadmap\n==========================\nhttp://computer-vision-talks.com/opencv-tutorial-roadmap/\n\nCopyright\n==========================\nIdea and development by Eugene Khvedchenya <ekhvedchenya@gmail.com>\n\nhttp://computer-vision-talks.com\n\nThis application is provided via BSD licence, it is free for both academic and commercial use.\n\nThis application is provided as-is, with no warranty expressed or implied.  Use this application at your own risk.\nThe author assumes no liability for any loss associated with the use of this application.\nIf you do not agree with the terms of this license, do not install this application.\n\nContributors\n==========================\n * Anton Belodedenko <anton.belodedenko@gmail.com> \n * Emmanuel d'Angelo <http://www.computersdontsee.net>\n * Daniel J. Pinter <datazombies@gmail.com>\n"
 },
 {
  "repo": "wzh191920/License-Plate-Recognition",
  "language": "Python",
  "readme_contents": "# License-Plate-Recognition\nLicense Plate Recognition For Car With Python And OpenCV\n\n#### \u7528python3+opencv3\u505a\u7684\u4e2d\u56fd\u8f66\u724c\u8bc6\u522b\uff0c\u5305\u62ec\u7b97\u6cd5\u548c\u5ba2\u6237\u7aef\u754c\u9762\uff0c\u53ea\u67092\u4e2a\u6587\u4ef6\uff0csurface.py\u662f\u754c\u9762\u4ee3\u7801\uff0cpredict.py\u662f\u7b97\u6cd5\u4ee3\u7801\uff0c\u754c\u9762\u4e0d\u662f\u91cd\u70b9\u6240\u4ee5\u7528tkinter\u5199\u5f97\u5f88\u7b80\u5355\u3002\n\n### \u4f7f\u7528\u65b9\u6cd5\uff1a\n\u7248\u672c\uff1apython3.4.4\uff0copencv3.4\u548cnumpy1.14\u548cPIL5<br>\n\u4e0b\u8f7d\u6e90\u7801\uff0c\u5e76\u5b89\u88c5python\u3001numpy\u3001opencv\u7684python\u7248\u3001PIL\uff0c\u8fd0\u884csurface.py\u5373\u53ef\n\n### \u7b97\u6cd5\u5b9e\u73b0\uff1a\n\u7b97\u6cd5\u601d\u60f3\u6765\u81ea\u4e8e\u7f51\u4e0a\u8d44\u6e90\uff0c\u5148\u4f7f\u7528\u56fe\u50cf\u8fb9\u7f18\u548c\u8f66\u724c\u989c\u8272\u5b9a\u4f4d\u8f66\u724c\uff0c\u518d\u8bc6\u522b\u5b57\u7b26\u3002\u8f66\u724c\u5b9a\u4f4d\u5728predict\u65b9\u6cd5\u4e2d\uff0c\u4e3a\u8bf4\u660e\u6e05\u695a\uff0c\u5b8c\u6210\u4ee3\u7801\u548c\u6d4b\u8bd5\u540e\uff0c\u52a0\u4e86\u5f88\u591a\u6ce8\u91ca\uff0c\u8bf7\u53c2\u770b\u6e90\u7801\u3002\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\u4e5f\u5728predict\u65b9\u6cd5\u4e2d\uff0c\u8bf7\u53c2\u770b\u6e90\u7801\u4e2d\u7684\u6ce8\u91ca\uff0c\u9700\u8981\u8bf4\u660e\u7684\u662f\uff0c\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\u4f7f\u7528\u7684\u7b97\u6cd5\u662fopencv\u7684SVM\uff0c opencv\u7684SVM\u4f7f\u7528\u4ee3\u7801\u6765\u81ea\u4e8eopencv\u9644\u5e26\u7684sample\uff0cStatModel\u7c7b\u548cSVM\u7c7b\u90fd\u662fsample\u4e2d\u7684\u4ee3\u7801\u3002SVM\u8bad\u7ec3\u4f7f\u7528\u7684\u8bad\u7ec3\u6837\u672c\u6765\u81ea\u4e8egithub\u4e0a\u7684EasyPR\u7684c++\u7248\u672c\u3002\u7531\u4e8e\u8bad\u7ec3\u6837\u672c\u6709\u9650\uff0c\u4f60\u6d4b\u8bd5\u65f6\u4f1a\u53d1\u73b0\uff0c\u8f66\u724c\u5b57\u7b26\u8bc6\u522b\uff0c\u53ef\u80fd\u5b58\u5728\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u7b2c\u4e00\u4e2a\u4e2d\u6587\u5b57\u7b26\u51fa\u73b0\u7684\u8bef\u5dee\u6982\u7387\u8f83\u5927\u3002\u6e90\u7801\u4e2d\uff0c\u6211\u4e0a\u4f20\u4e86EasyPR\u4e2d\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5728train\\\u76ee\u5f55\u4e0b\uff0c\u5982\u679c\u8981\u91cd\u65b0\u8bad\u7ec3\u8bf7\u89e3\u538b\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\uff0c\u5e76\u5220\u9664\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u6587\u4ef6svm.dat\u548csvmchinese.dat\u3002\n\n##### \u989d\u5916\u8bf4\u660e\uff1a\u7b97\u6cd5\u4ee3\u7801\u53ea\u6709500\u884c\uff0c\u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c\u8f66\u724c\u5b9a\u4f4d\u7b97\u6cd5\u7684\u53c2\u6570\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u8272\u504f\u3001\u8f66\u8ddd\u5f71\u54cd\uff08test\u76ee\u5f55\u4e0b\u7684\u8f66\u724c\u7684\u50cf\u7d20\u90fd\u6bd4\u8f83\u5c0f\uff0c\u5176\u4ed6\u56fe\u7247\u5f88\u53ef\u80fd\u56e0\u4e3a\u50cf\u7d20\u7b49\u95ee\u9898\u8bc6\u522b\u4e0d\u4e86\uff0c\u8bc6\u522b\u5176\u4ed6\u50cf\u7d20\u7684\u8f66\u724c\u9700\u8981\u4fee\u6539config\u6587\u4ef6\u91cc\u9762\u7684\u53c2\u6570\uff0c\u6b64\u9879\u76ee\u4ec5\u662f\u629b\u7816\u5f15\u7389\uff0c\u63d0\u4f9b\u4e00\u4e2a\u601d\u8def\uff09\u3002\n##### \u6709\u4efb\u4f55\u7591\u95ee\u8bf7\u90ae\u4ef6\u81f3 wzh191920@sina.com\n\n##### \u754c\u9762\u6548\u679c\uff1a\n![](https://github.com/wzh191920/License-Plate-Recognition/blob/master/Screenshots/3.png)\n![](https://github.com/wzh191920/License-Plate-Recognition/blob/master/Screenshots/5.png)\n"
 },
 {
  "repo": "abidrahmank/OpenCV2-Python",
  "language": "Python",
  "readme_contents": "This repo contains some samples codes on OpenCV-Python with new 'cv2' interface.\n\nIt contains two folders:\n1) Official_Tutorial_Python_version : It contains the corresponding Python codes for C++ codes in official tutorials. So explanation of the code can be found at 'http://opencv.itseez.com/doc/tutorials/tutorials.html' \n\n2) Blog_codes : It contains some codes which I have explained in my blog 'www.opencvpython.blogspot.com'. For explanation, visit the blog.\n\nYou can contact me on abidrahman2@gmail.com\n\nWith Regards,\nAbid Rahman K."
 },
 {
  "repo": "atulapra/Emotion-detection",
  "language": "Python",
  "readme_contents": "# Emotion detection using deep learning\n\n## Introduction\n\nThis project aims to classify the emotion on a person's face into one of **seven categories**, using deep convolutional neural networks. The model is trained on the **FER-2013** dataset which was published on International Conference on Machine Learning (ICML). This dataset consists of 35887 grayscale, 48x48 sized face images with **seven emotions** - angry, disgusted, fearful, happy, neutral, sad and surprised.\n\n## Dependencies\n\n* Python 3, [OpenCV](https://opencv.org/), [Tensorflow](https://www.tensorflow.org/)\n* To install the required packages, run `pip install -r requirements.txt`.\n\n## Basic Usage\n\nThe repository is currently compatible with `tensorflow-2.0` and makes use of the Keras API using the `tensorflow.keras` library.\n\n* First, clone the repository and enter the folder\n\n```bash\ngit clone https://github.com/atulapra/Emotion-detection.git\ncd Emotion-detection\n```\n\n* Download the FER-2013 dataset from [here](https://drive.google.com/file/d/1X60B-uR3NtqPd4oosdotpbDgy8KOfUdr/view?usp=sharing) and unzip it inside the `src` folder. This will create the folder `data`.\n\n* If you want to train this model, use:  \n\n```bash\ncd src\npython emotions.py --mode train\n```\n\n* If you want to view the predictions without training again, you can download the pre-trained model from [here](https://drive.google.com/file/d/1FUn0XNOzf-nQV7QjbBPA6-8GLoHNNgv-/view?usp=sharing) and then run:  \n\n```bash\ncd src\npython emotions.py --mode display\n```\n\n* The folder structure is of the form:  \n  src:\n  * data (folder)\n  * `emotions.py` (file)\n  * `haarcascade_frontalface_default.xml` (file)\n  * `model.h5` (file)\n\n* This implementation by default detects emotions on all faces in the webcam feed. With a simple 4-layer CNN, the test accuracy reached 63.2% in 50 epochs.\n\n![Accuracy plot](imgs/accuracy.png)\n\n## Data Preparation (optional)\n\n* The [original FER2013 dataset in Kaggle](https://www.kaggle.com/deadskull7/fer2013) is available as a single csv file. I had converted into a dataset of images in the PNG format for training/testing and provided this as the dataset in the previous section.\n\n* In case you are looking to experiment with new datasets, you may have to deal with data in the csv format. I have provided the code I wrote for data preprocessing in the `dataset_prepare.py` file which can be used for reference.\n\n## Algorithm\n\n* First, the **haar cascade** method is used to detect faces in each frame of the webcam feed.\n\n* The region of image containing the face is resized to **48x48** and is passed as input to the CNN.\n\n* The network outputs a list of **softmax scores** for the seven classes of emotions.\n\n* The emotion with maximum score is displayed on the screen.\n\n## Example Output\n\n![Mutiface](imgs/multiface.png)\n\n## References\n\n* \"Challenges in Representation Learning: A report on three machine learning contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n   Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,  \n   X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n   M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n   Y. Bengio. arXiv 2013.\n"
 },
 {
  "repo": "feichtenhofer/gpu_flow",
  "language": "C++",
  "readme_contents": "GPU based optical flow extraction in OpenCV\n====================\n### Features:\n* OpenCV wrapper for Real-Time optical flow extraction on GPU\n* Automatic directory handling using Qt\n* Allows saving of optical flow to disk, \n** either with clipping large displacements \n** or by adaptively scaling the displacements to the radiometric resolution of the output image\n\n### Dependencies\n* [OpenCV 2.4] (http://opencv.org/downloads.html) (if you want OpenCV 3.1, tell me, I'll do the port)\n* [Qt 5.4] (https://www.qt.io/qt5-4/)\n* [cmake] (https://cmake.org/)\n\n### Installation\n1. `mkdir -p build`\n2. `cd build`\n3. `cmake ..`\n4. `make`\n\n### Configuration:\nYou should adjust the input and output directories by passing in `vid_path` and `out_path`. Note that vid_path must exist, Qt will create out_path. Use -h option t for more.\nIn the CMakeLists.txt there is an option called WARP. This selects if you want warped optical flow or not. The warped optical flow file also outputs optical flows as a single BGR image (red is the flow magnitude). In the compute_flow_si_warp file itself there is a warp variable that you can set to false to just compute normal flow. If you want grayscale for images (x and y) use compute_flow.\n\n### Usage:\n```\n./compute_flow [OPTION]...\n```\n```\n./compute_flow_si_warp [OPTION] ..\n```\n\nAvailable options:\n* `start_video`: start with video number in `vid_path` directory structure [1]\n* `gpuID`: use this GPU ID [0]\n* `type`: use this flow method Brox = 0, TVL1 = 1 [1] \n* `skip`: the number of frames that are skipped between flow calcuation [1]\n* `vid_path`: folder with input videos\n* `out_path`: folder where a folder per video containing optical flow frames will be created\n\nAdditional features in `compute_flow.cpp`:\n* `float MIN_SZ = 256`: defines the smallest side of the frame for optical flow computation\n* `float OUT_SZ = 256`: defines the smallest side of the frame for saving as .jpeg \n* `bool clipFlow = true;`: defines whether to clip the optical flow larger than [-20 20] pixels and maps the interval [-20 20] to  [0 255] in grayscale image space. If no clipping is performed the mapping to the image space is achieved by finding the frame-wise minimum and maximum displacement and mapping to [0 255] via an adaptive scaling, where the scale factors are saved as a binary file to `out_path`.\n\n### Example:\n```\n./compute_flow --gpuID=0 --type=1 --vid_path=test --vid_path=test_out --stride=2\n```\n\n\n"
 },
 {
  "repo": "huihut/OpenCV-MinGW-Build",
  "language": null,
  "readme_contents": "# OpenCV-MinGW-Build\n\nMinGW 32bit and 64bit version of OpenCV compiled on Windows.\n\n## Releases\n\n### [OpenCV-4.5.0-with-contrib](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.5.0-with-contrib) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.5.0-with-contrib.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.5.0-with-contrib.tar.gz)\n\n```\ngit clone -b OpenCV-4.5.0-with-contrib git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.18.4\n\n```\nGeneral configuration for OpenCV 4.5.0 =====================================\n  Version control:               unknown\n\n  Extra modules:\n    Location (extra):            E:\\\\opencv-4.5.0\\\\opencv_contrib-4.5.0\\\\modules\n    Version control (extra):     unknown\n\n  Platform:\n    Timestamp:                   2020-11-21T18:57:32Z\n    Host:                        Windows 10.0.19041 AMD64\n    CMake:                       3.18.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (18 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ standard:                11\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 aruco bgsegm bioinspired calib3d ccalib core cvv datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 alphamat cnn_3dobj cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev freetype hdf java js julia matlab ovis python2 python3 sfm viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         YES\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.5-62)\n    WEBP:                        build (ver encoder: 0x020f)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 2.3.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.91.100)\n      avformat:                  YES (58.45.100)\n      avutil:                    YES (56.51.100)\n      swscale:                   YES (5.7.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.5.0/opencv-4.5.0/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/.windows-build-tools/python27/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.5.0/opencv-4.5.0-qt-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.1.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.1-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-08-07T16:07:07Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (29 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.1/opencv-4.1.1/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.1/opencv-4.1.1-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.1.0-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.0-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.0-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.0 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-04-09T16:48:54Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (29 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.0/opencv-4.1.0/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.0/opencv-4.1.0-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.1.0](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.1.0) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.1.0.tar.gz)\n\n```\ngit clone -b OpenCV-4.1.0 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 4.1.0 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-07-24T13:59:00Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (16 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.35.100)\n      avformat:                  YES (58.20.100)\n      avutil:                    YES (56.22.100)\n      swscale:                   YES (5.3.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.1.0/opencv-4.1.0/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.1.0/opencv-4.1.0-x86-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.1-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 4.0.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-03-27T15:25:27Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (7 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (13 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.35.100)\n      avformat:                  YES (ver 58.20.100)\n      avutil:                    YES (ver 56.22.100)\n      swscale:                   YES (ver 5.3.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.1/opencv-4.0.1/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.1/opencv-4.0.1-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.0-rc-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.0-rc-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-rc-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-rc-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.0-rc-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86_64-8.1.0-posix-seh-rt_v6-rev0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 4.0.0-rc =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-11-17T09:33:40Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (6 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (12 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator stitching ts video videoio\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 java js python2 python3\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.35.100)\n      avformat:                  YES (ver 58.20.100)\n      avutil:                    YES (ver 56.22.100)\n      swscale:                   YES (ver 5.3.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.0-rc/opencv-4.0.0-rc/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.0-rc/opencv-4.0.0-rc-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 4.0.0-alpha-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-4.0.0-alpha-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-alpha-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-4.0.0-alpha-x64.tar.gz)\n\n```\ngit clone -b OpenCV-4.0.0-alpha-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x64-4.8.1-release-posix-seh-rev5\n* Windows-10-64bit\n* CMake-3.12.0\n\n```\nGeneral configuration for OpenCV 4.0.0-alpha =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-09-25T08:37:52Z\n    Host:                        Windows 10.0.17134 AMD64\n    CMake:                       3.12.0\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (4 files):          + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (10 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ Compiler:                E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/g++.exe  (ver 4.8.1)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    js world\n    Disabled by dependency:      -\n    Unavailable:                 java python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.34)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    Video for Windows:           YES\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 58.18.100)\n      avformat:                  YES (ver 58.12.100)\n      avutil:                    YES (ver 56.14.100)\n      swscale:                   YES (ver 5.1.100)\n      avresample:                YES (ver 4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-4.0.0-alpha/opencv-4.0.0-alpha/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-4.0.0-alpha/opencv-4.0.0-mingw64-build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 3.4.9](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.9) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.9.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.9.tar.gz)\n```\ngit clone -b OpenCV-3.4.9 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary>\n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.9 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-12-26T15:33:59Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (16 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.9/opencv-windows/sources/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.9/build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.8 x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.8-x64) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.8-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.8-x64.tar.gz)\n```\ngit clone -b OpenCV-3.4.8-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary>\n\n* MinGW-x64-7.0.0\n* Windows-10-64bit\n* CMake-3.16.0-rc3\n\n```\nGeneral configuration for OpenCV 3.4.8 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-11-19T16:58:28Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.16.0-rc3\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            C:/PROGRA~1/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (15 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (28 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                C:/Program Files/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/g++.exe  (ver 8.1.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  C:/Program Files/mingw-w64/x86_64-8.1.0-posix-seh-rt_v6-rev0/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                C:/Library/opencv-3.4.8/source/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Program Files/python27/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    C:/Library/opencv-3.4.8/build/install\n-----------------------------------------------------------------\n```\n</details>\n\n\n### [OpenCV 3.4.7](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.7) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.7.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.7.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.7 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.7 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-09-01T15:19:48Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (15 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.7/opencv-3.4.7/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.7/opencv-3.4.7-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.6](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.6) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.6.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.6.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.6 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-7.3.0\n* Windows-10-64bit\n* CMake-3.14.1\n\n```\nGeneral configuration for OpenCV 3.4.6 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-07-22T17:25:51Z\n    Host:                        Windows 10.0.18362 AMD64\n    CMake:                       3.14.1\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (15 files):         + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/g++.exe  (ver 7.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.13.0/Tools/mingw730_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc ml objdetect photo shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.13.0)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.13.0)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 2.0.2-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.36)\n    TIFF:                        build (ver 42 - 4.0.10)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.6/opencv-3.4.6/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.6/opencv-3.4.6-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.5](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.5) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.5.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.5.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.5 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.12.4\n\n```\nGeneral configuration for OpenCV 3.4.5 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2019-02-11T05:45:28Z\n    Host:                        Windows 10.0.17763 AMD64\n    CMake:                       3.12.4\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2\n      requested:                 SSE2\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX\n      requested:                 SSE4_1 SSE4_2 AVX FP16\n      SSE4_1 (7 files):          + SSE3 SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (6 files):             + SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/g++.exe  (ver 5.3.0)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/Qt/Qt5.11.2/Tools/mingw530_32/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -mfpmath=sse -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         YES\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java js python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    QT:                          YES (ver 5.11.2)\n      QT OpenGL support:         YES (Qt5::OpenGL 5.11.2)\n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.35)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv-3.4.5/opencv-3.4.5/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            C:/Users/huihu/AppData/Local/Programs/Python/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files/Java/jdk1.8.0_191/include C:/Program Files/Java/jdk1.8.0_191/include/win32 C:/Program Files/Java/jdk1.8.0_191/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    E:/opencv-3.4.5/opencv-3.4.5-build/install\n-----------------------------------------------------------------\n```\n</details>\n\n### [OpenCV 3.4.1-x64](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.1-x64)  | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1-x64.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1-x64.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.1-x64 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x64-4.8.1-release-posix-seh-rev5\n* Windows-10-64bit\n* CMake-3.12.0\n\n```\nGeneral configuration for OpenCV 3.4.1 =====================================\n  Version control:               unknown\n\n  Platform:\n    Timestamp:                   2018-07-31T02:14:11Z\n    Host:                        Windows 10.0.17134 AMD64\n    CMake:                       3.12.0\n    CMake generator:             MinGW Makefiles\n    CMake build tool:            E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/mingw32-make.exe\n    Configuration:               Release\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (3 files):          + SSSE3 SSE4_1\n      SSE4_2 (1 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (2 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (9 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++11:                       YES\n    C++ Compiler:                E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/g++.exe  (ver 4.8.1)\n    C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG\n    C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG\n    C Compiler:                  E:/MinGW-w64/x64-4.8.1-release-posix-seh-rev5/mingw64/bin/gcc.exe\n    C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG\n    C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG\n    Linker flags (Release):      -Wl,--gc-sections  \n    Linker flags (Debug):        -Wl,--gc-sections  \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          opengl32 glu32\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 calib3d core dnn features2d flann highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python_bindings_generator shape stitching superres ts video videoio videostab\n    Disabled:                    js world\n    Disabled by dependency:      -\n    Unavailable:                 cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev java python2 python3 viz\n    Applications:                tests perf_tests apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.11)\n    JPEG:                        build (ver 90)\n    WEBP:                        build (ver encoder: 0x020e)\n    PNG:                         build (ver 1.6.34)\n    TIFF:                        build (ver 42 - 4.0.9)\n    JPEG 2000:                   build (ver 1.900.1)\n    OpenEXR:                     build (ver 1.7.1)\n\n  Video I/O:\n    Video for Windows:           YES\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (ver 57.107.100)\n      avformat:                  YES (ver 57.83.100)\n      avutil:                    YES (ver 55.78.100)\n      swscale:                   YES (ver 4.8.100)\n      avresample:                YES (ver 3.7.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n\n  Parallel framework:            none\n\n  Trace:                         YES (built-in)\n\n  Other third-party libraries:\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.5.1)\n\n  NVIDIA CUDA:                   NO\n\n  OpenCL:                        YES (no extra features)\n    Include path:                E:/opencv_341/opencv/sources/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python (for build):            E:/Python37-32/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         C:/Program Files (x86)/Java/jdk1.8.0_181/include C:/Program Files (x86)/Java/jdk1.8.0_181/include/win32 C:/Program Files (x86)/Java/jdk1.8.0_181/include\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Matlab:                        NO\n\n  Install to:                    E:/opencv_341/opencv_mingw64_build/install\n-----------------------------------------------------------------\n```\n\n</details>\n\n### [OpenCV 3.4.1](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.4.1) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.4.1.tar.gz)\n\n```\ngit clone -b OpenCV-3.4.1 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.9.2\n\n</details>\n\n### [OpenCV 3.3.1](https://github.com/huihut/OpenCV-MinGW-Build/tree/OpenCV-3.3.1) | [zip](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.3.1.zip) | [tar.gz](https://github.com/huihut/OpenCV-MinGW-Build/archive/OpenCV-3.3.1.tar.gz)\n\n```\ngit clone -b OpenCV-3.3.1 git@github.com:huihut/OpenCV-MinGW-Build.git\n```\n\n<details><summary>Configuration</summary> \n\n* MinGW-x86-5.3.0\n* Windows-10-64bit\n* CMake-3.9.2\n\n</details>\n\n## Tutorials\n\n### How to compile OpenCV\n\n* [blog.huihut . OpenCV\u4f7f\u7528CMake\u548cMinGW-w64\u7684\u7f16\u8bd1\u5b89\u88c5](https://blog.huihut.com/2018/07/31/CompiledOpenCVWithMinGW64/)\n* [blog.huihut . OpenCV\u4f7f\u7528CMake\u548cMinGW\u7684\u7f16\u8bd1\u5b89\u88c5\u53ca\u5176\u5728Qt\u914d\u7f6e\u8fd0\u884c](https://blog.huihut.com/2017/12/03/CompiledOpenCVRunInQt/)\n* [wiki.qt . How to setup Qt and openCV on Windows](https://wiki.qt.io/How_to_setup_Qt_and_openCV_on_Windows)\n* [Tutorial: Installation from source for Windows with Mingw-w64](http://visp-doc.inria.fr/doxygen/visp-daily/tutorial-install-win10-mingw64.html)\n\n### Using OpenCV in Visual Studio Code\n\n* [Running a C++ program with OpenCV 3.4.1 using MinGW-w64 g++ in Visual Studio Code on Windows 10 x64](https://stackoverflow.com/questions/51622111/opencv-c-mingw-vscode-fatal-error-to-compile/51801863#51801863)\n\n### Using OpenCV in Visual Studio\n\n* [How to build applications with OpenCV inside the Microsoft Visual Studio](https://docs.opencv.org/2.4/doc/tutorials/introduction/windows_visual_studio_Opencv/windows_visual_studio_Opencv.html)\n\n### Using OpenCV in Qt\n* [wiki.qt . How to setup Qt and openCV on Windows](https://wiki.qt.io/How_to_setup_Qt_and_openCV_on_Windows)\n"
 },
 {
  "repo": "Breta01/handwriting-ocr",
  "language": "Jupyter Notebook",
  "readme_contents": "# Handwriting OCR\nThe project tries to create software for recognition of a handwritten text from photos (also for Czech language). It uses computer vision and machine learning. And it experiments with different approaches to the problem. It started as a school project which I got a chance to present on Intel ISEF 2018.\n\n<p align=\"center\"><img src =\"doc/imgs/poster.png?raw=true\" height=\"340\" alt=\"Sublime's custom image\" /></p>\n\n## Program Structure\nProces of recognition is divided into 4 steps. The initial input is a photo of page with text.\n\n1. Detection of page and removal of background\n2. Detection and separation of words\n3. Normalization of words\n4. Separation and recegnition of characters (recognition of words)\n\nMain files combining all the steps are [OCR.ipynb](notebooks/OCR.ipynb) or [OCR-Evaluator.ipynb](notebooks/ocr_evaluator.ipynb). Naming of files goes by step representing - name of machine learning model.\n\n## Getting Started\n### 1. Clone the repository\n```\ngit clone https://github.com/Breta01/handwriting-ocr.git\n```\nAfter downloading the repo, you have to download the datasets and models (for more info look into [data](data/) and [models](models/) folders).\n\n### 2. Requirements\nThe project is created using Python 3.6 with Jupyter Notebook. I recommend using Anaconda. If you have it, you can run the installation as:\n```\nconda create --name ocr-env --file environment.yml\nconda activate ocr-env\n```\nMain libraries (all required libraries are in [environment.yml](environment.yml)):\n* Numpy (1.13)\n* Tensorflow (1.4)\n* OpenCV (3.1)\n* Pandas (0.21)\n* Matplotlib (2.1)\n\n### Run\nWith all required libraries installed and cloned repo, run `jupyter notebook` in the directory of the project. Then you can work on the particular notebook.\n\n## Contributing\nBest way how to get involved is through creating [GitHub issues](https://github.com/Breta01/handwriting-ocr/issues) or solving one! If there aren't any issues you can contact me directly on email.\n\n## License\n**MIT**\n\n## Support the project\nIf this project helped you or you want to support quick answers to questions and issues. Or you just think it is an interesting project. Please consider a small donation.\n\n[![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif)](https://paypal.me/bretahajek/2)\n"
 },
 {
  "repo": "casparwylie/Perceptron",
  "language": "Python",
  "readme_contents": " NOTE: This is an incomplete project. I may decide to spend a lot more time on it if people persist giving it positive attention.\r\n\r\n# Perceptron\r\nA flexible artificial neural network builder to analysis performance, and optimise the best model. \r\n\r\nPerceptron is a software that will help researchers, students, and programmers to\r\ndesign, compare, and test artificial neural networks. As it stands, there are few visual\r\ntools that do this for free, and with simplicity.\r\n<b>This software is largely for educational purposes</b>, allowing people to experiment\r\nand understand how the different parameters within an ANN can result in\r\ndifferent performances and better results. I have not used libraries like TensorFlow specifally so people\r\ncan see what goes on lower level, all within the code. \r\n\r\n![screenshotnew](https://user-images.githubusercontent.com/7353547/27341298-c611ba96-55d4-11e7-9da9-9cfd6045ae5c.png)\r\n\r\n\r\n\r\n![untitled](https://cloud.githubusercontent.com/assets/7353547/25346609/effb5106-290f-11e7-8426-788a10fd4e2f.png)\r\n"
 },
 {
  "repo": "lixiaoshaxing/MultiMediaLearn",
  "language": "C",
  "readme_contents": "# MultiMediaLearn\n\u591a\u5a92\u4f53\u5b66\u4e60\uff1a\u56fe\u7247\u5904\u7406\uff0c\u97f3\u89c6\u9891\u5904\u7406\uff0c\u76f8\u673a\u4f7f\u7528\uff0cOpenGL\uff0cOpenSL\uff0cOpenCV\uff0cFFMpeg\u7b49\u5b66\u4e60\uff0c\u6240\u6709\u4ee3\u7801\u90fd\u4e0d\u5c01\u88c5\uff0c\u4e00\u822c\u7531\u4e00\u4e2a\u7c7b\u5b8c\u6210\u529f\u80fd\uff0c\u65b9\u4fbf\u5b66\u4e60\u3002\n+ \u56fe\u7247\u5904\u7406\n    - 1.SurfaceView\uff0cGLSurfaceView,TextureView\uff0cImageView\uff0c\u81ea\u5b9a\u4e49View\u6e32\u67d3Bitmap\n    - 2.Bitmap\u538b\u7f29\uff0c\u526a\u88c1\uff0cRenderscript\u6ee4\u955c\u5904\u7406(\u5f85)\n+ \u76f8\u673a\u4f7f\u7528\n    - 1.SurfaceView\uff0cGLSurfaceView\uff0cTextureView\u9884\u89c8Camera\uff0c\u62cd\u7167\n    - 2.SurfaceView\uff0cGLsurfaceView\uff0cTextureView\u540c\u65f6\u9884\u89c8Camera\uff0c\u5171\u4eab\u6570\u636e\n    - 3.\u591aGLSurfaceView\u5171\u4eabCamera\uff0c\u6a21\u4eff\u5c0f\u7c73\u7cfb\u7edf\u76f8\u673a\u4e5d\u4e2a\u6ee4\u955c\u540c\u65f6\u9884\u89c8\n+ \u97f3\u89c6\u9891\u5904\u7406\n    - 1.MediaPlayer\u7ed3\u5408SurfaceView\uff0cGLSurfaceView\uff0cTextureView\u64ad\u653e\u89c6\u9891\n    - 2.MediaPlayer\uff0cMediaRecord\u5f55\u5236\u89c6\u9891\n    - 3.AudioRecord\uff0cAudioTrack\u5f55\u5236\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\n    - 4.Fmod\u5904\u7406\u97f3\u9891\uff0c\u53d8\u58f0\n    - 5.MediaExtractor\uff0cMediaCodec\u63d0\u53d6\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\uff0c\u7ed3\u5408MediaMux\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\n    - 6.AudioRecoder\uff0cCamera\uff0cMediaCodec\u5355\u72ec\u5f55\u5236\u97f3\u9891\uff0c\u89c6\u9891\n    - 7.AudioRecoder\uff0cCamera\uff0cMediaCodec\uff0cAudioMux\u5f55\u5236\u89c6\u9891\n    - 8.\u5f55\u5236\u89c6\u9891\uff0c\u4f7f\u7528OpenGL\u5904\u7406\uff0c\u6dfb\u52a0\u6ee4\u955c\uff0c\u6c34\u5370\n    - 9.\u89c6\u9891\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\uff0c\u8f6cgif\uff0c\u65ad\u70b9\u5f55\u5236\uff08\u5f85\uff09\n    - 10.\u5f55\u5236\u89c6\u9891\uff0c\u4f7f\u7528FFmpeg\u589e\u52a0\u6ee4\u955c\uff0c\u6c34\u5370\uff0c\u526a\u88c1\uff08\u5f85\uff09\n+ OpenGL\u5b66\u4e60\n    - 1.\u51b0\u7403\u6e38\u620f\uff0c\u57fa\u672c\u56fe\u5f62\u7ed8\u5236\uff0c\u57fa\u672c\u77e9\u9635\u53d8\u6362\uff0c\u89e6\u63a7\n    - 2.\u7c92\u5b50\u55b7\u6cc9\uff0c\u70df\u706b\u7cfb\u7edf\uff0c\u5929\u7a7a\u76d2\uff0c\u52a8\u6001\u58c1\u7eb8\uff0c\u9640\u87ba\u4eea\uff0c\u9ad8\u5ea6\u56fe\u7b49\n    - 3.\u56fe\u7247\u5904\u7406\uff0cOpenGL\u4f7f\u7528glsl\u5904\u7406\u56fe\u7247\uff0c\u6539\u8272\u8c03\uff0c\u653e\u5927\uff0c\u865a\u5316\u7b49\n    - 4.OpenGL\u9ad8\u7ea7\u7279\u6027\uff1aVBO\uff0cPBO\uff0cFBO\u4f7f\u7528\n    - 5.Camera\u9884\u89c8\u754c\u9762\u89e3\u6790ETC\uff0c\u52a8\u753b\uff0c\u6ee4\u955c\uff0c\u7f8e\u989c\uff0c\u6c34\u5370\n    - 6.\u52a0\u8f7dSTL\u6a21\u578b\uff0c\u6e32\u67d33D\u6a21\u578b\n    - 7.\u52a0\u8f7dOBJ\u6a21\u578b\n    - 8.\u52a0\u8f7dOBJ-MTL\u6a21\u578b\uff0c\u5305\u542b\u6a21\u578b\uff0c\u7eb9\u7406\n    - 9.\u7f16\u8bd1assimp\uff0c\u52a0\u8f7d\u6a21\u578b\uff08\u5f85\uff09\n    - 10.\u52a0\u8f7d\u9aa8\u67b6\uff08\u5f85\uff09\n+ FFMpeg\u5b66\u4e60\n    - 1.\u672c\u5730\u89c6\u9891\u63a8\u6d41\uff0c\u4f7f\u7528libRtmp\uff0cffmpeg\u89e3\u7801\uff0clibYuv\u8f6c\u5411\n    - 2.\u76f4\u64ad\uff0c\u91c7\u96c6\u97f3\u9891\uff0c\u89c6\u9891\uff0c\u4f7f\u7528x264\uff0cfaac\u7f16\u7801\uff0clibrtmp\u63a8\u6d41\uff0clibYuv\u8f6c\u5411\n    - 3.\u89e3\u6790\u97f3\u9891\u4e3apcm\uff0cffmpeg\u89e3\u7801\uff0c\u672c\u5730\u65b9\u6cd5\u8c03\u7528java\u5c42AudioTrack\u64ad\u653e\u97f3\u9891\n    - 4.FFMpeg\u89e3\u7801\u672c\u5730\u89c6\u9891\uff0c\u4f7f\u7528\u961f\u5217\uff0c\u4fdd\u6301\u97f3\u89c6\u9891\u540c\u6b65\u64ad\u653e\n    - 5.OpenSL\u5f55\u5236\u97f3\u9891\uff0c\u89e3\u7801\u97f3\u9891\uff0c\u64ad\u653e\u97f3\u9891\n+ OpenCV\u5165\u95e8\n    - 1. \u4f7f\u7528OpenCV\u8fdb\u884c\u8fb9\u6846\u8bc6\u522b\uff0c\u5e76\u526a\u88c1Bitmap"
 },
 {
  "repo": "jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV",
  "language": "Jupyter Notebook",
  "readme_contents": "# Tutorial: Computer Vision and Machine Learning with Python, Keras and OpenCV\n\n<a href=\"https://tracking.gitads.io/?repo=Computer-Vision-Basics-with-Python-Keras-and-OpenCV\"><img src=\"https://images.gitads.io/Computer-Vision-Basics-with-Python-Keras-and-OpenCV\" alt=\"GitAds\"/></a>\n\n### Includes a demonstration of concepts with Gesture Recognition.\nThis was created as part of an educational for the [Western Founders Network](https://foundersnetwork.ca/) computer vision and machine learning educational session.\n\n## Demo\n\nThe final demo can be seen [here](https://www.youtube.com/watch?v=IJV11OGTNT8) and below:\n\n<a href=\"https://imgflip.com/gif/22n3o6\"><img src=\"https://i.imgflip.com/22n3o6.gif\"/></a>\n\n## Contents\n[notebook.ipynb](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/blob/master/notebook.ipynb) contains a full tutorial of basic computer vision and machine learning concepts, including:\n\n* *What computers see*\n* Image Filters and Functions\n  - Blurring\n  - Dilating\n  - Erosion\n  - Canny Edge Detectors\n  - Thresholding\n* Background Subtraction Techniques\n  - Using a background image to find differences\n  - Using motion based background subtraction algorithms\n* Contours\n  - Finding and sorting contours\n* Tracking\n* (Deep) Neural Networks \n* (Deep) Convolutional Neural Networks\n* Demo Project: Gesture Recognition\n  - Extracting the subject\n  - Tracking the hand\n  - Collecting data\n  - Building the Neural Network\n  - Preparing Data for Training\n  - Training the Network\n  - Plotting Model History\n  \n*Note: Please check the [issues](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/issues) on this repo if you're having problems with the notebook.*\n\n## Installation Instructions ('$' means run this in terminal/command prompt, do not type '$')\n### Windows:\n* Install Anaconda (https://www.anaconda.com/download/) or Miniconda (https://conda.io/miniconda.html) to save hard drive space\n* Start an Anaconda Prompt. (Search Anaconda in the start menu.)\n#### Option 1: Exact source package installs\n* Use the spec-file.txt provided, install identical packages\n\n        $ conda create -n [ENV_NAME] --file spec-file.txt # create new env with same packages\n    or, if you have an existing environment\n\n        $ conda install -n [ENV_NAME] --file spec-file.txt # install packages into an existing env\n* Then activate the environment\n\n        $ activate cv\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install package\n\n          $ pip install [file path]\n#### Option 2: Package installs\n* Using the environment.yml file provided, run\n\n        $ conda create -n cv --file environment.yml\n    or, if you have an existing environment\n\n        $ conda install -n [ENV_NAME] --file environment.yml # install packages into an existing env\n* Activate the environment\n\n        $ activate cv\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install the package\n\n          $ pip install [file path]\n#### Option 3: Manually installing packages\n* Create and activate a Python 3.5 conda environment called cv.\n\n        $ conda create -n cv python=3.5\n\n        $ activate cv\n* Install Numpy (http://www.numpy.org/)\n\n        $ conda install numpy\n* Install Matplotlib (https://matplotlib.org/)\n\n        $ conda install matplotlib\n* Install Keras (https://keras.io/) \n\n        $ conda install keras\n    - This should also install tensorflow\n* Install h5py (http://www.h5py.org/)\n\n        $ conda install h5py\n* Install OpenCV3 (https://opencv.org/)\n    - Download whl file https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n    - Download \u201copencv_python 3.4.0+contrib cp35 cp35m win32.whl\u201d or \u201copencv_python 3.4.0+contrib cp35 cp35m win_amd64.whl\u201d for 32bit and 64bit respectively\n    - Install package\n\n          $ pip install [file path]\n* Install Jupyter Notebook (http://jupyter.org/)\n\n        $ conda install jupyter notebook\n* Install IPython (https://ipython.org/)\n\n        $ conda install ipython\n        \n### Mac/Linux: Manually installing packages\n* Install Anaconda (https://www.anaconda.com/download/) or Miniconda (https://conda.io/miniconda.html) to save hard drive space\n#### Mac:\n* For Miniconda, open terminal and navigate to the directory you downloaded Miniconda3-latest-MacOSX-x86_64.sh to and run:\n\n        $ bash Miniconda3-latest-MacOSX-x86_64.sh\n\n* For Anaconda, double click the Anaconda3-5.0.1-MacOSX-x86_64.pkg file you downloaded\n\n#### Linux:\n* For Miniconda, open a terminal and navigate to the directory you downloaded Miniconda3-latest-Linux-x86_64.sh to and run:\n\n        $ bash Miniconda3-latest-Linux-x86_64.sh\n\n* For Anaconda, open a terminal and navigate to the directory you downloaded Anaconda3-5.0.1-Linux-x86_64.sh to and run:\n\n        $ bash Anaconda3-5.0.1-Linux-x86_64.sh\n\n#### Both:\n* Create and activate a Python 3.5 conda environment called cv.\n\n        $ conda create -n cv python=3.5\n\n        $ source activate cv\n* Install Numpy (http://www.numpy.org/)\n\n        $ conda install numpy\n* Install Matplotlib (https://matplotlib.org/)\n\n        $ conda install matplotlib\n* Install Keras (https://keras.io/) \n\n        $ conda install keras\n    - This should also install tensorflow\n* Install h5py (http://www.h5py.org/)\n\n        $ conda install h5py\n* Install Jupyter Notebook (http://jupyter.org/)\n\n        $ conda install jupyter notebook\n* Install IPython (https://ipython.org/)\n\n        $ conda install ipython\n* Install OpenCV3 (https://opencv.org/)\n        \n        $ conda install -c conda-forge opencv \n    \n    if the `import cv2` does not work with this install, try instead:\n    \n        $ conda install -c https://conda.anaconda.org/menpo opencv3\n"
 },
 {
  "repo": "niw/iphone_opencv_test",
  "language": "C++",
  "readme_contents": "Using OpenCV on iPhone\n----------------------\nThis source repository includes pre-compiled OpenCV library and headeres so that you can get started easily!\nMore documents you can see on [this article](http://niw.at/articles/2009/03/14/using-opencv-on-iphone/).\n\nBuilding Static Link Version of OpenCV\n--------------------------------------\nIf you want to build it from source code, you can do by next steps.\n\n1.  Building OpenCV requiers [CMake](http://www.cmake.org/).\n    You can easily install it by using [Homebrew](http://mxcl.github.com/homebrew/) or [MacPorts](http://www.macports.org/).\n\n        # Using Homebrew\n        % brew install cmake\n        # Using MacPorts\n        % sudo port install cmake\n\n2.  Clone this project from github.com, then move into the project directory\n\n        % git clone git://github.com/niw/iphone_opencv_test.git\n\n3.  Getting source code from sourceforge. I tested with [OpenCV-2.2.0.tar.bz2](http://sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.2/OpenCV-2.2.0.tar.bz2/download).\n\n4.  Extract downloaded archive on the top of demo project directory\n\n        % tar xjvf OpenCV-2.2.0.tar.bz2\n\n5.  Apply patch for iPhone SDK\n\n        % cd OpenCV-2.2.0\n        % patch -p1 < ../OpenCV-2.2.0.patch\n\n6.  Following next steps to build OpenCV static library for simulator.\n    All files are installed into ``opencv_simulator`` directory.\n    When running ``make`` command, you've better assign ``-j`` option and number according to number of your CPU cores.\n    Without ``-j`` option, it takes a long time.\n\n        % cd ..\n        % mkdir build_simulator\n        % cd build_simulator\n        % ../opencv_cmake.sh Simulator ../OpenCV-2.2.0\n        % make -j 4\n        % make install\n\n7.  Following next steps to build OpenCV static library for device\n    All files are installed into ``opencv_device`` directory.\n\n        % cd ..\n        % mkdir build_device\n        % cd build_device\n        % ../opencv_cmake.sh Device ../OpenCV-2.2.0\n        % make -j 4\n        % make install\n\nBuild support script\n--------------------\n\nuild support script ``opencv_cmake.sh`` has some options to build OpenCV with iOS SDK.\nTry ``--help`` option to get the all options of it.\n\nChange Log\n----------\n *  04/11/2011 - Supprot OpenCV 2.2.0 + iOS SDK 4.3 + XCode 4\n *  10/30/2010 - Support iOS SDK 4.1\n *  08/22/2010 - Support OpenCV 2.1.0 + iOS SDK 4.0\n *  12/21/2009 - Support Snow Leopard + iPhone SDK 3.1.2, Thank you Hyon!\n *  11/15/2009 - Support OpenCV to 2.0.0 + iPhone SDK 3.x\n *  03/14/2009 - Release this project with OpenCV 1.0.0 + iPhone SDK 2.x\n"
 },
 {
  "repo": "WL-Amigo/waifu2x-converter-cpp",
  "language": "C++",
  "readme_contents": "# waifu2x (converter only version)\n\nThis is a reimplementation of waifu2x ([original](https://github.com/nagadomi/waifu2x)) converter function, in C++, using OpenCV.\nThis is also a reimplementation of [waifu2x python version](https://marcan.st/transf/waifu2x.py) by [Hector Martin](https://marcan.st/blog/).\nYou can use this as command-line tool of image noise reduction or/and scaling.\n\n\n## Prebuilt binary-form release\n\nPlease see [releases](https://github.com/WL-Amigo/waifu2x-converter-cpp/releases) of this repository.\nThere is only for-windows binary, now. Sorry.\n\n### works using waifu2x-converter\n\n * [waifu2x_win_koroshell](http://inatsuka.com/extra/koroshell/)\n   - waifu2x-converter GUI frontend that is easy to use, and so cute. You need only drag&drop to convert your image. (and also you can set converting mode, noise reduction level, scale ratio, etc..., on GUI)\n   - Both waifu2x-converter x86 and x64 are included this package, and GUI see your windows architecture(x86|x64) and selects automatically which to use. \n   - For windows only.\n\n\n## Dependencies\n\n### Platform\n\n * Ubuntu\n * Mac OS X?\n * Windows\n \n(This program probably can be built under MacOSX, because OpenCV and other libraries support OS X)\n\n### Libraries\n\n * [OpenCV](http://opencv.org/)(C++, version 3.0.0 rc1)\n\nThis programs also depends on libraries shown below, but these are already included in this repository.\n*CUDA Support in OpenCV is optional, since not required. (in version 1.0.0, CUDA Support is not used.)*\n\n * [picojson](https://github.com/kazuho/picojson)\n * [TCLAP(Templatized C++ Command Line Parser Library)](http://tclap.sourceforge.net/)\n\n## How to build\n\n### for Ubuntu\n\nSorry, under construction...\n\nThese are hints for building :\n\n * I recommend to install OpenCV from sources. (build instruction is found [here](http://opencv.org/quickstart.html))\n * include path : `include/` `(/path/to/opencv/installed/directory)/include`\n * library path : `(/path/to/opencv/installed/directory)/lib` \n     - if you have built and installed OpenCV from source, and have changed install directory(by using `CMAKE_INSTALL_PREFIX`), you may need to set environment variable `LD_LIBRARY_PATH` for your OpenCV installed directory.\n * libraries to link : `opencv_core` `opencv_imgproc` `opencv_imgcodecs` `opencv_features2d`\n * standard of C++ : `c++11`\n\n\n\n## Usage\n\nUsage of this program can be seen by executing this with `--help` option.\n\n\n\n(My native language is not English, then I'm sorry for my broken English.)\n"
 },
 {
  "repo": "jerry1900/faceRecognition",
  "language": "Python",
  "readme_contents": "# faceRecognition\n\u5229\u7528OpenCV\u3001CNN\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\n\u4e07\u58d1\uff0c497899960@qq.com\n"
 },
 {
  "repo": "Laex/Delphi-OpenCV",
  "language": "Pascal",
  "readme_contents": "# Delphi-OpenCV\n* OpenCV version - 2.4.13<br>\n* Development environment - Delphi 2010-10.3, FPC 3.0.4<br>\n\n<b>Contributors:</b>\n\n Laentir Valetov (email: laex@bk.ru)<br>\n Mikhail Grigorev (email: sleuthhound@gmail.com)\n\n## Requirements:\n* Visual C++ Redistributable for Visual Studio 2015<br>\nFiles: msvcp140.dll, msvcp140d.dll in \"Delphi-OpenCV\\redist\\\" or [here, but it is not exactly][2]<br>\nor from the repository (Delphi-OpenCV\\redist\\VC14):\n```\n(1) 32-bit in the \"\\x86\"\n(2) 64-bit in the \"\\x64\"\n```\n* Shared library FFMPEG 4.2.1 for Windows can be downloaded from [here][5]<br>\nor from the repository (Delphi-OpenCV\\redist\\ffmpeg):\n```\n(3) FFmpeg 32-bit Shared \"\\x86\"\n(4) FFmpeg 64-bit Shared \"\\x64\"\n```\n* Dynamic library OpenCV need to download [here][4]<br>\nFiles: *2413.dll and *2413d.dll<br>\nAfter installing OpenCV:\n```\n(5) 32-bit in the C:\\OpenCV\\build\\x86\\vc14\\bin\\*.dll\n(6) 64-bit in the C:\\OpenCV\\build\\x64\\vc14\\bin\\*.dll\n```\n* Some examples (FFMPEG) required [SDL 2.0 and SDL 1.2][3]<br>\nor from the repository (Delphi-OpenCV\\redist\\SDL\\1.2 and \\2.0):\n```\n(7) 32-bit - SDL.dll and SDL2.dll \"\\x86\"\n(8) 64-bit - SDL.dll and SDL2.dll \"\\x64\"\n```\n# How to install:\n## 1. Delphi environment setting\nDownload and unzip the [archive][1] or clone repository<br>\n```\ngit clone https://github.com/Laex/Delphi-OpenCV.git\n```\nGet the following directory structure<br>\n```\n<PROJECT_ROOT> - Directory, for example, \"C:\\Delphi\\OpenCV\\\"\n\t<bin>\t\t- here are the executable files of the examples\n\t<CheckCVDep>\t- program for checking the environment\n\t<Delphi-FFMPEG>\t- empty directory for \"Delphi-FFMPEG\"\n\t<packages>\t- packages for D10-D10.3\n\t<redist>\t- redistributable packages\n\t<resource>\t- media for working examples and Haar-cascades\n\t<samples>\t- examples\n\t<source>\t- object pascal sources for OpenCV, SDL, OpenGL\n\t<source3>\t- attempt for opencv 3\n\t<source4>\t- blank for OpenCV 4\n```\nRun <b>InitDelphiFFMPEG.cmd</b> to initialize the <b>Delphi-FFMPEG</b> submodule. The <b><Delphi-FFMPEG></b> directory should be populated with sources for <b>Delphi-FFMPEG</b>.\nIf it didn\ufffdt work, then\n```\ngit clone https://github.com/Laex/Delphi-FFMPEG.git\n```\nAdd the search path for the modules of the project in Delphi IDE<br>\n\"Tools-Options-Delphi Options-Library-Library path\" or \"Tools-Options-Language-Delphi-Library\"\n```\n<PROJECT_ROOT>\\source\n<PROJECT_ROOT>\\source\\opengl\n<PROJECT_ROOT>\\source\\sdl\n<PROJECT_ROOT>\\source\\sdl2\n<PROJECT_ROOT>\\packages\n<PROJECT_ROOT>\\resource\n<PROJECT_ROOT>\\resource\\facedetectxml\n<PROJECT_ROOT>\\source3\n<PROJECT_ROOT>\\Delphi-FFMPEG\\source\n```\nwhere ```<PROJECT_ROOT>``` directory, which was unzipped (or cloned) repository.<br>\n## 2. Copy dynamic libraries files\n<b>OS Windows 64-bit</b><br>\n```\nTarget platform 64-bit: (2),(4),(6),(8) -> \"C:\\Windows\\System32\\\"\nTarget platform 32-bit: (1),(3),(5),(7) -> \"C:\\Windows\\SysWOW64\\\"\n```\n<b>OS Windows 32-bit</b><br>\n```\nTarget platform 32-bit: (1),(3),(5),(7) -> \"C:\\Windows\\System32\\\"\n```\n<b>Alternatively</b>, dynamic libraries can be placed next to an executable file.\n## 3. Verify that the environment is configured correctly\nRun from the repository\n```\n Delphi-OpenCV/CheckCVDep/CheckCVDep.exe\n```\nThe program checks the availability of dynamic libraries\n```\n------- Verifying Microsoft DLL -------\nOK\n------- OpenCV DLL -------\nOK\n------- Delphi-OpenCV classes DLL -------\nOK\n------- FFMPEG DLL -------\nOK\n------- SDL DLL -------\nOK\n```\nTo successfully install components and run most of the examples, the availability of FFMPEG DLL, Microsoft DLL and OpenCV DLL is sufficient\n## 4. Install the components\n\nTo install the components, open and install\n```\n<PROJECT_ROOT>\\packages\\Delphi XXX\\rtpFFMPEG.dpk\n<PROJECT_ROOT>\\packages\\Delphi XXX\\rclVCLOpenCV.dpk\n<PROJECT_ROOT>\\packages\\Delphi XXX\\rclFMXOpenCV.dpk\n<PROJECT_ROOT>\\packages\\Delphi XXX\\dclVCLOpenCV.dpk\n<PROJECT_ROOT>\\packages\\Delphi XXX\\dclFMXOpenCV.dpk\n```\n## Examples\nOpen in Delphi IDE and compile:<br>\nExamples of the use of certain functions and procedures \n```\n<PROJECT_ROOT>\\samples\\LibDemo\\LibDemo.groupproj\n```\nExamples of the use of video processing algorithms\n```\n<PROJECT_ROOT>\\samples\\MultiDemo\\MultiDemo.groupproj\n```\nExamples of the use of video processing algorithms using VCL.Forms\n```\n<PROJECT_ROOT>\\samples\\VCLDemo\\VCLDemo.groupproj\n```\nExamples of using FFMPEG library header files are in the\n```\n<PROJECT_ROOT>\\Delphi-FFMPEG\\examples\n```\nExamples of use of components\n```\n<PROJECT_ROOT>\\samples\\Components\\ComponentsDemo.groupproj\n```\n<b>Donate</b><br>\n<a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=5Z5JQ7C9JCJQN\">PayPal USD</a><br>\n<a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WQYST8J8PR4K2\">PayPal EUR</a><br>\n<a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=XN8D6TJMSXPFL\">PayPal RUB</a><br>\n<a href=\"https://money.yandex.ru/to/410011600173245\">Yandex Money</a><br>\n[![Donatecoins](http://donatecoins.org/btc/3MTXVtRgQnA22EtBxP97Nws6GS8autp38s.svg)](http://donatecoins.org/btc/3MTXVtRgQnA22EtBxP97Nws6GS8autp38s)\n\n\n[1]: https://github.com/Laex/Delphi-OpenCV/archive/master.zip\n[2]: https://www.microsoft.com/en-us/download/details.aspx?id=48145\n[3]: https://www.libsdl.org/index.php\n[4]: https://github.com/opencv/opencv/releases/tag/2.4.13.6\n[5]: http://ffmpeg.zeranoe.com/builds/\n"
 },
 {
  "repo": "informramiz/opencv-face-recognition-python",
  "language": "HTML",
  "readme_contents": "\n# Face Recognition with OpenCV and Python\n\n## Introduction\n\nWhat is face recognition? Or what is recognition? When you look at an apple fruit, your mind immediately tells you that this is an apple fruit. This process, your mind telling you that this is an apple fruit is recognition in simple words. So what is face recognition then? I am sure you have guessed it right. When you look at your friend walking down the street or a picture of him, you recognize that he is your friend Paulo. Interestingly when you look at your friend or a picture of him you look at his face first before looking at anything else. Ever wondered why you do that? This is so that you can recognize him by looking at his face. Well, this is you doing face recognition. \n\nBut the real question is how does face recognition works? It is quite simple and intuitive. Take a real life example, when you meet someone first time in your life you don't recognize him, right? While he talks or shakes hands with you, you look at his face, eyes, nose, mouth, color and overall look. This is your mind learning or training for the face recognition of that person by gathering face data. Then he tells you that his name is Paulo. At this point your mind knows that the face data it just learned belongs to Paulo. Now your mind is trained and ready to do face recognition on Paulo's face. Next time when you will see Paulo or his face in a picture you will immediately recognize him. This is how face recognition work. The more you will meet Paulo, the more data your mind will collect about Paulo and especially his face and the better you will become at recognizing him. \n\nNow the next question is how to code face recognition with OpenCV, after all this is the only reason why you are reading this article, right? OK then. You might say that our mind can do these things easily but to actually code them into a computer is difficult? Don't worry, it is not. Thanks to OpenCV, coding face recognition is as easier as it feels. The coding steps for face recognition are same as we discussed it in real life example above.\n\n- **Training Data Gathering:** Gather face data (face images in this case) of the persons you want to recognize\n- **Training of Recognizer:** Feed that face data (and respective names of each face) to the face recognizer so that it can learn.\n- **Recognition:** Feed new faces of the persons and see if the face recognizer you just trained recognizes them.\n\nOpenCV comes equipped with built in face recognizer, all you have to do is feed it the face data. It's that simple and this how it will look once we are done coding it.\n\n![visualization](output/tom-shahrukh.png)\n\n## OpenCV Face Recognizers\n\nOpenCV has three built in face recognizers and thanks to OpenCV's clean coding, you can use any of them by just changing a single line of code. Below are the names of those face recognizers and their OpenCV calls. \n\n1. EigenFaces Face Recognizer Recognizer - `cv2.face.createEigenFaceRecognizer()`\n2. FisherFaces Face Recognizer Recognizer - `cv2.face.createFisherFaceRecognizer()`\n3. Local Binary Patterns Histograms (LBPH) Face Recognizer - `cv2.face.createLBPHFaceRecognizer()`\n\nWe have got three face recognizers but do you know which one to use and when? Or which one is better? I guess not. So why not go through a brief summary of each, what you say? I am assuming you said yes :) So let's dive into the theory of each. \n\n### EigenFaces Face Recognizer\n\nThis algorithm considers the fact that not all parts of a face are equally important and equally useful. When you look at some one you recognize him/her by his distinct features like eyes, nose, cheeks, forehead and how they vary with respect to each other. So you are actually focusing on the areas of maximum change (mathematically speaking, this change is variance) of the face. For example, from eyes to nose there is a significant change and same is the case from nose to mouth. When you look at multiple faces you compare them by looking at these parts of the faces because these parts are the most useful and important components of a face. Important because they catch the maximum change among faces, change the helps you differentiate one face from the other. This is exactly how EigenFaces face recognizer works.  \n\nEigenFaces face recognizer looks at all the training images of all the persons as a whole and try to extract the components which are important and useful (the components that catch the maximum variance/change) and discards the rest of the components. This way it not only extracts the important components from the training data but also saves memory by discarding the less important components. These important components it extracts are called **principal components**. Below is an image showing the principal components extracted from a list of faces.\n\n**Principal Components**\n![eigenfaces_opencv](visualization/eigenfaces_opencv.png)\n**[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**\n\nYou can see that principal components actually represent faces and these faces are called **eigen faces** and hence the name of the algorithm. \n\nSo this is how EigenFaces face recognizer trains itself (by extracting principal components). Remember, it also keeps a record of which principal component belongs to which person. One thing to note in above image is that **Eigenfaces algorithm also considers illumination as an important component**. \n\nLater during recognition, when you feed a new image to the algorithm, it repeats the same process on that image as well. It extracts the principal component from that new image and compares that component with the list of components it stored during training and finds the component with the best match and returns the person label associated with that best match component. \n\nEasy peasy, right? Next one is easier than this one. \n\n### FisherFaces Face Recognizer \n\nThis algorithm is an improved version of EigenFaces face recognizer. Eigenfaces face recognizer looks at all the training faces of all the persons at once and finds principal components from all of them combined. By capturing principal components from all the of them combined you are not focusing on the features that discriminate one person from the other but the features that represent all the persons in the training data as a whole.\n\nThis approach has drawbacks, for example, **images with sharp changes (like light changes which is not a useful feature at all) may dominate the rest of the images** and you may end up with features that are from external source like light and are not useful for discrimination at all. In the end, your principal components will represent light changes and not the actual face features. \n\nFisherfaces algorithm, instead of extracting useful features that represent all the faces of all the persons, it extracts useful features that discriminate one person from the others. This way features of one person do not dominate over the others and you have the features that discriminate one person from the others. \n\nBelow is an image of features extracted using Fisherfaces algorithm.\n\n**Fisher Faces**\n![eigenfaces_opencv](visualization/fisherfaces_opencv.png)\n**[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**\n\nYou can see that features extracted actually represent faces and these faces are called **fisher faces** and hence the name of the algorithm. \n\nOne thing to note here is that **even in Fisherfaces algorithm if multiple persons have images with sharp changes due to external sources like light they will dominate over other features and affect recognition accuracy**. \n\nGetting bored with this theory? Don't worry, only one face recognizer is left and then we will dive deep into the coding part. \n\n### Local Binary Patterns Histograms (LBPH) Face Recognizer \n\nI wrote a detailed explaination on Local Binary Patterns Histograms in my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/) using local binary patterns histograms. So here I will just give a brief overview of how it works.\n\nWe know that Eigenfaces and Fisherfaces are both affected by light and in real life we can't guarantee perfect light conditions. LBPH face recognizer is an improvement to overcome this drawback.\n\nIdea is to not look at the image as a whole instead find the local features of an image. LBPH alogrithm try to find the local structure of an image and it does that by comparing each pixel with its neighboring pixels. \n\nTake a 3x3 window and move it one image, at each move (each local part of an image), compare the pixel at the center with its neighbor pixels. The neighbors with intensity value less than or equal to center pixel are denoted by 1 and others by 0. Then you read these 0/1 values under 3x3 window in a clockwise order and you will have a binary pattern like 11100011 and this pattern is local to some area of the image. You do this on whole image and you will have a list of local binary patterns. \n\n**LBP Labeling**\n![LBP labeling](visualization/lbp-labeling.png)\n\nNow you get why this algorithm has Local Binary Patterns in its name? Because you get a list of local binary patterns. Now you may be wondering, what about the histogram part of the LBPH? Well after you get a list of local binary patterns, you convert each binary pattern into a decimal number (as shown in above image) and then you make a [histogram](https://www.mathsisfun.com/data/histograms.html) of all of those values. A sample histogram looks like this. \n\n**Sample Histogram**\n![LBP labeling](visualization/histogram.png)\n\n\nI guess this answers the question about histogram part. So in the end you will have **one histogram for each face** image in the training data set. That means if there were 100 images in training data set then LBPH will extract 100 histograms after training and store them for later recognition. Remember, **algorithm also keeps track of which histogram belongs to which person**.\n\nLater during recognition, when you will feed a new image to the recognizer for recognition it will generate a histogram for that new image, compare that histogram with the histograms it already has, find the best match histogram and return the person label associated with that best match histogram. \n<br><br>\nBelow is a list of faces and their respective local binary patterns images. You can see that the LBP images are not affected by changes in light conditions.\n\n**LBP Faces**\n![LBP faces](visualization/lbph-faces.jpg)\n**[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**\n\n\nThe theory part is over and now comes the coding part! Ready to dive into coding? Let's get into it then. \n\n# Coding Face Recognition with OpenCV\n\nThe Face Recognition process in this tutorial is divided into three steps.\n\n1. **Prepare training data:** In this step we will read training images for each person/subject along with their labels, detect faces from each image and assign each detected face an integer label of the person it belongs to.\n2. **Train Face Recognizer:** In this step we will train OpenCV's LBPH face recognizer by feeding it the data we prepared in step 1.\n3. **Testing:** In this step we will pass some test images to face recognizer and see if it predicts them correctly.\n\n**[There should be a visualization diagram for above steps here]**\n\nTo detect faces, I will use the code from my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/). So if you have not read it, I encourage you to do so to understand how face detection works and its Python coding. \n\n### Import Required Modules\n\nBefore starting the actual coding we need to import the required modules for coding. So let's import them first. \n\n- **cv2:** is _OpenCV_ module for Python which we will use for face detection and face recognition.\n- **os:** We will use this Python module to read our training directories and file names.\n- **numpy:** We will use this module to convert Python lists to numpy arrays as OpenCV face recognizers accept numpy arrays.\n\n\n```python\n#import OpenCV module\nimport cv2\n#import os module for reading training data directories and paths\nimport os\n#import numpy to convert python lists to numpy arrays as \n#it is needed by OpenCV face recognizers\nimport numpy as np\n\n#matplotlib for display our images\nimport matplotlib.pyplot as plt\n%matplotlib inline \n```\n\n### Training Data\n\nThe more images used in training the better. Normally a lot of images are used for training a face recognizer so that it can learn different looks of the same person, for example with glasses, without glasses, laughing, sad, happy, crying, with beard, without beard etc. To keep our tutorial simple we are going to use only 12 images for each person. \n\nSo our training data consists of total 2 persons with 12 images of each person. All training data is inside _`training-data`_ folder. _`training-data`_ folder contains one folder for each person and **each folder is named with format `sLabel (e.g. s1, s2)` where label is actually the integer label assigned to that person**. For example folder named s1 means that this folder contains images for person 1. The directory structure tree for training data is as follows:\n\n```\ntraining-data\n|-------------- s1\n|               |-- 1.jpg\n|               |-- ...\n|               |-- 12.jpg\n|-------------- s2\n|               |-- 1.jpg\n|               |-- ...\n|               |-- 12.jpg\n```\n\nThe _`test-data`_ folder contains images that we will use to test our face recognizer after it has been successfully trained.\n\nAs OpenCV face recognizer accepts labels as integers so we need to define a mapping between integer labels and persons actual names so below I am defining a mapping of persons integer labels and their respective names. \n\n**Note:** As we have not assigned `label 0` to any person so **the mapping for label 0 is empty**. \n\n\n```python\n#there is no label 0 in our training data so subject name for index/label 0 is empty\nsubjects = [\"\", \"Tom Cruise\", \"Shahrukh Khan\"]\n```\n\n### Prepare training data\n\nYou may be wondering why data preparation, right? Well, OpenCV face recognizer accepts data in a specific format. It accepts two vectors, one vector is of faces of all the persons and the second vector is of integer labels for each face so that when processing a face the face recognizer knows which person that particular face belongs too. \n\nFor example, if we had 2 persons and 2 images for each person. \n\n```\nPERSON-1    PERSON-2   \n\nimg1        img1         \nimg2        img2\n```\n\nThen the prepare data step will produce following face and label vectors.\n\n```\nFACES                        LABELS\n\nperson1_img1_face              1\nperson1_img2_face              1\nperson2_img1_face              2\nperson2_img2_face              2\n```\n\n\nPreparing data step can be further divided into following sub-steps.\n\n1. Read all the folder names of subjects/persons provided in training data folder. So for example, in this tutorial we have folder names: `s1, s2`. \n2. For each subject, extract label number. **Do you remember that our folders have a special naming convention?** Folder names follow the format `sLabel` where `Label` is an integer representing the label we have assigned to that subject. So for example, folder name `s1` means that the subject has label 1, s2 means subject label is 2 and so on. The label extracted in this step is assigned to each face detected in the next step. \n3. Read all the images of the subject, detect face from each image.\n4. Add each face to faces vector with corresponding subject label (extracted in above step) added to labels vector. \n\n**[There should be a visualization for above steps here]**\n\nDid you read my last article on [face detection](https://www.superdatascience.com/opencv-face-detection/)? No? Then you better do so right now because to detect faces, I am going to use the code from my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/). So if you have not read it, I encourage you to do so to understand how face detection works and its coding. Below is the same code.\n\n\n```python\n#function to detect face using OpenCV\ndef detect_face(img):\n    #convert the test image to gray image as opencv face detector expects gray images\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    #load OpenCV face detector, I am using LBP which is fast\n    #there is also a more accurate but slow Haar classifier\n    face_cascade = cv2.CascadeClassifier('opencv-files/lbpcascade_frontalface.xml')\n\n    #let's detect multiscale (some images may be closer to camera than others) images\n    #result is a list of faces\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);\n    \n    #if no faces are detected then return original img\n    if (len(faces) == 0):\n        return None, None\n    \n    #under the assumption that there will be only one face,\n    #extract the face area\n    (x, y, w, h) = faces[0]\n    \n    #return only the face part of the image\n    return gray[y:y+w, x:x+h], faces[0]\n```\n\nI am using OpenCV's **LBP face detector**. On _line 4_, I convert the image to grayscale because most operations in OpenCV are performed in gray scale, then on _line 8_ I load LBP face detector using `cv2.CascadeClassifier` class. After that on _line 12_ I use `cv2.CascadeClassifier` class' `detectMultiScale` method to detect all the faces in the image. on _line 20_, from detected faces I only pick the first face because in one image there will be only one face (under the assumption that there will be only one prominent face). As faces returned by `detectMultiScale` method are actually rectangles (x, y, width, height) and not actual faces images so we have to extract face image area from the main image. So on _line 23_ I extract face area from gray image and return both the face image area and face rectangle.\n\nNow you have got a face detector and you know the 4 steps to prepare the data, so are you ready to code the prepare data step? Yes? So let's do it. \n\n\n```python\n#this function will read all persons' training images, detect face from each image\n#and will return two lists of exactly same size, one list \n# of faces and another list of labels for each face\ndef prepare_training_data(data_folder_path):\n    \n    #------STEP-1--------\n    #get the directories (one directory for each subject) in data folder\n    dirs = os.listdir(data_folder_path)\n    \n    #list to hold all subject faces\n    faces = []\n    #list to hold labels for all subjects\n    labels = []\n    \n    #let's go through each directory and read images within it\n    for dir_name in dirs:\n        \n        #our subject directories start with letter 's' so\n        #ignore any non-relevant directories if any\n        if not dir_name.startswith(\"s\"):\n            continue;\n            \n        #------STEP-2--------\n        #extract label number of subject from dir_name\n        #format of dir name = slabel\n        #, so removing letter 's' from dir_name will give us label\n        label = int(dir_name.replace(\"s\", \"\"))\n        \n        #build path of directory containin images for current subject subject\n        #sample subject_dir_path = \"training-data/s1\"\n        subject_dir_path = data_folder_path + \"/\" + dir_name\n        \n        #get the images names that are inside the given subject directory\n        subject_images_names = os.listdir(subject_dir_path)\n        \n        #------STEP-3--------\n        #go through each image name, read image, \n        #detect face and add face to list of faces\n        for image_name in subject_images_names:\n            \n            #ignore system files like .DS_Store\n            if image_name.startswith(\".\"):\n                continue;\n            \n            #build image path\n            #sample image path = training-data/s1/1.pgm\n            image_path = subject_dir_path + \"/\" + image_name\n\n            #read image\n            image = cv2.imread(image_path)\n            \n            #display an image window to show the image \n            cv2.imshow(\"Training on image...\", image)\n            cv2.waitKey(100)\n            \n            #detect face\n            face, rect = detect_face(image)\n            \n            #------STEP-4--------\n            #for the purpose of this tutorial\n            #we will ignore faces that are not detected\n            if face is not None:\n                #add face to list of faces\n                faces.append(face)\n                #add label for this face\n                labels.append(label)\n            \n    cv2.destroyAllWindows()\n    cv2.waitKey(1)\n    cv2.destroyAllWindows()\n    \n    return faces, labels\n```\n\nI have defined a function that takes the path, where training subjects' folders are stored, as parameter. This function follows the same 4 prepare data substeps mentioned above. \n\n**(step-1)** On _line 8_ I am using `os.listdir` method to read names of all folders stored on path passed to function as parameter. On _line 10-13_ I am defining labels and faces vectors. \n\n**(step-2)** After that I traverse through all subjects' folder names and from each subject's folder name on _line 27_ I am extracting the label information. As folder names follow the `sLabel` naming convention so removing the  letter `s` from folder name will give us the label assigned to that subject. \n\n**(step-3)** On _line 34_, I read all the images names of of the current subject being traversed and on _line 39-66_ I traverse those images one by one. On _line 53-54_ I am using OpenCV's `imshow(window_title, image)` along with OpenCV's `waitKey(interval)` method to display the current image being traveresed. The `waitKey(interval)` method pauses the code flow for the given interval (milliseconds), I am using it with 100ms interval so that we can view the image window for 100ms. On _line 57_, I detect face from the current image being traversed. \n\n**(step-4)** On _line 62-66_, I add the detected face and label to their respective vectors.\n\nBut a function can't do anything unless we call it on some data that it has to prepare, right? Don't worry, I have got data of two beautiful and famous celebrities. I am sure you will recognize them!\n\n![training-data](visualization/tom-shahrukh.png)\n\nLet's call this function on images of these beautiful celebrities to prepare data for training of our Face Recognizer. Below is a simple code to do that.\n\n\n```python\n#let's first prepare our training data\n#data will be in two lists of same size\n#one list will contain all the faces\n#and other list will contain respective labels for each face\nprint(\"Preparing data...\")\nfaces, labels = prepare_training_data(\"training-data\")\nprint(\"Data prepared\")\n\n#print total faces and labels\nprint(\"Total faces: \", len(faces))\nprint(\"Total labels: \", len(labels))\n```\n\n    Preparing data...\n    Data prepared\n    Total faces:  23\n    Total labels:  23\n\n\nThis was probably the boring part, right? Don't worry, the fun stuff is coming up next. It's time to train our own face recognizer so that once trained it can recognize new faces of the persons it was trained on. Read? Ok then let's train our face recognizer. \n\n### Train Face Recognizer\n\nAs we know, OpenCV comes equipped with three face recognizers.\n\n1. EigenFace Recognizer: This can be created with `cv2.face.createEigenFaceRecognizer()`\n2. FisherFace Recognizer: This can be created with `cv2.face.createFisherFaceRecognizer()`\n3. Local Binary Patterns Histogram (LBPH): This can be created with `cv2.face.LBPHFisherFaceRecognizer()`\n\nI am going to use LBPH face recognizer but you can use any face recognizer of your choice. No matter which of the OpenCV's face recognizer you use the code will remain the same. You just have to change one line, the face recognizer initialization line given below. \n\n\n```python\n#create our LBPH face recognizer \nface_recognizer = cv2.face.createLBPHFaceRecognizer()\n\n#or use EigenFaceRecognizer by replacing above line with \n#face_recognizer = cv2.face.createEigenFaceRecognizer()\n\n#or use FisherFaceRecognizer by replacing above line with \n#face_recognizer = cv2.face.createFisherFaceRecognizer()\n```\n\nNow that we have initialized our face recognizer and we also have prepared our training data, it's time to train the face recognizer. We will do that by calling the `train(faces-vector, labels-vector)` method of face recognizer. \n\n\n```python\n#train our face recognizer of our training faces\nface_recognizer.train(faces, np.array(labels))\n```\n\n**Did you notice** that instead of passing `labels` vector directly to face recognizer I am first converting it to **numpy** array? This is because OpenCV expects labels vector to be a `numpy` array. \n\nStill not satisfied? Want to see some action? Next step is the real action, I promise! \n\n### Prediction\n\nNow comes my favorite part, the prediction part. This is where we actually get to see if our algorithm is actually recognizing our trained subjects's faces or not. We will take two test images of our celeberities, detect faces from each of them and then pass those faces to our trained face recognizer to see if it recognizes them. \n\nBelow are some utility functions that we will use for drawing bounding box (rectangle) around face and putting celeberity name near the face bounding box. \n\n\n```python\n#function to draw rectangle on image \n#according to given (x, y) coordinates and \n#given width and heigh\ndef draw_rectangle(img, rect):\n    (x, y, w, h) = rect\n    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n    \n#function to draw text on give image starting from\n#passed (x, y) coordinates. \ndef draw_text(img, text, x, y):\n    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n```\n\nFirst function `draw_rectangle` draws a rectangle on image based on passed rectangle coordinates. It uses OpenCV's built in function `cv2.rectangle(img, topLeftPoint, bottomRightPoint, rgbColor, lineWidth)` to draw rectangle. We will use it to draw a rectangle around the face detected in test image.\n\nSecond function `draw_text` uses OpenCV's built in function `cv2.putText(img, text, startPoint, font, fontSize, rgbColor, lineWidth)` to draw text on image. \n\nNow that we have the drawing functions, we just need to call the face recognizer's `predict(face)` method to test our face recognizer on test images. Following function does the prediction for us.\n\n\n```python\n#this function recognizes the person in image passed\n#and draws a rectangle around detected face with name of the \n#subject\ndef predict(test_img):\n    #make a copy of the image as we don't want to chang original image\n    img = test_img.copy()\n    #detect face from the image\n    face, rect = detect_face(img)\n\n    #predict the image using our face recognizer \n    label= face_recognizer.predict(face)\n    #get name of respective label returned by face recognizer\n    label_text = subjects[label]\n    \n    #draw a rectangle around face detected\n    draw_rectangle(img, rect)\n    #draw name of predicted person\n    draw_text(img, label_text, rect[0], rect[1]-5)\n    \n    return img\n```\n\n* **line-6** read the test image\n* **line-7** detect face from test image\n* **line-11** recognize the face by calling face recognizer's `predict(face)` method. This method will return a lable\n* **line-12** get the name associated with the label\n* **line-16** draw rectangle around the detected face\n* **line-18** draw name of predicted subject above face rectangle\n\nNow that we have the prediction function well defined, next step is to actually call this function on our test images and display those test images to see if our face recognizer correctly recognized them. So let's do it. This is what we have been waiting for. \n\n\n```python\nprint(\"Predicting images...\")\n\n#load test images\ntest_img1 = cv2.imread(\"test-data/test1.jpg\")\ntest_img2 = cv2.imread(\"test-data/test2.jpg\")\n\n#perform a prediction\npredicted_img1 = predict(test_img1)\npredicted_img2 = predict(test_img2)\nprint(\"Prediction complete\")\n\n#create a figure of 2 plots (one for each test image)\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n#display test image1 result\nax1.imshow(cv2.cvtColor(predicted_img1, cv2.COLOR_BGR2RGB))\n\n#display test image2 result\nax2.imshow(cv2.cvtColor(predicted_img2, cv2.COLOR_BGR2RGB))\n\n#display both images\ncv2.imshow(\"Tom cruise test\", predicted_img1)\ncv2.imshow(\"Shahrukh Khan test\", predicted_img2)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\ncv2.waitKey(1)\ncv2.destroyAllWindows()\n```\n\n    Predicting images...\n    Prediction complete\n\n\n\n![png](output_43_1.png)\n\n\nwohooo! Is'nt it beautiful? Indeed, it is! \n\n## End Notes\n\nFace Recognition is a fascinating idea to work on and OpenCV has made it extremely simple and easy for us to code it. It just takes a few lines of code to have a fully working face recognition application and we can switch between all three face recognizers with a single line of code change. It's that simple. \n\nAlthough EigenFaces, FisherFaces and LBPH face recognizers are good but there are even better ways to perform face recognition like using Histogram of Oriented Gradients (HOGs) and Neural Networks. So the more advanced face recognition algorithms are now a days implemented using a combination of OpenCV and Machine learning. I have plans to write some articles on those more advanced methods as well, so stay tuned! \n\n\n```python\n\n```\n"
 },
 {
  "repo": "sourishg/stereo-calibration",
  "language": "C++",
  "readme_contents": "## OpenCV C++ Stereo Camera Calibration\n\nThis repository contains some sources to calibrate the intrinsics of individual cameras and also the extrinsics of a stereo pair.\n\n### Dependencies\n\n- OpenCV\n- popt\n\n### Compilation\n\nCompile all the files using the following commands.\n\n```bash\nmkdir build && cd build\ncmake ..\nmake\n```\n\nMake sure your are in the `build` folder to run the executables.\n\n### Get images from webcams\n\nThis is a small helper tool to grab frames from two webcams operating as a stereo pair. Run the following command to use it.\n\n```bash\n./read -w [img_width] -h [img_height] -d [imgs_directory] -e [file_extension]\n```\n\nOnce it is running, hit any key to grab frames. Images are saved with prefixes `left` and `right` in the desired directory.\n\n### Intrinsic calibration of a single camera\n\nThis is only for lenses which follow the pinhole model. If you have fisheye lenses with a very wide field of view then see [this](https://github.com/sourishg/fisheye_stereo_calibration) repository. The calibration saves the camera matrix and the distortion coefficients in a YAML file. The datatype for these matrices is `Mat`.\n\nOnce you have compiled the sources run the following command to calibrate the intrinsics.\n\n```bash\n./calibrate -w [board_width] -h [board_height] -n [num_imgs] -s [square_size] -d [imgs_directory] -i [imgs_filename] -o [file_extension] -e [output_filename]\n```\n\nFor example, the command for the test images in `calib_imgs/1/` would be\n\n```bash\n./calibrate -w 9 -h 6 -n 27 -s 0.02423 -d \"../calib_imgs/1/\" -i \"left\" -o \"cam_left.yml\" -e \"jpg\"\n```\n\n### Stereo calibration for extrinisics\n\nOnce you have the intrinsics calibrated for both the left and the right cameras, you can use their intrinsics to calibrate the extrinsics between them.\n\n```bash\n./calibrate_stereo -n [num_imgs] -u [left_cam_calib] -v [right_cam_calib] -L [left_img_dir] -R [right_img_dir] -l [left_img_prefix] -r [right_img_prefix] -o [output_calib_file] -e [file_extension]\n```\n\nFor example, if you calibrated the left and the right cameras using the images in the `calib_imgs/1/` directory, the following command to compute the extrinsics.\n\n```bash\n./calibrate_stereo -n 27 -u cam_left.yml -v cam_right.yml -L ../calib_imgs/1/ -R ../calib_imgs/1/ -l left -r right -o cam_stereo.yml -e jpg\n```\n\n### Undistortion and Rectification\n\nOnce you have the stereo calibration data, you can remove the distortion and rectify any pair of images so that the resultant epipolar lines become scan lines.\n\n```bash\n./undistort_rectify -l [left_img_path] -r [right_img_path] -c [stereo_calib_file] -L [output_left_img] -R [output_right_img]\n```\n\nFor example\n\n```bash\n./undistort_rectify -l ../calib_imgs/1/left1.jpg -r ../calib_imgs/1/right1.jpg -c cam_stereo.yml -L left.jpg -R right.jpg\n```\n"
 },
 {
  "repo": "EnoxSoftware/OpenCVForUnity",
  "language": "C#",
  "readme_contents": "OpenCVForUnity Examples\r\n====================\r\n\r\nOverview\r\n-----\r\nOpenCVForUnity is required to run example code of this repository.  \r\n[https://assetstore.unity.com/packages/tools/integration/opencv-for-unity-21088](https://assetstore.unity.com/packages/tools/integration/opencv-for-unity-21088?aid=1011l4ehR)  \r\n\r\nSetup Tutorial & Demo Video\r\n-----\r\n[![](http://img.youtube.com/vi/HnXGIvHvU9I/0.jpg)](https://www.youtube.com/watch?v=HnXGIvHvU9I)\r\n\r\nDemo\r\n-----\r\n- WebGL\r\n<https://enoxsoftware.github.io/OpenCVForUnity/webgl_example/index.html>\r\n- Android\r\n<https://play.google.com/store/apps/details?id=com.enoxsoftware.opencvforunityexample>\r\n\r\nManual\r\n-----\r\n[ReadMe.pdf](/Assets/OpenCVForUnity/ReadMe.pdf)\r\n\r\nAPI Reference\r\n-----\r\n<https://enoxsoftware.github.io/OpenCVForUnity/3.0.0/doc/html/index.html>\r\n\r\nOfficial Site\r\n-----\r\n<https://enoxsoftware.com/opencvforunity/>\r\n\r\n"
 },
 {
  "repo": "AdamSpannbauer/python_video_stab",
  "language": "Python",
  "readme_contents": "# Python Video Stabilization <img src='https://s3.amazonaws.com/python-vidstab/logo/vidstab_logo_hex.png' width=125 align='right'/>\n\n[![Build Status](https://travis-ci.org/AdamSpannbauer/python_video_stab.svg?branch=master)](https://travis-ci.org/AdamSpannbauer/python_video_stab)\n[![codecov](https://codecov.io/gh/AdamSpannbauer/python_video_stab/branch/master/graph/badge.svg)](https://codecov.io/gh/AdamSpannbauer/python_video_stab)\n[![Maintainability](https://api.codeclimate.com/v1/badges/f3a17d211a2a437d21b1/maintainability)](https://codeclimate.com/github/AdamSpannbauer/python_video_stab/maintainability)\n[![PyPi version](https://pypip.in/v/vidstab/badge.png)](https://pypi.org/project/vidstab/)\n[![Last Commit](https://img.shields.io/github/last-commit/AdamSpannbauer/python_video_stab.svg)](https://github.com/AdamSpannbauer/python_video_stab/commits/master)\n[![Downloads](https://pepy.tech/badge/vidstab)](https://pepy.tech/project/vidstab)\n\n Python video stabilization using OpenCV. Full [searchable documentation here](https://adamspannbauer.github.io/python_video_stab).\n \n This module contains a single class (`VidStab`) used for video stabilization. This class is based on the work presented by Nghia Ho in [SIMPLE VIDEO STABILIZATION USING OPENCV](http://nghiaho.com/?p=2093). The foundation code was found in a comment on Nghia Ho's post by the commenter with username koala.\n \n Input                           |  Output\n:-------------------------------:|:-------------------------:\n![](https://s3.amazonaws.com/python-vidstab/readme/input_ostrich.gif)    |  ![](https://s3.amazonaws.com/python-vidstab/readme/stable_ostrich.gif)\n \n*[Video](https://www.youtube.com/watch?v=9pypPqbV_GM) used with permission from [HappyLiving](https://www.facebook.com/happylivinginfl/)*\n\n## Contents:\n1. [Installation](#installation)\n   * [Install `vidstab` without installing OpenCV](#install-vidstab-without-installing-opencv)\n   * [Install vidstab & OpenCV](#install-vidstab-opencv)   \n2. [Basic Usage](#basic-usage)\n   * [Using from command line](#using-from-command-line)\n   * [Using VidStab class](#using-vidstab-class)\n3. [Advanced Usage](#advanced-usage)\n   * [Plotting frame to frame transformations](#plotting-frame-to-frame-transformations)\n   * [Using borders](#using-borders)\n   * [Using Frame Layering](#using-frame-layering)\n   * [Stabilizing a frame at a time](#stabilizing-a-frame-at-a-time)\n   * [Working with live video](#working-with-live-video)\n   * [Transform File Writing & Reading](#transform-file-writing--reading)\n\n## Installation\n\n> ```diff\n> + Please report issues if you install/try to install and run into problems!\n> ```\n\n### Install `vidstab` without installing OpenCV\n\nIf you've already built OpenCV with python bindings on your machine it is recommended to install `vidstab` without installing the pypi versions of OpenCV.  The `opencv-python` python module can cause issues if you've already built OpenCV from source in your environment.\n\nThe below commands will install `vidstab` without OpenCV included.\n\n#### From PyPi\n\n```bash\npip install vidstab\n```\n\n#### From GitHub\n\n```bash\npip install git+https://github.com/AdamSpannbauer/python_video_stab.git\n```\n\n### Install `vidstab` & OpenCV\n\nIf you don't have OpenCV installed already there are a couple options.  \n\n1. You can build OpenCV using one of the great online tutorials from [PyImageSearch](https://www.pyimagesearch.com/), [LearnOpenCV](https://www.learnopencv.com/), or [OpenCV](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup.html#py-table-of-content-setup) themselves.  When building from source you have more options (e.g. [platform optimization](https://www.pyimagesearch.com/2017/10/09/optimizing-opencv-on-the-raspberry-pi/)), but more responsibility.  Once installed you can use the pip install command shown above.\n2. You can install a pre-built distribution of OpenCV from pypi as a dependency for `vidstab` (see command below)\n\nThe below commands will install `vidstab` with `opencv-contrib-python` as dependencies.\n\n#### From PyPi\n\n```bash\npip install vidstab[cv2]\n```\n\n#### From Github\n\n```bash\n pip install -e git+https://github.com/AdamSpannbauer/python_video_stab.git#egg=vidstab[cv2]\n```\n\n## Basic usage\n\nThe `VidStab` class can be used as a command line script or in your own custom python code.\n\n### Using from command line\n\n```bash\n# Using defaults\npython3 -m vidstab --input input_video.mov --output stable_video.avi\n```\n\n```bash\n# Using a specific keypoint detector\npython3 -m vidstab -i input_video.mov -o stable_video.avi -k GFTT\n```\n\n### Using `VidStab` class\n\n```python\nfrom vidstab import VidStab\n\n# Using defaults\nstabilizer = VidStab()\nstabilizer.stabilize(input_path='input_video.mov', output_path='stable_video.avi')\n\n# Using a specific keypoint detector\nstabilizer = VidStab(kp_method='ORB')\nstabilizer.stabilize(input_path='input_video.mp4', output_path='stable_video.avi')\n\n# Using a specific keypoint detector and customizing keypoint parameters\nstabilizer = VidStab(kp_method='FAST', threshold=42, nonmaxSuppression=False)\nstabilizer.stabilize(input_path='input_video.mov', output_path='stable_video.avi')\n```\n\n## Advanced usage\n\n### Plotting frame to frame transformations\n\n```python\nfrom vidstab import VidStab\nimport matplotlib.pyplot as plt\n\nstabilizer = VidStab()\nstabilizer.stabilize(input_path='input_video.mov', output_path='stable_video.avi')\n\nstabilizer.plot_trajectory()\nplt.show()\n\nstabilizer.plot_transforms()\nplt.show()\n```\n\nTrajectories                     |  Transforms\n:-------------------------------:|:-------------------------:\n![](https://s3.amazonaws.com/python-vidstab/readme/trajectory_plot.png)  |  ![](https://s3.amazonaws.com/python-vidstab/readme/transforms_plot.png)\n\n### Using borders\n\n```python\nfrom vidstab import VidStab\n\nstabilizer = VidStab()\n\n# black borders\nstabilizer.stabilize(input_path='input_video.mov', \n                     output_path='stable_video.avi', \n                     border_type='black')\nstabilizer.stabilize(input_path='input_video.mov', \n                     output_path='wide_stable_video.avi', \n                     border_type='black', \n                     border_size=100)\n\n# filled in borders\nstabilizer.stabilize(input_path='input_video.mov', \n                     output_path='ref_stable_video.avi', \n                     border_type='reflect')\nstabilizer.stabilize(input_path='input_video.mov', \n                     output_path='rep_stable_video.avi', \n                     border_type='replicate')\n```\n\n<table>\n  <tr>\n    <td><p align='center'><code>border_size=0</code></p></td>\n    <td><p align='center'><code>border_size=100</code></p></td>\n  </tr>\n  <tr>\n    <td><p align='center'><img src='https://s3.amazonaws.com/python-vidstab/readme/stable_ostrich.gif'></p></td>\n    <td><p align='center'><img src='https://s3.amazonaws.com/python-vidstab/readme/wide_stable_ostrich.gif'></p></td>\n  </tr>\n</table>\n\n`border_type='reflect'`                 |  `border_type='replicate'`\n:--------------------------------------:|:-------------------------:\n![](https://s3.amazonaws.com/python-vidstab/readme/reflect_stable_ostrich.gif)  |  ![](https://s3.amazonaws.com/python-vidstab/readme/replicate_stable_ostrich.gif)\n\n*[Video](https://www.youtube.com/watch?v=9pypPqbV_GM) used with permission from [HappyLiving](https://www.facebook.com/happylivinginfl/)*\n\n### Using Frame Layering\n\n```python\nfrom vidstab import VidStab, layer_overlay, layer_blend\n\n# init vid stabilizer\nstabilizer = VidStab()\n\n# use vidstab.layer_overlay for generating a trail effect\nstabilizer.stabilize(input_path=INPUT_VIDEO_PATH,\n                     output_path='trail_stable_video.avi',\n                     border_type='black',\n                     border_size=100,\n                     layer_func=layer_overlay)\n\n\n# create custom overlay function\n# here we use vidstab.layer_blend with custom alpha\n#   layer_blend will generate a fading trail effect with some motion blur\ndef layer_custom(foreground, background):\n    return layer_blend(foreground, background, foreground_alpha=.8)\n\n# use custom overlay function\nstabilizer.stabilize(input_path=INPUT_VIDEO_PATH,\n                     output_path='blend_stable_video.avi',\n                     border_type='black',\n                     border_size=100,\n                     layer_func=layer_custom)\n```\n\n`layer_func=vidstab.layer_overlay`     |  `layer_func=vidstab.layer_blend`\n:--------------------------------------:|:-------------------------:\n![](https://s3.amazonaws.com/python-vidstab/readme/trail_stable_ostrich.gif)  |  ![](https://s3.amazonaws.com/python-vidstab/readme/blend_stable_ostrich.gif)\n\n*[Video](https://www.youtube.com/watch?v=9pypPqbV_GM) used with permission from [HappyLiving](https://www.facebook.com/happylivinginfl/)*\n\n\n### Automatic border sizing\n\n```python\nfrom vidstab import VidStab, layer_overlay\n\nstabilizer = VidStab()\n\nstabilizer.stabilize(input_path=INPUT_VIDEO_PATH,\n                     output_path='auto_border_stable_video.avi', \n                     border_size='auto',\n                     # frame layering to show performance of auto sizing\n                     layer_func=layer_overlay)\n```\n\n<p align='center'>\n  <img width='45%' src='https://s3.amazonaws.com/python-vidstab/readme/auto_border_stable_ostrich.gif'>\n</p>\n\n\n### Stabilizing a frame at a time\n\nThe method `VidStab.stabilize_frame()` can accept `numpy` arrays to allow stabilization processing a frame at a time.\nThis can allow pre/post processing for each frame to be stabilized; see examples below.\n\n#### Simplest form\n\n```python\nfrom vidstab.VidStab import VidStab\n\nstabilizer = VidStab()\nvidcap = cv2.VideoCapture('input_video.mov')\n\nwhile True:\n     grabbed_frame, frame = vidcap.read()\n     \n     if frame is not None:\n        # Perform any pre-processing of frame before stabilization here\n        pass\n     \n     # Pass frame to stabilizer even if frame is None\n     # stabilized_frame will be an all black frame until iteration 30\n     stabilized_frame = stabilizer.stabilize_frame(input_frame=frame,\n                                                   smoothing_window=30)\n     if stabilized_frame is None:\n         # There are no more frames available to stabilize\n         break\n     \n     # Perform any post-processing of stabilized frame here\n     pass\n```\n\n#### Example with object tracking\n\n```python\nimport os\nimport cv2\nfrom vidstab import VidStab, layer_overlay, download_ostrich_video\n\n# Download test video to stabilize\nif not os.path.isfile(\"ostrich.mp4\"):\n    download_ostrich_video(\"ostrich.mp4\")\n\n# Initialize object tracker, stabilizer, and video reader\nobject_tracker = cv2.TrackerCSRT_create()\nstabilizer = VidStab()\nvidcap = cv2.VideoCapture(\"ostrich.mp4\")\n\n# Initialize bounding box for drawing rectangle around tracked object\nobject_bounding_box = None\n\nwhile True:\n    grabbed_frame, frame = vidcap.read()\n\n    # Pass frame to stabilizer even if frame is None\n    stabilized_frame = stabilizer.stabilize_frame(input_frame=frame, border_size=50)\n\n    # If stabilized_frame is None then there are no frames left to process\n    if stabilized_frame is None:\n        break\n\n    # Draw rectangle around tracked object if tracking has started\n    if object_bounding_box is not None:\n        success, object_bounding_box = object_tracker.update(stabilized_frame)\n\n        if success:\n            (x, y, w, h) = [int(v) for v in object_bounding_box]\n            cv2.rectangle(stabilized_frame, (x, y), (x + w, y + h),\n                          (0, 255, 0), 2)\n\n    # Display stabilized output\n    cv2.imshow('Frame', stabilized_frame)\n\n    key = cv2.waitKey(5)\n\n    # Select ROI for tracking and begin object tracking\n    # Non-zero frame indicates stabilization process is warmed up\n    if stabilized_frame.sum() > 0 and object_bounding_box is None:\n        object_bounding_box = cv2.selectROI(\"Frame\",\n                                            stabilized_frame,\n                                            fromCenter=False,\n                                            showCrosshair=True)\n        object_tracker.init(stabilized_frame, object_bounding_box)\n    elif key == 27:\n        break\n\nvidcap.release()\ncv2.destroyAllWindows()\n```\n\n<p align='center'>\n  <img width='50%' src='https://s3.amazonaws.com/python-vidstab/readme/obj_tracking_vidstab_1.gif'>\n</p>\n\n\n### Working with live video\n\nThe `VidStab` class can also process live video streams.  The underlying video reader is `cv2.VideoCapture`([documentation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_video_display/py_video_display.html)).\nThe relevant snippet from the documentation for stabilizing live video is:\n\n> *Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on.*\n\nThe `input_path` argument of the `VidStab.stabilize` method can accept integers that will be passed directly to `cv2.VideoCapture` as a device index.  You can also pass a device index to the `--input` argument for command line usage.\n\nOne notable difference between live feeds and video files is that webcam footage does not have a definite end point.\nThe options for ending a live video stabilization are to set the max length using the `max_frames` argument or to manually stop the process by pressing the <kbd>Esc</kbd> key or the <kbd>Q</kbd> key.\nIf `max_frames` is not provided then no progress bar can be displayed for live video stabilization processes.\n\n#### Example\n\n```python\nfrom vidstab import VidStab\n\nstabilizer = VidStab()\nstabilizer.stabilize(input_path=0,\n                     output_path='stable_webcam.avi',\n                     max_frames=1000,\n                     playback=True)\n```\n\n<p align='center'>\n  <img width='50%' src='https://s3.amazonaws.com/python-vidstab/readme/webcam_stable.gif'>\n</p>\n\n### Transform file writing & reading \n\n#### Generating and saving transforms to file\n\n```python\nimport numpy as np\nfrom vidstab import VidStab, download_ostrich_video\n\n# Download video if needed\ndownload_ostrich_video(INPUT_VIDEO_PATH)\n\n# Generate transforms and save to TRANSFORMATIONS_PATH as csv (no headers)\nstabilizer = VidStab()\nstabilizer.gen_transforms(INPUT_VIDEO_PATH)\nnp.savetxt(TRANSFORMATIONS_PATH, stabilizer.transforms, delimiter=',')\n```\n\nFile at `TRANSFORMATIONS_PATH` is of the form shown below.  The 3 columns represent delta x, delta y, and delta angle respectively.\n\n```\n-9.249733913760086068e+01,2.953221378387767970e+01,-2.875918912994855636e-02\n-8.801434576214279559e+01,2.741942225927152776e+01,-2.715232319470826938e-02\n```\n\n#### Reading and using transforms from file\n\nBelow example reads a file of transforms and applies to an arbitrary video.  The transform file is of the form shown in [above section](#generating-and-saving-transforms-to-file).\n\n```python\nimport numpy as np\nfrom vidstab import VidStab\n\n# Read in csv transform data, of form (delta x, delta y, delta angle):\ntransforms = np.loadtxt(TRANSFORMATIONS_PATH, delimiter=',')\n\n# Create stabilizer and supply numpy array of transforms\nstabilizer = VidStab()\nstabilizer.transforms = transforms\n\n# Apply stabilizing transforms to INPUT_VIDEO_PATH and save to OUTPUT_VIDEO_PATH\nstabilizer.apply_transforms(INPUT_VIDEO_PATH, OUTPUT_VIDEO_PATH)\n```\n"
 },
 {
  "repo": "SixQuant/nowatermark",
  "language": "Python",
  "readme_contents": "# nowatermark\n\n[![PyPI Version](https://img.shields.io/pypi/v/nowatermark.svg)](https://pypi.python.org/pypi/nowatermark)\n[![Build Status](https://img.shields.io/travis/SixQuant/nowatermark/master.svg)](https://travis-ci.org/SixQuant/nowatermark)\n[![Wheel Status](https://img.shields.io/badge/wheel-yes-brightgreen.svg)](https://pypi.python.org/pypi/nowatermark)\n[![Coverage report](https://img.shields.io/codecov/c/github/SixQuant/nowatermark/master.svg)](https://codecov.io/github/SixQuant/nowatermark?branch=master)\n[![Powered by SixQuant](https://img.shields.io/badge/powered%20by-SixQuant-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://sixquant.cn)\n\n## Overview\nremove watermark. \n\u6839\u636e\u6c34\u5370\u6a21\u677f\u56fe\u7247\u81ea\u52a8\u5bfb\u627e\u5e76\u53bb\u9664\u56fe\u7247\u4e2d\u5bf9\u5e94\u7684\u6c34\u5370\uff0c\u5229\u7528 Python \u548c OpenCV \u5feb\u901f\u5b9e\u73b0\u3002\n\n\n## Install\n\n### Mac OS Install OpenCV for Python3\n\n- with-python3\u7528\u6765\u544a\u8bc9homebrew\u7528\u6765\u8ba9opencv\u652f\u6301python3\uff0c\n- C++11 \u7528\u6765\u544a\u8bc9homebrew\u63d0\u4f9bc++11\u652f\u6301\uff0c\n- with-contrib \u7528\u6765\u5b89\u88c5opencv\u7684contrib\u652f\u6301\u3002\n\n```bash\n$ brew install opencv3 --without-python --with-python3 --c++11 --with-contrib  \n```\n\nVerifying the installation\uff1a\n\n```python\nimport cv2\nprint(cv2.__version__)\n```\n\nIf you got this error: \"ImportError: No module named 'cv2'\", then your symlink might be corrupted, you need to link your opencv to python site-packages:\n```bash\n$ brew link --force opencv3\n```\n\n### Install nowatermark\n```bash\n$ pip3 install nowatermark\n```\n\n## Usage\n\n```python\nfrom nowatermark import WatermarkRemover\n\npath = './data/'\n\nwatermark_template_filename = path + 'anjuke-watermark-template.jpg'\nremover = WatermarkRemover()\nremover.load_watermark_template(watermark_template_filename)\n\nremover.remove_watermark(path + 'anjuke3.jpg', path + 'anjuke3-result.jpg')\nremover.remove_watermark(path + 'anjuke4.jpg', path + 'anjuke4-result.jpg')\n\n```\n\n---\n\n### Original\n![Original](https://github.com/SixQuant/nowatermark/blob/master/data/anjuke2.jpg)\n\n### Removed watermark\n![Removed watermark](https://github.com/SixQuant/nowatermark/blob/master/data/anjuke2-result.jpg)\n\n---\n\n## Procedure\n\n### Feature Matching(\u7279\u5f81\u5339\u914d)\n* \u5bf9\u6c34\u5370\u6a21\u677f\u56fe\u7247\u8fdb\u884c\u4e86\u4e00\u4e9b\u521d\u59cb\u5316\u5904\u7406\uff0c\u6bd4\u5982\u4e8c\u503c\u5316\u540e\u53bb\u9664\u975e\u6587\u5b57\u90e8\u5206\u7b49\n* \u5c1d\u8bd5\u4e86 OpenCV \u7684\u591a\u79cd\u7b97\u6cd5\n  - \u6bd4\u5982 ORB + Brute-Force\uff0c\u5373\u86ee\u529b\u5339\u914d\uff0c\u5bf9\u5e94 cv2.BFMatcher() \u65b9\u6cd5\n  - \u6bd4\u5982 SIFT + FLANN\uff0c\u5373\u5feb\u901f\u6700\u8fd1\u90bb\u5339\u914d\uff0c\u5bf9\u5e94 cv2.BFMatcher() \u65b9\u6cd5\n  - \u6bd4\u5982 Template Matching\uff0c\u5373\u6a21\u677f\u5339\u914d\uff0c\u5bf9\u5e94 cv2.matchTemplate() \u65b9\u6cd5\n* \u6700\u540e\u53d1\u73b0 Template Matching \u6700\u7b80\u5355\u65b9\u4fbf\uff0c\u6548\u679c\u4e5f\u6700\u597d\u3002 \n* \u5982\u679c\u6c34\u5370\u4f4d\u7f6e\u56fa\u5b9a\u7684\u8bdd\u5219\u53ef\u4ee5\u8df3\u8fc7Feature Matching(\u7279\u5f81\u5339\u914d)\uff0c\u76f4\u63a5\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684Inpainting(\u56fe\u7247\u4fee\u590d)\n\n### Inpainting(\u56fe\u7247\u4fee\u590d)\n* \u4fee\u590d\u56fe\u7247\u524d\u9700\u8981\u505a\u4e00\u4e9b\u524d\u7f6e\u5904\u7406\n  - \u9996\u5148\u8981\u5f97\u5230\u56fe\u7247\u7684\u53bb\u6c34\u5370 Mask \u56fe\u7247\uff0c\u5373\u548c\u5f85\u5904\u7406\u56fe\u7247\u4e00\u6837\u5927\u5c0f\u7684\u9664\u4e86\u6c34\u5370\u90e8\u5206\u7684\u6587\u5b57\u90e8\u5206\u5916\u5176\u4ed6\u90e8\u5206\u5168\u90e8\u662f\u9ed1\u8272\u7684\u4f4d\u56fe\n  - \u56e0\u4e3a\u524d\u9762\u5bf9\u6c34\u5370\u505a\u4e86\u4e8c\u503c\u5316\u7b49\u5904\u7406\uff0c\u6700\u7ec8\u6548\u679c\u53d1\u73b0\u4f1a\u6709\u6c34\u5370\u8f6e\u5ed3\uff0c\u6240\u4ee5\u9700\u8981\u5bf9 Mask \u56fe\u7247\u505a\u4e00\u6b21\u81a8\u80c0\u5904\u7406\u8986\u76d6\u6389\u8f6e\u5ed3\n* \u9009\u7528\u4e86Telea\u57282004\u5e74\u63d0\u51fa\u7684Telea\u7b97\u6cd5\uff0c\u5373\u57fa\u4e8e\u5feb\u901f\u884c\u8fdb\uff08FMM\uff09\u7684\u4fee\u590d\u7b97\u6cd5\n  - \u5bf9\u5e94 cv2.inpaint(img, mask, 5, cv2.INPAINT_TELEA)\n  - \u5bf9\u5e94\u8bba\u6587\uff1a[An Image Inpainting Technique Based on the Fast Marching Method (2004)](http://www.cs.rug.nl/~alext/PAPERS/JGT04/paper.pdf)\n\n## Todo\n\n* \u7531\u4e8e\u67d0\u4e9b\u56fe\u7247\u7684\u6c34\u5370\u548c\u80cc\u666f\u56fe\u7247\u76f8\u4f3c\u7a0b\u5ea6\u592a\u9ad8\uff0c\u5982\u4f55\u63d0\u9ad8\u6c34\u5370\u4f4d\u7f6e\u7684\u8bc6\u522b\u6b63\u786e\u7387\n* \u6539\u8fdb\u4fee\u590d\u56fe\u7247\u7b97\u6cd5\uff0c\u53ef\u4ee5\u8003\u8651\u7528\u6df1\u5ea6\u5b66\u4e60\u6765\u505a\u505a\u770b\uff1f\n* Google CVPR 2017, [\u300aOn the Effectiveness of Visible Watermarks\u300b](https://watermark-cvpr17.github.io)\u8fd9\u4e2a\u636e\u8bf4\u5f88\u725b\u7684\uff0c\u56de\u5934\u53ef\u4ee5\u8bfb\u4e00\u8bfb\n\n## License\n\n[MIT](https://tldrlegal.com/license/mit-license)\n"
 },
 {
  "repo": "BBuf/Image-processing-algorithm",
  "language": "C++",
  "readme_contents": "\ufeff# \u672c\u5de5\u7a0b\u8bb0\u5f55\u4e00\u4e9b\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u8bba\u6587\u590d\u73b0\u53ca\u6570\u5b57\u56fe\u50cf\u5904\u7406\u77e5\u8bc6\u70b9\n\n## 1. Correction Algorithm \u590d\u73b0\u4e86\u4e00\u4e9b\u56fe\u50cf\u77eb\u6b63\u7b97\u6cd5\n\n## 2. ImageFiletering \u590d\u73b0\u4e86\u4e00\u4e9b\u56fe\u50cf\u6ee4\u6ce2\u7b97\u6cd5\n\n## 3. Feature Extraction \u590d\u73b0\u4e86\u4e00\u4e9b\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\n\n## 4. License Plate Recognition System \u5b9e\u73b0\u8f66\u724c\u53f7\u7801\u8bc6\u522b\u7b97\u6cd5\n\n## 5. Color Space Conversion \u5b9e\u73b0\u548c\u4f18\u5316\u5404\u79cd\u8272\u5f69\u7a7a\u95f4\u8f6c\u6362\u7b97\u6cd5\n\n## 6. Algorithm optimization \u4f18\u5316\u4e00\u4e9b\u5e38\u89c1\u7684OpenCV\u7b97\u6cd5\n\n## 7.PhotoShop Algorithm \u7834\u89e3\u4e00\u4e9bPhotoShop\u7b97\u6cd5\n\n- Retinex MSRCR.cpp \u5e26\u8272\u5f69\u6062\u590d\u7684\u591a\u5c3a\u5ea6\u89c6\u7f51\u819c\u589e\u5f3a\u7b97\u6cd5\u3002 \u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttp://www.cnblogs.com/Imageshop/archive/2013/04/17/3026881.html\n- ImageDataIncrease.cpp \u5e38\u89c1\u7684\u56fe\u7247\u6570\u636e\u6269\u5145\u3002\u5305\u62ec\u4e00\u4e9bPS\u7b97\u6cd5 \u5177\u4f53\u4e3a\u65cb\u8f6c\uff0c\u6dfb\u52a0\u9ad8\u65af\uff0c\u6912\u76d0\u566a\u58f0\uff0c\u589e\u52a0\u8001\u7167\u7247\u6548\u679c\uff0c\u589e\u52a0\u548c\u964d\u4f4e\u56fe\u50cf\u9971\u548c\u5ea6\uff0c\u5bf9\u539f\u56fe\u7f29\u653e\uff0c\u4eae\u5ea6\u589e\u5f3a\uff0c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\uff0c\u78e8\u76ae\u7f8e\u767d\uff0c\u504f\u8272\u77eb\u6b63\uff0c\u540c\u6001\u6ee4\u6ce2\uff0c\u8fc7\u66dd\uff0c\u7070\u5ea6\u5316\uff0c\u8f6e\u6362\u901a\u9053\uff0c\u56fe\u50cf\u9519\u5207\uff0c\u8fd0\u52a8\u6a21\u7cca\uff0c\u949d\u5316\u8499\u7248\uff0cPS\u6ee4\u955c\u7b97\u6cd5\u4e4b\u7403\u9762\u5316 (\u51f8\u51fa\u548c\u51f9\u9677\u6548\u679c)\n- HDR.cpp C++\u590d\u73b0\u300aAdaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images\u300b\uff0c \u5b9e\u73b0\u4f4e\u7167\u5ea6\u5f69\u8272\u56fe\u50cf\u6062\u590d\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84030723\n- Adaptive Logarithmic Mapping For Displaying High Contrast Scenes.cpp C++\u590d\u73b0\u4e86\u300aAdaptive Logarithmic Mapping For Displaying High Contrast Scenes\u300b \u5b9e\u73b0\u4f4e\u7167\u5ea6\u5f69\u8272\u56fe\u50cf\u6062\u590d\uff0c\u6548\u679c\u8d85\u68d2\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84066390\n- Single Image Haze Removal Using Dark Channel Prior.cpp C++\u590d\u73b0\u4e86\u300aSingle Image Haze Removal Using Dark Channel Prior\u300b\uff0c\u5b9e\u73b0\u6697\u901a\u9053\u53bb\u96fe\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84110518\n- Local Color Correction.cpp C++\u590d\u73b0\u4e86\u300aLocal Color Correction\u300b\u8bba\u6587\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84539295\n- PartialcolorJudge.cpp C++\u590d\u73b0\u4e86\u300a\u57fa\u4e8e\u56fe\u50cf\u5206\u6790\u7684\u504f\u8272\u68c0\u6d4b\u53ca\u989c\u8272\u6821\u6b63\u65b9\u6cd5\u300b\u8bba\u6587\uff0c\u5b9e\u73b0\u5feb\u901f\u5224\u65ad\u56fe\u7247\u662f\u5426\u5b58\u5728\u504f\u8272\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84897976\n- Optimized contrast enhancement for real-time image and video dehazin.cpp C++\u590d\u73b0\u4e86\u300aOptimized contrast enhancement for real-time image and video dehazin\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u76f8\u5bf9\u4e8eHe Kaiming\u7684\u6697\u901a\u9053\u53bb\u96fe\uff0c\u5bf9\u5929\u7a7a\u5177\u6709\u5929\u7136\u7684\u514d\u75ab\u529b\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84932848\n- AutoLevelAndAutoContrast.cpp C++\u590d\u73b0\u4e86\u81ea\u52a8\u8272\u9636\u8c03\u6574\u548c\u81ea\u52a8\u5bf9\u6bd4\u5ea6\u8c03\u6574\uff0c\u5176\u4e2d\u81ea\u52a8\u8272\u9636\u8c03\u6574\u53ef\u4ee5\u7528\u4e8e\u53bb\u96fe\u548c\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://www.cnblogs.com/Imageshop/archive/2011/11/13/2247614.html\n- Contrast Image Correction Method.cpp C++\u590d\u73b0\u4e86\u300aContrast Image Correction Method\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u53ef\u4ee5\u81ea\u9002\u5e94\u77eb\u6b63\u56fe\u50cf\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/85005510\n- MultiScaleDetailBoosting.cpp C++\u590d\u73b0\u4e86\u300aDARK IMAGE ENHANCEMENT BASED ON PAIRWISE TARGET CONTRAST AND MULTI-SCALE DETAIL BOOSTING\u300b\u8bba\u6587\uff0c\u53ef\u4ee5\u7528\u4e8e\u63d0\u5347\u56fe\u50cf\u4e0d\u540c\u7a0b\u5ea6\u7684\u7ec6\u8282\u4fe1\u606f\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/85007555\n- Inrbl.cpp C++\u590d\u73b0\u4e86\u300a\u6539\u8fdb\u975e\u7ebf\u6027\u4eae\u5ea6\u63d0\u5347\u6a21\u578b\u7684\u9006\u5149\u56fe\u50cf\u6062\u590d\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u53ef\u4ee5\u505a\u9006\u5149\u56fe\u50cf\u6062\u590d\u3002\u7b97\u6cd5\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/86681325\n- unevenLightCompensate.cpp C++\u590d\u73b0\u4e86\u300a\u4e00\u79cd\u57fa\u4e8e\u4eae\u5ea6\u5747\u8861\u7684\u56fe\u50cf\u9608\u503c\u5206\u5272\u6280\u672f\u300b\u8fd9\u7bc7\u8bba\u6587\u7684\u5149\u7167\u8865\u507f\u90e8\u5206\uff0c\u53ef\u4ee5\u5bf9\u5149\u7167\u4e0d\u5747\u5300\uff0c\u66dd\u5149\uff0c\u9006\u5149\u56fe\u50cf\u505a\u4eae\u5ea6\u5747\u8861\uff0c\u6548\u679c\u4e0d\u9519\u3002\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/88551771\n- Adaptive correction algorithm for illumination inhomogeneity image based on two-dimensional gamma function.cpp C++\u590d\u73b0\u4e86\u300a\u57fa\u4e8e\u4e8c\u7ef4\u4f3d\u9a6c\u51fd\u6570\u7684\u5149\u7167\u4e0d\u5747\u5300\u56fe\u50cf\u81ea\u9002\u5e94\u6821\u6b63\u7b97\u6cd5\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u5bf9\u5149\u7167\u4e0d\u5747\u5300\u7684\u56fe\u50cf\u6709\u8f83\u597d\u7684\u6821\u6b63\u6548\u679c\uff0c\u4e14\u4e0d\u4f1a\u50cfRetiex\u90a3\u6837\u51fa\u73b0\u5149\u6655\u3002\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/88569129\n- Real-time adaptive contrast enhancement for imaging sensors.cpp C++\u590d\u73b0\u4e86\u300aReal-time adaptive contrast enhancement for imaging sensors\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u5b9e\u65f6\u81ea\u9002\u5e94\u5c40\u90e8\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u7b97\u6cd5\u3002\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/85208124\n- AutomaticWhiteBalanceMethod.cpp C++\u590d\u73b0\u4e86\u300aA Novel Automatic White Balance Method For Digital Still Cameras\u300b\u8fd9\u7bc7\u8bba\u6587\uff0c\u5b9e\u73b0\u4e86\u6548\u679c\u6bd4\u5b8c\u7f8e\u53cd\u5c04\u66f4\u597d\u5f97\u767d\u5e73\u8861\u6548\u679c\u3002\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/89183909\n- Automatic Color Equalization(ACE) and its Fast Implementation.cpp C++\u590d\u73b0\u4e86IPOL\u300aAutomatic Color  Equalization(ACE) and its Fast Implementation\u300b\u8bba\u6587\uff0c\u7528\u4e8e\u81ea\u52a8\u8272\u5f69\u5747\u8861\u3002\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/85237711\n- Single Image Haze Removal Using Dark Channel Prior(Guided Filter).cpp C++\u590d\u73b0\u4e86\u300aSingle Image Haze Removal Using Dark Channel Prior\u300b\u8bba\u6587\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u4f55\u535a\u58eb\u63d0\u5230\u5bfc\u5411\u6ee4\u6ce2\u6765\u4f30\u8ba1\u900f\u5c04\u7387\uff0c\u6bd4\u539f\u59cb\u5b9e\u73b0\u6548\u679c\u66f4\u597d\u3002\u7b97\u6cd5\u539f\u7406\uff1ahttps://blog.csdn.net/just_sort/article/details/89470403\n- MedianFilterFogRemoval.cpp C++\u590d\u73b0\u4e86\u300a[\u4e00\u79cd\u5355\u5e45\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5](http://wenku.baidu.com/link?url=ZoNmd4noFbWZOGKCHus4anP83t8gcc0xWDu9QCfgQuzwn7LxUoBbZmMxrUAFYM3_YEMoQH3DdvYD8j1hdcHt5Wz4LhdvDe4_GZYXrqCYco3)\u300b\u4f7f\u7528\u4e2d\u503c\u6ee4\u6ce2\u8fdb\u884c\u53bb\u96fe\uff0c\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/89520776\n- FastDefoggingBasedOnSingleImage.cpp C++\u590d\u73b0\u4e86\u300a\u57fa\u4e8e\u5355\u5e45\u56fe\u50cf\u7684\u5feb\u901f\u53bb\u96fe\u300b\u8bba\u6587\uff0c\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/90205686\n- BoxSideWindowFilter.cpp C++\u590d\u73b0\u4e86CVPR2019\u300aSide Window Filter\u300b\u8bba\u6587(Box Filter)\uff0c\u5b9e\u73b0\u9738\u6c14\u7684\u5f3a\u5236\u4fdd\u8fb9\uff0c\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/93664078\n- MedianSideWindowFilter.cpp C++\u590d\u73b0\u4e86CVPR2019\u300aSide Window Filter\u300b\u8bba\u6587(Median Filter)\uff0c\u5b9e\u73b0\u9738\u6c14\u7684\u5f3a\u5236\u4fdd\u8fb9\uff0c\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/93664078\n- RectangleDetection.cpp C++\u590d\u73b0\u4e86StackOverFlow\u4e0a\u9762\u7684\u4e00\u4e2a\u6709\u8da3\u7684\u77e9\u5f62\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5e76\u4e14\u914d\u5408Side Window Filter\u53ef\u4ee5\u53d6\u5f97\u66f4\u597d\u7684\u6548\u679c\uff0c\u539f\u7406\u8bf7\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/104754937\n\n\n\n\n\n![\u6211\u7684\u516c\u4f17\u53f7\uff0c\u6b22\u8fce\u5173\u6ce8](image/weixin.jpg)\n\n"
 },
 {
  "repo": "district10/cmake-templates",
  "language": "C++",
  "readme_contents": "# cmake-templates\n\nSome CMake Templates.\n\n## 1. Overview\n\n**Conventions**\n\n-   :smile: means tested okay/good\n-   :rage: means test result not sufficiently good\n-   :question: means not tested yet\n\n| Project                       | Linux + GCC 4.8+ | Win + VS2010 | Win + VS2015 | macOS   |\n| :---------------------------: | :--------------: | :----------: | :----------: | :---:   |\n| [c][refc]                     | :smile:          |  :smile:     | :smile:      | :smile: |\n| [c++][refcpp]                 | :smile:          |  :smile:     | :smile:      | :smile: |\n| [c++11][refcpp11]             | :smile:          |  :rage:      | :smile:      | :smile: |\n| [c++11vs2010][refcpp11vs2010] | :smile:          |  :smile:     | :smile:      | :smile: |\n| [module][refmodule]           | :smile:          |  :smile:     | :smile:      | :smile: |\n| [opencv][refocv]              | :question:       |  :smile:     | :question:   | :rage:  |\n| [opencv3][refocv3]            | :smile:          |  :question:  | :smile:      | :smile: |\n| [boost][refboost]             | :smile:          |  :smile:     | :question:   | :smile: |\n| [qt4-console][refqt4c]        | :smile:          |  :smile:     | :smile:      | :rage:  |\n| [qt4-gui][refqt4g]            | :smile:          |  :smile:     | :smile:      | :rage:  |\n| [qt4-project][refqt4p]        | :smile:          |  :smile:     | :smile:      | :rage:  |\n| [qt5-project][refqt5p]        | :smile:          |  :question:  | :smile:      | :smile: |\n\n\n[refc]: #31-c-example\n[refcpp]: #32-c-example\n[refcpp11]: #33-c11-example\n[refcpp11vs2010]: #33-c11-example\n[refmodule]: #34-example-to-show-how-to-modualize-your-project\n[refboost]: #35-example-with-support-of-boost\n[refocv]: #36-example-with-support-of-opencv\n[refocv3]: #36-example-with-support-of-opencv\n[refqt4c]: #37-example-with-support-of-qt4\n[refqt4g]: #37-example-with-support-of-qt4\n[refqt4p]: #37-example-with-support-of-qt4\n[refqt5p]: #38-example-with-support-of-qt5\n\n## 2. Usage\n\n### 2.1. Windows\n\nUse `CMake-GUI` to generate Visual Studio 2010 project, then use Visual Studio to compile & run.\n\nHere is a Tutorial: [HOWTO: Win + CMake + Visual Studio 2010 \u00b7 Issue #1 \u00b7 district10/cmake-templates](https://github.com/district10/cmake-templates/issues/1).\n\n### 2.2. Linux\n\nMost commonly, we build Makefile project:\n\n```bash\n# cd source dir (there should be a CMakeLists.txt)\nmkdir build && cd build\ncmake ..            # want a release build? try `cmake -DCMAKE_BUILD_TYPE=Release ..'\nmake\n\n# then checkout the generated binary files\n```\n\nBut we can build CodeBlocks projects too, see my tutorial:\n[HOWTO: Linux + CMake + CodeBlocks + GNU Make \u00b7 Issue #2 \u00b7 district10/cmake-templates](https://github.com/district10/cmake-templates/issues/2),\nor use qt-creator to open CMakeLists.txt directly, see my tutorial:\n[HOWTO: Use Qt creator to Open CMakeLists.txt directly (will generate proper project files) \u00b7 Issue #5 \u00b7 district10/cmake-templates](https://github.com/district10/cmake-templates/issues/5).\n\n## 3. Examples\n\n### 3.1. C Example\n\nSimple C project.\n\n```cmake\nproject( C )\ncmake_minimum_required( VERSION 2.6 )\nadd_executable( ${PROJECT_NAME} main.c )\n```\n\n`cmake_minimum_required( ... )` is needed in root CMakeLists.txt, always.\n\nThe `${PROJECT_NAME}` is variable with value `C`,\nwhich is set by the `project( C )`.\n\nSee [c](c).\n\n### 3.2. C++ Example\n\nSimple C++ project.\n\n```cmake\nproject( CPP )\nmake_minimum_required( VERSION 2.6 )\nfile( GLOB SRCS *.c *.cpp *.cc *.h *.hpp )  # a variable called SRCS with all files whose path match \"*.c *.cpp...\"\nadd_executable( ${PROJECT_NAME} ${SRCS} )\n```\n\nSee [cpp](cpp).\n\n### 3.3. C++11 Example\n\nC++11 project.\n\n```cmake\ninclude( CheckCXXCompilerFlag )\ncheck_cxx_compiler_flag( \"-std=c++11\"   COMPILER_SUPPORTS_CXX11 )\ncheck_cxx_compiler_flag( \"-std=c++0x\"   COMPILER_SUPPORTS_CXX0X )\nif( COMPILER_SUPPORTS_CXX11 )\n    if( CMAKE_COMPILER_IS_GNUCXX )\n        set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=gnu++11\" )\n    else()\n        set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\" )\n    endif()\nelseif( COMPILER_SUPPORTS_CXX0X)\n    set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++0x\" )\nelse()\n    # MSVC, On by default (if available)\nendif()\n```\n\nSee [cpp11](cpp11).\n\nI recommend Visual Studio 2015 Community Edition.\n\n### 3.4. Example to Show How to Modualize Your Project\n\n```cmake\n# root cmakelists.txt\nproject( MODULES )\ncmake_minimum_required( VERSION 2.8.3 )\n\ninclude_directories( ${CMAKE_SOURCE_DIR}/includes )\n\nadd_subdirectory( src )\nadd_subdirectory( demo )\n\n# src dir\nadd_subdirectory( cubic )\nadd_subdirectory( extras )\nadd_subdirectory( square )\n\n# cubic\nadd_library( LibCubic ${CUBICS} cubic.c )\n\n# demo\nproject( CALC )\ncmake_minimum_required( VERSION 2.6 )\n\nset( EXTRA_LIBS ${EXTRA_LIBS} LibSquare )\nset( EXTRA_LIBS ${EXTRA_LIBS} LibExtras )\nset( EXTRA_LIBS ${EXTRA_LIBS} LibCubic )\n\nadd_executable( Calc calc.c )\ntarget_link_libraries( Calc  ${EXTRA_LIBS} )\n```\n\nSee [modules](modules).\n\n### 3.5. Example with Support of Boost\n\n```cmake\nproject( BOOST )\ncmake_minimum_required( VERSION 2.6 )\n\nfind_package( Boost REQUIRED )\nINCLUDE_DIRECTORIES( ${Boost_INCLUDE_DIR} )\nLINK_DIRECTORIES( ${Boost_LIBRARY_DIRS} )\nset( Boost_USE_STATIC_LIBS        OFF )\nset( Boost_USE_MULTITHREADED      ON )\nset( Boost_USE_STATIC_RUNTIME     OFF )\nset( BOOST_ALL_DYN_LINK           ON ) # force dynamic linking for all libraries\n\nadd_executable( ${PROJECT_NAME} main.cpp )\ntarget_link_libraries( ${PROJECT_NAME} ${Boost_LIBRARIES} )\n```\n\nUbuntu install: `sudo apt-get install libboost-all-dev`.\n\nSee [boost](boost).\n\n### 3.6. Example with Support of OpenCV\n\nWant to how to configure both opencv 2 & 3 on your system?\nCheckout my tutorial: [HOWTO: OpenCV 2 & OpenCV 3 \u00b7 Issue #4 \u00b7 district10/cmake-templates](https://github.com/district10/cmake-templates/issues/4).\n\nopencv 2 or less\n\n```cmake\nproject( OPENCV )\ncmake_minimum_required( VERSION 2.6 )\n\ninclude( $ENV{OpenCV2_DIR}/OpenCVConfig.cmake ) # find_package( OpenCV REQUIRED )\n\nmessage( STATUS \"OpenCV library status:\" )\nmessage( STATUS \"    version: ${OpenCV_VERSION}\" )\nmessage( STATUS \"    libraries: ${OpenCV_LIBS}\" )\nmessage( STATUS \"    include path: ${OpenCV_INCLUDE_DIRS}\" )\n\ninclude_directories( ${OpenCV_INCLUDE_DIRS} )\n\nadd_executable( ${PROJECT_NAME}  minarea.c )\ntarget_link_libraries( ${PROJECT_NAME} ${OpenCV_LIBS} )\n```\n\nopencv 3\n\n```cmake\nproject( OPENCV3 )\ncmake_minimum_required( VERSION 2.8 )\n\ninclude( $ENV{OpenCV3_DIR}/OpenCVConfig.cmake ) # find_package( OpenCV REQUIRED )\n\nmessage( STATUS \"OpenCV library status:\" )\nmessage( STATUS \"    version: ${OpenCV_VERSION}\" )\nmessage( STATUS \"    libraries: ${OpenCV_LIBS}\" )\nmessage( STATUS \"    include path: ${OpenCV_INCLUDE_DIRS}\" )\n\ninclude_directories( ${OpenCV_INCLUDE_DIRS} )\n\nadd_executable( ${PROJECT_NAME} example.cpp )\ntarget_link_libraries( ${PROJECT_NAME} ${OpenCV_LIBS} )\n```\n\nSee\n\n-   [opencv](opencv): for opencv2 or less (VS2010 :smile:, Linux :question:)\n-   [opencv3](opencv3): for opencv3 (VS2010 :question:, Linux :smile:)\n\n### 3.7. Example with Support of Qt4\n\nBe sure to make `qmake` caught by CMake, put it in your `$PATH`.\n\nqt4 console\n\n```cmake\nfind_package( Qt4 REQUIRED )\ninclude( ${QT_USE_FILE} )\nset( QT_DONT_USE_QTGUI TRUE )\n\nadd_executable( ${PROJECT_NAME} main.cpp )\ntarget_link_libraries( ${PROJECT_NAME}  ${QT_LIBRARIES} )\n```\n\nconfigure file\n\n```cmake\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/Configs.h.in\"\n    \"${PROJECT_BINARY_DIR}/Configs.h\" )\n```\n\nmoc, uic\n\n```cmake\nfile( GLOB_RECURSE HDRS_FILES *.h *.hpp )\nfile( GLOB_RECURSE SRCS_FILES *.cpp )\nfile( GLOB_RECURSE UI_FILES *.ui )\n\nqt4_wrap_cpp( MOC_SRCS ${HDRS_FILES} )\nqt4_wrap_ui( UI_HDRS ${UI_FILES} )\n\nsource_group( \"UI Files\" FILES ${UI_FILES} )\nsource_group( \"Generated Files\" FILES ${MOC_SRCS} ${UI_HDRS} )\n\nadd_library( ${PROJECT_NAME} STATIC ${SRCS_FILES} ${UI_FILES} ${HDRS_FILES} ${MOC_SRCS} ${UI_HDRS} )\ntarget_link_libraries( ${PROJECT_NAME} ${QT_LIBRARIES} )\n```\n\nWorks like `qmake -project`, one ring to rule them all:\n\n```cmake\nproject( QT4 )\ncmake_minimum_required( VERSION 2.6 )\n\nfind_package( Qt4 REQUIRED )\ninclude( ${QT_USE_FILE} )\n\ninclude_directories( ${CMAKE_SOURCE_DIR}/ )\ninclude_directories( ${CMAKE_BINARY_DIR}/ )\n\n# based on: https://cmake.org/Wiki/CMakeMacroFilterOut\nmacro( filter_out FILTERS INPUTS OUTPUTS )\n    set( FOUT \"\" )\n    foreach( INP ${INPUTS} )\n        set( FILTERED 0 )\n        foreach( FILT ${FILTERS} )\n            if( ${FILTERED} EQUAL 0 )\n                if( \"${FILT}\" STREQUAL \"${INP}\" )\n                    set( FILTERED 1 )\n                endif( \"${FILT}\" STREQUAL \"${INP}\" )\n                if( ${INP} MATCHES ${FILT} )\n                    set( FILTERED 1 )\n                endif( ${INP} MATCHES ${FILT} )\n            endif( ${FILTERED} EQUAL 0 )\n        endforeach( FILT ${FILTERS} )\n        if( ${FILTERED} EQUAL 0 )\n            set( FOUT ${FOUT} ${INP} )\n        endif( ${FILTERED} EQUAL 0 )\n    endforeach( INP ${INPUTS} )\n    set( ${OUTPUTS} ${FOUT} )\nendmacro( filter_out FILTERS INPUTS OUTPUTS )\n\nfile( GLOB_RECURSE UI_FILES *.ui )\nfile( GLOB_RECURSE HDRS_FILES *.h *.hpp )\nfile( GLOB_RECURSE SRCS_FILES *.cpp *.c )\nfile( GLOB_RECURSE RCS_FILES *.qrc )\n\nset( FILTERS \".*CompilerId.*\" )\nset( FILTERS \".*CMakeFiles/.*\" )\nfilter_out(\"${FILTERS}\" \"${SRCS_FILES}\" SRCS_FILES )\n\nqt4_wrap_cpp( MOC_SRCS ${HDRS_FILES} )\nqt4_wrap_ui( UI_HDRS ${UI_FILES} )\nqt4_add_resources( RCS ${RCS_FILES} )\n\nsource_group( \"UI Files\" FILES ${UI_FILES} )\nsource_group( \"Generated Files\" FILES ${MOC_SRCS} ${UI_HDRS} )\nsource_group( \"All Resource Files\" FILES ${RCS} )\n\nadd_executable( ${PROJECT_NAME}\n    ${MOC_SRCS}\n    ${HDRS_FILES}\n    ${SRCS_FILES}\n    ${UI_FILES}\n    ${UI_HDRS} ${RCS} )\ntarget_link_libraries( ${PROJECT_NAME} ${QT_LIBRARIES} )\n```\n\nSee\n\n-   [qt4 console application](qt4-console) (VS2010 :smile:, Linux :smile:)\n-   [qt4 GUI application](qt4-gui) (check out the [configs.h.in](qt4-gui/configs.h.in) file) (VS2010 :smile:, Linux :smile:)\n-   [qt4 application for lazy people](qt4-project), works like `qmake -project && qmake && make` on Linux (VS2010 :smile:, Linux :smile:)\n\n### 3.8. Example with Support of Qt5\n\n```cmake\nproject( Qt5Project )\ncmake_minimum_required( VERSION 2.8.11 )\n\n#                                           root of your msvc14 x64 prebuild\nset( CMAKE_PREFIX_PATH ${CMAKE_PREFIX_PATH} \"C:/Qt/Qt5-msvc14/5.6/msvc2015_64\" )\nset( CMAKE_INCLUDE_CURRENT_DIR ON )\nset( CMAKE_AUTOMOC ON )\n\nfind_package( Qt5Widgets REQUIRED )\nqt5_wrap_ui( UI_HEADERS mainwindow.ui )\nqt5_add_resources( QRCS resources.qrc )\nadd_executable( ${PROJECT_NAME} main.cpp mainwindow.cpp ${UI_HEADERS} ${QRCS} )\ntarget_link_libraries( ${PROJECT_NAME} Qt5::Widgets )\n```\n\nSee [qt5 project](qt5-project).\n\n### 3.9. Get'em Together (advanced examples)\n\nThis part is called CMake in Action.\n\n-   [ToyAuthor/luapp: Using lua in C++ style. Build system is CMake.](https://github.com/ToyAuthor/luapp)\n    -   a great cmake wrapper for lua, a great c++ wrapper for lua\n    -   I forked it, and annotated (in chinese), it's really great! My fork: [4ker/luapp: Using lua in C++ style. Build system is CMake.](https://github.com/4ker/luapp).\n-   [district10/algo: \u91cd\u590d\u9020\u8f6e\u5b50\u3002](https://github.com/district10/algo)\n    - Libs\n        -   google test (gmock), for testing and benchmarking, etc\n        -   cppformat, the missing string formating lib\n    -   modulized\n    -   advanced linking style\n-   <https://github.com/district10/bcp/tree/standalone>\n\n## 4. TODO\n\n-   More documentation\n-   More elegant & illustrative examples\n-   Planned Examples\n    -   for Windows, link `*.lib` files\n    -   for Linux, link `*.a`, `*.so` files, set `rpath`\n    -   *etc.*\n\n## 5. Snippets & Helper Functions\n\ncpp -> exe\n\n```cmake\nfile( GLOB SRCS src/*.cpp)\nforeach( src ${SRCS} )\n    string( REGEX REPLACE \"(^.*/|.cpp$)\" \"\" exe ${src} )\n    message( STATUS \"${exe} <-- ${src}\" )\n    add_executable( ${exe} ${src} )\nendforeach( src )\n```\n\n There are some utility functions in [`utilities.cmake`](utilities.cmake), use `include(utilities.cmake)` to include, then use\n - `print_include_directories()` to print all included directories,\n - `print_all_linked_libraries(your_exe_or_lib)` to print all linked libs,\n - `print_all_variables()` to print all variables\n\nTip, use `cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=YES ..` to generate compile commands (a json file).\n\n## 6. ReadingList\n\nThese links may be useful:\n\n-   [Search \u00b7 cmake templates](https://github.com/search?utf8=%E2%9C%93&q=cmake+templates)\n-   [giddie/qt-cmake-template: Project template using CMake / Qt / NSIS or WiX / MinGW or MSVS combined in easy-to-use form](https://github.com/giddie/qt-cmake-template)\n-   [cginternals/cmake-init: Template for reliable, cross-platform C++ project setup using cmake.](https://github.com/cginternals/cmake-init)\n\n## 7. Koan\n\n-   CMake's documentation is not for human. It really smells\n-   Adapt to various standards is by no means easy, it's kind of brain fucking\n"
 },
 {
  "repo": "PacktPublishing/Mastering-OpenCV-4-Third-Edition",
  "language": "Assembly",
  "readme_contents": "# Mastering OpenCV 4 - Third Edition\n\n<a href=\"https://www.packtpub.com/application-development/mastering-opencv-4-third-edition?utm_source=github&utm_medium=repository&utm_campaign=9781789533576 \"><img src=\"https://dz13w8afd47il.cloudfront.net/sites/default/files/imagecache/ppv4_main_book_cover/B11672_MockupCover_1.png\" alt=\"Mastering OpenCV 4 - Third Edition\" height=\"256px\" align=\"right\"></a>\n\nThis is the code repository for [Mastering OpenCV 4 - Third Edition](https://www.packtpub.com/application-development/mastering-opencv-4-third-edition?utm_source=github&utm_medium=repository&utm_campaign=9781789533576), published by Packt.\n\n**A comprehensive guide to building computer vision and image processing applications with C++**\n\n## What is this book about?\nMastering OpenCV, now in its third edition, targets computer vision engineers taking their first steps toward mastering OpenCV. Keeping the mathematical formulations to a solid but bare minimum, the book delivers complete projects from ideation to running code, targeting current hot topics in computer vision such as face recognition, landmark detection and pose estimation, and number recognition with deep convolutional networks.\n\nThis book covers the following exciting features:\n* Build real-world computer vision problems with working OpenCV code samples \n* Uncover best practices in engineering and maintaining OpenCV projects \n* Explore algorithmic design approaches for complex computer vision tasks \n* Work with OpenCV\u2019s most updated API (v4.0.0)through projects \n* Understand 3D scene reconstruction and Structure from Motion (SfM) \n* Study camera calibration and overlay AR using the ArUco Module \n\nIf you feel this book is for you, get your [copy](https://www.amazon.com/dp/1789533570) today!\n\n<a href=\"https://www.packtpub.com/?utm_source=github&utm_medium=banner&utm_campaign=GitHubBanner\"><img src=\"https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png\" \nalt=\"https://www.packtpub.com/\" border=\"5\" /></a>\n\n## Instructions and Navigations\nAll of the code is organized into folders. For example, Chapter02.\n\nThe code will look like the following:\n```\nMat bigImg; \nresize(smallImg, bigImg, size, 0,0, INTER_LINEAR); \ndst.setTo(0); \nbigImg.copyTo(dst, mask);\n```\n\n**Following is what you need for this book:**\nThis book is for those who have a basic knowledge of OpenCV and are competent C++ programmers. You need to have an understanding of some of the more theoretical/mathematical concepts, as we move quite quickly throughout the book.\n\nWith the following software and hardware list you can run all code files present in the book (Chapter 1-10).\n\n### Software and Hardware List\n\nEach chapter folder contains individual instruction on building and running the code.\nChapter 10: Avoiding Common Pitfalls in OpenCV, is without code.\n\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. [Click here to download it](http://www.packtpub.com/sites/default/files/downloads/9781789533576_ColorImages.pdf).\n\n### Related products\n* Learn OpenCV 4 By Building Projects - Second Edition [[Packt]](https://www.packtpub.com/application-development/learn-opencv-4-building-projects-second-edition?utm_source=github&utm_medium=repository&utm_campaign=9781789341225) [[Amazon]](https://www.amazon.com/dp/B07J9LYR9S)\n\n* Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA [[Packt]](https://www.packtpub.com/application-development/hands-gpu-accelerated-computer-vision-opencv-and-cuda?utm_source=github&utm_medium=repository&utm_campaign=9781789348293) [[Amazon]](https://www.amazon.com/dp/1789348293)\n\n## Get to Know the Authors\n**Roy Shilkrot**\nis an assistant professor of computer science at Stony Brook University, where he leads the Human Interaction group. Dr. Shilkrot's research is in computer vision, human-computer interfaces, and the cross-over between these two domains, funded by US federal, New York State, and industry grants. Dr. Shilkrot graduated from the Massachusetts Institute of Technology (MIT) with a PhD, and has authored more than 25 peer-reviewed papers published at premier computer science conferences, such as CHI and SIGGRAPH, as well as in leading academic journals such as ACM Transaction on Graphics (TOG) and ACM Transactions on Computer-Human Interaction (ToCHI). Dr. Shilkrot is also a co-inventor of several patented technologies, a co-author of a number of books, serves on the scientific advisory board of numerous start-up companies, and has over 10 years of experience as an engineer and an entrepreneur.\n\n**David Millan Escriva**\nwas eight years old when he wrote his first program on an 8086 PC in Basic, which enabled the 2D plotting of basic equations. In 2005, he finished his studies in IT through the Universitat Polit\u00e9cnica de Valenci with honors in human-computer interaction supported by computer vision with OpenCV (v0.96). He had a final project based on this subject and published it on HCI Spanish congress. He has worked with Blender, an open source, 3D software project, and worked on his first commercial movie, Plumiferos - Aventuras voladoras, as a computer graphics software developer. David now has more than 10 years of experience in IT, with experience in computer vision, computer graphics, and pattern recognition, working with different projects and start-ups, applying his knowledge of computer vision, optical character recognition, and augmented reality. He is the author of the DamilesBlog blog, where he publishes research articles and tutorials about OpenCV, computer vision in general, and optical character recognition algorithms.\n\n## Other books by the authors\n[Mastering OpenCV 3 - Second Edition](https://www.packtpub.com/application-development/mastering-opencv-3-second-edition?utm_source=github&utm_medium=repository&utm_campaign=9781786467171)\n\n[Mastering OpenCV with Practical Computer Vision Projects](https://www.packtpub.com/application-development/mastering-opencv-practical-computer-vision-projects?utm_source=github&utm_medium=repository&utm_campaign=9781849517829)\n\n### Suggestions and Feedback\n[Click here](https://docs.google.com/forms/d/e/1FAIpQLSdy7dATC6QmEL81FIUuymZ0Wy9vH1jHkvpY57OiMeKGqib_Ow/viewform) if you have any feedback or suggestions.\n"
 },
 {
  "repo": "bytefish/libfacerec",
  "language": "C++",
  "readme_contents": "# Description\n\n[libfacerec](http://www.github.com/bytefish/libfacerec) is a library for face recognition in OpenCV. It has been merged into OpenCV 2.4 (contrib module) and both implementations are synchronized. So if you are in (a recent) OpenCV 2.4: There is no need to compile libfacerec yourself, you have everything to get started. Note: Make sure to work on a recent OpenCV revision, if you want to be compatible with the very latest libfacerec version.\n\n<table>\n<tr>\n    <td><img src=\"https://github.com/bytefish/libfacerec/raw/master/doc/source/img/eigenfaces_opencv.png\" width=\"200\"></td>\n    <td><img src=\"https://github.com/bytefish/libfacerec/raw/master/doc/source/img/fisherface_reconstruction_opencv.png\" width=\"200\"></td>\n    <td><img src=\"https://github.com/bytefish/libfacerec/raw/master/doc/source/img/tutorial/facerec_video/facerec_video.png\" width=\"200\"></td>\n</tr>\n</table>\n\nThe library comes with an extensive documentation, which can be found at\n\n* [http://docs.opencv.org/trunk/modules/contrib/doc/facerec/index.html](http://docs.opencv.org/trunk/modules/contrib/doc/facerec/index.html)\n  \nThe documentation includes:\n\n* [The API (cv::FaceRecognizer)](http://docs.opencv.org/trunk/modules/contrib/doc/facerec/facerec_api.html)\n* [Guide to Face Recognition with OpenCV](http://docs.opencv.org/trunk/modules/contrib/doc/facerec/facerec_tutorial.html)\n* [Tutorial on Gender Classification](http://docs.opencv.org/trunk/modules/contrib/doc/facerec/tutorial/facerec_gender_classification.html)\n* **[Face Recognition in Videos](http://docs.opencv.org/trunk/modules/contrib/doc/facerec/tutorial/facerec_video_recognition.html)**\n\nThere are no additional dependencies to build the library. The Eigenfaces, Fisherfaces method and Local Binary Patterns Histograms (LBPH) are implemented and most parts of the library are covered by unit tests. As of OpenCV 2.4+ this library has been merged into the OpenCV contrib module, so if you are using OpenCV 2.4+ you can [start right away](http://code.opencv.org/projects/opencv/repository/entry/trunk/opencv/samples/cpp/facerec_demo.cpp). \n\nAgain note: This library is included in the contrib module of OpenCV.\n\n\n# Issues and Feature Requests\n\nThis project is now open for bug reports and feature requests.\n\n# Building the library with Microsoft Visual Studio 2008/2010\n\nIf you have problems with building libfacerec with Microsoft Visual Studio 2008/2010, then please read my blog post at:\n\n* [http://www.bytefish.de/blog/opencv_visual_studio_and_libfacerec](http://www.bytefish.de/blog/opencv_visual_studio_and_libfacerec)\n\nThis is based on version 0.04 of the libfacerec, available here:\n\n* [https://github.com/bytefish/libfacerec/zipball/v0.04](https://github.com/bytefish/libfacerec/zipball/v0.04)\n  \n# Literature\n\n* Eigenfaces (Turk, M., and Pentland, A. \"Eigenfaces for recognition.\". Journal of Cognitive Neuroscience 3 (1991), 71\u201386.)\n* Fisherfaces (Belhumeur, P. N., Hespanha, J., and Kriegman, D. \"Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection.\". IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 7 (1997), 711\u2013720.)\n* Local Binary Patterns Histograms (Ahonen, T., Hadid, A., and Pietikainen, M. \"Face Recognition with Local Binary Patterns.\". Computer Vision - ECCV 2004 (2004), 469\u2013481.)\n\n"
 },
 {
  "repo": "bluquar/cubr",
  "language": "Python",
  "readme_contents": "cubr\n====\n\nRubik's Cube Solver -- Uses OpenCV\n\n[Video and Description](http://cbarker.net/blog/projects/applications/cubr)\n\n[Live Web Release](http://cbarker.net/cubr)\n\nTo run:\n\n```\n> python qbr.py\n```\n\nDependencies:\n\n* Python 2.7\n* [OpenCV](http://opencv.org/downloads.html)\n"
 },
 {
  "repo": "krasserm/face-recognition",
  "language": "Jupyter Notebook",
  "readme_contents": "## Deep face recognition with Keras, Dlib and OpenCV\n\nThis repository hosts the [companion notebook](http://nbviewer.jupyter.org/github/krasserm/face-recognition/blob/master/face-recognition.ipynb?flush_cache=true) to the article [Deep face recognition with Keras, Dlib and OpenCV](https://krasserm.github.io/2018/02/07/deep-face-recognition/).\n"
 },
 {
  "repo": "bikz05/object-tracker",
  "language": "Python",
  "readme_contents": "# object-tracker\n\n![Image](docs/images/cover-image-1.png)\n\n_Real-Time Object Tracker written in Python using dlib and OpenCV_ | [Video 1](https://www.youtube.com/watch?v=uLZquACzXVg) [Video 2](https://youtu.be/VVo-5kVmDEY)\n\n---\n\n## Quick Links\n\n* [Dependencies](#dependencies)\n* [Starting the code](#starting-the-code)\n* [How to perform tracking](#how-to-perform-tracking)\n\n## Dependencies\n\n* [`Dlib` with Python support](http://dlib.net/)\n* [`OpenCV` with Python support](http://opencv.org)\n\n## Starting the code\n\nTracking can either be done using a live video from a webcam or using a video file.\n\n### Tracking using Video File\n\nTo run the code using a video file use the following command line argument --\n\n```shell\npython object-tracker-single.py -v <path-2-video-file>\n```\n\nFor example, you can use the demo video provided with this code as --\n\n```shell\npython object-tracker-single.py -v demo-video-single.avi\n```\n\n### Tracking using Live Video\n\nTo run the code using live video use the following command line argument --\n\n```shell\npython object-tracker-single.py -d <device-id>\n```\n\nFor example, on most systems device id 0 is the webcam attached.\n\n```shell:\npython object-tracker-single.py -v docs/demo-single-video.avi\n```\n\nUse the `-l` or `--dispLoc' command line option to display the locations of the tracked object as show below - \n\n\n```shell:\npython object-tracker-single.py -v docs/demo-single-video.avi -l\n```\n\n__NOTE__ -- If you want to do multi object tracking code, use the file `object-tracker-multi.py` instead of `object-tracker-single.py`. This is a hack to do multi-object tracking and __hence the code slows down__.\n\n## How to perform tracking\n\nOnce the code starts, it will start the video file or the live stream. To select the objects to be tracked, pause the video by pressing the <kbd>p</kbd> key.The next step is to create a bounding box around the object(s) to be tracked. Press the mouse to select the top-left pixel location of the object to be tracked and then release the mouse on the bottom-right location of the object to be tracked. Once, this is done press <kbd>p</kbd> to start the tracking. Also, if you want to discard the object, press the <kbd>d</kbd> key. In SINGLE OBJECT TRACKING MODE, you can only select one object but in MULTI OBJECT TRACKING MODE, you can select as many objects you want but at the cost of speed. Press <kbd>esc</kbd> anytime to gracefully quit the code.\n"
 },
 {
  "repo": "lzane/Fingers-Detection-using-OpenCV-and-Python",
  "language": "Python",
  "readme_contents": "> for people using python2 and opencv2, please check out the [`lzane:py2_opencv2`](https://github.com/lzane/Fingers-Detection-using-OpenCV-and-Python/tree/py2_opencv2) branch.\n\n> for people using opencv4, please change line 96 in the `new.py` to `contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)` according to the [opencv api change](https://github.com/lzane/Fingers-Detection-using-OpenCV-and-Python/issues/7#issuecomment-509925971).\n\n\n## Environment\n- OS: MacOS El Capitan\n- Platform: Python 3\n- Librarys: \n\t- OpenCV 3\n\t- appscript\n\n\n## Demo Videos\n- Youtube: [Click here](https://youtu.be/CmBxUnp7XwM)\n- Youku: [Click here](http://v.youku.com/v_show/id_XMTc3MjI4MjQwOA==.html)\n\n## How to run it?\n- run it in python\n- press `'b'` to capture the background model (Remember to move your hand out of the blue rectangle)\n- press `'r'` to reset the backgroud model\n- press `'ESC'` to exit\n\n## Process\n#### Capture original image\n\nCapture video from camera and pick up a frame.\n\n![Alt text](material/-1474508814843.png)\n\n#### Capture background model & Background subtraction\nUse background subtraction method called **Gaussian Mixture-based Background/Foreground Segmentation Algorithm** to subtract background. \n\nFor more information about the method, check [Zivkovic2004](http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf)\n\nHere I use the OpenCV's built-in function `BackgroundSubtractorMOG2` to subtract background.\n\n```python\nbgModel = cv2.BackgroundSubtractorMOG2(0, bgSubThreshold)\n```\n\nBuild a background subtractor model\n\n\n\n```python\nfgmask = bgModel.apply(frame)\n```\nApply the model to a frame\n\n\n```python\nres = cv2.bitwise_and(frame, frame, mask=fgmask)\n```\n\nGet the foreground(hand) image\n\n![Alt text](material/-1474508613267.png)\n\n#### Gaussian blur & Threshold\n```python\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n```\nFirst convert the image to gray scale.\n\n```python\nblur = cv2.GaussianBlur(gray, (blurValue, blurValue), 0)\n```\nBy Gaussian blurring, we create smooth transition from one color to another and reduce the edge content.\n\n![Alt text](material/-1474508640877.png)\n\n```python\nret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY)\n```\nWe use thresholding to create binary images from grayscale images. \n\n![Alt text](material/-1474508661044.png)\n\n\n#### Contour & Hull & Convexity \nWe now need to find out the hand contour from the binary image we created before and detect fingers (or in other words, recognize gestures)\n\n```python\ncontours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n```\nThis function will find all the contours from the binary image. We need to get the biggest contours (our hand) based on their area since we can assume that our hand will be the biggest contour in this situation. (it's obvious)\n\nAfter picking up our hand, we can create its hull and detect the defects by calling :\n```python\nhull = cv2.convexHull(res)\ndefects = cv2.convexityDefects(res, hull)\n```\n\n![Alt text](material/-1474508788185.png)\n\n\nNow we have the number of fingers. How to use this information? It's based on your imagination...\n\nI add in a keyboard simulation package named **appscript** as interface to control Chrome's dinosaur game.\n\n![Alt text](material/-1474522195081.png)\n\n----------------------\n## References & Tutorials\n\n1. OpenCV documentation: \nhttp://docs.opencv.org/2.4.13/\n2. Opencv python hand gesture recognition:\nhttp://creat-tabu.blogspot.com/2013/08/opencv-python-hand-gesture-recognition.html\n3. Mahaveerverma's hand gesture recognition project:\n[hand-gesture-recognition-opencv](https://github.com/mahaveerverma/hand-gesture-recognition-opencv)\n\n"
 },
 {
  "repo": "petercunha/Emotion",
  "language": "Python",
  "readme_contents": "# Emotion\nThis software recognizes human faces and their corresponding emotions from a video or webcam feed. Powered by OpenCV and Deep Learning.\n\n![Demo](https://github.com/petercunha/Emotion/blob/master/demo/demo.gif?raw=true)\n\n\n## Installation\n\nClone the repository:\n```\ngit clone https://github.com/petercunha/Emotion.git\ncd Emotion/\n```\n\nInstall these dependencies with `pip3 install <module name>`\n-\ttensorflow\n-\tnumpy\n-\tscipy\n-\topencv-python\n-\tpillow\n-\tpandas\n-\tmatplotlib\n-\th5py\n-\tkeras\n\nOnce the dependencies are installed, you can run the project.\n`python3 emotions.py`\n\n\n## To train new models for emotion classification\n\n- Download the fer2013.tar.gz file from [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n- Move the downloaded file to the datasets directory inside this repository.\n- Untar the file:\n`tar -xzf fer2013.tar`\n- Download train_emotion_classifier.py from orriaga's repo [here](https://github.com/oarriaga/face_classification/blob/master/src/train_emotion_classifier.py)\n- Run the train_emotion_classification.py file:\n`python3 train_emotion_classifier.py`\n\n\n## Deep Learning Model\n\nThe model used is from this [research paper](https://github.com/oarriaga/face_classification/blob/master/report.pdf) written by Octavio Arriaga, Paul G. Pl\u00f6ger, and Matias Valdenegro.\n\n![Model](https://i.imgur.com/vr9yDaF.png?1)\n\n\n## Credit\n\n* Computer vision powered by OpenCV.\n* Neural network scaffolding powered by Keras with Tensorflow.\n* Convolutional Neural Network (CNN) deep learning architecture is from this [research paper](https://github.com/oarriaga/face_classification/blob/master/report.pdf).\n* Pretrained Keras model and much of the OpenCV code provided by GitHub user [oarriaga](https://github.com/oarriaga).\n"
 },
 {
  "repo": "YuliangXiu/PoseFlow",
  "language": "C++",
  "readme_contents": "# Pose Flow\n\nOfficial implementation of [Pose Flow: Efficient Online Pose Tracking ](https://arxiv.org/abs/1802.00977).\n\n<p align='center'>\n    <img src=\"posetrack1.gif\", width=\"360\">\n    <img src=\"posetrack2.gif\", width=\"344\">\n</p>\n\nResults on PoseTrack Challenge validation set:\n\n1. Task2: Multi-Person Pose Estimation (mAP)\n<center>\n\n| Method | Head mAP | Shoulder mAP | Elbow mAP | Wrist mAP | Hip mAP | Knee mAP | Ankle mAP | Total mAP |\n|:-------|:-----:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Detect-and-Track(FAIR) | **67.5** | 70.2 | 62 | 51.7 | 60.7 | 58.7 | 49.8 | 60.6 |\n| **AlphaPose** | 66.7 | **73.3** | **68.3** | **61.1** | **67.5** | **67.0** | **61.3** | **66.5** |\n\n</center>\n\n2. Task3: Pose Tracking (MOTA)\n<center>\n\n| Method | Head MOTA | Shoulder MOTA | Elbow MOTA | Wrist MOTA | Hip MOTA | Knee MOTA | Ankle MOTA | Total MOTA | Total MOTP| Speed(FPS) |\n|:-------|:-----:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Detect-and-Track(FAIR) | **61.7** | 65.5 | 57.3 | 45.7 | 54.3 | 53.1 | 45.7 | 55.2 | 61.5 |Unknown|\n| **PoseFlow(DeepMatch)** | 59.8 | **67.0** | 59.8 | 51.6 | **60.0** | **58.4** | **50.5** | **58.3** | **67.8**|8|\n| **PoseFlow(OrbMatch)** | 59.0 | 66.8 | **60.0** | **51.8** | 59.4 | **58.4** | 50.3 | 58.0 | 62.2|24|\n\n</center>\n\n## Latest Features\n- Dec 2018: <strong>PoseFlow(General Version)</strong> released! Support ANY DATASET and pose tracking results visualization.\n- Oct 2018: Support generating correspondence files with ORB(OpenCV), 3X FASTER and no need to compile DeepMatching library. \n\n## Requirements\n\n- Python 2.7.13\n- OpenCV 3.4.2.16\n- OpenCV-contrib 3.4.2.16\n- tqdm 4.19.8\n\n## Installation\n\n1. Download PoseTrack Dataset from [PoseTrack](https://posetrack.net/) to `AlphaPose/PoseFlow/posetrack_data/`\n2. (Optional) Use [DeepMatching](http://lear.inrialpes.fr/src/deepmatching/) to extract dense correspondences between adjcent frames in every video, please refer to [DeepMatching Compile Error](https://github.com/MVIG-SJTU/AlphaPose/issues/97) to compile DeepMatching correctly\n\n```shell\npip install -r requirements.txt\n\ncd deepmatching\nmake clean all\nmake\ncd ..\n```\n\n## For Any Datasets (General Version)\n\n1. Using [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) to generate multi-person pose estimation results.\n\n```shell\n# pytorch version\npython demo.py --indir ${image_dir}$ --outdir ${results_dir}$\n\n# torch version\n./run.sh --indir ${image_dir}$ --outdir ${results_dir}$\n```\n\n2. Run pose tracking\n\n\n```shell\n# pytorch version\npython tracker-general.py --imgdir ${image_dir}$ \n                          --in_json ${results_dir}$/alphapose-results.json \n                          --out_json ${results_dir}$/alphapose-results-forvis-tracked.json\n                          --visdir ${render_dir}$\n\n# torch version\npython tracker-general.py --imgdir ${image_dir}$ \n                          --in_json ${results_dir}$/POSE/alpha-pose-results-forvis.json \n                          --out_json ${results_dir}$/POSE/alpha-pose-results-forvis-tracked.json\n                          --visdir ${render_dir}$\n```\n\n\n## For PoseTrack Dataset Evaluation (Paper Baseline)\n\n1. Using [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) to generate multi-person pose estimation results on videos with format like `alpha-pose-results-sample.json`.\n2. Using DeepMatching/ORB to generate correspondence files.\n\n```shell\n# Generate correspondences by DeepMatching\n# (More Robust but Slower)\npython matching.py --orb=0 \n\nor\n\n# Generate correspondences by Orb\n# (Faster but Less Robust)\npython matching.py --orb=1\n```\n\n3. Run pose tracking\n\n\n```shell\npython tracker-baseline.py --dataset=val/test  --orb=1/0\n```\n4. Evaluation\n\nOriginal [poseval](https://github.com/leonid-pishchulin/poseval) has some instructions on how to convert annotation files from MAT to JSON.\n\nEvaluate pose tracking results on validation dataset:\n\n```shell\ngit clone https://github.com/leonid-pishchulin/poseval.git --recursive\ncd poseval/py && export PYTHONPATH=$PWD/../py-motmetrics:$PYTHONPATH\ncd ../../\npython poseval/py/evaluate.py --groundTruth=./posetrack_data/annotations/val \\\n                    --predictions=./${track_result_dir}/ \\\n                    --evalPoseTracking --evalPoseEstimation\n```\n\n\n## Citation\n\nPlease cite these papers in your publications if it helps your research:\n\n    @inproceedings{xiu2018poseflow,\n      author = {Xiu, Yuliang and Li, Jiefeng and Wang, Haoyu and Fang, Yinghong and Lu, Cewu},\n      title = {{Pose Flow}: Efficient Online Pose Tracking},\n      booktitle={BMVC},\n      year = {2018}\n    }\n\n\n\n\n\n"
 },
 {
  "repo": "opencv/opencv_3rdparty",
  "language": null,
  "readme_contents": "### OpenCV: Open Source Computer Vision Library\n\nThis repository contains 3rdparty libraries used by OpenCV.\n\n#### OpenCV Resources\n\n* OpenCV repository: <https://github.com/opencv/opencv>\n* Homepage: <https://opencv.org>\n* Docs: <https://docs.opencv.org>\n* Q&A forum: <https://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n"
 },
 {
  "repo": "srianant/kalman_filter_multi_object_tracking",
  "language": "Python",
  "readme_contents": "Multi Object Tracker Using Kalman Filter & Hungarian Algorithm\n----\n- Author: Srini Ananthakrishnan  \n- Project: DSCI-6008 Final Project\n- Date: 07/14/2017\n\n\n- Usage:  \n$ python2.7 objectTracking.py  \n\n- Design:  \n\n<img src=\"images/KF_arch.png\" height=\"400\"/>  \n\n- Output:  \n\n<img src=\"images/KF_output.png\" height=\"400\"/>  \n\n- Pre-requisite:  \n    - Python2.7  \n    - Numpy  \n    - SciPy  \n    - Opencv 3.0 for Python - [Installation](http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/)\n    \n- References:    \n  - [Excellent MATLAB tutorial by Student Dave on object tracking](http://studentdavestutorials.weebly.com/multi-bugobject-tracking.html)   \n  - [OpenCV Tutorial: Multiple Object Tracking in Real Time by Kyle Hounslow](https://www.youtube.com/watch?annotation_id=annotation_307976421&feature=iv&src_vid=RS_uQGOQIdg&v=bSeFrPrqZ2A)  \n    \n"
 },
 {
  "repo": "richmondu/libfaceid",
  "language": "Python",
  "readme_contents": "# libfaceid, a Face Recognition library for everybody\r\n\r\n<p>\r\n    <b> FaceRecognition Made Easy.</b> libfaceid is a Python library for facial recognition that seamlessly integrates multiple face detection and face recognition models. \r\n</p>\r\n<p>\r\n    <b> From Zero to Hero.</b> Learn the basics of Face Recognition and experiment with different models.\r\n    libfaceid enables beginners to learn various models and simplifies prototyping of facial recognition solutions by providing a comprehensive list of models to choose from.\r\n    Multiple models for detection and encoding/embedding including classification models are supported from the basic models (Haar Cascades + LBPH) to the more advanced models (MTCNN + FaceNet).\r\n    The models are seamlessly integrated so that user can mix and match models. Each detector model has been made compatible with each embedding model to abstract you from the differences.\r\n    Each model differs in speed, accuracy, memory requirements and 3rd-party library dependencies.\r\n    This enables users to easily experiment with various solutions appropriate for their specific use cases and system requirements.\r\n    In addition, face liveness detection models are also provided for anti-face spoofing attacks (photo-based, video-based, 3d-mask-based attacks).\r\n</p>\r\n<p>\r\n    <b> Awesome Design.</b> The library is designed so that it is easy to use, modular and robust.\r\n    Selection of model is done via the constructors while the expose function is simply detect() or estimate() making usage very easy.\r\n    The files are organized into modules so it is very intuitive to understand and debug.\r\n    The robust design allows supporting new models in the future to be very straightforward.\r\n</p> \r\n<p>\r\n    <b> Extra Cool Features.</b> The library contains models for predicting your age, gender, emotion and facial landmarks.\r\n    It also contains TTS text-to-speech (speech synthesizer) and STT speech-to-text (speech recognition) models for voice-enabled and voice-activated capabilities.\r\n    Voice-enabled feature allows system to speak your name after recognizing your face.\r\n    Voice-activated feature allows system to listen for a specified word or phrase to trigger the system to do something (wake-word/trigger-word/hotword detection).\r\n    Web app is also supported for some test applications using Flask so you would be able to view the video capture remotely on another computer in the same network via a web browser. \r\n</p>\r\n\r\n\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid2.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid3.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid4.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid5.jpg)\r\n\r\n\r\n# News:\r\n\r\n| Date | Milestones |\r\n| --- | --- |\r\n| 2018, Dec 29 | Integrated [Colorspace histogram concatenation](https://github.com/ee09115/spoofing_detection) for anti-face spoofing (face liveness detection) |\r\n| 2018, Dec 26 | Integrated Google Cloud's STT speech-to-text (speech recognition) for voice-activated capability |\r\n| 2018, Dec 19 | Integrated Google's [Tacotron](https://github.com/keithito/tacotron) TTS text-to-speech (speech synthesis) for voice-enabled capability |\r\n| 2018, Dec 13 | Integrated Google's [FaceNet](https://github.com/davidsandberg/facenet) face embedding |\r\n| 2018, Nov 30 | Committed libfaceid to Github |\r\n\r\n\r\n# Background:\r\n\r\n<p>\r\nWith Apple incorporating face recognition technology in iPhone X last year, 2017 \r\nand with China implementing nation-wide wide-spread surveillance for social credit system in a grand scale, \r\nFace Recognition has become one of the most popular technologies where Deep Learning is used. \r\nFace recognition is used for identity authentication, access control, passport verification in airports, \r\nlaw enforcement, forensic investigations, social media platforms, disease diagnosis, police surveillance, \r\ncasino watchlists and many more.\r\n</p>\r\n\r\n<p>\r\nModern state of the art Face Recognition solutions leverages graphics processor technologies, GPU, \r\nwhich has dramatically improved over the decades. (In particular, Nvidia released the CUDA framework which allowed C and C++ applications to utilize the GPU for massive parallel computing.)\r\nIt utilizes Deep Learning (aka Neural Networks) which requires GPU power to perform massive compute operations in parallel. \r\nDeep Learning is one approach to Artificial Intelligence that simulates how the brain functions by teaching software through examples, several examples (big data), instead of harcoding the logic rules and decision trees in the software. \r\n(One important contribution in Deep Learning is the creation of ImageNet dataset. It pioneered the creation of millions of images, a big data collection of images that were labelled and classified to teach computer for image classifications.) \r\nNeural networks are basically layers of nodes where each nodes are connected to nodes in the next layer feeding information. \r\nDeepnets are very deep neural networks with several layers made possible using GPU compute power. \r\nMany neural networks topologies exists such as Convolutional Neural Networks (CNN) architecture \r\nwhich particulary applies to Computer Vision, from image classification to face recognition.\r\n</p>\r\n\r\n\r\n# Introduction:\r\n\r\n<p>\r\n    \r\nA facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. At a minimum, a simple real-time facial recognition system is composed of the following pipeline:\r\n\r\n0. <b>Face Enrollment.</b> Registering faces to a database which includes pre-computing the face embeddings and training a classifier on top of the face embeddings of registered individuals. \r\n1. <b>Face Capture.</b> Reading a frame image from a camera source.\r\n2. <b>Face Detection.</b> Detecting faces in a frame image.\r\n3. <b>Face Encoding/Embedding.</b> Generating a mathematical representation of each face (coined as embedding) in the frame image.\r\n4. <b>Face Identification.</b> Infering each face embedding in an image with face embeddings of known people in a database.\r\n\r\nMore complex systems include features such as <b>Face Liveness Detection</b> (to counter spoofing attacks via photo, video or 3d mask), face alignment, <b>face augmentation</b> (to increase the number of dataset of images) and face verification (to confirm prediction by comparing cosine similarity or euclidean distance with each database embedding).\r\n</p>\r\n\r\n\r\n# Problem:\r\n\r\n<p>\r\nlibfaceid democratizes learning Face Recognition. Popular models such as FaceNet and OpenFace are not straightforward to use and don't provide easy-to-follow guidelines on how to install and setup. So far, dlib has been the best in terms of documentation and usage but installation is not straightforward, it is slow on CPU and is highly abstracted (abstracts OpenCV as well). Simple models such as OpenCV is good but too basic and lacks documentation of the parameter settings, on classification algorithms and end-to-end pipeline. Pyimagesearch has been great having several tutorials with easy to understand explanations but not much emphasis on model comparisons and seems to aim to sell books so intentions to help the community are not so pure after all (I hate the fact that you need to wait for 2 marketing emails to arrive just to download the source code for the tutorials. But I love the fact that he replies to all questions in the threads). With all this said, I've learned a lot from all these resources so I'm sure you will learn a lot too. \r\n\r\nlibfaceid was created to somehow address these problems and fill-in the gaps from these resources. It seamlessly integrates multiple models for each step of the pipeline enabling anybody specially beginners in Computer Vision and Deep Learning to easily learn and experiment with a comprehensive face recognition end-to-end pipeline models. No strings attached. Once you have experimented will all the models and have chosen specific models for your specific use-case and system requirements, you can explore the more advanced models like FaceNet.\r\n\r\n</p>\r\n\r\n\r\n# Design:\r\n\r\n<p>\r\nlibfaceid is designed so that it is easy to use, modular and robust. Selection of model is done via the constructors while the expose function is simply detect() or estimate() making usage very easy. The files are organized into modules so it is very intuitive to understand and debug. The robust design allows supporting new models in the future to be very straightforward.\r\n\r\nOnly pretrained models will be supported. [Transfer learning](http://cs231n.github.io/transfer-learning/) is the practice of applying a pretrained model (that is trained on a very large dataset) to a new dataset. It basically means that it is able to generalize models from one dataset to another when it has been trained on a very large dataset, such that it is 'experienced' enough to generalize the learnings to new environment to new datasets. It is one of the major factors in the explosion of popularity in Computer Vision, not only for face recognition but most specially for object detection. And just recently, mid-2018 this year, transfer learning has been making good advances to Natural Language Processing ( [BERT by Google](https://github.com/google-research/bert) and [ELMo by Allen Institute](https://allennlp.org/elmo) ). Transfer learning is really useful and it is the main goal that the community working on Reinforcement Learning wants to achieve for robotics.\r\n</p>\r\n\r\n\r\n# Features:\r\n\r\nHaving several dataset of images per person is not possible for some use cases of Face Recognition. So finding the appropriate model for that balances accuracy and speed on target hardware platform (CPU, GPU, embedded system) is necessary. The trinity of AI is Data, Algorithms and Compute. libfaceid allows selecting each model/algorithm in the pipeline.\r\n\r\nlibfaceid library supports several models for each step of the Face Recognition pipeline. Some models are faster while some models are more accurate. You can mix and match the models for your specific use-case, hardware platform and system requirements. \r\n\r\n### Face Detection models for detecting face locations\r\n- [Haar Cascade Classifier via OpenCV](https://github.com/opencv/opencv/blob/master/samples/python/facedetect.py)\r\n- [Histogram of Oriented Gradients (HOG) via DLIB](http://dlib.net/face_detector.py.html)\r\n- [Deep Neural Network via DLIB](http://dlib.net/cnn_face_detector.py.html)\r\n- [Single Shot Detector with ResNet-10 via OpenCV](https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/resnet_ssd_face_python.py)\r\n- [Multi-task Cascaded CNN (MTCNN) via Tensorflow](https://github.com/ipazc/mtcnn/blob/master/tests/test_mtcnn.py)\r\n- [FaceNet MTCNN via Tensorflow](https://github.com/davidsandberg/facenet)\r\n\r\n### Face Encoding models for generating face embeddings on detected faces\r\n- [Local Binary Patterns Histograms (LBPH) via OpenCV](https://www.python36.com/face-recognition-using-opencv-part-3/)\r\n- [OpenFace via OpenCV](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/)\r\n- [ResNet-34 via DLIB](http://dlib.net/face_recognition.py.html)\r\n- [FaceNet (Inception ResNet v1) via Tensorflow](https://github.com/davidsandberg/facenet)\r\n- [VGG-Face (VGG-16, ResNet-50) via Keras](https://github.com/rcmalli/keras-vggface) - TODO\r\n- [OpenFace via Torch and Lua](https://github.com/cmusatyalab/openface) - TODO\r\n\r\n### Classification algorithms for Face Identification using face embeddings\r\n- [Na\u00efve Bayes](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\r\n- Linear SVM\r\n- RVF SVM\r\n- Nearest Neighbors\r\n- Decision Tree\r\n- Random Forest\r\n- Neural Net\r\n- Adaboost\r\n- QDA\r\n\r\n### Face Liveness Detection models for preventing spoofing attacks\r\n- [Eye Movement](https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/)\r\n- [Mouth Movement](https://github.com/mauckc/mouth-open)\r\n- [Colorspace Histogram Concatenation](https://github.com/ee09115/spoofing_detection)\r\n\r\n### Additional models (bonus features for PR): \r\n- TTS Text-To-Speech <b>(speech synthesis)</b> models for voice-enabled capability\r\n    - [PyTTSX3](https://pypi.org/project/pyttsx3/)\r\n    - [Tacotron](https://github.com/keithito/tacotron)\r\n    - [gTTS](https://pypi.org/project/gTTS/)\r\n- STT Speech-To-Text <b>(speech recognition)</b> models for voice-activated capability\r\n    - [GoogleCloud](https://pypi.org/project/SpeechRecognition/)\r\n    - [Wit.ai](https://wit.ai/)\r\n    - [Houndify](https://www.houndify.com/)\r\n    - PocketSphinx - TODO\r\n    - Snoyboy - TODO\r\n    - Precise - TODO\r\n- Face Pose estimator models for predicting face landmarks <b>(face landmark detection)</b>\r\n- Face Age estimator models for predicting age <b>(age detection)</b>\r\n- Face Gender estimator models for predicting gender <b>(gender detection)</b>\r\n- Face Emotion estimator models for predicting facial expression <b>(emotion detection)</b>\r\n\r\n\r\n# Compatibility:\r\n\r\n<p>\r\nThe library and example applications have been tested on Raspberry Pi 3B+ (Python 3.5.3) and Windows 7 (Python 3.6.6)\r\nusing <b>OpenCV</b> 3.4.3.18, <b>Tensorflow</b> 1.8.0 and <b>Keras</b> 2.0.8. \r\nFor complete dependencies, refer to requirements.txt. \r\nTested with built-in laptop camera and with a Logitech C922 Full-HD USB webcam.\r\n\r\nI encountered DLL issue with OpenCV 3.4.3.18 on my Windows 7 laptop. \r\nIf you encounter such issue, use OpenCV 3.4.1.15 or 3.3.1.11 instead.\r\nAlso note that opencv-python and opencv-contrib-python must always have the same version.\r\n</p>\r\n\r\n\r\n# Usage:\r\n\r\n### Installation:\r\n\r\n        1. Install Python 3 and Python PIP\r\n           Use Python 3.5.3 for Raspberry Pi 3B+ and Python 3.6.6 for Windows\r\n        2. Install the required Python PIP package dependencies using requirements.txt\r\n           pip install -r requirements.txt\r\n\r\n           This will install the following dependencies below:\r\n           opencv-python==3.4.3.18\r\n           opencv-contrib-python==3.4.3.18\r\n           numpy==1.15.4\r\n           imutils==0.5.1\r\n           scipy==1.1.0\r\n           scikit-learn==0.20.0\r\n           mtcnn==0.0.8\r\n           tensorflow==1.8.0\r\n           keras==2.0.8\r\n           h5py==2.8.0\r\n           facenet==1.0.3\r\n           flask==1.0.2\r\n           dlib==19.16.0 # requires CMake\r\n           \r\n           // Installing dlib\r\n           1. Install cmake from https://cmake.org/download/ OR \r\n           2. pip install https://files.pythonhosted.org/packages/0e/ce/f8a3cff33ac03a8219768f0694c5d703c8e037e6aba2e865f9bae22ed63c/dlib-19.8.1-cp36-cp36m-win_amd64.whl#sha256=794994fa2c54e7776659fddb148363a5556468a6d5d46be8dad311722d54bfcf \r\n\r\n\r\n        3. Optional: Install the required Python PIP package dependencies for speech synthesizer and speech recognition for voice capability \r\n           pip install -r requirements_with_voicecapability.txt\r\n\r\n           This will install additional dependencies below:\r\n           playsound==1.2.2\r\n           inflect==0.2.5\r\n           librosa==0.4.2\r\n           unidecode==0.4.20\r\n           pyttsx3==2.7\r\n           gtts==2.0.3\r\n           speechrecognition==3.8.1\r\n\r\n           Additional items to install: \r\n           On Windows, install pypiwin32 using \"pip install pypiwin32==223\"\r\n           On RPI, \r\n               sudo apt-get install espeak\r\n               sudo apt-get install python-espeak\r\n               sudo apt-get install portaudio19-dev\r\n               pip3 install pyaudio\r\n               [Microphone Setup on RPI](https://iotbytes.wordpress.com/connect-configure-and-test-usb-microphone-and-speaker-with-raspberry-pi/)\r\n\r\n\r\n### Quickstart (Dummy Guide):\r\n\r\n        1. Add your dataset\r\n           ex. datasets/person1/1.jpg, datasets/person2/1.jpg\r\n        2. Train your model with your dataset\r\n           Update training.bat to specify your chosen models\r\n           Run training.bat\r\n        3. Test your model\r\n           Update testing_image.bat to specify your chosen models\r\n           Run testing_image.bat\r\n\r\n\r\n### Folder structure:\r\n\r\n        libfaceid\r\n        |\r\n        |   agegenderemotion_webcam.py\r\n        |   testing_image.py\r\n        |   testing_webcam.py\r\n        |   testing_webcam_livenessdetection.py\r\n        |   testing_webcam_voiceenabled.py\r\n        |   testing_webcam_voiceenabled_voiceactivated.py\r\n        |   training.py\r\n        |   requirements.txt\r\n        |   requirements_with_voicecapability.txt\r\n        |   \r\n        +---libfaceid\r\n        |   |   age.py\r\n        |   |   classifier.py\r\n        |   |   detector.py\r\n        |   |   emotion.py\r\n        |   |   encoder.py\r\n        |   |   gender.py\r\n        |   |   liveness.py\r\n        |   |   pose.py\r\n        |   |   speech_synthesizer.py\r\n        |   |   speech_recognizer.py\r\n        |   |   __init__.py\r\n        |   |   \r\n        |   \\---tacotron\r\n        |           \r\n        +---models\r\n        |   +---detection\r\n        |   |       deploy.prototxt\r\n        |   |       haarcascade_frontalface_default.xml\r\n        |   |       mmod_human_face_detector.dat\r\n        |   |       res10_300x300_ssd_iter_140000.caffemodel\r\n        |   |       \r\n        |   +---encoding\r\n        |   |       dlib_face_recognition_resnet_model_v1.dat\r\n        |   |       facenet_20180402-114759.pb\r\n        |   |       openface_nn4.small2.v1.t7\r\n        |   |       shape_predictor_5_face_landmarks.dat\r\n        |   |           \r\n        |   +---estimation\r\n        |   |       age_deploy.prototxt\r\n        |   |       age_net.caffemodel\r\n        |   |       emotion_deploy.json\r\n        |   |       emotion_net.h5\r\n        |   |       gender_deploy.prototxt\r\n        |   |       gender_net.caffemodel\r\n        |   |       shape_predictor_68_face_landmarks.dat\r\n        |   |       shape_predictor_68_face_landmarks.jpg\r\n        |   |               \r\n        |   +---liveness\r\n        |   |       colorspace_ycrcbluv_print.pkl\r\n        |   |       colorspace_ycrcbluv_replay.pkl\r\n        |   |       shape_predictor_68_face_landmarks.dat\r\n        |   |               \r\n        |   +---synthesis\r\n        |   |   \\---tacotron-20180906\r\n        |   |           model.ckpt.data-00000-of-00001\r\n        |   |           model.ckpt.index\r\n        |   |           \r\n        |   \\---training // This is generated during training (ex. facial_recognition_training.py)\r\n        |           dlib_le.pickle\r\n        |           dlib_re.pickle\r\n        |           facenet_le.pickle\r\n        |           facenet_re.pickle\r\n        |           lbph.yml\r\n        |           lbph_le.pickle\r\n        |           openface_le.pickle\r\n        |           openface_re.pickle\r\n        |\r\n        +---audiosets // This is generated during training (ex. facial_recognition_training.py)\r\n        |       Person1.wav or Person1.mp3\r\n        |       Person2.wav or Person2.mp3\r\n        |       Person3.wav or Person3.mp3\r\n        |       \r\n        +---datasets // This is generated by user\r\n        |   +---Person1\r\n        |   |       1.jpg\r\n        |   |       2.jpg\r\n        |   |       ...\r\n        |   |       X.jpg\r\n        |   |       \r\n        |   +---Person2\r\n        |   |       1.jpg\r\n        |   |       2.jpg\r\n        |   |       ...\r\n        |   |       X.jpg\r\n        |   |       \r\n        |   \\---Person3\r\n        |           1.jpg\r\n        |           2.jpg\r\n        |           ...\r\n        |           X.jpg\r\n        |           \r\n        \\---templates\r\n\r\n\r\n### Pre-requisites:\r\n\r\n        1. Add the dataset of images under the datasets directory\r\n           The datasets folder should be in the same location as the test applications.\r\n           Having more images per person makes accuracy much better.\r\n           If only 1 image is possible, then do data augmentation.\r\n             Example:\r\n             datasets/Person1 - contain images of person name Person1\r\n             datasets/Person2 - contain images of person named Person2 \r\n             ...\r\n             datasets/PersonX - contain images of person named PersonX \r\n        2. Train the model using the datasets. \r\n           Can use training.py\r\n           Make sure the models used for training is the same for actual testing for better accuracy.\r\n\r\n\r\n### Examples:\r\n\r\n        detector models:           0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\r\n        encoder models:            0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\r\n        classifier algorithms:     0-NAIVE_BAYES, 1-LINEAR_SVM, 2-RBF_SVM, 3-NEAREST_NEIGHBORS, 4-DECISION_TREE, 5-RANDOM_FOREST, 6-NEURAL_NET, 7-ADABOOST, 8-QDA\r\n        liveness models:           0-EYESBLINK_MOUTHOPEN, 1-COLORSPACE_YCRCBLUV\r\n        speech synthesizer models: 0-TTSX3, 1-TACOTRON, 2-GOOGLECLOUD\r\n        speech recognition models: 0-GOOGLECLOUD, 1-WITAI, 2-HOUNDIFY\r\n        camera resolution:         0-QVGA, 1-VGA, 2-HD, 3-FULLHD\r\n\r\n        1. Training with datasets\r\n            Usage: python training.py --detector 0 --encoder 0 --classifier 0\r\n            Usage: python training.py --detector 0 --encoder 0 --classifier 0 --setsynthesizer True --synthesizer 0\r\n\r\n        2. Testing with images\r\n            Usage: python testing_image.py --detector 0 --encoder 0 --image datasets/rico/1.jpg\r\n\r\n        3. Testing with a webcam\r\n            Usage: python testing_webcam.py --detector 0 --encoder 0 --webcam 0 --resolution 0\r\n            Usage: python testing_webcam_flask.py\r\n                   Then open browser and type http://127.0.0.1:5000 or http://ip_address:5000\r\n                \r\n        4. Testing with a webcam with anti-spoofing attacks\r\n            Usage: python testing_webcam_livenessdetection.py --detector 0 --encoder 0 --liveness 0 --webcam 0 --resolution 0\r\n\r\n        5. Testing with voice-control\r\n            Usage: python testing_webcam_voiceenabled.py --detector 0 --encoder 0 --speech_synthesizer 0 --webcam 0 \r\n            Usage: python testing_webcam_voiceenabled_voiceactivated.py --detector 0 --encoder 0 --speech_synthesizer 0 --speech_recognition 0 --webcam 0 --resolution 0\r\n\r\n        6. Testing age/gender/emotion detection\r\n            Usage: python agegenderemotion_webcam.py --detector 0 --webcam 0 --resolution 0\r\n            Usage: python agegenderemotion_webcam_flask.py\r\n                   Then open browser and type http://127.0.0.1:5000 or http://ip_address:5000\r\n\r\n\r\n### Training models with dataset of images:\r\n\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.classifier  import FaceClassifierModels\r\n\r\n        INPUT_DIR_DATASET         = \"datasets\"\r\n        INPUT_DIR_MODEL_DETECTION = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING  = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING  = \"models/training/\"\r\n\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=True)\r\n        face_encoder.train(face_detector, path_dataset=INPUT_DIR_DATASET, verify=verify, classifier=FaceClassifierModels.NAIVE_BAYES)\r\n\r\n        // generate audio samples for image datasets using text to speech synthesizer\r\n        OUTPUT_DIR_AUDIOSET       = \"audiosets/\"\r\n        INPUT_DIR_MODEL_SYNTHESIS = \"models/synthesis/\"\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=INPUT_DIR_MODEL_SYNTHESIS, path_output=OUTPUT_DIR_AUDIOSET)\r\n        speech_synthesizer.synthesize_datasets(INPUT_DIR_DATASET)\r\n\r\n\r\n### Face Recognition on images:\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n\r\n        INPUT_DIR_MODEL_DETECTION = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING  = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING  = \"models/training/\"\r\n\r\n        image = cv2.VideoCapture(imagePath)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n\r\n        frame = image.read()\r\n        faces = face_detector.detect(frame)\r\n        for (index, face) in enumerate(faces):\r\n            face_id, confidence = face_encoder.identify(frame, face)\r\n            label_face(frame, face, face_id, confidence)\r\n        cv2.imshow(window_name, frame)\r\n        cv2.waitKey(5000)\r\n\r\n        image.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Basic Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n\r\n        INPUT_DIR_MODEL_DETECTION = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING  = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING  = \"models/training/\"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Real-Time Face Recognition With Liveness Detection (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.liveness import FaceLivenessModels, FaceLiveness\r\n\r\n        INPUT_DIR_MODEL_DETECTION  = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING   = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING   = \"models/training/\"\r\n        INPUT_DIR_MODEL_ESTIMATION = \"models/estimation/\"\r\n        INPUT_DIR_MODEL_LIVENESS   = \"models/liveness/\"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        face_liveness = FaceLiveness(model=model_liveness, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_liveness2 = FaceLiveness(model=FaceLivenessModels.COLORSPACE_YCRCBLUV, path=INPUT_DIR_MODEL_LIVENESS)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n\r\n                // Check if eyes are close and if mouth is open\r\n                eyes_close, eyes_ratio = face_liveness.is_eyes_close(frame, face)\r\n                mouth_open, mouth_ratio = face_liveness.is_mouth_open(frame, face)\r\n\r\n                // Detect if frame is a print attack or replay attack based on colorspace\r\n                is_fake_print  = face_liveness2.is_fake(frame, face)\r\n                is_fake_replay = face_liveness2.is_fake(frame, face, flag=1)\r\n\r\n                // Identify face only if it is not fake and eyes are open and mouth is close\r\n                if is_fake_print or is_fake_replay:\r\n                    face_id, confidence = (\"Fake\", None)\r\n                elif not eyes_close and not mouth_open:\r\n                    face_id, confidence = face_encoder.identify(frame, face)\r\n\r\n                label_face(frame, face, face_id, confidence)\r\n\r\n            // Monitor eye blinking and mouth opening for liveness detection\r\n            total_eye_blinks, eye_counter = monitor_eye_blinking(eyes_close, eyes_ratio, total_eye_blinks, eye_counter, eye_continuous_close)\r\n            total_mouth_opens, mouth_counter = monitor_mouth_opening(mouth_open, mouth_ratio, total_mouth_opens, mouth_counter, mouth_continuous_open)\r\n\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Voice-Enabled Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n\r\n        INPUT_DIR_MODEL_DETECTION = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING  = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING  = \"models/training/\"\r\n        INPUT_DIR_AUDIOSET        = \"audiosets\"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=None, path_output=None, training=False)\r\n\r\n        frame_count = 0\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n                if (frame_count % 120 == 0):\r\n                    // Speak the person's name\r\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n            frame_count += 1\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Voice-Activated and Voice-Enabled Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n        from libfaceid.speech_recognizer  import SpeechRecognizerModels,  SpeechRecognizer\r\n\r\n        trigger_word_detected = False\r\n        def speech_recognizer_callback(word):\r\n            print(\"Trigger word detected! '{}'\".format(word))\r\n            trigger_word_detected = True\r\n\r\n        INPUT_DIR_MODEL_DETECTION = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING  = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING  = \"models/training/\"\r\n        INPUT_DIR_AUDIOSET        = \"audiosets\"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder  = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=None, path_output=None, training=False)\r\n        speech_recognizer  = SpeechRecognizer(model=SpeechRecognizerModels.DEFAULT, path=None)\r\n\r\n        // Wait for trigger word/wake word/hot word before starting face recognition\r\n        TRIGGER_WORDS = [\"Hey Google\", \"Alexa\", \"Activate\", \"Open Sesame\"]\r\n        print(\"\\nWaiting for a trigger word: {}\".format(TRIGGER_WORDS))\r\n        speech_recognizer.start(TRIGGER_WORDS, speech_recognizer_callback)\r\n        while (trigger_word_detected == False):\r\n            time.sleep(1)\r\n        speech_recognizer.stop()\r\n\r\n        // Start face recognition\r\n        frame_count = 0\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n                if (frame_count % 120 == 0):\r\n                    // Speak the person's name\r\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n            frame_count += 1\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Real-Time Face Pose/Age/Gender/Emotion Estimation (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.pose import FacePoseEstimatorModels, FacePoseEstimator\r\n        from libfaceid.age import FaceAgeEstimatorModels, FaceAgeEstimator\r\n        from libfaceid.gender import FaceGenderEstimatorModels, FaceGenderEstimator\r\n        from libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\r\n\r\n        INPUT_DIR_MODEL_DETECTION       = \"models/detection/\"\r\n        INPUT_DIR_MODEL_ENCODING        = \"models/encoding/\"\r\n        INPUT_DIR_MODEL_TRAINING        = \"models/training/\"\r\n        INPUT_DIR_MODEL_ESTIMATION      = \"models/estimation/\"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_pose_estimator = FacePoseEstimator(model=FacePoseEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_age_estimator = FaceAgeEstimator(model=FaceAgeEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_gender_estimator = FaceGenderEstimator(model=FaceGenderEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_emotion_estimator = FaceEmotionEstimator(model=FaceEmotionEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                age = face_age_estimator.estimate(frame, face_image)\r\n                gender = face_gender_estimator.estimate(frame, face_image)\r\n                emotion = face_emotion_estimator.estimate(frame, face_image)\r\n                shape = face_pose_estimator.detect(frame, face)\r\n                face_pose_estimator.add_overlay(frame, shape)\r\n                label_face(age, gender, emotion)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n\r\n# Case Study - Face Recognition for Identity Authentication:\r\n\r\nOne of the use cases of face recognition is for security identity authentication.\r\nThis is a convenience feature to authenticate with system using one's face instead of inputting passcode or scanning fingerprint. Passcode is often limited by the maximum number of digits allowed while fingerprint scanning often has problems with wet fingers or dry skin. Face authentication offers a more reliable and secure way to authenticate.\r\n\r\nWhen used for identity authentication, face recognition specifications will differ a lot from general face recognition systems like Facebook's automated tagging and Google's search engine; it will be more like Apple's Face ID in IPhone X. Below are guidelines for drafting specifications for your face recognition solution. Note that [Apple's Face ID technology](https://support.apple.com/en-us/HT208109) will be used as the primary baseline in this case study of identity authentication use case of face recognition. Refer to this [Apple's Face ID white paper](https://www.apple.com/business/site/docs/FaceID_Security_Guide.pdf) for more information.\r\n\r\n\r\n### Face Enrollment\r\n\r\n- Should support dynamic enrollment of faces. Tied up with the maximum number of users the existing system supports.\r\n- Should ask user to move/rotate face (in a circular motion) in order to capture different angles of the face. This gives the system enough flexbility to recognize you at different face angles.\r\n- IPhone X Face ID face enrollment is done twice for some reason. It is possible that the first scan is for liveness detection only.\r\n- How many images should be captured? We can store as much image as possible for better accuracy but memory footprint is the limiting factor. Estimate based on size of 1 picture and the maximum number of users.\r\n- For security purposes and memory related efficiency, images used during enrollment should not be saved. \r\nOnly the mathematical representations (128-dimensional vector) of the face should be used.\r\n\r\n\r\n### Face Capture\r\n\r\n- Camera will be about 1 foot away from user (Apple Face ID: 10-20 inches).\r\n- Camera resolution will depend on display panel size and display resolutions. QVGA size is acceptable for embedded solutions. \r\n- Take into consideration a bad lighting and extremely dark situation. Should camera have a good flash/LED to emit some light. Iphone X has an infrared light to better perform on dark settings.\r\n\r\n\r\n### Face Detection\r\n\r\n- Only 1 face per frame is detected.\r\n- Face is expected to be within a certain location (inside a fixed box or circular region).\r\n- Detection of faces will be triggered by a user action - clicking some button. (Not automatic detection).\r\n- Face alignment may not be helpful as users can be enforced or directed to have his face inside a fixed box or circular region so face is already expected to be aligned for the most cases. But if adding this feature does not affect speed performance, then face alignment ahould be added if possible.\r\n- Should verify if face is alive via anti-spoofing techniques against picture-based attacks, video-based attacks and 3D mask attacks. Two popular example of liveness detection is detecting eye blinking and mouth opening. \r\n\r\n\r\n### Face Encoding/Embedding\r\n\r\n- Speed is not a big factor. Face embedding and face identification can take 3-5 seconds.\r\n- Accuracy is critically important. False match rate should be low as much as possible. \r\n- Can do multiple predictions and get the highest count. Or apply different models for predictions for double checking.\r\n\r\n\r\n### Face Identification\r\n\r\n- Recognize only when eyes are not closed and mouth is not open\r\n- Images per person should at least be 50 images. Increase the number of images per person by cropping images with different face backgound margin, slight rotations, flipping and scaling.\r\n- Classification model should consider the maximum number of users to support. For example, SVM is known to be good for less than 100k classes/persons only.\r\n- Should support unknown identification by setting a threshold on the best prediction. If best prediction is too low, then consider as Unknown.\r\n- Set the number of consecutive failed attempts allowed before disabling face recognition feature. Should fallback to passcode authentication if identification encounters trouble recognizing people.\r\n- Images used for successful scan should be added to the existing dataset images during face enrollment making it adaptive and updated so that a person can be recognized with better accuracy in the future even with natural changes in the face appearance (hairstyle, mustache, pimples, etc.)\r\n\r\nIn addition to these guidelines, the face recognition solution should provide a way to disable/enable this feature as well as resetting the stored datasets during face enrollment.\r\n\r\n\r\n\r\n# Case Study - Face Recognition for Home/Office/Hotel Greeting System:\r\n\r\nOne of the use cases of face recognition is for greeting system used in smart homes, office and hotels.\r\nTo enable voice capability feature, we use text-to-speech synthesis to dynamically create audio files given some input text. \r\n\r\n### Speech Synthesis\r\n\r\nSpeech synthesis is the artificial simulation of human speech by a computer device.\r\nIt is mostly used for translating text into audio to make the system voice-enabled.\r\nProducts such as Apple's Siri, Microsoft's Cortana, Amazon Echo and Google Assistant uses speech synthesis.\r\nA good speech synthesizer is one that produces accurate outputs that naturally sounds like a real human in near real-time.\r\nState-of-the-art speech synthesis includes [Deepmind's WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) \r\nand [Google's Tacotron](https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1452.html).\r\n\r\nSpeech Synthesis can be used for some use-cases of Face Recognition to enable voice capability feature.\r\nOne example is to greet user as he approaches the terminal or kiosk system.\r\nGiven some input text, the speech synthesizer can generate an audio which can be played upon recognizing a face.\r\nFor example, upon detecting person arrival, it can be set to say 'Hello PersonX, welcome back...'. \r\nUpon departure, it can be set to say 'Goodbye PersonX, see you again soon...'.\r\nIt can be used in smart homes, office lobbies, luxury hotel rooms, and modern airports. \r\n\r\n### Face Enrollment\r\n\r\n- For each person who registers/enrolls to the system, create an audio file \"PersonX.wav\" for some input text such as \"Hello PersonX\".\r\n  \r\n### Face Identification\r\n\r\n- When a person is identified to be part of the database, we play the corresponding audio file \"PersonX.wav\". \r\n\r\n\r\n\r\n# Performance Optimizations:\r\n\r\nSpeed and accuracy is often a trade-off. Performance can be optimized depending on your specific use-case and system requirements. Some models are optimized for speed while others are optimized for accuracy. Be sure to test all the provided models to determine the appropriate model for your specific use-case, target platform (CPU, GPU or embedded) and specific requirements. Below are additional suggestions to optimize performance.\r\n\r\n### Speed\r\n- Reduce the frame size for face detection.\r\n- Perform face recognition every X frames only\r\n- Use threading in reading camera source frames or in processing the camera frames.\r\n- Update the library and configure the parameters directly.\r\n\r\n### Accuracy\r\n- Add more datasets if possible (ex. do data augmentation). More images per person will often result to higher accuracy.\r\n- Add face alignment if faces in the datasets are not aligned or when faces may be unaligned in actual deployment.\r\n- Update the library and configure the parameters directly.\r\n\r\n\r\n\r\n# References:\r\n\r\nBelow are links to valuable resoures. Special thanks to all of these guys for sharing their work on Face Recognition. Without them, learning Face Recognition would be difficult.\r\n\r\n### Codes\r\n- [OpenCV tutorials by Adrian Rosebrock](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/)\r\n- [Dlib by Davis King](https://github.com/davisking/dlib)\r\n- [Face Recognition (Dlib wrapper) by Adam Geitgey](https://github.com/ageitgey/face_recognition)\r\n- [FaceNet implementation by David Sandberg](https://github.com/davidsandberg/facenet)\r\n- [OpenFace (FaceNet implementation) by Satyanarayanan](https://github.com/cmusatyalab/openface)\r\n- [VGG-Face implementation by Refik Can Malli](https://github.com/rcmalli/keras-vggface)\r\n\r\nGoogle and Facebook have access to large database of pictures being the best search engine and social media platform, respectively. Below are the face recognition models they have designed for their own system. Be sure to take time to read these papers for better understanding of high-quality face recognition models. \r\n\r\n### Papers\r\n- [FaceNet paper by Google](https://arxiv.org/pdf/1503.03832.pdf)\r\n- [DeepFace paper by Facebook](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)\r\n\r\n\r\n\r\n# Contribute:\r\n\r\nHave a good idea for improving libfaceid? Please message me in [twitter](https://twitter.com/richmond_umagat).\r\nIf libfaceid has helped you in learning or prototyping face recognition system, please be kind enough to give this repository a 'Star'.\r\n"
 },
 {
  "repo": "danforthcenter/plantcv",
  "language": "Python",
  "readme_contents": "![builds](https://github.com/danforthcenter/plantcv/workflows/builds/badge.svg)\n[![codecov](https://codecov.io/gh/danforthcenter/plantcv/branch/master/graph/badge.svg)](https://codecov.io/gh/danforthcenter/plantcv)\n[![Documentation Status](http://readthedocs.org/projects/plantcv/badge/?version=latest)](http://plantcv.readthedocs.io/en/latest/?badge=latest)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/danforthcenter/plantcv-binder.git/master?filepath=index.ipynb)\n[![Docker Pulls](https://img.shields.io/docker/pulls/danforthcenter/plantcv.svg)](https://hub.docker.com/r/danforthcenter/plantcv/)\n[![GitHub release](https://img.shields.io/github/release/danforthcenter/plantcv.svg)](https://github.com/danforthcenter/plantcv/releases)\n[![PyPI version](https://badge.fury.io/py/plantcv.svg)](https://badge.fury.io/py/plantcv)\n![Conda](https://img.shields.io/conda/v/conda-forge/plantcv)\n[![license](https://img.shields.io/github/license/danforthcenter/plantcv.svg)](https://github.com/danforthcenter/plantcv/blob/master/LICENSE)\n\n# PlantCV: Plant phenotyping using computer vision\n\nPlease use, cite, and [contribute to](http://plantcv.readthedocs.io/en/latest/CONTRIBUTING/) PlantCV!\nIf you have questions, please submit them via the\n[GitHub issues page](https://github.com/danforthcenter/plantcv/issues).\nFollow us on twitter [@plantcv](https://twitter.com/plantcv).\n\n***\n\n## Introduction to PlantCV\n\nPlantCV is an open-source image analysis software package targeted for plant phenotyping. PlantCV provides a common\nprogramming and documentation interface to a collection of image analysis techniques that are integrated from a variety\nof source packages and algorithms. PlantCV utilizes a modular architecture that enables flexibility in the design of\nanalysis workflows and rapid assimilation and integration of new methods. For more information about the project,\nlinks to recorded presentations, and publications using PlantCV, please visit our homepage: \n<https://plantcv.danforthcenter.org/>.\n\n### Quick Links\n\n* [Documentation](http://plantcv.readthedocs.io/)\n* [Interactive Documentation](https://mybinder.org/v2/gh/danforthcenter/plantcv-binder.git/master?filepath=index.ipynb)\n* [Installation Instructions](https://plantcv.readthedocs.io/en/stable/installation/)\n* [Updating/Changelog](https://plantcv.readthedocs.io/en/stable/updating/)\n* [Public Image Datasets](http://plantcv.danforthcenter.org/pages/data.html)\n* [Contribution Guide](https://plantcv.readthedocs.io/en/stable/CONTRIBUTING/)\n* [Code of Conduct](https://plantcv.readthedocs.io/en/stable/CODE_OF_CONDUCT/)\n* Downloads\n  * [GitHub](https://github.com/danforthcenter/plantcv)\n  * [PyPI](https://pypi.org/project/plantcv/)\n  * [Conda-forge](https://anaconda.org/conda-forge/plantcv)\n  * [Docker](https://hub.docker.com/r/danforthcenter/plantcv)\n  * [Zenodo](https://doi.org/10.5281/zenodo.595522)\n\n### Citing PlantCV\n\nIf you use PlantCV, please cite the [PlantCV publications](https://plantcv.danforthcenter.org/#plantcv-publications)\nrelevant to your work. To see how others have used PlantCV in their research, check out our list of \n[publications using PlantCV](https://plantcv.danforthcenter.org/#publications-using-plantcv).\n\n***\n\n## Issues with PlantCV\n\nPlease file any PlantCV suggestions/issues/bugs via our \n[GitHub issues page](https://github.com/danforthcenter/plantcv/issues). Please check to see if any related \nissues have already been filed.\n\n***\n"
 },
 {
  "repo": "WPIRoboticsProjects/GRIP",
  "language": "Java",
  "readme_contents": "![logo](https://cloud.githubusercontent.com/assets/3964980/11156885/6fa1967a-8a1c-11e5-8c78-e552ffba31c0.png)\n\n[![Join the chat at https://gitter.im/WPIRoboticsProjects/GRIP](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/WPIRoboticsProjects/GRIP?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://dev.azure.com/wpiroboticsprojects/GRIP/_apis/build/status/WPIRoboticsProjects.GRIP?branchName=master)](https://dev.azure.com/wpiroboticsprojects/GRIP/_build/latest?definitionId=1?branchName=master)\n[![codecov.io](http://codecov.io/github/WPIRoboticsProjects/GRIP/coverage.svg?branch=master)](http://codecov.io/github/WPIRoboticsProjects/GRIP?branch=master)\n[![Github Releases](https://img.shields.io/github/downloads/WPIRoboticsProjects/GRIP/total.svg)](https://github.com/WPIRoboticsProjects/GRIP/releases/latest)\n\n\n# GRIP Computer Vision Engine\n\nGRIP (the Graphically Represented Image Processing engine) is an application for rapidly prototyping and deploying computer\nvision algorithms, primarily for robotics applications.\nDeveloping a vision program can be difficult because it is hard to visualize the intermediate results. \nGRIP simplifies and accelerates the creation of vision systems for experienced users and reduces the barrier to entry for inexperienced users.\nAs a result, many teams with minimal computer vision knowledge have successfully used GRIP since the 2016 FIRST Robotics Competition game.\n\n# Features\n\n - Intuitive drag and drop UI.\n - Active development community.\n - Generates Java, C++, and Python code directly from the pipeline ([Example usage here](https://github.com/WPIRoboticsProjects/GRIP-code-generation))!\n - Extensible!\n - Deploys and runs headless.\n - Supports various network protocols\n   - [Network Tables](https://github.com/wpilibsuite/allwpilib/tree/master/ntcore/)\n   - [Robot Operating System (ROS)](http://www.ros.org/)\n   - HTTP\n - CUDA acceleration\n - OS Support:\n     - Windows\n     - OSX\n     - Linux (minimum Ubuntu 18.04 or libc version 2.27+)\n     - Embedded Linux ARM (NI RoboRIO)\n\n\n## Getting Started\n\n1. Download the newest [release](https://github.com/WPIRoboticsProjects/GRIP/releases) for your operating system.\n2. Run the included installer.\n3. Open GRIP\n\nCheck out the release notes and [the wiki](https://github.com/WPIRoboticsProjects/GRIP/wiki) for more information.\n\nNote for Linux users: GRIP requires GTK2 to be installed. Most Ubuntu-based distributions include it,\nbut some other distros such as Arch may require it to be manually installed. GRIP also requires libc version 2.27\nor higher; for Ubuntu-based distributions, this requires Ubuntu 18.04 or newer.\n\n## Building GRIP From Source\n\nTo build and run, use the included Gradle wrapper script on a Unix System:\n\n    ./gradlew :ui:run\n\nOn Windows:\n\n    gradlew.bat :ui:run\n\nIf you don't have an internet connection you can build using the `--offline` flag if you have built GRIP once before.\n\n## CUDA Support\nTo enable CUDA acceleration, CUDA 10.0 needs to be installed on your computer. CUDA 10.1 may work on\nLinux systems, but Windows _must_ use 10.0.\n\nWhen running or building from source, add the Gradle flag `-Pcuda` to enable CUDA acceleration (eg `./gradlew :ui:run -Pcuda`)\n\nNote that CUDA acceleration is not available for all operations.\n\nCode generation does not support CUDA - it is only used for operations running in GRIP.\n\n## Contributing\n\nSee the guide on [setting up build tools](https://github.com/WPIRoboticsProjects/GRIP/wiki/Setting-up-build-tools) in the wiki.\n"
 },
 {
  "repo": "huihut/Facemoji",
  "language": "C#",
  "readme_contents": "# Facemoji\n\n**A voice chatbot that can imitate your expression.**\n\nThis is a **Unity** project (just for Android now), which has two modules.\n\n* One is **FaceTracking**, which using [OpenCV](https://enoxsoftware.com/opencvforunity/) and [Dlib](https://enoxsoftware.com/dlibfacelandmarkdetector/) to detects facial expressions, converts them into [Live2D](http://sites.cybernoids.jp/cubism-sdk2_e/unity_2-1) model, and [record](https://github.com/Chman/Moments) gif. \n* The other is **AI** ~~(chatbot)~~, which uses [Turing Robot](https://github.com/huihut/TuringRobot), [Iflytek IAT](http://www.xfyun.cn/services/voicedictation) and [Iflytek TTS](http://www.xfyun.cn/services/online_tts) to make a voice chat.\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](README_zh_CN.md)\n\n## Releases\n\n* ~~[Google Play . Facemoji \u5e9f\u840c](https://play.google.com/store/apps/details?id=com.huihut.facemoji)~~\n* [\u9177\u5b89 . Facemoji \u5e9f\u840c](https://www.coolapk.com/apk/192260)\n* [Github . Facemoji/releases](https://github.com/huihut/Facemoji/releases)\n* [Drive.Google . Facemoji/Platform](https://drive.google.com/open?id=1ofJMFIdzXCdYYO3qO5hvrTQPJUumgSY-)\n* [Pan.Baidu . Facemoji/Platform](https://pan.baidu.com/s/1U08B_wPY67Zh1RTwFhrihA)\n\n## Setup\n\n* Download `shape_predictor_68_face_landmarks.dat`(Facial Landmark Detector) and `Facemoji_Plugins_Assets_1.5.0.unitypackage`(Streamlined OpenCV, Dlib, Live2D and Iflytek Assets Library) from [Drive.Google](https://drive.google.com/open?id=1ofJMFIdzXCdYYO3qO5hvrTQPJUumgSY-) or [Pan.Baidu](https://pan.baidu.com/s/1U08B_wPY67Zh1RTwFhrihA)\n* `git clone https://github.com/huihut/Facemoji.git`\n* Create new Unity project (called `FacemojiDemo`)\n* Copy `Facemoji/Assets` and `Facemoji/ProjectSettings` to your unity project (`FacemojiDemo/`)\n* Copy `shape_predictor_68_face_landmarks.dat` to your `FacemojiDemo/Assets/StreamingAssets/`\n* Import `Facemoji_Plugins_Assets_1.5.0.unitypackage`\n* Select Android platform\n* Build & Run\n\n## Usage\n\n### FaceTracking\n\nUsing **OpenCV** and **Dlib** to detects facial expressions, converts them into **Live2D** model.\n\nShe can move with your face and you can try shaking your head.\n\n### Record gif\n\nThe middle of the above is the record button, you can record 3 seconds **gif**.\n\nRecorder State : **Recording**(Ready to record) -> **PreProcessing**(Is recording) -> **Paused**(Compressing gif) -> **Recording**(Ready to record)\n\nSave the gif in `Application.dataPath`\n\n(Android in `/storage/emulated/0/Android/data/com.huihut.facemoji/files/`)\n\n### Voice and text chat (~~chatbot?~~ She said she is AI, not Robot! hhhh...)\n\nUsing **Turing Robot**, **Iflytek IAT** and **Iflytek TTS**.\n\nYou can chat with her by voice or text.\n\nShe's a great AI ~~(robot)~~, and she can:\n\n* Chat\n* Encyclopedia\n* Calculate\n* Tell a story\n* Tell a joke\n* Idiom Solitaire\n* Horoscope\n* Weather forecast\n* ...\n\nBut because she is a Chinese robot(**Turing robot only supports Chinese**), she can **only chat in Chinese**.\n\n~~She will chat in English later.~~\n\n## Preview\n\n![](Images/Capture_Facemoji.png)\n\n## Gif\n\n* Come On !\n    \n    ![](Images/GifCapture-ComeOn.gif)\n\n* No~ No~\n    \n    ![](Images/GifCapture-NoNo.gif)\n\n* Wink !\n    \n    ![](Images/GifCapture-Spark.gif)\n\n## License\n\n[GPL v3.0](https://github.com/huihut/Facemoji/blob/master/LICENSE)"
 },
 {
  "repo": "xikuqi/OpenCV",
  "language": "Java",
  "readme_contents": "# seetafaceJNI\n\n#### \u9879\u76ee\u4ecb\u7ecd\n\u57fa\u4e8e\u4e2d\u79d1\u9662seetaface2\u8fdb\u884c\u5c01\u88c5\u7684JAVA\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5e93\uff0c\u652f\u6301\u4eba\u8138\u8bc6\u522b\u30011:1\u6bd4\u5bf9\u30011:N\u6bd4\u5bf9\u3002\nseetaface2\uff1ahttps://github.com/seetaface/SeetaFaceEngine2\n\n#### \u73af\u5883\u914d\u7f6e\n1\u3001\u4e0b\u8f7dmodel\uff08 https://pan.baidu.com/s/1HJj8PEnv3SOu6ZxVpAHPXg \uff09 \u6587\u4ef6\u5230\u672c\u5730\uff0c\u5e76\u89e3\u538b\u51fa\u6765\uff1b\n\n2\u3001\u4e0b\u8f7ddoc\u76ee\u5f55\u4e2d\u5bf9\u5e94\u7684lib\u5305\u5230\u672c\u5730\u5e76\u89e3\u538b\uff1aWindows(64\u4f4d)\u73af\u5883\u4e0b\u8f7dlib-win-x64.zip\u3001Linux(64\u4f4d)\u4e0b\u8f7dlib-linux-x64.tar.bz2\uff0cLinux\u73af\u5883\u8fd8\u9700\u8981\u5b89\u88c5\u4f9d\u8d56\u5e93\uff0c\u8be6\u89c1\uff1ahttps://my.oschina.net/u/1580184/blog/3042404 \uff1b\n\n3\u3001\u5c06doc\u4e2d\u7684faces-data.db\u4e0b\u8f7d\u5230\u672c\u5730\uff1b\uff08PS\uff1a\u5982\u679c\u4e0d\u9700\u8981\u4f7f\u75281:N\u4eba\u8138\u641c\u7d22,\u4e0d\u9700\u8981\u6b64\u6587\u4ef6\uff0c\u9700\u8981\u5c06seetafce.properties\u4e2d\u7684sqlite.db.file\u914d\u7f6e\u6ce8\u91ca\u6389\uff09\uff1b\n\n4\u3001\u5c06src/main/resources/\u4e2d\u7684seetaface.properties\u6587\u4ef6\u653e\u5230\u9879\u76ee\u7684resources\u6839\u76ee\u5f55\u4e2d\uff1b\n\n```properties\n#linux\u7cfb\u7edf\u4e2d\u4f9d\u8d56\u7684lib\u540d\u79f0\nlibs=holiday,SeetaFaceDetector200,SeetaPointDetector200,SeetaFaceRecognizer200,SeetaFaceCropper200,SeetaFace2JNI\n#Windows\u7cfb\u7edf\u4e2d\u4f9d\u8d56\u7684lib\u540d\u79f0\n#libs=libgcc_s_sjlj-1,libeay32,libquadmath-0,ssleay32,libgfortran-3,libopenblas,holiday,SeetaFaceDetector200,SeetaPointDetector200,SeetaFaceRecognizer200,SeetaFaceCropper200,SeetaFace2JNI\n\n#lib\u5b58\u653e\u76ee\u5f55\nlibs.path=/usr/local/seetaface2/lib\n#model\u5b58\u653e\u76ee\u5f55\nbindata.dir=/usr/local/seetaface2/bindata\n\n##sqlite\u914d\u7f6e(\u5982\u679c\u4e0d\u75281:N\u4eba\u8138\u641c\u7d22\u529f\u80fd\uff0c\u8bf7\u5220\u9664\u4e0b\u97625\u9879sqlite\u5f00\u5934\u7684\u914d\u7f6e)\nsqlite.db.file=/data/faces-data.db\nsqlite.conn.maxTotal=50\nsqlite.conn.maxIdle=5\nsqlite.conn.minIdle=0\nsqlite.conn.maxWaitMillis=60000\n```\n\n\n5\u3001\u5c06seetafaceJNI-2.0.jar\u548c\u4f9d\u8d56\u5305\u5bfc\u5165\u5230\u9879\u76ee\u4e2d\uff0cpom\u5982\u4e0b:\n\n```xml\n   <properties>\n       <spring.version>4.2.8.RELEASE</spring.version>\n       <log4j.version>2.8.2</log4j.version>\n       <slf4j.version>1.7.25</slf4j.version>\n   </properties>\n  \n   <dependencies>\n       <dependency>\n            <groupId>com.cnsugar.ai</groupId>\n            <artifactId>seetafaceJNI</artifactId>\n            <version>2.0</version>\n            <!--<scope>system</scope>-->\n            <!--<systemPath>${project.basedir}/lib/seetafaceJNI-2.0.jar</systemPath>-->\n       </dependency>\n       <dependency>\n           <groupId>org.springframework</groupId>\n           <artifactId>spring-core</artifactId>\n           <version>${spring.version}</version>\n       </dependency>\n  \n       <dependency>\n           <groupId>org.slf4j</groupId>\n           <artifactId>slf4j-api</artifactId>\n           <version>${slf4j.version}</version>\n       </dependency>\n  \n       <!-- sqlite -->\n       <dependency>\n           <groupId>org.xerial</groupId>\n           <artifactId>sqlite-jdbc</artifactId>\n           <version>3.25.2</version>\n       </dependency>\n       <dependency>\n           <groupId>org.apache.commons</groupId>\n           <artifactId>commons-pool2</artifactId>\n           <version>2.4.2</version>\n       </dependency>\n   </dependencies> \n```\n\n6\u3001\u8c03\u7528FaceHelper\u4e2d\u7684\u65b9\u6cd5\u3002\n\n\n#### \u4f7f\u7528\u65b9\u6cd5\n\u6240\u6709\u65b9\u6cd5\u90fd\u5c01\u88c5\u5230\u4e86FaceHelper\u5de5\u5177\u7c7b\u4e2d\n```java\n    /**\n     * \u4eba\u8138\u6bd4\u5bf9\n     *\n     * @param img1\n     * @param img2\n     * @return \u76f8\u4f3c\u5ea6\n     */\n    float compare(File img1, File img2);\n    float compare(byte[] img1, byte[] img2);\n    float compare(BufferedImage image1, BufferedImage image2);\n    \n    /**\n     * \u6ce8\u518c\u4eba\u8138\uff08\u4f1a\u88c1\u526a\u56fe\u7247\uff09\n     *\n     * @param key \u4eba\u8138\u7167\u7247\u552f\u4e00\u6807\u8bc6\n     * @param img \u4eba\u8138\u7167\u7247\n     * @return \n     */\n    boolean register(String key, byte[] img);\n    /**\n     * \u6ce8\u518c\u4eba\u8138\uff08\u4e0d\u88c1\u526a\u56fe\u7247\uff09\n     *\n     * @param key \u4eba\u8138\u7167\u7247\u552f\u4e00\u6807\u8bc6\n     * @param image \u4eba\u8138\u7167\u7247\n     * @return \n     */\n    boolean register(String key, BufferedImage image)\n    \n    /**\n     * \u641c\u7d22\u4eba\u8138\n     *\n     * @param img \u4eba\u8138\u7167\u7247\n     * @return\n     */\n    Result search(byte[] img);\n    Result search(BufferedImage image);\n    \n    /**\n     * \u4eba\u8138\u63d0\u53d6\uff08\u88c1\u526a\uff09\n     *\n     * @param img\n     * @return return cropped face\n     */\n    BufferedImage crop(byte[] img);\n    BufferedImage crop(BufferedImage image);\n    \n    /**\n     * \u4eba\u8138\u8bc6\u522b\n     *\n     * @param img\n     * @return\n     */\n    SeetaRect[] detect(byte[] img);\n    SeetaRect[] detect(BufferedImage image);\n\n    /**\n     * \u4eba\u8138\u8bc6\u522b(\u5305\u542b5\u4e2a\u7279\u5f81\u70b9\u4f4d\u7f6e)\n     *\n     * @param image\n     * @return\n     */\n    FaceLandmark detectLandmark(BufferedImage image);\n    \n    /**\n     * \u5220\u9664\u5df2\u6ce8\u518c\u7684\u4eba\u8138\n     * @param keys\n     */\n    void removeRegister(String... keys);  \n    \n    /**\n     * \u6e05\u9664\u4eba\u8138\u5e93\u6570\u636e\n     */\n    void clear();    \n    \n```\n\n- \u793a\u4f8b\u4ee3\u7801\uff1a1:1\u4eba\u8138\u6bd4\u5bf9\n```java\n    @org.junit.Test\n    public void testCompare() throws Exception {\n        String img1 = \"F:\\\\ai\\\\demo-pic39.jpg\";\n        String img2 = \"F:\\\\ai\\\\left_pic_one.jpg\";\n        System.out.println(\"result:\"+FaceHelper.compare(new File(img1), new File(img2)));\n    }\n```\n\n- \u793a\u4f8b\u4ee3\u7801\uff1a1:N\u4eba\u8138\u641c\u7d22\n  \u5148\u8c03\u7528FaceHelper.register()\u65b9\u6cd5\u5c06\u4eba\u8138\u56fe\u7247\u6ce8\u518c\u5230seetaface2\u7684\u4eba\u8138\u5e93(\u5185\u5b58)\u4e2d\uff0c\u540c\u65f6\u4f1a\u5c06\u56fe\u7247\u5b58\u5728sqlite\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u6301\u4e45\u5316\uff0c\u4e0b\u6b21\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u65f6\u4f1a\u81ea\u52a8\u4ecesqlite\u4e2d\u628a\u56fe\u7247\u8bfb\u53d6\u51fa\u6765\u91cd\u65b0\u6ce8\u518c\u5230seetafce2\u7684\u5185\u5b58\u5e93\u4e2d\n\n```java\n    @org.junit.Test\n    public void testRegister() throws IOException {\n        //\u5c06F:\\ai\\star\u76ee\u5f55\u4e0b\u7684jpg\u3001png\u56fe\u7247\u90fd\u6ce8\u518c\u5230\u4eba\u8138\u5e93\u4e2d\uff0c\u4ee5\u6587\u4ef6\u540d\u4e3akey\n        Collection<File> files = FileUtils.listFiles(new File(\"F:\\\\ai\\\\star\"), new String[]{\"jpg\", \"png\"}, false);\n        for (File file : files) {\n            String key = file.getName();\n            try {\n                FaceHelper.register(key, FileUtils.readFileToByteArray(file));\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    @org.junit.Test\n    public void testSearch() throws IOException {\n        SeetafaceBuilder.build();//\u7cfb\u7edf\u542f\u52a8\u65f6\u5148\u8c03\u7528\u521d\u59cb\u5316\u65b9\u6cd5\n\n        //\u7b49\u5f85\u521d\u59cb\u5316\u5b8c\u6210\n        while (SeetafaceBuilder.getFaceDbStatus() == SeetafaceBuilder.FacedbStatus.LOADING || SeetafaceBuilder.getFaceDbStatus() == SeetafaceBuilder.FacedbStatus.READY) {\n            try {\n                Thread.sleep(100);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n        \n        long l = System.currentTimeMillis();\n        Result result = FaceHelper.search(FileUtils.readFileToByteArray(new File(\"F:\\\\ai\\\\gtl.jpg\")));\n        System.out.println(\"\u641c\u7d22\u7ed3\u679c\uff1a\" + result + \"\uff0c \u8017\u65f6\uff1a\" + (System.currentTimeMillis() - l));\n    }\n```"
 },
 {
  "repo": "sthanhng/yoloface",
  "language": "Python",
  "readme_contents": "# YOLOFace\n\n# Deep learning based Face detection using the YOLOv3 algorithm\n\n\n## Getting started\n\nThe YOLOv3 (You Only Look Once) is a state-of-the-art, real-time object detection algorithm. The published model recognizes 80 different objects in images and videos. For more details, you can refer to this [paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf).\n\n## YOLOv3's architecture\n\n![Imgur](assets/yolo-architecture.png)\n\nCredit: [Ayoosh Kathuria](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)\n\n## OpenCV Deep Neural Networks (dnn module)\n\nOpenCV `dnn` module supports running inference on pre-trained deep learning models from popular frameworks such as TensorFlow, Torch, Darknet and Caffe.\n\n## Prerequisites\n\n* Tensorflow\n* opencv-python\n* opencv-contrib-python\n* Numpy\n* Keras\n* Matplotlib\n* Pillow\n\nDevelopment for this project will be isolated in Python virtual environment. This allows us to experiment with different versions of dependencies.\n\nThere are many ways to install `virtual environment (virtualenv)`, see the [Python Virtual Environments: A Primer](https://realpython.com/python-virtual-environments-a-primer/) guide for different platforms, but here are a couple:\n\n- For Ubuntu\n```bash\n$ pip install virtualenv\n```\n\n- For Mac\n```bash\n$ pip install --upgrade virtualenv\n```\n\nCreate a Python 3.6 virtual environment for this project and activate the virtualenv:\n```bash\n$ virtualenv -p python3.6 yoloface\n$ source ./yoloface/bin/activate\n```\n\nNext, install the dependencies for the this project:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Usage\n\n* Clone this repository\n```bash\n$ git clone https://github.com/sthanhng/yoloface\n```\n\n* For face detection, you should download the pre-trained YOLOv3 weights file which trained on the [WIDER FACE: A Face Detection Benchmark](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html) dataset from this [link](https://drive.google.com/file/d/1xYasjU52whXMLT5MtF7RCPQkV66993oR/view?usp=sharing) and place it in the `model-weights/` directory.\n\n* Run the following command:\n\n>**image input**\n```bash\n$ python yoloface.py --image samples/outside_000001.jpg --output-dir outputs/\n```\n\n>**video input**\n```bash\n$ python yoloface.py --video samples/subway.mp4 --output-dir outputs/\n```\n\n>**webcam**\n```bash\n$ python yoloface.py --src 1 --output-dir outputs/\n```\n\n## Sample outputs\n\n![Imgur](assets/outside_000001_yoloface.jpg)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for more details.\n\n## References\n\n"
 },
 {
  "repo": "Raymondhhh90/idcardocr",
  "language": "Python",
  "readme_contents": "# \u7b2c\u4e8c\u4ee3\u8eab\u4efd\u8bc1\u4fe1\u606f\u8bc6\u522b\n\u53ef\u8bc6\u522b\u8eab\u4efd\u8bc1\u4e0a\u6240\u6709\u4fe1\u606f\uff1a\u59d3\u540d\uff0c\u6027\u522b\uff0c\u6c11\u65cf\uff0c\u51fa\u751f\u65e5\u671f\uff0c\u4f4f\u5740\uff0c\u8eab\u4efd\u8bc1\u53f7\u7801\u3002\u63d0\u4f9bDocker\u955c\u50cf\u90e8\u7f72\u65b9\u5f0f\n* 2018/8/21 \u66f4\u65b0\uff0c\u5e94\u8be5\u8f83\u5927\u63d0\u5347\u4e86\u51c6\u786e\u7387\n* 2018/11/8 \u66f4\u65b0\uff0c\u4f18\u5316\u7b97\u6cd5\uff0c\u66f4\u65b0\u81f3tesseract4.0, \u5355\u5f20\u56fe\u7247\u8bc6\u522b\u65f6\u95f4\u964d\u4f4e\u5230*3s*\u4ee5\u4e0b\n# \u4f9d\u8d56\uff1a\n> \u672c\u9879\u76ee\u5728Ubuntu 18.10\u57fa\u4e8etesseract 4.0 rc3\uff0cOpenCV2; \u4f7f\u7528Python3.6\u8fdb\u884c\u5f00\u53d1<br>\n> apt\u4f9d\u8d56\u5b89\u88c5\uff1a<br>\n>`sudo apt install python3 python3-pip tesseract-ocr tesseract-ocr-chi-sim tzdata libsm6 libxext6 python3-tk -y` <br><br>\n> Python\u4f9d\u8d56\u5b89\u88c5\uff1a<br>\n>`sudo pip3 install -r idcardocr/requirements.txt`<br><br>\n> ~~tessdata\u914d\u7f6e\uff1a~~<br>\n> ~~`sudo cp tessdata/* /usr/share/tesseract-ocr/tessdata`~~<br>\n# \u4f7f\u7528\u65b9\u6cd5\uff1a\n> \u8bc6\u522b\u672c\u5730\u56fe\u7247<br>\n> `import idcard_recognize;print idcard_recognize.process('testimages/3.jpg')`<br><br>\n> http_server\u8fdc\u7a0b\u63a5\u6536\u56fe\u7247<br>\n> `python3 idcard_recognize.py`  <br>\n> \u9ed8\u8ba4\u76d1\u542c\u7aef\u53e3\u4e3a8080 <br><br>\n> Docker\u8fd0\u884chttp_server:  <br>\n> `docker pull raymondwong/idcardocr;docker run -d -p 8080:8080 raymondwong/idcardocr`  <br>\n>> \u6d4b\u8bd5:  <br>\n>>> \u4f7f\u7528curl\u5411\u670d\u52a1\u5668\u53d1\u9001\u56fe\u7247:  <br>\n>>>`curl --request POST \\\n  --url http://127.0.0.1:8080 \\\n  --header 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \\\n  --form 'pic=@./testimages/3.jpg'`  <br><br>\n>>> \u4f7f\u7528Postman\uff1a  <br>\n>>> ![avatar](postman.jpg) <br>\n\n# \u6027\u80fd<br>\n> \u5e73\u53f0\uff1a I5 8259u + 16g macOS 13.14 \u5173\u95edOpenCL<br>\n\u5904\u7406\u5355\u5f20\u56fe\u7247\u65f6\u95f4\u57282.5\u79d2\u5de6\u53f3\uff08\u5355\u5f20\u56fe\u7247\u53ea\u80fd\u4f7f\u7528\u5355\u6838\u5fc3\uff09  <br>\n~~\u5904\u74064\u5f20\u56fe\u7247\u65f6\u95f4\u4e5f\u662f4\u79d2\u5de6\u53f3\uff084\u6838\u5fc3\uff09~~  <br>\n\u5173\u4e8eOPENCL: \u5f00\u542f\u5e76\u4e0d\u4f1a\u4f7f\u5355\u5f20\u56fe\u7247\u5904\u7406\u901f\u5ea6\u52a0\u5feb\uff0c\u4f46\u662f\u80fd\u8ba9\u4f60\u5728\u540c\u65f6\u95f4\u5904\u7406\u66f4\u591a\u56fe\u7247\uff08\u8b6c\u5982I5 6500\u6bcf\u79d2\u80fd\u5904\u74064\u5f20\u56fe\u7247\uff0c\u5f00\u542fOPENCL\u540e\u6bcf\u79d2\u80fd\u5904\u74066\u5f20\u56fe\u7247\uff09 <br> \n\u5f00\u542fOPENCL\uff1a \u9ed8\u8ba4\u5173\u95ed\uff0c\u53ef\u4ee5\u81ea\u884c\u4fee\u6539`idcard_recognize.http_server`\u4e2d\u7684`cv2.ocl.setUseOpenCL(False)`\u5f00\u542f\n"
 },
 {
  "repo": "SimonCherryGZ/face-landmark-android",
  "language": "Java",
  "readme_contents": "\nAR\u76f8\u673a\n===================================\n\u6839\u636e\u4eba\u8138\u56fe\u7247\u6784\u5efa\u7b80\u5355\u76843D\u4eba\u8138\u6a21\u578b\uff0c\u7136\u540e\u5728\u6444\u50cf\u5934\u9884\u89c8\u753b\u9762\u4e2d\u5c55\u793a\u3002\u57fa\u4e8e\u8be5\u4eba\u8138\u6a21\u578b\u8fdb\u884c\u6362\u8138\u6216\u8005\u6dfb\u52a0\u88c5\u9970\u54c1\u3002\n  \n\u57fa\u4e8e \n----------------------------------- \n* [dlib-android-app](https://github.com/tzutalin/dlib-android-app) \n* [dlib-android](https://github.com/tzutalin/dlib-android) \u63d0\u4f9bAndroid\u5e73\u53f0\u53ef\u7528\u7684Dlib\u5e93\u3002\n* More Than Technical[\u8fd9\u7bc7\u6587\u7ae0](http://www.morethantechnical.com/2012/10/17/head-pose-estimation-with-opencv-opengl-revisited-w-code/) \u7684\u5934\u90e8\u59ff\u6001\u4f30\u7b97\u7b97\u6cd5\u3002\n* [LearnOpenCV.com](http://www.learnopencv.com/face-swap-using-opencv-c-python/) \u7684\u6362\u8138\u7b97\u6cd5\u3002\n* [Rajawali](https://github.com/Rajawali/Rajawali) OpenGL ES\u5f15\u64ce\u3002\n\n\u5e94\u7528\u622a\u56fe \n-----------------------------------\n### GIF\u6f14\u793a\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/GIF_1.gif)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/GIF_2.gif)\n### \u663e\u793a\u4eba\u81383D\u6a21\u578b\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/1.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/2.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/3.jpg)\n### \u663e\u793a\u88c5\u9970\u54c1\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/4.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/5.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/6.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/7.jpg)\n### \u6d4b\u8bd5\u9875\u9762\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/8.jpg)\n![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/9.jpg)\n\n\u539f\u7406\n-----------------------------------  \n* \u6839\u636e\u4eba\u8138\u56fe\u7247\u6784\u5efa3D\u4eba\u8138\u6a21\u578b\n\n\t\u4f7f\u7528Dlib\u53ef\u4ee5\u68c0\u6d4b\u51fa\u4eba\u8138\u768468\u4e2a\u5173\u952e\u70b9\uff1a  \n  \n\t![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/landmarks.jpg)\n  \n\t\u6839\u636e\u8fd968\u4e2a\u70b9\u53ef\u4ee5\u8fde\u7ebf\u5f97\u5230\u4e00\u4e2a\u4f4e\u9762\u6570\u7684\u4eba\u8138\u6a21\u578b\uff1a  \n  \n\t![image](https://github.com/SimonCherryGZ/face-landmark-android/raw/master/screenshots/base_model.jpg)\n  \n\t\u6211\u4f7f\u7528[Blender](https://www.blender.org/) \u5efa\u7acb\u4e86\u8be5\u6a21\u578b\uff0c\u8fd9\u91cc\u79f0\u5176\u4e3aBaseModel\uff0c\u683c\u5f0f\u4e3aobj\u3002\n  \n* \u52a8\u6001\u4fee\u6539BaseModel\n\n\t\u67e5\u8be2obj\u6587\u4ef6\u7ed3\u6784\u53ef\u77e5\uff0c\u4ee5\u201cv\u201d\u5f00\u5934\u7684\u884c\u63cf\u8ff0\u7684\u662f\u6a21\u578b\u7684\u9876\u70b9\uff0c\u4ee5\u201cvt\u201d\u5f00\u5934\u7684\u884c\u63cf\u8ff0\u7684\u662f\u6a21\u578b\u7684\u8d34\u56fe\u5750\u6807\u70b9\u3002  \n  \n  \u90a3\u4e48\u53ea\u8981\u627e\u5230\u8fd9\u4e9b\u70b9\u4e0e\u4eba\u7c7b\u5173\u952e\u70b9\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c31\u53ef\u4ee5\u7b80\u5355\u5730\u901a\u8fc7\u66ff\u6362\u5bf9\u5e94\u884c\u7684\u6570\u636e\uff0c\u6765\u8fbe\u5230\u52a8\u6001\u4fee\u6539\u6a21\u578b\u7684\u76ee\u7684\u3002\n\n\n\u4e0d\u8db3\n-----------------------------------  \n* Dlib\u5e93\u4f9d\u8d56shape_predictor_68_face_landmarks.dat\uff0c\u5176\u5927\u5c0f\u7ea6100M\uff0c\u52a0\u8f7d\u9700\u8981\u82b1\u8d39\u6570\u79d2\u3002\n* Dlib\u5e93\u68c0\u6d4b\u4eba\u8138\u7684\u901f\u5ea6\u4e0e\u56fe\u50cf\u7684\u5927\u5c0f\u6210\u53cd\u6bd4\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6444\u50cf\u5934\u7684\u9884\u89c8\u753b\u9762\u8fdb\u884c\u622a\u53d6\u3002  \n\n  \u622a\u53d6\u5f97\u592a\u5c0f\u7684\u8bdd\u4e5f\u68c0\u6d4b\u4e0d\u51fa\u4eba\u8138\u3002\u5728\u5927\u5c0f\u4e0d\u5f71\u54cd\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u68c0\u6d4b\u901f\u5ea6\u4f9d\u7136\u4e0d\u7406\u60f3\u3002\n* \u7531\u4e8e\u662f\u4ece\u5355\u5f20\u56fe\u7247\u6784\u5efa3D\u4eba\u8138\uff0c\u65e0\u6cd5\u83b7\u53d6Z\u8f74\u7684\u6570\u503c\uff0c\u6240\u4ee5\u5728\u4f30\u7b97\u4e09\u7ef4\u59ff\u6001\u65f6\u7528\u7684\u662f\u6a21\u62df\u6570\u636e\uff0c\u5f97\u51fa\u7684\u6570\u503c\u53ef\u80fd\u4e0d\u51c6\u786e\u3002\n\n\u6784\u5efa\n-----------------------------------  \n1. \u6309\u7167[dlib-android](https://github.com/tzutalin/dlib-android) \u63cf\u8ff0\u7684\u6b65\u9aa4\u6765\u6784\u5efaDlib-Android\u5e93\u3002\n2. \u4ece[dlib-android-app](https://github.com/tzutalin/dlib-android-app/blob/d0170613f36046b8e122a5de651029ecb1af947e/app/src/main/res/raw/shape_predictor_68_face_landmarks.dat) \u4e2d\u627e\u5230shape_predictor_68_face_landmarks.dat\uff0c\u5c06\u5176\u590d\u5236\u5230\u624b\u673a\u6839\u76ee\u5f55\u4e2d\u3002\n\n\u4f9d\u8d56\u5e93 \n-----------------------------------  \n  * [RxJava](https://github.com/ReactiveX/RxJava)\n  * [fastjson](https://github.com/alibaba/fastjson)\n  * [Realm](https://github.com/realm/realm-java)\n  * [Glide](https://github.com/bumptech/glide)\n\n\n> Written with [StackEdit](https://stackedit.io/).\n"
 },
 {
  "repo": "rmislam/PythonSIFT",
  "language": "Python",
  "readme_contents": "# PythonSIFT\n\nThis is an implementation of SIFT (David G. Lowe's scale-invariant feature transform) done entirely in Python with the help of NumPy. This implementation is based on OpenCV's implementation and returns OpenCV `KeyPoint` objects and descriptors, and so can be used as a drop-in replacement for OpenCV SIFT. This repository is intended to help computer vision enthusiasts learn about the details behind SIFT.\n\n### *Update 2/11/2020*\n\nPythonSIFT has been reimplemented (and greatly improved!) in Python 3. You can find the original Python 2 version in the `legacy` branch. However, I strongly recommend you use `master` (the new Python 3 implementation). It's much better.\n\n## Dependencies\n\n`Python 3`\n\n`NumPy`\n\n`OpenCV-Python`\n\nLast tested successfully using `Python 3.7.6` and `OpenCV-Python 4.2.0`.\n\n## Usage\n\n```python\nimport cv2\nimport pysift\n\nimage = cv2.imread('your_image.png', 0)\nkeypoints, descriptors = pysift.computeKeypointsAndDescriptors(image)\n```\n\nIt's as simple as that. Just like OpenCV.\n\nThe returned `keypoints` are a list of OpenCV `KeyPoint` objects, and the corresponding `descriptors` are a list of `128` element NumPy vectors. They can be used just like the objects returned by OpenCV-Python's SIFT `detectAndCompute` member function. Note that this code is not optimized for speed, but rather designed for clarity and ease of understanding, so it will take a few minutes to run on most images.\n\n## Tutorial\n\nYou can find a step-by-step, detailed explanation of the code in this repo in my two-part tutorial:\n\n[Implementing SIFT in Python: A Complete Guide (Part 1)](https://medium.com/@russmislam/implementing-sift-in-python-a-complete-guide-part-1-306a99b50aa5)\n\n[Implementing SIFT in Python: A Complete Guide (Part 2)](https://medium.com/@russmislam/implementing-sift-in-python-a-complete-guide-part-2-c4350274be2b)\n\nI'll walk you through each function, printing and plotting things along the way to develop a solid understanding of SIFT and its implementation details.\n\n## Template Matching Demo\n\nI've adapted OpenCV's SIFT template matching demo to use PythonSIFT instead. The OpenCV images used in the demo are included in this repo for your convenience.\n```python\npython template_matching_demo.py\n```\n\n## Questions, Concerns, Bugs\n\nAnyone is welcome to report and/or fix any bugs. I will resolve any opened issues as soon as possible.\n\nAny questions about the implementation, no matter how simple, are welcome. I will patiently explain my code to you.\n\n### *Original Paper*\n\n[\"Distinctive Image Features from Scale-Invariant Keypoints\", David G. Lowe](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)\n\nDefinitely worth a read!\n\n### *Legal Notice*\n\nSIFT *was* patented, but it has expired.\nThis repo is primarily meant for educational purposes, but feel free to use my code any way you want, commercial or otherwise. All I ask is that you cite or share this repo.\n\nYou can find the original (now expired) patent [here](https://patents.google.com/patent/US6711293B1/en) (Inventor: David G. Lowe. Assignee: University of British Columbia.).\n"
 },
 {
  "repo": "aperrault/DetectText",
  "language": "C++",
  "readme_contents": "DetectText\n==========\n\nDetect text with stroke width transform.\n\n## Dependencies\nOpenCV 2.4+, boost.\n\n## Compile\n\n    g++ -o DetectText TextDetection.cpp FeaturesMain.cpp -lopencv_core -lopencv_highgui -lopencv_imgproc -I/path/to/current/directory\n\nwhere /path/to/current/directory is replaced with the absolute path to the current directory.\n\n### Using CMake\n\n    mkdir build\n    cd build\n    cmake ..\n    make\n\n## To run\n./TextDetection input_file output_file dark_on_light\nwhere dark_on_light is 1 or 0, indicating whether the text is darker or lighter than the background.\n\n## More \nDetails on the algorithm can be found in:\nhttp://www.cs.cornell.edu/courses/cs4670/2010fa/projects/final/results/group_of_arp86_sk2357/Writeup.pdf"
 },
 {
  "repo": "Canjie-Luo/Text-Image-Augmentation",
  "language": "C++",
  "readme_contents": "# Text Image Augmentation\n\n[![Build Status](https://travis-ci.org/Canjie-Luo/Text-Image-Augmentation.svg?branch=master)](https://travis-ci.org/Canjie-Luo/Text-Image-Augmentation)\n\nA general geometric augmentation tool for text images in the CVPR 2020 paper \"[Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition](https://arxiv.org/abs/2003.06606)\". We provide the tool to avoid overfitting and gain robustness of text recognizers. \n\n***Note that this is a general toolkit. Please customize for your specific task. If the repo benefits your work, please [cite the papers](https://github.com/Canjie-Luo/Text-Image-Augmentation#citation).***\n\n## News\n- 2020-02 The paper \"Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition\" was accepted to ***CVPR 2020***. It is a preliminary attempt for smart augmentation. \n\n- 2019-11 The paper \"Decoupled Attention Network for Text Recognition\" ([Paper](https://arxiv.org/abs/1912.10205) [Code](https://github.com/Wang-Tianwei/Decoupled-attention-network)) was accepted to ***AAAI 2020***. This augmentation tool was used in the experiments of handwritten text recognition.\n\n- 2019-04 We applied this tool in the ReCTS competition of ***ICDAR 2019***. Our ensemble model won the championship.\n\n- 2019-01 The similarity transformation was specifically customized for geomeric augmentation of text images. \n\n## Requirements\n\n- [GCC](https://gcc.gnu.org/gcc-4.8/) 4.8.*\n- [Python](https://www.python.org/) 2.7.*\n- [Boost](https://www.boost.org/) 1.67\n- [OpenCV](https://opencv.org/) 2.4.*\n\nWe recommend [Anaconda](https://www.anaconda.com/) to manage the version of your dependencies. For example:\n\n```bash\n     conda install boost=1.67.0\n```\n\n## Installation\nBuild library:\n\n```bash\n    mkdir build\n    cd build\n    cmake -D CUDA_USE_STATIC_CUDA_RUNTIME=OFF ..\n    make\n```\n\nCopy the **Augment.so** to the target folder and follow **demo.py** to use the tool.\n\n```bash\n    cp Augment.so ..\n    cd ..\n    python demo.py\n```\n\n## Demo\n\n- Distortion\n\n![](pic/gif_Distort.gif) \n\n- Stretch\n\n![](pic/gif_Stretch.gif)\n\n- Perspective\n\n![](pic/gif_Perspective.gif)\n\n## Speed\n\nTo transform an image with size (H:64, W:200), it takes less than 3ms using a 2.0GHz CPU. It is possible to accelerate the process by calling multi-process batch samplers in an on-the-fly manner, such as setting [**\\\"num_workers\\\"**](https://pytorch.org/docs/0.3.1/data.html?highlight=dataset#torch.utils.data.DataLoader) in [PyTorch](https://pytorch.org/docs/0.3.1/data.html?highlight=dataset#torch.utils.data.DataLoader).\n\n## Improvement for Recognition\n\nWe compare the accuracies of [CRNN](https://github.com/meijieru/crnn.pytorch) trained using only the corresponding small training set.\n\n| <center>Dataset</center> | <center>IIIT5K</center> | <center>IC13</center> | <center>IC15</center> |\n| :---: | :---: | :---: | :---:|\n| Without Data Augmentation | <center>40.8%</center> | <center>6.8%</center> | <center>8.7%</center> |\n| <center>With Data Augmentation</center> | <center>53.4%</center> | <center>9.6%</center> | <center>24.9%</center> | \n\n\n## Citation\n\n```\n@inproceedings{luo2020learn,\n  author = {Canjie Luo and Yuanzhi Zhu and Lianwen Jin and Yongpan Wang},\n  title = {Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition},\n  booktitle = {CVPR},\n  year = {2020}\n}\n\n@inproceedings{wang2020decoupled,\n  author = {Tianwei Wang and Yuanzhi Zhu and Lianwen Jin and Canjie Luo and Xiaoxue Chen and Yaqiang Wu and Qianying Wang and Mingxiang Cai}, \n  title = {Decoupled attention network for text recognition}, \n  booktitle ={AAAI}, \n  year = {2020}\n}\n\n@article{schaefer2006image,\n  title={Image deformation using moving least squares},\n  author={Schaefer, Scott and McPhail, Travis and Warren, Joe},\n  journal={ACM Transactions on Graphics (TOG)},\n  volume={25},\n  number={3},\n  pages={533--540},\n  year={2006},\n  publisher={ACM New York, NY, USA}\n}\n```\n\n## Acknowledgment\n\nThanks for the contribution of the following developers.\n\n[@keeofkoo](https://github.com/keeofkoo)\n\n[@cxcxcxcx](https://github.com/cxcxcxcx)\n\n[@Yati Sagade](https://github.com/yati-sagade) \n## Attention\nThe tool is only free for academic research purposes.\n"
 },
 {
  "repo": "PINTO0309/MobileNet-SSD-RealSense",
  "language": "Python",
  "readme_contents": "# MobileNet-SSD-RealSense\nRaspberryPi3(Raspbian Stretch) or Ubuntu16.04/UbuntuMate + Neural Compute Stick(NCS/NCS2) + RealSense D435(or USB Camera or PiCamera) + MobileNet-SSD(MobileNetSSD)  \n\n**\u3010Notice\u3011December 19, 2018 OpenVINO has supported RaspberryPi + NCS2 !!  \nhttps://software.intel.com/en-us/articles/OpenVINO-RelNotes#inpage-nav-2-2**  \n  \n**\u3010Dec 31, 2018\u3011 `USB Camera + MultiStick + MultiProcess mode` correspondence with NCS2 is completed.**<br>\n**\u3010Jan 04, 2019\u3011 Tune performance four times. MultiStickSSDwithRealSense_OpenVINO_NCS2.py. Core i7 -> NCS2 x1, 48 FPS**<br>\n**\u3010Nov 12, 2019\u3011 Compatible with OpenVINO 2019 R3 + RaspberryPi3/4 + Raspbian Buster.**<br>\n<br><br>\nMeasure the distance to the object with RealSense D435 while performing object detection by MobileNet-SSD(MobileNetSSD) with RaspberryPi 3 boosted with Intel Movidius Neural Compute Stick.<br>\n\"USB Camera mode / PiCamera mode\" can not measure the distance, but it operates at high speed.<br>\nAnd, This is support for MultiGraph and FaceDetection, MultiProcessing, Background transparentation.<br>\nAnd, This is support for simple clustering function. (To prevent thermal runaway)<br><br>\n## My blog\n**\u3010Japanese Article1\u3011  \n[RaspberryPi3 (Raspbian Stretch) + Intel Movidius Neural Compute Stick(NCS) + RealSenseD435 + MobileNet-SSD(MobileNetSSD) \u3067\u9ad8\u901f\u306b\u7269\u4f53\u691c\u51fa\u3057\u3064\u3064\u609f\u7a7a\u3084\u30e2\u30cb\u30bf\u307e\u3067\u306e\u8ddd\u96e2\u3092\u6e2c\u308b](https://qiita.com/PINTO/items/1828f97d95fdda45f57d)**<br>\n**\u3010Japanese / English Article2\u3011  \n[Intel also praised me again \u30fd(\uff9f\u2200\uff9f)\uff89 Yeah MobileNet-SSD(MobileNetSSD) object detection and RealSense distance measurement (640x480) with RaspberryPi3 At least 25FPS playback frame rate + 12FPS prediction rate](https://qiita.com/PINTO/items/40abcf33af3ae7ef579d#-english-article)**<br>\n**\u3010Japanese / English Article3\u3011  \n[Detection rate approx. 30FPS RaspberryPi3 Model B(plus none) is slightly later than TX2, acquires object detection rate of MobilenetSSD and corresponds to MultiModel VOC+WIDER FACE](https://qiita.com/PINTO/items/190daa4fddfd2a21f959#-detection-rate-approx-30fps-raspberrypi3-model-bplus-none-is-slightly-later-than-tx2-acquires-object-detection-rate-of-mobilenetssd-and-corresponds-to-multimodel-vocwider-face)**<br>\n**\u3010Japanese Article4\u3011  \n[RaspberryPi3\u3067\u8907\u6570\u306eMovidius Neural Compute Stick \u3092\u30b7\u30fc\u30e0\u30ec\u30b9\u306b\u30af\u30e9\u30b9\u30bf\u5207\u308a\u66ff\u3048\u3057\u3066\u9ad8\u901f\u63a8\u8ad6\u6027\u80fd\u3092\u7dad\u6301\u3057\u3064\u3064\u71b1\u66b4\u8d70(\u5185\u90e8\u6e29\u5ea670\u2103\u524d\u5f8c)\u3092\u56de\u907f\u3059\u308b](https://qiita.com/PINTO/items/62859125c5381690623c)**<br>\n**\u3010Japanese Article5\u3011  \n[Caffe\u3067\u8d85\u8efd\u91cf\u306a \"Semantic Segmentation\" \u306e\u30e2\u30c7\u30eb\u3092\u751f\u6210\u3059\u308b Sparse-Quantized CNN 512x1024_10MB_\u8efd\u91cf\u30e2\u30c7\u30eb_\u305d\u306e\uff11](https://qiita.com/PINTO/items/127c84319822a0776420)**<br>\n**\u3010Japanese / English Article6\u3011  \n[Boost RaspberryPi3 with Neural Compute Stick 2 (1 x NCS2) and feel the explosion performance of MobileNet-SSD\u3000(If it is Core i7, 21 FPS)](https://qiita.com/PINTO/items/fc1fcecce4d5600c20bb#boost-raspberrypi3-with-neural-compute-stick-2-1-x-ncs2-and-feel-the-explosion-performance-of-mobilenet-ssdif-it-is-core-i7-21-fps)**<br>\n**\u3010Japanese / English Article7\u3011  \n[[24 FPS] Boost RaspberryPi3 with four Neural Compute Stick 2 (NCS2) MobileNet-SSD / YoloV3 [48 FPS for Core i7]](https://qiita.com/PINTO/items/94d5557fca9911cc892d#24-fps-boost-raspberrypi3-with-four-neural-compute-stick-2-ncs2-mobilenet-ssd--yolov3-48-fps-for-core-i7)**<br>\n**\u3010Japanese / English Article8\u3011  \n[[24 FPS, 48 FPS] RaspberryPi3 + Neural Compute Stick 2, The day when the true power of one NCS2 was drawn out and \"Goku\" became a true \"super saiya-jin\"](https://qiita.com/PINTO/items/cb7ba1dae4bfc74a5966#24-fps-48-fps-raspberrypi3--neural-compute-stick-2-the-day-when-the-true-power-of-one-ncs2-was-drawn-out-and-goku-became-a-true-super-saiya-jin)**<br><br>\n\n## Table of contents\n**1. [Summary](#summary)**  \n\u3000**1.1 [Verification environment NCSDK (1)](#verification-environment-1)**  \n\u3000**1.2 [Result of detection rate NCSDK (1)](#result-of-detection-rate-1)**  \n\u3000**1.3 [Verification environment NCSDK (2)](#verification-environment-2)**  \n\u3000**1.4 [Result of detection rate NCSDK (2)](#result-of-detection-rate-2)**  \n**2. [Performance comparison as a mobile application (Based on sensory comparison)](#performance-comparison-as-a-mobile-application-based-on-sensory-comparison)**  \n**3. [Change history](#change-history)**  \n**4. [Motion image](#motion-image)**  \n\u3000**4-1. NCSDK ver**  \n\u3000\u3000**4-1-1. [RealSense Mode about 6.5 FPS \uff08Synchronous screen drawing\uff09](#realsense-mode-about-65-fps-detection--synchronous-screen-drawing--singlestickssdwithrealsensepy)**  \n\u3000\u3000**4-1-2. [RealSense Mode about 25.0 FPS \uff08Asynchronous screen drawing\uff09](#realsense-mode-about-250-fps-asynchronous-screen-drawing--multistickssdwithrealsensepy)**  \n\u3000\u3000**4-1-3. [USB Camera Mode MultiStick x4 Boosted 16.0 FPS+ \uff08Asynchronous screen drawing\uff09](#usb-camera-mode-multistick-x4-boosted-160-fps-asynchronous-screen-drawing--multistickssdwithrealsensepy)**  \n\u3000\u3000**4-1-4. [RealSense Mode SingleStick about 5.0 FPS\uff08Transparent background / Asynchronous screen drawing](#realsense-mode-singlestick-about-50-fpstransparent-background-in-real-time--asynchronous-screen-drawing--multistickssdwithrealsensepy)**  \n\u3000\u3000**4-1-5. [USB Camera Mode MultiStick x3 Boosted \uff08Asynchronous screen drawing / MultiGraph](#usb-camera-mode-multistick-x3-boosted-asynchronous-screen-drawing--multigraphssdfacedetection--facedetection--multistickssdwithrealsensepy)**  \n\u3000\u3000**4-1-6. [Simple clustering function (MultiStick / MultiCluster / Cluster switch cycle / Cluster switch temperature)](#simple-clustering-function-multistick--multicluster--cluster-switch-cycle--cluster-switch-temperature)**  \n\u3000**4-2. OpenVINO ver**  \n\u3000\u3000**4-2-1. [USB Camera Mode NCS2 x 1 Stick + RaspberryPi3\uff08Synchronous screen drawing\uff09](#usb-camera-mode-ncs2-singlestick--raspberrypi3synchronous-screen-drawing--singlestickssdwithusbcamera_openvino_ncs2py)**  \n\u3000\u3000**4-2-2. [USB Camera Mode NCS2 x 1 Stick + Core i7\uff08Synchronous screen drawing\uff09](#usb-camera-mode-ncs2-singlestick--core-i7synchronous-screen-drawing--singlestickssdwithusbcamera_openvino_ncs2py)**  \n\u3000\u3000**4-2-3. [USB Camera Mode NCS2 x 1 Stick + Core i7\uff08Asynchronous screen drawing\uff09](#usb-camera-mode-ncs2-x-1-stick--core-i7asynchronous-screen-drawing--multistickssdwithrealsense_openvino_ncs2py)**  \n\u3000\u3000**4-2-4. [USB Camera Mode NCS2 x 1 Stick + RaspberryPi3\uff08Asynchronous screen drawing\uff09](#usb-camera-mode-ncs2-x-1-stick--raspberrypi3asynchronous-screen-drawing--multistickssdwithrealsense_openvino_ncs2py)**  \n\u3000\u3000**4-2-5. [USB Camera Mode NCS2 x 1 Stick + LattePanda Alpha\uff08Asynchronous screen drawing\uff0948 FPS](#usb-camera-mode-ncs2-x-1-stick--lattepanda-alphaasynchronous-screen-drawing--multistickssdwithrealsense_openvino_ncs2py48-fps)**  \n\u3000\u3000**4-2-6. [PiCamera Mode NCS2 x 1 Stick + RaspberryPi3\uff08Asynchronous screen drawing\uff09](#picamera-mode-ncs2-x-1-stick--raspberrypi3asynchronous-screen-drawing--multistickssdwithpicamera_openvino_ncs2py)**  \n\u3000\u3000**4-2-7. [USB Camera Mode NCS2 x 1 Stick + RaspberryPi4\uff08Asynchronous screen drawing\uff0940 FPS](#usb-camera-mode-ncs2-x-1-stick--raspberrypi4asynchronous-screen-drawing--multistickssdwithusbcamera_openvino_ncs2py)**  \n**5. [Motion diagram of MultiStick](#motion-diagram-of-multistick)**  \n**6. [Environment](#environment)**  \n**7. [Firmware update with Windows 10 PC](#firmware-update-with-windows-10-pc)**  \n**8. [Work with RaspberryPi3 (or PC + Ubuntu16.04 / RaspberryPi + Ubuntu Mate)](#work-with-raspberrypi3-or-pc--ubuntu1604--raspberrypi--ubuntu-mate)**  \n\u3000**8-1. [NCSDK ver (Not compatible with NCS2)](#1ncsdk-ver-not-compatible-with-ncs2)**  \n\u3000**8-2. [OpenVINO ver (Corresponds to NCS2)](#2openvino-ver-corresponds-to-ncs2)**  \n**9. [Execute the program](#execute-the-program)**  \n**10. [\u3010Reference\u3011 MobileNetv2 Model (Caffe) Great Thanks!!](#reference-mobilenetv2-model-caffe-great-thanks)**  \n**11. [Conversion method from Caffe model to NCS model (NCSDK)](#conversion-method-from-caffe-model-to-ncs-model---ncsdk)**  \n**12. [Conversion method from Caffe model to NCS model (OpenVINO)](#conversion-method-from-caffe-model-to-ncs-model---openvino)**  \n**13. [Construction of learning environment and simple test for model (Ubuntu16.04 x86_64 PC + GPU NVIDIA Geforce)](#construction-of-learning-environment-and-simple-test-for-model-ubuntu1604-x86_64-pc--gpunvidia-geforce)**  \n**14. [Reference articles, thanks](#reference-articles-thanks)**  \n\n## Summary\n**Performance measurement result each number of sticks. (It is Detection rate. It is not a Playback rate.)**<br>\n**The best performance can be obtained with QVGA + 5 Sticks.**<br>\n**However, It is important to use a good quality USB camera.**<br><br>\n### Verification environment (1)\n|No.|Item|Contents|\n|:-:|:-|:-|\n|1|Video device|USB Camera (No RealSense D435) **ELP-USB8MP02G-L75 $70**|\n|2|Auxiliary equipment|(Required) self-powered USB2.0 HUB|\n|3|Input resolution|640x480|\n|4|Output resolution|640x480|\n|5|Execution parameters|$ python3 MultiStickSSDwithRealSense.py -mod 1 -wd 640 -ht 480|\n### Result of detection rate (1)\n|No.|Stick count|FPS|Youtube Movie|Note|\n|:-:|:-|:-|:-|:-|\n|1|1 Stick|6 FPS|**https://youtu.be/lNbhutT8hkA**|base line|\n|2|2 Sticks|12 FPS|**https://youtu.be/zuJOhKWoLwc**|6 FPS increase|\n|3|3 Sticks|16.5 FPS|**https://youtu.be/8UDFIJ1Z4v8**|4.5 FPS increase|\n|4|4 Sticks|16.5 FPS|**https://youtu.be/_2xIZ-IZwZc**|No improvement|\n\n### Verification environment (2)\n|No.|Item|Contents|\n|:-:|:-|:-|\n|1|Video device|USB Camera (No RealSense D435) **PlayStationEye $5**|\n|2|Auxiliary equipment|(Required) self-powered USB2.0 HUB|\n|3|Input resolution|320x240|\n|4|Output resolution|320x240|\n|5|Execution parameters|$ python3 MultiStickSSDwithRealSense.py -mod 1 -wd 320 -ht 240|\n### Result of detection rate (2)\n|No.|Stick count|FPS|Youtube Movie|Note|\n|:-:|:-|:-|:-|:-|\n|1|4 Sticks|\u3000 25 FPS|**https://youtu.be/v-Cei1TW88c**||\n|2|5 Sticks|:star: 30 FPS|**https://youtu.be/CL6PTNgWibI**|best performance|\n\n## Performance comparison as a mobile application (Based on sensory comparison)\n\u25ef=HIGH, \u25b3=MEDIUM, \u00d7=LOW  \n\n|No.|Model|Speed|Accuracy|Adaptive distance|\n|:-:|:-|:-:|:-:|:-|\n|1|SSD|\u00d7|\u25ef|ALL|\n|2|**[MobileNet-SSD](https://github.com/PINTO0309/MobileNet-SSD-RealSense.git)**|\u25b3|\u25b3|Short distance|\n|3|**[YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u00d7|\u25ef|ALL|\n|4|**[tiny-YoloV3](https://github.com/PINTO0309/OpenVINO-YoloV3.git)**|\u25ef|\u00d7|Long distance|\n\n## Change history\n<details><summary>Change history</summary><div>\n[July 14, 2018]\u3000Corresponds to NCSDK v2.05.00.02<br>\n[July 17, 2018]\u3000Corresponds to OpenCV 3.4.2<br>\n[July 21, 2018]\u3000Support for multiprocessing [MultiStickSSDwithRealSense.py]<br>\n[July 23, 2018]\u3000Support for USB Camera Mode [MultiStickSSDwithRealSense.py]<br>\n[July 29, 2018]\u3000Added steps to build learning environment<br>\n[Aug\u30003, 2018]\u3000Background Multi-transparent mode implementation [MultiStickSSDwithRealSense.py]<br>\n[Aug  11, 2018]\u3000CUDA9.0 + cuDNN7.2 compatible with environment construction procedure<br>\n[Aug 14, 2018]\u3000Reference of MobileNetv2 Model added to README and added Facedetection Model<br>\n[Aug 15, 2018]\u3000Bug Fixed. `MultiStickSSDwithRealSense.py` depth_scale be undefined. Pull Requests merged. Thank you Drunkar!!<br>\n[Aug 19, 2018]\u3000\u3010Experimental\u3011 Update Facedetection model [DeepFace] (graph.facedetectXX)<br>\n[Aug 22, 2018]\u3000Separate environment construction procedure of \"Raspbian Stretch\" and \"Ubuntu16.04\"<br>\n[Aug 22, 2018]\u3000\u3010Experimental\u3011 FaceDetection model replaced [resnet] (graph.facedetection)<br>\n[Aug 23, 2018]\u3000Added steps to build NCSDKv2<br>\n[Aug 25, 2018]\u3000Added \"Detection FPS View\" [MultiStickSSDwithRealSense.py]<br>\n[Sep 01, 2018]\u3000FaceDetection model replaced [Mobilenet] (graph.fullfacedetection / graph.shortfacedetection)<br>\n[Sep 01, 2018]\u3000Added support for MultiGraph and FaceDetection mode [MultiStickSSDwithRealSense.py]<br>\n[Sep 04, 2018]\u3000Performance measurement result with 5 sticks is posted<br>\n[Sep 08, 2018]\u3000To prevent thermal runaway, simple clustering function of stick was implemented.<br>\n[Sep 16, 2018]\u3000\u3010Experimental\u3011 Added Semantic Segmentation model [Tensorflow-UNet] (semanticsegmentation_frozen_person.pb)<br>\n[Sep 20, 2018]\u3000\u3010Experimental\u3011 Updated Semantic Segmentation model [Tensorflow-UNet]<br>\n[Oct 07, 2018]\u3000\u3010Experimental\u3011 Added Semantic Segmentation model [caffe-jacinto] (cityscapes5_jsegnet21v2_iter_60000.caffemodel)<br>\n[Oct 10, 2018]\u3000Corresponds to NCSDK 2.08.01<br>\n[Oct 12, 2018]\u3000\u3010Experimental\u3011 Added Semantic Segmentation model [Tensorflow-ENet] (semanticsegmentation_enet.pb) https://github.com/PINTO0309/TensorFlow-ENet.git<br>\n[Dec 22, 2018]\u3000Only \"USB Camera + single thread mode\" correspondence with NCS 2 is completed<br>\n[Dec 31, 2018]\u3000\"USB Camera + MultiStick + MultiProcess mode\" correspondence with NCS2 is completed<br>\n[Jan 04, 2019]\u3000Tune performance four times. MultiStickSSDwithRealSense_OpenVINO_NCS2.py<br>\n[Feb 01, 2019]\u3000Pull request merged. Fix Typo. Thanks, nguyen-alexa!!<br>\n[Feb 09, 2019]\u3000Corresponds to PiCamera.<br>\n[Feb 10, 2019]\u3000Added support for SingleStickSSDwithRealSense_OpenVINO_NCS2.py<br>\n[Feb 10, 2019]\u3000Firmware v5.9.13 -> v5.10.6, RealSenseSDK v2.13.0 -> v2.16.5<br>\n[May 01, 2019]\u3000Corresponds to OpenVINO 2019 R1.0.1<br>\n[Nov 12, 2019]\u3000Corresponds to OpenVINO 2019 R3.0<br>\n</div></details><br><br>\n\n## Motion image\n### **RealSense Mode about 6.5 FPS \uff08Detection + Synchronous screen drawing / SingleStickSSDwithRealSense.py\uff09**<br>\n**\u3010YouTube Movie\u3011 https://youtu.be/77cV9fyqJ1w**<br><br>\n![03](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/03.gif)\n![04](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/04.png)<br><br>\n### **RealSense Mode about 25.0 FPS \uff08Asynchronous screen drawing / MultiStickSSDwithRealSense.py\uff09**<br>\n**However, the prediction rate is fairly low.(about 6.5 FPS)**<br>\n**\u3010YouTube Movie\u3011 https://youtu.be/tAf1u9DKkh4**<br><br>\n![09](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/09.gif)<br><br>\n### **USB Camera Mode MultiStick x4 Boosted 16.0 FPS+ \uff08Asynchronous screen drawing / MultiStickSSDwithRealSense.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/GedDpAc0JyQ**<br><br>\n![10](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/10.gif) ![11](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/11.png)<br>\n### **RealSense Mode SingleStick about 5.0 FPS\uff08Transparent background in real time / Asynchronous screen drawing / MultiStickSSDwithRealSense.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/ApyX-mN_dYA**<br><br>\n![12](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/12.gif)<br>\n### **USB Camera Mode MultiStick x3 Boosted \uff08Asynchronous screen drawing / MultiGraph(SSD+FaceDetection) / FaceDetection / MultiStickSSDwithRealSense.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/fQZpuD8mWok**<br><br>\n![13](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/13.gif)<br>\n### **Simple clustering function (MultiStick / MultiCluster / Cluster switch cycle / Cluster switch temperature)**<br>\n![14](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/14.png)<br>\n**[Execution log]**<br>\n![15](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/15.png)\n### **USB Camera Mode NCS2 SingleStick + RaspberryPi3\uff08Synchronous screen drawing / SingleStickSSDwithUSBCamera_OpenVINO_NCS2.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/GJNkX-ZBuC8**<br><br>\n![16](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/16.gif)<br>\n### **USB Camera Mode NCS2 SingleStick + Core i7\uff08Synchronous screen drawing / SingleStickSSDwithUSBCamera_OpenVINO_NCS2.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/1ogge90EuqI**<br><br>\n![17](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/17.gif)<br>\n### **USB Camera Mode NCS2 x 1 Stick + Core i7\uff08Asynchronous screen drawing / MultiStickSSDwithRealSense_OpenVINO_NCS2.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/Nx_rVDgT8uY**<br>\n```bash\n$ python3 MultiStickSSDwithRealSense_OpenVINO_NCS2.py -mod 1 -numncs 1\n```\n![23](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/23.gif)<br>\n### **USB Camera Mode NCS2 x 1 Stick + RaspberryPi3\uff08Asynchronous screen drawing / MultiStickSSDwithRealSense_OpenVINO_NCS2.py\uff09**<br>\n**\u3010YouTube Movie\u3011\u3000https://youtu.be/Xj2rw_5GwlI**<br>\n```bash\n$ python3 MultiStickSSDwithRealSense_OpenVINO_NCS2.py -mod 1 -numncs 1\n```\n![24](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/24.gif)<br>\n### **USB Camera Mode NCS2 x 1 Stick + LattePanda Alpha\uff08Asynchronous screen drawing / MultiStickSSDwithRealSense_OpenVINO_NCS2.py\uff09[48 FPS]**<br>\n**https://twitter.com/PINTO03091/status/1081575747314057219**<br>\n### **PiCamera Mode NCS2 x 1 Stick + RaspberryPi3\uff08Asynchronous screen drawing / MultiStickSSDwithPiCamera_OpenVINO_NCS2.py\uff09**<br>\n```bash\n$ python3 MultiStickSSDwithPiCamera_OpenVINO_NCS2.py\n```\n![25](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/25.gif)<br>\n### **USB Camera Mode NCS2 x 1 Stick + RaspberryPi4\uff08Asynchronous screen drawing / MultiStickSSDwithUSBCamera_OpenVINO_NCS2.py\uff09**<br>\n```\n$ python3 MultiStickSSDwithUSBCamera_OpenVINO_NCS2.py\n```\n![26](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/26.gif)<br>\n<br>\n<br>\n## Motion diagram of MultiStick\n![20](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/20.png)<br>\n## Environment\n1\uff0eRaspberryPi3 + Raspbian Stretch (USB2.0 Port) or RaspberryPi3 + Ubuntu Mate or PC + Ubuntu16.04<br>\n2\uff0eIntel RealSense D435 (Firmware Ver 5.10.6) or USB Camera or PiCamera [Official stable version firmware](https://realsense.intel.com/intel-realsense-downloads/#firmware)<br>\n3\uff0eIntel Neural Compute Stick v1/v2 x\uff11piece or more<br>\n4-1\uff0eOpenCV 3.4.2 (NCSDK)  \n4-2\uff0eOpenCV 4.1.1-openvino (OpenVINO)  \n5\uff0eVFPV3 or TBB (Intel Threading Building Blocks)<br>\n6\uff0eNumpy<br>\n7\uff0ePython3.5<br>\n8\uff0eNCSDK v2.08.01 (It does not work with NCSDK v1.\u3000[v1 version is here](https://github.com/PINTO0309/MobileNet-SSD-RealSense/tree/v1.0))<br>\n9. OpenVINO 2019 R2.0.1  \n10\uff0eRealSenseSDK v2.16.5 (The latest version is unstable) [Official stable version SDK](https://realsense.intel.com/intel-realsense-downloads/#firmware)<br>\n11\uff0eHDMI Display<br>\n\n## Firmware update with Windows 10 PC\n1\uff0eZIP 2 types [(1) Firmware update tool for Windows 10](https://downloadmirror.intel.com/27514/eng/Intel%20RealSense%20D400%20Series%20DFU%20Tool%20for%20Windows.zip)\u3000[(2) The latest firmware bin file](https://downloadmirror.intel.com/28237/eng/Intel%C2%AE%20RealSense%E2%84%A2D400%20Series%20Signed%20Production%20Firmware%20v5_10_6.zip) Download and decompress<br>\n2\uff0eCopy Signed_Image_UVC_5_10_6_0.bin to the same folder as intel-realsense-dfu.exe<br>\n3\uff0eConnect RealSense D435 to USB port<br>\n4\uff0eWait for completion of installation of device driver<br>\n5\uff0eExecute intel-realsense-dfu.exe<br>\n6\uff0e\u300c1\u300d Type and press Enter and follow the instructions on the screen to update<br>\n![01](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/01.png)<br>\n7\uff0eFirmware version check \u300c2\u300d<br>\n![02](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/02.png)\n\n## Work with RaspberryPi3 (or PC + Ubuntu16.04 / RaspberryPi + Ubuntu Mate)\n### 1.NCSDK ver (Not compatible with NCS2)\n**Use of Virtualbox is not strongly recommended**<br>\n[Note] Japanese Article<br>\nhttps://qiita.com/akitooo/items/6aee8c68cefd46d2a5dc<br>\nhttps://qiita.com/kikuchi_kentaro/items/280ac68ad24759b4c091<br>\n<br>\n[Post of Official Forum]<br>\nhttps://ncsforum.movidius.com/discussion/950/problems-with-python-multiprocessing-using-sdk-2-0-0-4<br>\nhttps://ncsforum.movidius.com/discussion/comment/3921<br>\nhttps://ncsforum.movidius.com/discussion/comment/4316/#Comment_4316<br><br>\n\n1.Execute the following\n```bash\n$ sudo apt update;sudo apt upgrade\n$ sudo reboot\n```\n2.Extend the SWAP area (RaspberryPi+Raspbian Stretch / RaspberryPi+Ubuntu Mate Only)\n```bash\n$ sudo nano /etc/dphys-swapfile\nCONF_SWAPSIZE=2048\n\n$ sudo /etc/init.d/dphys-swapfile restart;swapon -s\n```\n3.Install NSCDK<br>\n```bash\n$ sudo apt install python-pip python3-pip\n$ sudo pip3 install --upgrade pip\n$ sudo pip2 install --upgrade pip\n\n$ cd ~/ncsdk\n$ make uninstall\n$ cd ~;rm -r -f ncsdk\n#=====================================================================================================\n# [Oct 10, 2018] NCSDK 2.08.01 , Tensorflow 1.9.0\n$ git clone -b ncsdk2 http://github.com/Movidius/ncsdk\n#=====================================================================================================\n$ cd ncsdk\n$ nano ncsdk.conf\n\n#MAKE_NJOBS=1\n\u2193\nMAKE_NJOBS=1\n\n$ sudo apt install cython\n$ sudo -H pip3 install cython\n$ sudo -H pip3 install numpy\n$ sudo -H pip3 install pillow\n$ make install\n\n$ cd ~\n$ wget https://github.com/google/protobuf/releases/download/v3.5.1/protobuf-all-3.5.1.tar.gz\n$ tar -zxvf protobuf-all-3.5.1.tar.gz\n$ cd protobuf-3.5.1\n$ ./configure\n$ sudo make -j1\n$ sudo make install\n$ cd python\n$ export LD_LIBRARY_PATH=../src/.libs\n$ python3 setup.py build --cpp_implementation \n$ python3 setup.py test --cpp_implementation\n$ sudo python3 setup.py install --cpp_implementation\n$ sudo ldconfig\n$ protoc --version\n\n# Before executing \"make examples\", insert Neural Compute Stick into the USB port of the device.\n$ cd ~/ncsdk\n$ make examples -j1\n```\n**\u3010Reference\u3011https://github.com/movidius/ncsdk**<br>\n\n4.Update udev rule\n```bash\n$ sudo apt install -y git libssl-dev libusb-1.0-0-dev pkg-config libgtk-3-dev\n$ sudo apt install -y libglfw3-dev libgl1-mesa-dev libglu1-mesa-dev\n\n$ cd /etc/udev/rules.d/\n$ sudo wget https://raw.githubusercontent.com/IntelRealSense/librealsense/master/config/99-realsense-libusb.rules\n$ sudo udevadm control --reload-rules && udevadm trigger\n```\n5.Upgrade to \"cmake 3.11.4\"\n```bash\n$ cd ~\n$ wget https://cmake.org/files/v3.11/cmake-3.11.4.tar.gz\n$ tar -zxvf cmake-3.11.4.tar.gz;rm cmake-3.11.4.tar.gz\n$ cd cmake-3.11.4\n$ ./configure --prefix=/home/pi/cmake-3.11.4\n$ make -j1\n$ sudo make install\n$ export PATH=/home/pi/cmake-3.11.4/bin:$PATH\n$ source ~/.bashrc\n$ cmake --version\ncmake version 3.11.4\n```\n6.Register LD_LIBRARY_PATH\n```bash\n$ nano ~/.bashrc\nexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n\n$ source ~/.bashrc\n```\n7.Install TBB (Intel Threading Building Blocks)\n```bash\n$ cd ~\n$ wget https://github.com/PINTO0309/TBBonARMv7/raw/master/libtbb-dev_2018U2_armhf.deb\n$ sudo dpkg -i ~/libtbb-dev_2018U2_armhf.deb\n$ sudo ldconfig\n```\n8.Uninstall old OpenCV (RaspberryPi Only)<br>\n**[Very Important] The highest performance can not be obtained unless VFPV3 is enabled.**\n```bash\n$ cd ~/opencv-3.x.x/build\n$ sudo make uninstall\n$ cd ~\n$ rm -r -f opencv-3.x.x\n$ rm -r -f opencv_contrib-3.x.x\n```\n9.Build install \"OpenCV 3.4.2\" or Install by deb package.<br>\n**[Very Important] The highest performance can not be obtained unless VFPV3 is enabled.**<br><br>\n**9.1 Build Install (RaspberryPi Only)**\n```bash\n$ sudo apt update && sudo apt upgrade\n$ sudo apt install build-essential cmake pkg-config libjpeg-dev libtiff5-dev \\\nlibjasper-dev libavcodec-dev libavformat-dev libswscale-dev \\\nlibv4l-dev libxvidcore-dev libx264-dev libgtk2.0-dev libgtk-3-dev \\\nlibcanberra-gtk* libatlas-base-dev gfortran python2.7-dev python3-dev\n\n$ cd ~\n$ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.4.2.zip\n$ unzip opencv.zip;rm opencv.zip\n$ wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.4.2.zip\n$ unzip opencv_contrib.zip;rm opencv_contrib.zip\n$ cd ~/opencv-3.4.2/;mkdir build;cd build\n$ cmake -D CMAKE_CXX_FLAGS=\"-DTBB_USE_GCC_BUILTINS=1 -D__TBB_64BIT_ATOMICS=0\" \\\n        -D CMAKE_BUILD_TYPE=RELEASE \\\n        -D CMAKE_INSTALL_PREFIX=/usr/local \\\n        -D INSTALL_PYTHON_EXAMPLES=OFF \\\n        -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.4.2/modules \\\n        -D BUILD_EXAMPLES=OFF \\\n        -D PYTHON_DEFAULT_EXECUTABLE=$(which python3) \\\n        -D INSTALL_PYTHON_EXAMPLES=OFF \\\n        -D BUILD_opencv_python2=ON \\\n        -D BUILD_opencv_python3=ON \\\n        -D WITH_OPENCL=OFF \\\n        -D WITH_OPENGL=ON \\\n        -D WITH_TBB=ON \\\n        -D BUILD_TBB=OFF \\\n        -D WITH_CUDA=OFF \\\n        -D ENABLE_NEON:BOOL=ON \\\n        -D ENABLE_VFPV3=ON \\\n        -D WITH_QT=OFF \\\n        -D BUILD_TESTS=OFF ..\n$ make -j1\n$ sudo make install\n$ sudo ldconfig\n```\n**9.2 Install by deb package (RaspberryPi Only) [I already activated VFPV3 and built it]**\n```bash\n$ cd ~\n$ sudo apt autoremove libopencv3\n$ wget https://github.com/PINTO0309/OpenCVonARMv7/raw/master/libopencv3_3.4.2-20180709.1_armhf.deb\n$ sudo apt install -y ./libopencv3_3.4.2-20180709.1_armhf.deb\n$ sudo ldconfig\n```\n\n10.Install Intel\u00ae RealSense\u2122 SDK 2.0\n```bash\n$ cd ~\n$ sudo apt update;sudo apt upgrade\n$ sudo apt install -y vulkan-utils libvulkan1 libvulkan-dev\n\n# Ubuntu16.04 Only\n$ sudo apt install -y mesa-utils* libglu1* libgles2-mesa-dev libopenal-dev gtk+-3.0\n\n# The latest version is unstable\n$ cd ~/librealsense/build\n$ sudo make uninstall\n$ cd ~\n$ sudo rm -rf librealsense\n\n$ git clone -b v2.16.5 https://github.com/IntelRealSense/librealsense.git\n$ cd ~/librealsense\n$ git checkout -b v2.16.5\n$ mkdir build;cd build\n\n$ cmake .. -DBUILD_EXAMPLES=true -DCMAKE_BUILD_TYPE=Release\n\n# For RaspberryPi3\n$ make -j1\nor\n# For LaptopPC\n$ make -j8\n\n$ sudo make install\n```\n11.Install Python binding\n```bash\n$ cd ~/librealsense/build\n\n#When using with Python 3.x series\n$ cmake .. -DBUILD_PYTHON_BINDINGS=bool:true -DPYTHON_EXECUTABLE=$(which python3)\n\nOR\n\n#When using with Python 2.x series\n$ cmake .. -DBUILD_PYTHON_BINDINGS=bool:true -DPYTHON_EXECUTABLE=$(which python)\n\n# For RaspberryPi3\n$ make -j1\nor\n# For LaptopPC\n$ make -j8\n\n$ sudo make install\n```\n12.Update PYTHON_PATH\n```bash\n$ nano ~/.bashrc\nexport PYTHONPATH=$PYTHONPATH:/usr/local/lib\n\n$ source ~/.bashrc\n```\n13.RealSense SDK import test\n```bash\n$ python3\nPython 3.5.3 (default, Jan 19 2017, 14:11:04) \n[GCC 6.3.0 20170124] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pyrealsense2\n>>> exit()\n```\n14.Installing the OpenGL package for Python\n```bash\n$ sudo apt-get install -y python-opengl\n$ sudo -H pip3 install pyopengl\n$ sudo -H pip3 install pyopengl_accelerate\n```\n15.Installation of the imutils package. (For PiCamera)\n```bash\n$ sudo apt-get install -y python3-picamera\n$ sudo -H pip3 install imutils --upgrade\n```\n16.Reduce the SWAP area to the default size (RaspberryPi+Raspbian Stretch / RaspberryPi+Ubuntu Mate Only)\n```bash\n$ sudo nano /etc/dphys-swapfile\nCONF_SWAPSIZE=100\n\n$ sudo /etc/init.d/dphys-swapfile restart;swapon -s\n```\n17.Clone a set of resources\n```bash\n$ git clone https://github.com/PINTO0309/MobileNet-SSD-RealSense.git\n```\n18.[Optional] Create a RAM disk folder for movie file placement\n```bash\n$ cd /etc\n$ sudo cp fstab fstab_org\n$ sudo nano fstab\n\n# Mount \"/home/pi/movie\" on RAM disk.\n# Add below.\ntmpfs /home/pi/movie tmpfs defaults,size=32m,noatime,mode=0777 0 0\n\n$ sudo reboot\n```\n<br>\n<br>\n\n### 2.OpenVINO ver (Corresponds to NCS2)\n1.Execute the following\n```bash\n$ sudo apt update;sudo apt upgrade\n$ sudo reboot\n```\n2.Extend the SWAP area (RaspberryPi+Raspbian Stretch / RaspberryPi+Ubuntu Mate Only)\n```bash\n$ sudo nano /etc/dphys-swapfile\nCONF_SWAPSIZE=2048\n\n$ sudo /etc/init.d/dphys-swapfile restart;swapon -s\n```\n3.Install OpenVINO\n```bash\n$ curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1rBl_3kU4gsx-x2NG2I5uIhvA3fPqm8uE\" > /dev/null\n$ CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n$ curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1rBl_3kU4gsx-x2NG2I5uIhvA3fPqm8uE\" -o l_openvino_toolkit_ie_p_2018.5.445.tgz\n$ tar -zxvf l_openvino_toolkit_ie_p_2018.5.445.tgz\n$ rm l_openvino_toolkit_ie_p_2018.5.445.tgz\n$ sed -i \"s|<INSTALLDIR>|$(pwd)/inference_engine_vpu_arm|\" inference_engine_vpu_arm/bin/setupvars.sh\n$ nano ~/.bashrc\n### Add 1 row below\nsource /home/pi/inference_engine_vpu_arm/bin/setupvars.sh\n\n$ source ~/.bashrc\n### Successful if displayed as below\n[setupvars.sh] OpenVINO environment initialized\n\n$ sudo usermod -a -G users \"$(whoami)\"\n$ sudo reboot\n\n$ uname -a\nLinux raspberrypi 4.14.79-v7+ #1159 SMP Sun Nov 4 17:50:20 GMT 2018 armv7l GNU/Linux\n\n$ sh inference_engine_vpu_arm/install_dependencies/install_NCS_udev_rules.sh\n### It is displayed as follows\nUpdate udev rules so that the toolkit can communicate with your neural compute stick\n[install_NCS_udev_rules.sh] udev rules installed\n```\n4.Update udev rule\n```bash\n$ sudo apt install -y git libssl-dev libusb-1.0-0-dev pkg-config libgtk-3-dev\n$ sudo apt install -y libglfw3-dev libgl1-mesa-dev libglu1-mesa-dev\n\n$ cd /etc/udev/rules.d/\n$ sudo wget https://raw.githubusercontent.com/IntelRealSense/librealsense/master/config/99-realsense-libusb.rules\n$ sudo udevadm control --reload-rules && udevadm trigger\n```\n5.Upgrade to \"cmake 3.11.4\"\n```bash\n$ cd ~\n$ wget https://cmake.org/files/v3.11/cmake-3.11.4.tar.gz\n$ tar -zxvf cmake-3.11.4.tar.gz;rm cmake-3.11.4.tar.gz\n$ cd cmake-3.11.4\n$ ./configure --prefix=/home/pi/cmake-3.11.4\n$ make -j1\n$ sudo make install\n$ export PATH=/home/pi/cmake-3.11.4/bin:$PATH\n$ source ~/.bashrc\n$ cmake --version\ncmake version 3.11.4\n```\n6.Register LD_LIBRARY_PATH\n```bash\n$ nano ~/.bashrc\nexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n\n$ source ~/.bashrc\n```\n7.Install Intel\u00ae RealSense\u2122 SDK 2.0\n```bash\n$ cd ~\n$ sudo apt update;sudo apt upgrade\n$ sudo apt install -y vulkan-utils libvulkan1 libvulkan-dev\n\n# Ubuntu16.04 Only\n$ sudo apt install -y mesa-utils* libglu1* libgles2-mesa-dev libopenal-dev gtk+-3.0\n\n# The latest version is unstable\n$ cd ~/librealsense/build\n$ sudo make uninstall\n$ cd ~\n$ sudo rm -rf librealsense\n\n$ git clone -b v2.16.5 https://github.com/IntelRealSense/librealsense.git\n$ cd ~/librealsense\n$ git checkout -b v2.16.5\n$ mkdir build;cd build\n\n$ cmake .. -DBUILD_EXAMPLES=false -DCMAKE_BUILD_TYPE=Release\n\n# For RaspberryPi3\n$ make -j1\nor\n# For LaptopPC\n$ make -j8\n\n$ sudo make install\n```\n8.Install Python binding\n```bash\n$ cd ~/librealsense/build\n\n#When using with Python 3.x series\n$ cmake .. -DBUILD_PYTHON_BINDINGS=bool:true -DPYTHON_EXECUTABLE=$(which python3)\n\nOR\n\n#When using with Python 2.x series\n$ cmake .. -DBUILD_PYTHON_BINDINGS=bool:true -DPYTHON_EXECUTABLE=$(which python)\n\n# For RaspberryPi3\n$ make -j1\nor\n# For LaptopPC\n$ make -j8\n\n$ sudo make install\n```\n9.Update PYTHON_PATH\n```bash\n$ nano ~/.bashrc\nexport PYTHONPATH=$PYTHONPATH:/usr/local/lib\n\n$ source ~/.bashrc\n```\n10.RealSense SDK import test\n```bash\n$ python3\nPython 3.5.3 (default, Jan 19 2017, 14:11:04) \n[GCC 6.3.0 20170124] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pyrealsense2\n>>> exit()\n```\n11.Installing the OpenGL package for Python\n```bash\n$ sudo apt-get install -y python-opengl\n$ sudo -H pip3 install pyopengl\n$ sudo -H pip3 install pyopengl_accelerate\n```\n12.Installation of the imutils package. (For PiCamera)\n```bash\n$ sudo apt-get install -y python3-picamera\n$ sudo -H pip3 install imutils --upgrade\n```\n13.Reduce the SWAP area to the default size (RaspberryPi+Raspbian Stretch / RaspberryPi+Ubuntu Mate Only)\n```bash\n$ sudo nano /etc/dphys-swapfile\nCONF_SWAPSIZE=100\n\n$ sudo /etc/init.d/dphys-swapfile restart;swapon -s\n```\n14.Clone a set of resources\n```bash\n$ git clone https://github.com/PINTO0309/MobileNet-SSD-RealSense.git\n```\n15.[Optional] Create a RAM disk folder for movie file placement\n```bash\n$ cd /etc\n$ sudo cp fstab fstab_org\n$ sudo nano fstab\n\n# Mount \"/home/pi/movie\" on RAM disk.\n# Add below.\ntmpfs /home/pi/movie tmpfs defaults,size=32m,noatime,mode=0777 0 0\n\n$ sudo reboot\n```\n<br>\n<br>\n\n## Execute the program\n```\n$ python3 MultiStickSSDwithRealSense.py <option1> <option2> ...\n\n<options>\n -grp MVNC graphs Path. (Default=./)\n -mod Camera Mode. (0:=RealSense Mode, 1:=USB Camera Mode. Defalut=0)\n -wd\u3000Width of the frames in the video stream. (USB Camera Mode Only. Default=320)\n -ht\u3000Height of the frames in the video stream. (USB Camera Mode Only. Default=240)\n -tp\u3000TransparentMode. (RealSense Mode Only. 0:=No background transparent, 1:=Background transparent. Default=0)\n -sd\u3000SSDDetectionMode. (0:=Disabled, 1:=Enabled. Default=1)\n -fd\u3000FaceDetectionMode. (0:=Disabled, 1:=Enabled. Default=0)\n -snc stick_num_of_cluster. Number of sticks to be clustered. (0:=Clustering invalid, n:=Number of sticks Default=0)\n -csc cluster_switch_cycle. Cycle of switching active cluster. (n:=millisecond Default=10000)\n -cst cluster_switch_temperature. Temperature threshold to switch active cluster. (n.n:=temperature(Celsius) Default=65.0)\n```\n(Example0) **[MobileNet-SSD + Neural Compute Stick + RealSense D435 Mode + Syncronous](#realsense-mode-about-65-fps-detection--synchronous-screen-drawing--singlestickssdwithrealsensepy)**\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 SingleStickSSDwithRealSense.py\n```\n\n\n(Example1) **[MobileNet-SSD + Neural Compute Stick + RealSense D435 Mode + Asynchronous](#realsense-mode-about-250-fps-asynchronous-screen-drawing--multistickssdwithrealsensepy)**\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 MultiStickSSDwithRealSense.py\n```\n\n(Example2) **[MobileNet-SSD + Neural Compute Stick + USB Camera Mode + Asynchronous](#usb-camera-mode-multistick-x4-boosted-160-fps-asynchronous-screen-drawing--multistickssdwithrealsensepy)**\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 MultiStickSSDwithRealSense.py -mod 1 -wd 640 -ht 480\n$ python3 MultiStickSSDwithRealSense.py -mod 1 -wd 320 -ht 240\n```\n\n(Example3) **[MobileNet-SSD + Neural Compute Stick + RealSense D435 Mode + Asynchronous + Transparent background in real time](#realsense-mode-singlestick-about-50-fpstransparent-background-in-real-time--asynchronous-screen-drawing--multistickssdwithrealsensepy)**\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 MultiStickSSDwithRealSense.py -tp 1\n```\n\n(Example4) **[MobileNet-SSD + FaceDetection + Neural Compute Stick + USB Camera Mode + Asynchronous ](#usb-camera-mode-multistick-x3-boosted-asynchronous-screen-drawing--multigraphssdfacedetection--facedetection--multistickssdwithrealsensepy)**\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 MultiStickSSDwithRealSense.py -mod 1 -wd 640 -ht 480 -fd 1\n```\n\n(Example5) **To prevent thermal runaway, simple clustering function (2 Stick = 1 Cluster)**<br><br>\nWhen a certain cycle or constant temperature is reached, the active cluster switches seamlessly automatically.<br>\nYou must turn on the clustering enable flag.<br>\nThe default switch period is 10 seconds, the default temperature threshold is 65\u00b0C.<br>\nThe number, cycle, and temperature of sticks constituting one cluster can be specified by the start parameter.<br>\nDepending on your environment, please tune to the optimum parameters yourself.<br><br>\n **[1] Number of all sticks = 5<br>\n [2] stick_num_of_cluster = 2<br>\n [3] cluster_switch_cycle = 10sec (10,000millisec)<br>\n [4] cluster_switch_temperature = 65.0\u2103**<br>\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n\n$ cd ~/MobileNet-SSD-RealSense\n$ python3 MultiStickSSDwithRealSense.py -mod 1 -snc 2 -csc 10000 -cst 65.0\n```\n**[Simplified drawing of cluster switching]**<br>\n![14](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/14.png)<br>\n**[Execution log]**<br>\n![15](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/15.png)<br><br>\n\n(Example6)\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G2 GL (Fake KMS)\"\n$ realsense-viewer\n```\n![05](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/05.gif)\n\n(Example7)\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n\n$ cd ~/librealsense/wrappers/opencv/build/grabcuts\n$ rs-grabcuts\n```\n![06](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/06.gif)\n\n(Example8)\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n\n$ cd ~/librealsense/wrappers/opencv/build/imshow\n$ rs-imshow\n```\n![07](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/07.gif)\n\n(Example9) MobileNet-SSD(OpenCV-DNN) + RealSense D435 + Without Neural Compute Stick\n```\n$ sudo raspi-config\n\"7.Advanced Options\" - \"A7 GL Driver\" - \"G3 Legacy\"\n\n$ cd ~/librealsense/wrappers/opencv/build/dnn\n$ rs-dnn\n```\n![08](https://github.com/PINTO0309/MobileNet-SSD-RealSense/blob/master/media/08.gif)\n<br>\n<br>\n\n## \u3010Reference\u3011 MobileNetv2 Model (Caffe) Great Thanks!!\n**https://github.com/xufeifeiWHU/Mobilenet-v2-on-Movidius-stick.git**\n\n## Conversion method from Caffe model to NCS model - NCSDK\n```bash\n$ cd ~/MobileNet-SSD-RealSense\n$ mvNCCompile ./caffemodel/MobileNetSSD/deploy.prototxt -w ./caffemodel/MobileNetSSD/MobileNetSSD_deploy.caffemodel -s 12\n$ mvNCCompile ./caffemodel/Facedetection/fullface_deploy.prototxt -w ./caffemodel/Facedetection/fullfacedetection.caffemodel -s 12\n$ mvNCCompile ./caffemodel/Facedetection/shortface_deploy.prototxt -w ./caffemodel/Facedetection/shortfacedetection.caffemodel -s 12\n```\n## Conversion method from Caffe model to NCS model - OpenVINO\n```bash\n$ cd ~/MobileNet-SSD-RealSense\n$ sudo python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n--input_model caffemodel/MobileNetSSD/MobileNetSSD_deploy.caffemodel \\\n--input_proto caffemodel/MobileNetSSD/MobileNetSSD_deploy.prototxt \\\n--data_type FP16 \\\n--batch 1\n```\nor\n```bash\n$ cd ~/MobileNet-SSD-RealSense\n$ sudo python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n--input_model caffemodel/MobileNetSSD/MobileNetSSD_deploy.caffemodel \\\n--input_proto caffemodel/MobileNetSSD/MobileNetSSD_deploy.prototxt \\\n--data_type FP32 \\\n--batch 1\n```\n## Construction of learning environment and simple test for model (Ubuntu16.04 x86_64 PC + GPU[NVIDIA Geforce])\n1.**\u3010Example\u3011** Introduction of NVIDIA-Driver, CUDA and cuDNN to the environment with GPU\n```\n$ sudo apt-get remove nvidia-*\n$ sudo apt-get remove cuda-*\n\n$ apt search \"^nvidia-[0-9]{3}$\"\n$ sudo apt install cuda-9.0\n$ sudo reboot\n$ nvidia-smi\n\n### Download cuDNN v7.2.1 NVIDIA Home Page\n### libcudnn7_7.2.1.38-1+cuda9.0_amd64.deb\n### libcudnn7-dev_7.2.1.38-1+cuda9.0_amd64.deb\n### cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb\n### cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb\n### cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb\n### cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-3_1.0-1_amd64.deb\n### cuda-repo-ubuntu1604-9-0-176-local-patch-4_1.0-1_amd64.deb\n\n$ sudo dpkg -i libcudnn7*\n$ sudo dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb\n$ sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n$ sudo apt update\n$ sudo dpkg -i cuda-repo-ubuntu1604-9*\n$ sudo apt update\n$ rm libcudnn7_7.2.1.38-1+cuda9.0_amd64.deb;rm libcudnn7-dev_7.2.1.38-1+cuda9.0_amd64.deb;rm cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb;rm cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb;rm cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb;rm cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-3_1.0-1_amd64.deb;rm cuda-repo-ubuntu1604-9-0-176-local-patch-4_1.0-1_amd64.deb\n\n$ echo 'export PATH=/usr/local/cuda-9.0/bin:${PATH}' >> ~/.bashrc\n$ echo 'export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:${LD_LIBRARY_PATH}' >> ~/.bashrc\n$ source ~/.bashrc\n$ sudo ldconfig\n$ nvcc -V\n$ cd ~;nano cudnn_version.cpp\n\n#include <cudnn.h>\n#include <iostream>\n\nint main(int argc, char** argv) {\n    std::cout << \"CUDNN_VERSION: \" << CUDNN_VERSION << std::endl;\n    return 0;\n}\n\n$ nvcc cudnn_version.cpp -o cudnn_version\n$ ./cudnn_version\n\n$ sudo pip2 uninstall tensorflow-gpu\n$ sudo pip2 install tensorflow-gpu==1.10.0\n$ sudo pip3 uninstall tensorflow-gpu\n$ sudo pip3 install tensorflow-gpu==1.10.0\n```\n2.**\u3010Example\u3011** Introduction of Caffe to environment with GPU\n```\n$ cd ~\n$ sudo apt install libopenblas-base libopenblas-dev\n$ git clone https://github.com/weiliu89/caffe.git\n$ cd caffe\n$ git checkout ssd\n$ cp Makefile.config.example Makefile.config\n$ nano Makefile.config\n```\n\n```\n# cuDNN acceleration switch (uncomment to build with cuDNN).\n#USE_CUDNN := 1\n\u2193\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1\n\n# Uncomment if you're using OpenCV 3\n# OPENCV_VERSION := 3\n\u2193\n# Uncomment if you're using OpenCV 3\nOPENCV_VERSION := 3\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n\u2193\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda-9.0\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the lines after *_35 for compatibility.\nCUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\\n             -gencode arch=compute_20,code=sm_21 \\\n             -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61\n\u2193\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the lines after *_35 for compatibility.\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include\n\u2193\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include \\\n                /usr/local/lib/python2.7/dist-packages/numpy/core/include\n\n\n# Uncomment to support layers written in Python (will link against Python libs)\n# WITH_PYTHON_LAYER := 1\n\u2193\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib\n\u2193\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include \\\n                /usr/include/hdf5/serial\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib \\\n                /usr/lib/x86_64-linux-gnu/hdf5/serial\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\u2193\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\nUSE_PKG_CONFIG := 1\n```\n\n```\n$ rm -r -f build\n$ rm -r -f .build_release\n$ make superclean\n$ make all -j4\n$ make test -j4\n$ make distribute -j4\n$ export PYTHONPATH=/home/<username>/caffe/python:$PYTHONPATH\n$ make py\n```\n\n3.Download of VGG model [My Example CAFFE_ROOT PATH = \"/home/\\<username\\>/caffe\"]\n```\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ cd $CAFFE_ROOT/models/VGGNet\n$ wget http://cs.unc.edu/~wliu/projects/ParseNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel\n```\n\n4.Download VOC 2007 and VOC 2012 datasets\n\n```\n# Download the data.\n$ cd ~;mkdir data;cd data\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar #<--- 1.86GB\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar #<--- 438MB\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar #<--- 430MB\n\n# Extract the data.\n$ tar -xvf VOCtrainval_11-May-2012.tar\n$ tar -xvf VOCtrainval_06-Nov-2007.tar\n$ tar -xvf VOCtest_06-Nov-2007.tar\n$ rm VOCtrainval_11-May-2012.tar;rm VOCtrainval_06-Nov-2007.tar;rm VOCtest_06-Nov-2007.tar\n```\n\n5.Generate lmdb file\n```\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ cd $CAFFE_ROOT\n# Create the trainval.txt, test.txt, and test_name_size.txt in $CAFFE_ROOT/data/VOC0712/\n$ ./data/VOC0712/create_list.sh\n\n# You can modify the parameters in create_data.sh if needed.\n# It will create lmdb files for trainval and test with encoded original image:\n#   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb\n#   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb\n# and make soft links at examples/VOC0712/\n\n$ ./data/VOC0712/create_data.sh\n```\n\n6.Execution of learning [My Example environment GPU x1, GeForce GT 650M = RAM:2GB]<br><br>\nAdjust according to the number of GPU\n```\n# It will create model definition files and save snapshot models in:\n#   - $CAFFE_ROOT/models/VGGNet/VOC0712/SSD_300x300/\n# and job file, log file, and the python script in:\n#   - $CAFFE_ROOT/jobs/VGGNet/VOC0712/SSD_300x300/\n# and save temporary evaluation results in:\n#   - $HOME/data/VOCdevkit/results/VOC2007/SSD_300x300/\n# It should reach 77.* mAP at 120k iterations.\n\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ export PYTHONPATH=/home/<username>/caffe/python:$PYTHONPATH\n$ cd $CAFFE_ROOT\n$ cp examples/ssd/ssd_pascal.py examples/ssd/BK_ssd_pascal.py\n$ nano examples/ssd/ssd_pascal.py\n```\n\n```\n# Solver parameters.\n# Defining which GPUs to use.\ngpus = \"0,1,2,3\"\n\u2193\n# Solver parameters.\n# Defining which GPUs to use.\ngpus = \"0\"\n```\n\nAdjust according to GPU performance (Memory Size) [My Example GeForce GT 650M x1 = RAM:2GB]\n```\n# Divide the mini-batch to different GPUs.\nbatch_size = 32\naccum_batch_size = 32\n\u2193\n# Divide the mini-batch to different GPUs.\nbatch_size = 1\naccum_batch_size = 1\n```\n\nExecution\n- The learned data is generated in \"$CAFFE_ROOT/models/VGGNet/VOC0712/SSD_300x300\"\n- VGG_VOC0712_SSD_300x300_iter_n.caffemodel\n- VGG_VOC0712_SSD_300x300_iter_n.solverstate\n```\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ export PYTHONPATH=/home/<username>/caffe/python:$PYTHONPATH\n$ cd $CAFFE_ROOT\n$ python examples/ssd/ssd_pascal.py\n```\n\n7.Evaluation of learning data (still image)\n```\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ export PYTHONPATH=/home/<username>/caffe/python:$PYTHONPATH\n$ cd $CAFFE_ROOT\n# If you would like to test a model you trained, you can do:\n$ python examples/ssd/score_ssd_pascal.py\n```\n\n8.Evaluation of learning data (USB camera)\n```\n$ export CAFFE_ROOT=/home/<username>/caffe\n$ export PYTHONPATH=/home/<username>/caffe/python:$PYTHONPATH\n$ cd $CAFFE_ROOT\n# If you would like to attach a webcam to a model you trained, you can do:\n$ python examples/ssd/ssd_pascal_webcam.py\n```\n\n## Reference articles, thanks\nhttps://github.com/movidius/ncappzoo/tree/master/caffe/SSD_MobileNet<br>\nhttps://github.com/FreeApe/VGG-or-MobileNet-SSD<br>\nhttps://github.com/chuanqi305/MobileNet-SSD<br>\nhttps://github.com/avBuffer/MobilenetSSD_caffe<br>\nhttps://github.com/Coldmooon/SSD-on-Custom-Dataset<br>\nhttps://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide#the-gpu-support-prerequisites<br>\nhttps://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training<br>\nhttps://github.com/CongWeilin/mtcnn-caffe<br>\nhttps://github.com/DuinoDu/mtcnn.git<br>\nhttps://www.hackster.io/mjrobot/real-time-face-recognition-an-end-to-end-project-a10826<br>\nhttps://github.com/Mjrovai/OpenCV-Face-Recognition.git<br>\nhttps://github.com/sgxu/face-detection-based-on-caffe.git<br>\nhttps://github.com/RiweiChen/DeepFace.git<br>\nhttps://github.com/KatsunoriWa/eval_faceDetectors<br>\nhttps://github.com/BeloborodovDS/MobilenetSSDFace<br>\nhttps://www.pyimagesearch.com/2018/09/03/semantic-segmentation-with-opencv-and-deep-learning/<br>\nhttps://github.com/TimoSaemann/ENet/tree/master/Tutorial<br>\nhttps://blog.amedama.jp/entry/2017/04/03/235901<br>\nhttps://github.com/NVIDIA/nvidia-docker<br>\nhttps://hub.docker.com/r/nvidia/cuda/<br>\nhttps://www.dlology.com/blog/how-to-run-keras-model-on-movidius-neural-compute-stick/<br>\nhttps://ncsforum.movidius.com/discussion/1106/ncs-temperature-issue<br>\nhttps://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend<br>\nhttps://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#raspbian-stretch<br>\nhttps://github.com/skhameneh/OpenVINO-ARM64<br>\n"
 },
 {
  "repo": "fengjian0106/hed-tutorial-for-document-scanning",
  "language": "C++",
  "readme_contents": "# HED-tutorial-for-document-scanning\nCode for blog [\u624b\u673a\u7aef\u8fd0\u884c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u6587\u6863\u68c0\u6d4b\u529f\u80fd(\u4e8c) -- \u4ece VGG \u5230 MobileNetV2 \u77e5\u8bc6\u68b3\u7406](http://fengjian0106.github.io/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/)\n\n## get code\n```\ngit clone https://github.com/fengjian0106/hed-tutorial-for-document-scanning\n```\n\n## how to run\n#### _1_ Prepare image resources, synthesize training samples\n\n_1.1_ background image is downloaded to the ./sample\\_images/background\\_images directory.  \n\n_1.2_ The foreground image is downloaded to the ./sample\\_images/rect\\_images directory.\n\n#### 2 Synthesize training samples using the iPhone simulator\n_2.1_ Open the ./generate\\_training\\_dataset/generate\\_training\\_dataset.xcodeproj project.\n\n- Check the loadImagePaths function of ViewController.m: make sure that self.backgroundImagesPath and self.rectImagesPath point to the directories mentioned in 1.1 and 1.2 respectively, then run The program (_dataset.xcodeproj project) by pressing Run button on the xcode. Choose an iphone simulator to run this code (Please dont run this code on apple device) , and based on the printed log information, find the directory corresponding to self.imageSaveFolder on the Mac, the generated sample image will be saved in this directory.\n\n- Basically you will observe two types of images : image1_color.jpg, image1_annotation.png. image2_color.jpg, image2_annotation.png ... etc. (I have attached few images in ```>> dataset->generate_sample_by_ios_image_size_256_256_thickness_0.2  (folder)``` for my reference)\n\n_2.2_ Move all the images generated in 2.1 to the ./dataset/generate\\_sample\\_by\\_ios\\_image\\_size\\_256\\_256\\_thickness\\_0.2 directory.\n\n_2.3_ The white rectangle border drawn on the UIView is smoothed. The white *Point* corresponds to a pixel value other than 255. Therefore, you need to binarize these white *Point*s and run the following program. :\n\n```\npython preprocess_generate_training_dataset.py \\\n\t\t\t--dataset_root_dir dataset \\\n\t\t\t--dataset_folder_name generate_sample_by_ios_image_size_256_256_thickness_0.2\n```                                        \n\nAfter this program is executed, you will get the ./dataset/generate\\_sample\\_by\\_ios\\_image\\_size\\_256\\_256\\_thickness\\_0.2.csv file.\n\n_2.4_ Use the *gshuf* tool to randomly mess up the contents of the ./dataset/generate\\_sample\\_by\\_ios\\_image\\_size\\_256\\_256\\_thickness\\_0.2.csv file and execute the following command:\n\n```\ngshuf ./dataset/generate_sample_by_ios_image_size_256_256_thickness_0.2.csv > ./dataset/temp.txt\ngshuf ./dataset/temp.txt > ./dataset/generate_sample_by_ios_image_size_256_256_thickness_0.2.csv\n```\n\nAfter performing this step, a batch of synthetic training sample images is obtained.\n\nThe process of preparing the training samples should be customized according to the specific needs. Here is just a reference method. \n\n#### _3_ Training Network\nRun the following program:\n\n```\npython train_hed.py --dataset_root_dir dataset \\\n                    --csv_path dataset/generate_sample_by_ios_image_size_256_256_thickness_0.2.csv \\\n                    --display_step 5\n```\nAfter the training is done you get additional two types of images in the dataset->generate_sample_by_ios_image_size_256_256_thickness_0.2 folder these new images have names image1_annotation_gray.png and image1_annotation_thresh_gray.png. image2_annotation_gray.png and image2_annotation_thresh_gray.png ... etc\n\n#### _4_ Testing HED networks in a python environment\nRun the following program to process a picture:\n\n```\npython evaluate_hed.py --checkpoint_dir checkpoint \\\n                       --image test_image/test27.jpg \\\n                       --output_dir test_image\n```\n\n#### _5_ Run a complete process in the iPhone real-world environment, including running the HED network and executing an OpenCV-based point-finding algorithm\n_5.1_ Export the model file in pb format and run the following program:\n\n```\npython freeze_model.py --checkpoint_dir checkpoint\n```\n\nAfter running successfully, you can see a model file named *hed_graph.pb* in the ./checkpoint directory, which will be loaded in the iOS program.\n\n_5.2_ Running the iOS demo program\n\n*./ios\\_demo/DemoWithStaticLib/DemoWithStaticLib.xcodeproj* is a demo program that contains compiled static libraries of various dependencies and can be run directly. There is a complete process in the demo. The first step is to call the HED network to get the edge detection map. The second step is to execute the algorithm for finding the quadrilateral vertices.\n\n_5.3_ Compile FMHEDNet static library\n\n*./ios\\_demo/FMHEDNet/FMHEDNet.xcodeproj* is a static library project that encapsulates the call to the HED network to avoid introducing TensorFlow source files into the project files of the business layer app. If you want to compile this FMHEDNet static library, you need to compile TensorFlow Mobile first. For how to compile TensorFlow Mobile, please see _5.4_ below. For the process of compiling FMHEDNet, please see [here] (https://github.com/fengjian0106/hed-tutorial-for-document-scanning/blob/master/ios_demo/FMHEDNet/FMHEDNet/FMHEDNet.mm).\n\n_5.4_ Compile TensorFlow Mobile\nTensorFlow's [official documentation] (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) has instructions for compiling. I am using a manually cropped version and have modified the namespace in the Protobuf source. For details, see [here] (https://github.com/fengjian0106/hed-tutorial-for-document-scanning/blob/master /how_to_build_tensorflow_and_change_namespace_of_protobuf.txt).\n\n"
 },
 {
  "repo": "beartung/tclip-android",
  "language": "C++",
  "readme_contents": "#Bitmap Smart Clipping using OpenCV\n\n### based on [http://code.taobao.org/p/tclip/](http://code.taobao.org/p/tclip/)\n\n#### recommend iOS version: [https://github.com/croath/UIImageView-BetterFace](https://github.com/croath/UIImageView-BetterFace)\n\n#Demo Screenshots\n\n![demo screenshots](https://raw.github.com/beartung/tclip-android/master/screenshots/s1.png \"Demo Screenshots\")\n\n\n#Features\n\n* using OpenCV to detect faces firstly, if have faces, won't cut faces off\n* using OpenCV to detect other characters secondly, if found significant zone, won't cut it off\n* using FAST feature detector instead of SURF, thanks for [@yanunon](https://github.com/yanunon)\n\n#Usage\n\n* copy config file to app dir\n\n    ```\n    String configPath = TClip.copyConfig(context, TClip.CONFIG, R.raw.haarcascade_frontalface_alt);\n    ```\n\n* get cropped bitmap\n\n    ```\n    //config: /data/data/com.demo.tclip.debug/haarcascade_frontalface_alt.xml\n    Bitmap ret = TClip.crop(configPath, sourceBitmap, width, height);\n    ```\n\n#Build\n\n* download OpenCV & unzip to /home/user/opencv-android-sdk\n* export OPENCV_PACKAGE_DIR=\"/home/user/opencv-android-sdk\"\n* ~~libnonfree.so & libopencv_java.so from [beartung/nonfree-opencv-android](https://github.com/beartung/nonfree-opencv-android)~~\n* using FAST feature detector without libnonfree\n* cd jni && ndk-build\n"
 },
 {
  "repo": "SimonShi1994/Princess-connection-farm",
  "language": "Python",
  "readme_contents": " <img src=\"webclient/src/assets/logo.jpg\" width = \"80\" height = \"80\" alt=\"LOGO\" align=center />\n\n # Princess connection \u516c\u4e3b\u8fde\u7ed3\u519c\u573a\u811a\u672cv2.1.20201116\n\n![](https://img.shields.io/badge/license-GPL--3.0-blue)![](https://img.shields.io/badge/opencv-2.0-blue)![](https://img.shields.io/badge/UIAutomator-2-blue)\n\n## \u7b80\u4ecb\n\n\u6b64\u9879\u76ee\u4e3a\u56fd\u670d\u516c\u4e3b\u8fde\u7ed3\u811a\u672c\uff0c\u4f7f\u7528opencv\u56fe\u50cf\u8bc6\u522b\u8fdb\u884c\u6309\u94ae\u5206\u6790\u3002\u672c\u9879\u76ee\u57fa\u4e8e\u516c\u4e3b\u8fde\u63a5opencv\u9ad8\u7ea7\u811a\u672c(https://github.com/bbpp222006/Princess-connection) \u5f00\u53d1\u3002\n\n**\u652f\u6301\u6a21\u62df\u5668\u591a\u5f00**\n\n**\u652f\u6301\u81ea\u52a8\u586b\u5199\u9a8c\u8bc1\u7801**\n\n**\u652f\u630124\u5c0f\u65f6\u6302\u673a**\n\n**\u652f\u630140to1mana\u519c\u573a**\n\n**\u652f\u6301\u6bcf\u65e5\u81ea\u52a8\u4e09\u6350**\n\n## \u8be6\u7ec6\u529f\u80fd\n\n1. \u884c\u4f1a\n\n- [x] \u7ec4\u5efa\u884c\u4f1a\n- [x] \u652f\u63f4\u52a9\u6218\n- [x] \u884c\u4f1a\u6350\u8d60\n- [x] \u884c\u4f1a\u70b9\u8d5e\n\n2. \u5730\u4e0b\u57ce\n\n- [x] \u5730\u4e0b\u57ce\u501f\u4eba\n- [x] \u901a\u5173\u5730\u4e0b\u57ce\n\n3. \u7ade\u6280\u573a\n\n- [x] \u6218\u6597\u7ade\u6280\u573a\n- [x] \u516c\u4e3b\u7ade\u6280\u573a\n\n4. \u65e5\u5e38\u4efb\u52a1\n\n- [x] \u5bb6\u56ed\u9886\u53d6\n- [x] \u514d\u8d39\u626d\u86cb\n- [x] \u514d\u8d39\u5341\u8fde\n- [x] \u6536\u53d6\u793c\u7269\n- [x] \u6536\u53d6\u4efb\u52a1\n- [x] \u8d2d\u4e70\u4f53\u529b\n- [x] \u8d2d\u4e70mana\n- [x] \u8d2d\u4e70\u7ecf\u9a8c\n- [x] \u63a2\u7d22\n- [x] \u5723\u8ff9\u8c03\u67e5\n\n5. \u5de5\u5177\n\n- [x] \u8d26\u53f7\u91cd\u547d\u540d\n- [x] box\u622a\u56fe\n- [x] OCR\u83b7\u53d6\u8d26\u53f7\u4fe1\u606f\n- [x] \u5356\u51fa\u8fc7\u91cf\u88c5\u5907\n- [x] \u6682\u505c\u624b\u64cd\n\n6. \u5237\u56fe\n\n- [x] \u5237\u7ecf\u9a8c\n- [x] \u526f\u672c\u626b\u8361\n- [x] \u521d\u59cb\u5316\n- [x] \u81ea\u52a8\u63a8\u56fe\n\n## \u73af\u5883\n\n\u9700\u8981 Python **64\u4f4d**\u7248\u672c>=3.6\uff08\u5b89\u88c5\u65f6\u8bb0\u5f97\u628a\u5e26\u6709**PATH**\u5b57\u6bcd\u9009\u9879\u7684\u52fe\u4e0a\uff09**\u4e0d\u89813.9\uff01\uff01\uff01**\n\n\u9700\u8981\u6267\u884c\u6307\u4ee4\u5b89\u88c5\u4f9d\u8d56:\n\n```\npip install -r requirements.txt\n```\n\n\u82e5\u4f7f\u7528\u6a21\u62df\u5668\uff0c\u5219\u53ef\u80fd\u9700\u8981\u5c06\u6a21\u62df\u5668\u8bbe\u7f6e\u4e3a\u6865\u63a5\u6a21\u5f0f\uff0c\u540c\u65f6\u9700\u8981\u6253\u5f00\u5f00\u53d1\u8005usb\u8c03\u8bd5\u3002\n\n\u5efa\u8bae\u4f7f\u7528\u96f7\u7535\u6a21\u62df\u56684\uff0c\u672c\u9879\u76ee\u4e2d\u5747\u4ee5\u96f7\u7535\u6a21\u62df\u56684\u4e3a\u793a\u4f8b\u3002\n\n**\u91cd\u8981\uff1a\u6a21\u62df\u5668\u5206\u8fa8\u7387\u8981\u6c42540*960**\n\n**\u91cd\u8981**\uff1a\u76ee\u524d\u5173\u4e8eAPI\u90e8\u5206\u5df2\u7ecf\u79fb\u5165 config.ini \u4e2d\uff0c\u5982\u4f55\u586b\u5165\u8bf7\u53c2\u8003\u76ee\u5f55\u4e0b\u7684md\u6587\u4ef6\n\n\u5982\u4f55\u7533\u8bf7\u767e\u5ea6\u6587\u5b57\u8bc6\u522bapikey\u548cSecret Key:(https://blog.csdn.net/biao197/article/details/102907492 )\n\nServer\u9171\u98df\u7528\u65b9\u6cd5\uff1a(http://sc.ftqq.com/3.version)\n\n## \u4f7f\u7528\u65b9\u5f0f\n\n- \u73af\u5883\u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u518d\u68c0\u67e5\u6a21\u62df\u5668\u5206\u8fa8\u7387\u4e3a540*960\u3002\u786e\u8ba4\u65e0\u8bef\n- \u4f7f\u7528OCR\u76f8\u5173\u7684\u670d\u52a1\uff0c\u8bf7\u5148\u542f\u52a8**app.py**(\u53cc\u51fb/`python app.py`)\n- \u8f93\u5165`python main_new.py`\uff0c\u542f\u52a8\u811a\u672c\u3002\u8be5\u9879\u76ee\u652f\u6301\u63a7\u5236\u53f0\uff0c\u53ef\u4ee5\u8f93\u5165help\u67e5\u770b\u5e2e\u52a9\u3002\n- \u51fa\u73b0\u201cNo module named 'XXX'\uff0c\u8bf7\u5728\u9879\u76ee\u76ee\u5f55\u6267\u884c`pip install -r requirements.txt`\u91cd\u65b0\u5b89\u88c5\u4f9d\u8d56\n- \u7b2c\u4e00\u6b21\u6b63\u5e38\u8fd0\u884c\u540e\uff0c\u5c31\u53ef\u4ee5\u901a\u8fc7**run.bat**\u6765\u4e00\u952e\u542f\u52a8\n- **\u7b2c\u4e00\u6b21\u4f7f\u7528\uff0c\u5b8c\u5168\u4e0d\u61c2\u600e\u4e48\u529e\uff1f** \n\n[Schedule\u4f7f\u7528\u65b9\u6cd5](docs/introduce_to_schedule.md)\n\n[\u5982\u4f55\u63a5\u5165\u6253\u7801\u5e73\u53f0](docs/\u5982\u4f55\u63a5\u5165\u6253\u7801\u5e73\u53f0.md)\n\n- \u611f\u89c9\u8fd8\u662f\u4e0d\u4f1a\u4f7f\u7528\uff0c\u600e\u4e48\u529e\uff1f\n\n\u66f4\u8be6\u7ec6\u7684\u4f7f\u7528\u65b9\u6cd5\u4f1a\u9646\u7eed\u66f4\u65b0\uff0c\u6211\u4eec\u4e5f\u4f1a\u5c3d\u5feb\u7b80\u5316\u4f7f\u7528\u65b9\u5f0f\u53ca\u4e0a\u7ebfWebGUI\u63a7\u5236\u7248\u672c\uff0c\u656c\u8bf7\u671f\u5f85\uff01\u4e5f\u6b22\u8fce\u5927\u5bb6\u5165\u7fa4\u4ea4\u6d41\u8ba8\u8bba\u3002\u2193\u2193\n\n\n## \u989d\u5916\u8bf4\u660e\n\n1. \u8bf7\u4e0d\u8981\u7528\u4e8e\u5546\u4e1a\u7528\u9014\u3002\u4ee3\u7801\u4ea4\u6d41\u548cbug\u53cd\u9988\u8bf7\u52a0\u7fa4\u52a0qq\u7fa4 1130884619\n\n   ![image](https://s1.ax1x.com/2020/06/26/NsXjh9.png)\n\n2. \u611f\u8c22CyiceK(https://github.com/1076472672) \u3001Dr-Bluemond(https://github.com/Dr-Bluemond) \u3001TheAutumnOfRice(https://github.com/TheAutumnOfRice) \u5bf9\u672c\u9879\u76ee\u7684\u503e\u529b\u5e2e\u52a9\u3002\n\n3. **\u6765\u4e2a star \u5427(*/\u03c9\uff3c*)\uff0c\u6709\u95ee\u9898\u8bf7\u63d0\u4ea4issue**\n\n4. \u60a8\u7684\u4e00\u70b9\u652f\u6301\u4f1a\u662f\u6211\u4eec\u5b8c\u5584\u672c\u9879\u76ee\u7684\u5f3a\u5927\u52a8\u529b\uff01(*/\u03c9\uff3c*)\n\n## \u66f4\u65b0\u8ba1\u5212\n\n- [x] \u6a21\u62df\u5668\u81ea\u542f\u52a8\u63a7\u5236\n- [x] \u7b80\u5316Schedule\u64cd\u4f5c\u6a21\u5f0f\n- [ ] WebGUI\u754c\u9762\n- [ ] \u63d0\u9ad8\u5237\u56fe\u6548\u7387\n- [ ] \u5237\u6d3b\u52a8\u672c\n\n## \u514d\u8d23\u58f0\u660e\n\n\u5f53\u4f60**\u4e0b\u8f7d\u6216\u4f7f\u7528**\u672c\u9879\u76ee\uff0c\u5c06\u9ed8\u8bb8\n\n\u672c\u9879\u76ee\u4ec5\u4f9b\u4ea4\u6d41\u548c\u5b66\u4e60\u4f7f\u7528\uff0c\u8bf7\u52ff\u7528\u6b64\u4ece\u4e8b \u8fdd\u6cd5/\u5546\u4e1a\u76c8\u5229\u7b49\uff0c\u5f00\u53d1\u8005\u56e2\u961f\u62e5\u6709\u672c\u9879\u76ee\u7684\u6700\u7ec8\u89e3\u91ca\u6743\n\n## \u66f4\u65b0\u5386\u53f2\n\n2020/11/11 By:TheAutumnOfRice\n\n- \u589e\u52a016\u56fe\n- \u4fee\u590d\u901a\u5173\u5730\u4e0b\u57ce\u4e2d\u8fdb\u5165\u5730\u4e0b\u57ce\u5bb9\u6613\u70b9\u8fdb4\u56fe\u7684Bug\n\n2020/11/7 By:Monkey\n\n- \u65b0\u589e\u65e5\u5e38\u5723\u8ff9\u8c03\u67e5\u7684\u626b\u8361\n- \u65b0\u589erun.bat\uff0c\u4e00\u952e\u542f\u52a8\u811a\u672c\n\n2020/11/4 By:CyiceK\n\n- \u4fee\u590d\u96f7\u7535\u4e09\u8bfb\u53d6\u4e0d\u5230\u63a7\u4ef6\u7684bug\uff08\u611f\u8c22\u7fa4\u53cbStack\u7684\u53d1\u73b0\uff09\n- \u4fee\u590d\u9a8c\u8bc1\u7801\u56db\u5750\u6807\u7684\u65e0\u6cd5\u8bc6\u522b\u95ee\u9898\n- \u589e\u52a0\u9a8c\u8bc1\u7801\u901f\u5ea6\u8bc6\u522b\u901f\u5ea6\u6a21\u5f0f\u5207\u6362\n- \u5347\u7ea7\u81ea\u52a8\u7533\u8bc9\u9898\u76ee\n\n2020/11/2 By:TheAutumnOfRice\n\n- \u4fee\u590d\u53ef\u63a8\u56fe\u63a2\u7d22\u5361\u5728jingyan\u7684BUG\n- \u6253\u7801\u914d\u7f6eBUG\u4fee\u590d\n- \u589e\u52a0\u6a21\u62df\u5668\u8fc7\u6ee4 ignore_serials\n- \u9632\u6b62\u201c\u6e38\u5ba2\u767b\u9646\u201d\n\n2020/11/1 By:CyiceK\n\n- \u63a5\u7801\u529f\u80fd\u5b8c\u5584\n\n2020/11/1 By:TheAutumnOfRice\n\n- \u5728\u6a21\u62df\u5668\u5f00\u7740\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u652f\u6301\u81ea\u52a8\u63a7\u5236\u4e86\u3002\n- \u4f18\u5316\u4e86Login\u4e2d\u5bf9\u4e8e\u9a8c\u8bc1\u7801\u7684\u5904\u7406\n\n2020/10/31 By:CyiceK\n\n- \u63a5\u5165\u6253\u7801\u5e73\u53f0\u5b9e\u73b0\u9a8c\u8bc1\u7801\u81ea\u52a8\u586b\u5199\uff0c\u6559\u7a0b\u5728docs-\u5982\u4f55\u63a5\u5165\u6253\u7801\u5e73\u53f0.md\n\n2020/10/28 By:TheAutumnOfRice\n\n- \u51fa\u73b0\u9a8c\u8bc1\u7801\u65f6\u5f39\u7a97\u63d0\u9192\uff08config - captcha_popup\uff09\n- \u589e\u52a0\u201c\u6682\u505c\u201d\u4efb\u52a1\uff0c\u4fbf\u4e8e\u624b\u52a8\u64cd\u4f5c\u8d26\u53f7\n- \u4fee\u590d\u901a\u5173\u5730\u4e0b\u57ce\u4e00\u7cfb\u5217\u8bef\u5165/\u5361\u4f4f\u7684BUG\n\n2020/10/24 By:CyiceK\n\n- \u4fee\u590d\u5f02\u6b65\u6682\u505c\u7ebf\u7a0b\u6ea2\u51fa\n- \u4fee\u590dtimeout\u4e0e\u6682\u505c\u51b2\u7a81\u95ee\u9898\n- OCR\u5730\u4e0b\u57ce\u5728\u65e0\u652f\u63f4\u4efb\u52a1\u65f6\uff0c\u4f1a\u81ea\u52a8\u9000\u51fa\u4e0d\u64a4\u9000\n\n2020/10/21 By:TheAutumnOfRice\n\n- \u589e\u52a0\u5730\u4e0b\u57ce1\u56fe\n- \u901a\u5173\u5730\u4e0b\u57ce\u589e\u52a0\u6a21\u5f0f3\uff1a\u53ea\u6253\u7b2c\u4e00\u5173\n\n2020/10/20 By:TheAutumnOfRice\n\n- \u589e\u52a0\u5356\u51fa\u88c5\u5907\n\n2020/10/18 By:TheAutumnOfRice\n\n- \u901a\u5173\u5730\u4e0b\u57ce\u63d0\u793a\u4fe1\u606f\u4fee\u590d\n- 8-15\u53c2\u6570\u7f3a\u5931\uff0c\u8fdb\u884c\u4e86\u8865\u5145\n- \u90e8\u5206\u4f53\u529b\u76f8\u5173BUG\u5bfc\u81f4\u65e0\u6cd5\u5237\u56fe\u7684\u4fee\u590d\n\n2020/10/10 By:TheAutumnOfRice\n\n- \u7d27\u6025\u4fee\u590dv2.0.20201009\u9a8c\u8bc1\u7801\u914d\u7f6e\u6df7\u4e71\u7684Bug\n- \u589e\u52a0\u4e86\u7b49\u5f85\u9a8c\u8bc1\u7801\u8f93\u5165\u7684\u65f6\u95f4\uff0c\u53ef\u4ee5\u901a\u8fc7`captcha_wait_time`\u914d\u7f6e\u63a7\u5236\n\n2020/10/9 By:TheAutumnOfRice\n\n- \u66f4\u65b015\u56fe\n- \u767b\u5f55\u5f3a\u9000\u65f6\u957f\u589e\u52a0180s->300s\n- \u4fee\u590d\u6587\u4ef6\u635f\u574f\u8bfb\u53d6Nonetype\u65f6\u7684\u62a5\u9519\n- \u4fee\u590d\u767b\u9646\u65f6\u5feb\u901f\u622a\u56fe\u5f02\u5e38\u5bfc\u81f4\u72c2\u70b9\u53f3\u4e0a\u89d2\u7684\u62a5\u9519\n- \u5feb\u901f\u622a\u56fe\u8fde\u63a5\u5931\u8d25\u65f6\uff0c\u5982\u679c\u914d\u7f6e\u4e86force_fastscreen_cut\u4ee5\u53ca\u6a21\u62df\u5668\u81ea\u52a8\u63a7\u5236\uff0c\u5c06\u81ea\u52a8\u91cd\u542f\u6a21\u62df\u5668\n- \u4fee\u590d\u9a8c\u8bc1\u7801\u65e0\u6cd5\u8df3\u8fc7\u7684Bug\n\n2020/10/1 By:CyiceK\n\n- \u5bf9\u9a8c\u8bc1\u7801\u7684\u95ee\u9898\u8fdb\u884c\u5904\u7406\uff0c\u901a\u8fc7Sever\u9171\u8fdb\u884c\u901a\u77e5\u5e76\u4e14\u8df3\u8fc7\u8be5\u8d26\u53f7\uff0c\u5199\u5165\u5f02\u5e38\u8bb0\u5f55\n\n2020/9/29 By:TheAutumnOfRice\n\n- \u7528\u6237\u534f\u8bae\u68c0\u6d4b\u65f6\u95f4\u589e\u52a0 10s -> 20s\n- \u4fee\u590d\u63a8\u56fe\u5927\u6982\u7387\u5361\u9650\u5b9a\u5546\u5e97\u7684BUG\n\n2020/9/28 By:TheAutumnOfRice\n\n- \u65b0\u589e\u8df3\u8fc7\u7528\u6237\u534f\u8bae\n\n2020/9/24 By:TheAutumnOfRice\n\n- \u901a\u8fc7\u5c06\u81ea\u5e26adb\u6dfb\u52a0\u5230\u73af\u5883\u53d8\u91cf\uff0c\u4e5f\u8bb8\u4fee\u590d\u4e86\u81ea\u542f\u52a8\u7684BUG\n- \u589e\u52a0timeout\uff0c\u5168\u9762\u9632\u6b62\u5f7b\u5e95\u5361\u6b7b\n- \u8fdb\u4e00\u6b65\u63d0\u5347u2\u8c03\u7528\u7a33\u5b9a\u6027\n\n2020/9/15 By:TheAutumnOfRice\n\n- \u66f4\u65b014\u56fe\n- \u4fee\u590d\u90e8\u5206BUG\uff0c\u89e3\u51b3\u63a8\u56fe\u9047\u5230\u9650\u5b9a\u5546\u5e97\u7684\u95ee\u9898\uff08\u5927\u6982\n\n2020/9/7By:CyiceK\n\n- \u672c\u5730OCR\u4e0e\u767e\u5ea6OCR\u5e76\u5165app.py\u4e2d\n- \u5bf9\u4e8e\u767e\u5ea6OCR\u7684QPS\u5e76\u53d1\u9650\u5236\u5df2\u505a\u5904\u7406\uff0c\u57fa\u672c\u6210\u529f\u7387\u4e3a100%\n\n2020/9/3 By:TheAutumnOfRice\n\n- \u63a7\u5236\u53f0\u4f7f\u7528\u66f4\u65b0\uff0c\u66f4\u52a0\u4e1d\u6ed1\uff0c\u6269\u5c55\u5b9e\u65f6\u63a7\u5236\u529f\u80fd\n- \u4e0a\u7ebf\u6a21\u62df\u5668\u63a7\u5236\n\n2020/9/2 By:TheAutumnOfRice\n\n- Schedule\u63a7\u5236\u6a21\u5f0f\u4e0a\u7ebf\n\n2020/8/24 By:CyiceK\n\n- \u672c\u5730OCR\u4e0a\u7ebf\uff0c\u6ce8\u610f\u9700\u8981\u91cd\u65b0pip\u3002\u4f1a\u6709\u5927\u7ea6500M\u7684\u4e0b\u8f7d\u91cf!\n\n2020/8/22 By:CyiceK\n\n- \u589e\u52a0XLS\u8868\u683c\u652f\u6301\uff08`add t3`)\uff0c\u4fee\u590d\u90e8\u5206bug\n\n2020/8/19 By:TheAutumnOfRice\n\n- \u63a8\u56fe\u7ec6\u8282\u4f18\u5316\uff0c\u81ea\u52a8\u63a8NORMAL\u56fe\u529f\u80fd\u4e0a\u7ebf\uff08`add s6`\uff09\n- \u81ea\u52a8\u53d1\u8d77\u88c5\u5907\u6350\u8d60\u4e0a\u7ebf\uff08`add h9`\uff09\n\n2020/8/19 By:CyiceK\n\n- \u968f\u5fc3\u6240\u6b32\u7684\u6682\u505c shift+p\n- \u6539\u52a8\u4e86 lock\u65b9\u6cd5\u7136\u540e\u5f02\u6b65\u5267\u60c5\u8df3\u8fc7\u5f03\u7528\n\n2020/8/18 By:TheAutumnOfRice\n\n- \u589e\u52a0box\u622a\u56fe\u529f\u80fd\n\n2020/8/17 By:TheAutumnOfRice\n\n- \u589e\u52a0\u8d85\u7ea7\u5237\u56fe\u529f\u80fd\uff1a\u52371-1\u6700\u5feb\u901f\u69a8\u5e72\u4f53\u529b\n- \u901a\u7528\u5237\u56fe/\u624b\u5237\u51fd\u6570\uff0c\u7528\u4e8e\u4e3b\u7ebfNORMAL,HARD\n- \u57fa\u4e8e\u901a\u7528\u5237\u56fe/\u624b\u5237\u51fd\u6570\u7684\u81ea\u52a8\u63a8\u56fe\u529f\u80fd\n- \u57fa\u4e8e\u901a\u7528\u5237\u56fe/\u624b\u5237\u51fd\u6570\u7684\u81ea\u52a8\u5f3a\u5316\u529f\u80fd\n- \u65b0\u589e\u521d\u59cb\u5316\u548c\u5feb\u901f\u521d\u59cb\u5316\u529f\u80fd\n\n2020/8/17 By:zsdostar\n\n- \u524d\u7aef\u7f16\u8f91\u5b50\u4efb\u52a1\u7684\u57fa\u672c\u4ea4\u4e92\n- \u540e\u7aef\u63d0\u4f9b VALID_TASK \u7684\u8f6c\u4e49\u6210\u5b50\u4efb\u52a1 schema \u7684\u63a5\u53e3, \u90e8\u5206\u63a5\u53e3\u8fd4\u56de\u503c\u5fae\u8c03\n\n2020/8/15 By:CyiceK\n\n- OCR\u7248\u5730\u4e0b\u57ce\u90e8\u5206\u4ee3\u7801\u4f7f\u7528\u5f02\u6b65\u81ea\u52a8\u65f6\u95f4\u5835\u585e\uff08\u6839\u636eCPU\u7684\u8d1f\u8f7d\u5224\u65ad\n- \u4fee\u590d\u5f02\u6b65+\u626d\u86cb\u7684\u4e00\u4e9bbug\n- \u4f7f\u7528\u5f02\u6b65\u81ea\u52a8\u65f6\u95f4\u5835\u585e\u589e\u52a0\u4e86\u8fd0\u884c\u6548\u7387\uff08\u5e94\u8be5\u5427\n\n2020/8/12 By:zsdostar\n\n- \u4efb\u52a1\u524d\u7aef\u9875\u9762\u9664\u5bf9\u4efb\u52a1\u8be6\u60c5\u7684\u7f16\u8f91\u67e5\u770b\u5916, \u5927\u90e8\u5206\u5b8c\u6210\n- \u8d26\u53f7\u5217\u8868\u589e\u52a0\u7f16\u8f91\u548c\u67e5\u770b\u4efb\u52a1\u7684\u529f\u80fd\n\n2020/8/10 By:Moment\n\n- guild_40to1.py\u4fee\u590d\u5e76\u6d4b\u8bd5\u5b8c\u6bd5\uff0c\u5199\u4e86readme\n\n2020/8/6 By:CyiceK\n\n- \u4fee\u590d\u5f02\u6b65\u7ebf\u7a0b\u6cc4\u6f0f\u548c\u4e00\u4e9bbug\n\n2020/8/6 By:zsdostar\n\n- \u8d26\u53f7\u524d\u7aef\u9875\u9762\u5b8c\u6210, \u524d\u540e\u7aef\u8054\u8c03\u5b8c\u6bd5\n- \u4f18\u5316\u9875\u9762\u6837\u5f0f, \u589e\u52a0\u9634\u5f71\u7b49, \u63d0\u9ad8\u7acb\u4f53\u611f\n- \u628a Moment \u624b\u7ed8\u7684 logo \u6362\u4e0a\u53bb\u4e86\n\n2020/8/4 By:CyiceK\n\n- \u4fee\u6539\u90e8\u5206\u5730\u4e0b\u57ceOCR\u903b\u8f91\uff0c\u4f7f\u5f97ocr\u7a33\u5b9a\u4e86\u4e9b\n- \u811a\u672c\u589e\u52a0\u6682\u505c\u51fd\u6570\uff08shift+p\uff09\u6682\u505c/\u6062\u590d\uff0c\u540e\u9762\u8ffd\u52a0\u81ea\u5b9a\u4e49\n\n2020/8/3 By:Moment\n\n- \u5feb\u901f\u622a\u56fe\u51fd\u6570\u6362\u6210\u957f\u8fde\u63a5\uff08\u4e00\u5806bug\uff0c\u5728\u4e4b\u540e\u7684\u51e0\u5929\u4e2d\u7fa4\u91cc\u7684\u5927\u4f6c\u4eec\u6162\u6162\u4fee\u590d\u4e86\uff0c\u81f38/9\u4fee\u590d\u5b8c\u6bd5\uff0c\u723d\u5230\u98de\u8d77\uff09\n\n2020/8/2 By:CyiceK\n\n- \u4e0a\u7ebfconfig\uff0c\u6dfb\u52a0\u622a\u56fe\u624b\u52a8\u7ea0\u9519\u65b9\u5411\uff08\u767e\u5ea6OCR\u7684\n\n2020/8/1 By:Moment\n\n- \u589e\u52a0\u9002\u7528\u4e8e\u65b0\u7248\u768440to1\uff08guild_40to1.py)\uff0c\u4f7f\u7528\u65b9\u6cd5\u89c1\u4e0a\u65b9readme\n\n2020/7/30 By:CyiceK\n\n- \u4fee\u6539\u4e86Server\u9171\u7684\u683c\u5f0f\u548c\u4fee\u590d\u4e86\u65e5\u5fd7\u591a\u8f93\u51fa (\u5e94\u8be5\n- cv.py \u589e\u52a0\u754c\u9762\u76f8\u4f3c\u5ea6\u7684\u5224\u65ad\uff0c\u5f02\u6b65\u6dfb\u52a0\u5361\u6b7b\u754c\u9762\u5224\u65ad\uff08\u6d4b\u8bd5\n\n2020/7/30 By:Moment\n\n- \u589e\u52a0N3-1\u7684\u5237\u7ecf\u9a8c\u51fd\u6570\n- \u5c06N\u56fe\u5750\u6807\u5b8c\u5168\u8865\u5168\n- \u5b8c\u5584\u8fdb\u5165\u5730\u56fe\u7684\u51fd\u6570\uff08\u5e94\u8be5\u4e0d\u4f1a\u5361\u5728\u5192\u9669\u754c\u9762\u4e86\uff09\n\n2020/7/28 By:TheAutumnOfRice\n\n- \u4fee\u590d\u4e86\u5f02\u6b65\u4e0d\u81ea\u542f\u52a8\u7684Bug\n- \u6dfb\u52a0\u622a\u56fe\u5c0f\u5de5\u5177\n\n2020/7/26 By:TheAutumnOfRice\n\n- \u811a\u672c\u4efb\u52a1\u7edf\u4e00\u7531\u7528\u6237json\u63a7\u5236\n- \u5408\u5e76\u4e86\u5237\u56fe\u51fd\u6570\uff0c\u5f03\u7528\u4e86shuatuXX\u548cDoXXtoXX\u51fd\u6570\uff0c\u6539\u7528shuatuNN\u548cshuatuHH\u51fd\u6570\n- \u652f\u6301\u7b80\u5355\u7684\u65ad\u70b9\u6062\u590d\u3002\u5f53\u7a0b\u5e8f\u8fd0\u884c\u5d29\u6e83\uff0c\u518d\u6b21\u6253\u5f00\u811a\u672c\uff0c\u4f1a\u91cd\u65b0\u5f00\u542f\u4e0a\u4e00\u6b21\u6ca1\u6709\u5b8c\u6210\u7684\u811a\u672c\u3002\n- \u4fee\u6539\u4e86\u5f02\u6b65\u903b\u8f91\uff1a\u5982\u679c\u5f02\u6b65\u7ebf\u7a0bBad_connecting\u53d1\u73b0\u9519\u8bef\uff0c\u5219\u76f4\u63a5\u91cd\u542f\u91cd\u8fdb\u3002\n- \u65b0\u589e\u4e86`\u975e\u5e38\u7b80\u964b`\u7684\u811a\u672c\u63a7\u5236\u5668\u3001\u7528\u6237\u4fe1\u606f\u7f16\u8f91\u5668\u548c\u4efb\u52a1\u7f16\u8f91\u5668\n- \u5927\u6539\u4e86Automator\u7684\u903b\u8f91\uff0c\u73b0\u5728Automator\u53ea\u9700\u8981\u8bfb\u5165address\u53c2\u6570\uff0c\u901a\u8fc7init_account\u5728\u5185\u90e8\u8bfb\u53d6\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff0c\u4ece\u800c\u4e00\u6c14\u559d\u6210\u5b8c\u6210\u6240\u6709\u4efb\u52a1\u3002\n  \u56e0\u4e3a\u6539\u52a8\u4e86Automator\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u76ee\u524dgoumaimana,juanzeng\u7b49\u7b49\u811a\u672c\u90fd\u65e0\u6cd5\u4f7f\u7528\uff0c\u9700\u8981\u5c3d\u5feb\u5199\u6210\u914d\u7f6e\u6587\u4ef6\u4ece\u800c\u4f7f\u7528\u3002\n\n2020/7/22 By:CyiceK\n\n- \u6539\u5584\u5f02\u6b65\u7684\u5360\u7528\uff0c\u5e94\u8be5\u5427\uff08\n- \u4fee\u6539\u65e5\u5fd7\u529f\u80fd\uff0c\u589e\u6dfbServer\u9171\uff08\u5fae\u4fe1\u63a8\u9001\uff09\n- \u5220\u9664baidu_ocr.txt\uff0c\u4e34\u65f6\u7ec4\u5efapcr_config.py\n- \u4fee\u590d\u4f20\u5165\u767e\u5ea6OCR\u7684\u622a\u56fe\u7384\u5b66\u95ee\u9898\n- \u5176\u4ed6\u5c0f\u65b9\u9762\u8c03\u6574\uff0c\u9003\uff09\n\n2020/7/20 By:Ailumar\n\n- \u518d\u6269\u5145\u4e86main.py\u4e2d<\u5237\u56fe\u63a7\u5236\u4e2d\u5fc3>\u5904\u7406\u8303\u56f4,\u53ef\u652f\u6301hard\u672c\u5237\u89d2\u8272\u788e\u7247\u548c\u4e3b\u7ebf\u5237\u88c5\u5907\u548c\u63a2\u7d22\u4e00\u8d77\u5904\u7406,\u63a2\u7d22\u65b0\u52a0\u7684,\u53ef\u80fd\u4e0d\u592a\u5b8c\u5584\u5148\u7528\u7528\u770b.\n- \u65b0\u589e\u4e3b\u7ebf12\u6751\u7684\u88c5\u5907\u626b\u8361.\n- \u65b0\u589e\u4e3b\u7ebf\u56f0\u672c12\u6751\u626b\u8361.\n\n2020/7/19 By:Ailumar\n\n- hard\u56fe\u548c\u4e3b\u7ebf\u56fe\u8981\u6c42\u7686\u4e3a3\u661f\u901a\u5173\u6216\u8005\u88ab\u6ce8\u91ca\u6389\u4e0d\u5237\n- \u6269\u5145\u4e86main.py\u4e2d<\u5237\u56fe\u63a7\u5236\u4e2d\u5fc3>\u5904\u7406\u8303\u56f4,\u53ef\u652f\u6301hard\u672c\u5237\u89d2\u8272\u788e\u7247\u548c\u4e3b\u7ebf\u5237\u88c5\u5907\u4e00\u8d77\u5904\u7406,\u6ce8\u610f\u4f53\u529b,\u6ce8\u610f\u8d26\u53f7\u586b\u5199\u65b9\u5f0f!!\n- \u65b0\u589e\u4e3b\u7ebf\u56f0\u96be1-11\u6751\u89d2\u8272\u788e\u7247\u626b\u8361.\n- \u65b0\u589e\u4e3b\u7ebf7\u6751\u7684\u88c5\u5907\u626b\u8361.\n- \u4fee\u6539\u4e868,10,11\u7684\u88c5\u5907\u626b\u8361\u903b\u8f91.\n- \u4f18\u5316\u4e86\u4e3b\u7ebf\u5237\u88c5\u5907\u5bb9\u6613\u5237\u9519\u56fe\u7684\u95ee\u9898\u5e76\u8fdb\u884c\u4e86\u7b80\u5355\u7684\u9632\u5361\u6b7b,\u9632\u6a21\u62df\u5668\u5361\u987f\u5904\u7406.\n\n2020/7/19 By:Yuki_Asuuna\n\n- \u7528logging\u5e93\u91cd\u5199\u4e86\u65e5\u5fd7\u7c7b\uff08log_handler.py\uff09\uff0c\u6240\u6709\u65e5\u5fd7\u91cd\u5b9a\u5411\u5230\u6807\u51c6\u8f93\u51fa\u548c\u6587\u4ef6\uff08\u65e5\u5fd7\u6587\u4ef6\u4f4d\u4e8elog\u6587\u4ef6\u5939\u4e0b\uff09\uff0c\u6bcf\u4e2a\u8d26\u53f7\u90fd\u5bf9\u5e94\u4e00\u4e2a\u65e5\u5fd7\u6587\u4ef6a.log\uff0c\u8c03\u7528\u65b9\u6cd5\u4e3aa.log.write_log(level,message)\uff0c\u4e24\u4e2a\u53c2\u6570\u90fd\u4e3astr\u7c7b\u578b\uff0c\u5176\u4e2dlevel\u8868\u793a\u8be5\u6761\u65e5\u5fd7\u4fe1\u606f\u7684\u7c7b\u578b\uff08['debug','info','warning','error','critical']\u4e4b\u4e00\uff09\uff0cmessage\u8868\u793a\u5185\u5bb9\u3002\n\n2020/7/17 By:CyiceK\n\n- \u4f18\u5316OCR\u5730\u4e0b\u57ce\u7684\u5224\u65ad\n- \u5f02\u6b65 \u591a\u8fdb\u7a0b\u534f\u8c03\u964d\u4f4e\u6027\u80fd\u5360\u7528\n- OCR\u5730\u4e0b\u57ce\u4e0e\u975eOCR\u7684\u81ea\u52a8\u5207\u6362\n- \u65b0\u589e \u5bb6\u56ed\u65b0\u5bb6\u5177\u63d0\u793a\u5173\u95ed\u7684\u5224\u65ad\uff0c\u5168\u5c40\u5f02\u6b65\u8df3\u8fc7\u53ef\u53ef\u841d\u5267\u60c5\u7684\u5f02\u6b65(\u5305\u62ec\u5730\u4e0b\u57ce\u5403\u5854\u5e01\u540e\u51fa\u53d1\u7684\u4f46\u672a\u6d4b\u8bd5)\n- \u5176\u4ed6\u65b9\u9762\u5c0f\u8c03\u6574\n\n2020/7/14 By:Dr_Bluemond\n\n- \u53e6\u5199\u4e00\u7248\u5730\u4e0b\u57ce\u51fd\u6570\uff0c\u53ef\u9009\u62e9\u66f4\u6362\u4e3adixiacheng_dev\uff0c\u4e0d\u4f7f\u7528ocr\uff0c\u4e0d\u5904\u7406\u83b7\u5f97\u5854\u5e01\u7684\u53ef\u53ef\u841d\u6559\u7a0b\n- \u4f18\u5316\u8d5b\u9a6c\n- lockimg\u589e\u52a0retry\u53c2\u6570\uff0celseclick\u8d85\u8fc7retry\u6b21\u5219\u8fd4\u56deFalse\n- \u589e\u52a0lock_no_img\u51fd\u6570\uff0c\u5728\u6709\u56fe\u7247\u65f6\u4e00\u76f4\u70b9\u51fbelseclick\uff0c\u56fe\u7247\u6d88\u5931\u65f6\u70b9\u51fbifclick\n- \u589e\u52a0\u533a\u57df\u5b9a\u4f4d\u8f85\u52a9\uff0c\u73b0\u5728\u5982\u679c\u641c\u56fe\u6ca1\u6709\u52a0\u4e0aat\u5219\u4f1a\u5728\u8fd0\u884c\u53d1\u73b0\u65f6\u63d0\u4f9bat\u7684\u5750\u6807\uff0c\u76f4\u63a5\u590d\u5236\u5373\u53ef\u4f7f\u7528\n- \u4f18\u5316\u521d\u59cb\u5316\u7a0b\u5e8f\uff0c\u5927\u5e45\u589e\u52a0\u5176\u6548\u7387\n\n2020/7/12 By:CyiceK\n\n- \u5c01\u88c5\u5f02\u6b65\u7c7b\uff0c\u542f\u52a8\u5f02\u6b65\u8df3\u8fc7\u5267\u60c5 \u5f02\u6b65\u5224\u65ad\u5904\u7406 connect/nowloading/\u8fd4\u56de\u6807\u9898 \u4e09\u79cd\u5f02\u5e38\u60c5\u51b5\n- \u5c1d\u8bd5\u6027\u4f7f\u7528\u534f\u7a0b\u6765\u521d\u59cb\u5316\n- \u4f18\u5316 \u6539\u52a8ocr\u51fd\u6570\uff0clog_handler.py \uff0ccv.py\uff0clock\u51fd\u6570\n- \u5b8c\u5168\u91cd\u5199\u5730\u4e0b\u57ce\uff0c\u53bb\u9664\u5197\u4f59\u4ee3\u7801**\u9700\u8981\u767e\u5ea6OCR\u6587\u5b57\u8bc6\u522b\u7684API**\n- \u53ea\u8981\u5728\u754c\u9762\u4e0a\u51fa\u73b0\u5267\u60c5\u90fd\u4f1a\u8fdb\u884c\u8df3\u8fc7\uff0c\u51fa\u73b0\u5f02\u5e38\u5373\u91cd\u542fapp\n\n2020/7/11 By:Yuki_Asuuna\n\n- \u533a\u57dfOCR\u51fd\u6570(x1,y1,x2,y2,size)\u589e\u52a0\u53c2\u6570size\uff08\u9ed8\u8ba4size=1.0\uff0c\u53c2\u6570\u53ef\u7701\u7565\uff09\u3002\n\n  \u8868\u793a\u5148\u5bf9\u9009\u5b9a\u533a\u57df(x1,y1)->(x2,y2)\u8fdb\u884c\u653e\u5927/\u7f29\u5c0f\uff0c\u518d\u8fdb\u884c\u8bc6\u522b\u3002\n\n  \u907f\u514d\u56e0\u56fe\u7247\u8fc7\u5927or\u8fc7\u5c0f\u800c\u5bfc\u81f4\u8bc6\u522b\u9519\u8bef\u3002\n\n  \u82e5size=2.0\u8868\u793a\u5c06\u622a\u53d6\u533a\u57df\u653e\u59272\u500d\uff0csize=0.5\u5219\u8868\u793a\u5c06\u622a\u53d6\u533a\u57df\u7f29\u5c0f\u4e00\u534a\u3002\n\n- \u589e\u52a0\u9879\u76ee\u66f4\u65b0\u7684\u811a\u672c\uff08updater.py\uff09\uff0c\u81ea\u52a8\u4e0b\u8f7d\u6700\u65b0\u7248\u672c\u7684\u9879\u76ee\u4ee3\u7801\u5230\u672c\u5730\u6587\u4ef6\u5939\uff0c\u6ee1\u8db3\u5feb\u901f\u66f4\u65b0\u7684\u9700\u6c42\uff08\u8be6\u60c5\u8bf7\u53c2\u8003AboutUpdater.md\uff09\uff0c\u4ec5\u4f9b\u5c0f\u767d\u4f7f\u7528\uff0c\u5f00\u53d1\u7ec4\u7684\u5927\u4f6c\u4eec\u8bf7\u65e0\u89c6\u3002\n\n2020/7/10\n\n* \u4fee\u590d\u884c\u4f1a\u591a\u6b21\u6350\u8d60bug\uff0c\u589e\u52a0\u8df3\u8fc7\u5730\u4e0b\u57ce\u6218\u6597\uff08\u53c2\u6570skip=True\uff09\n\n2020/7/8 By:Dr_Bluemond\n\n- \u4f18\u5316\u5237\u56fe\u63a7\u5236\n\n2020/7/7 By:Yuki_Asuuna\n\n- \u589e\u52a0\u65e5\u5fd7\u529f\u80fd\uff1a\u5e10\u53f7\u7684\u767b\u9646\u767b\u51fa\u4fe1\u606f\u5c06\u4f1a\u88ab\u8bb0\u5f55\u5728AccountRecord.txt\uff0c\u65b9\u4fbf\u5927\u5bb6\u5b9a\u4f4d\u54ea\u4e2a\u53f7\u5361\u4e86\n\n2020/7/6\n\n* \u66f4\u65b0\u519c\u573a\u53f7\u81ea\u52a8\u52a0\u5165\u6307\u5b9a\u884c\u4f1a\u529f\u80fd\uff08zujianhanghui.py\uff09\n\n2020/7/6 By:Dr_Bluemond\n\n- \u4f18\u5316\u8bc6\u56fe\u4ee3\u7801\n- \u4fee\u590d\u516c\u4f1a\u4e4b\u5bb6\u8df3\u8fc7\u5267\u60c5\n- \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u8bfb\u53d6\u8d26\u53f7\n\n2020/7/2 By:Yuki_Asuuna\n\n- \u589e\u52a0\u83b7\u53d6\u5f53\u524d\u4f53\u529b\u503c\u7684\u51fd\u6570\n- \u77e9\u5f62\u533a\u57dfOCR\u51fd\u6570\u5df2\u5b8c\u6210\uff08ocr_baidu\uff09\n\n2020/6/30 By:Dr_Bluemond\n\n* \u66f4\u65b0\u89d2\u8272\u5f3a\u5316\u4ee3\u7801\n* \u5c06mana\u519c\u573a\u53f7\u5efa\u7acb\u811a\u672c\u5b8c\u5584\u4e3a\u4ece\u5168\u65b0B\u7ad9\u8d26\u53f7\u5237\u52303-1\u3002\n\n2020/6/29 By:CyiceK\n\n- \u884c\u4f1a\u591a\u6350\u8d60\n- \u622a\u56fe\u5c0f\u66f4\u65b0\n\n2020/6/29\n\n* \u66f4\u65b040\u5bf91\u884c\u4f1a\u529f\u80fd\n* \u4fee\u6539adb\u8fde\u63a5\u51fd\u6570\uff0c\u73b0\u5728\u4f1a\u81ea\u52a8\u5ffd\u7565\u5904\u4e8eoffline\u7684\u8bbe\u5907\n\n2020/6/28  By:Dr_Bluemond\n\n- \u589e\u52a0\u514d\u8d39\u5341\u8fde\u529f\u80fd\uff0c\u9700\u624b\u52a8\u5f00\u542f\n- \u589e\u52a0\u521d\u59cb\u5316mana\u53f7\u529f\u80fd\uff0c\u4ece\u5df2\u5b8c\u6210\u65b0\u624b\u4efb\u52a1\u7684\u8d26\u53f7\uff08\u5df2\u6253\u8fc71-2\uff09\u53d8\u6210\u5237\u5b8c3-1\u7684\u8d26\u53f7\uff08\u591a\u7ebf\u7a0b\u672a\u8c03\u8bd5\uff09\n\n2020/6/26  By:CyiceK\n\n- \u91cd\u5199\u5237\u56fe\u903b\u8f91\uff0c\u73b0\u5728\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5237\u56fe\u4e86\uff08\u9002\u5408\u8fdb\u5ea6\u4e0d\u540c\u7684\u519c\u573a\u53f7\uff09\n\n2020/6/23  By:CyiceK\n\n- \u4f18\u5316\u4ee3\u7801\uff0c\u53bb\u9664\u9700\u8981\u624b\u52a8\u914d\u7f6eadb\u73af\u5883\u7684\u6b65\u9aa4\n- \u6539\u8fdb\u5730\u4e0b\u57ce\u548c\u5237\u56fe\u903b\u8f91\uff0c\u6548\u7387\u63d0\u5347\n- \u6539\u8fdb\u884c\u4f1a\u6350\u8d60\uff0c\u73b0\u5728\u7edd\u5927\u90e8\u5206\u4e0d\u4f1a\u5931\u8d25\u4e86\n- \u5237\u56fe\u65e0\u4f53\u529b\u65e0\u6b21\u6570\u4f1a\u7ed3\u675f\u5269\u4e0b\u7684\u5237\u56fe\u4efb\u52a1\n- \u8ddf\u8fdb\u5b98\u65b9\u66f4\u65b0\uff0c\u6dfb\u52a0 \u8df3\u8fc7\u626b\u8361\u8fc7\u7a0b\n- \u5730\u4e0b\u57ce\u903b\u8f91\u91cd\u5199\uff0c\u65e0\u6b21\u6570 \u5361\u4f4f\u4f1a\u8df3\u51fa\uff0c\u4e0d\u4f1a\u6b7b\u5faa\u73af\u4e86\n- \u8bc6\u522b\u5361\u6b7b\u4f1a\u81ea\u52a8\u8df3\u8f6c\n- \u5927\u90e8\u5206\u6b21\u6570 \u53ef\u4ee5\u81ea\u5b9a\u4e49\u4e86\n- \u6574\u5408\u4e86\u8d2d\u4e70mana\n\n2020/6/22\n\n* \u589e\u52a0\u4e86\u5bf9\u96f7\u7535\u6a21\u62df\u5668\u591a\u5f00\u7684\u652f\u6301\n* \u514d\u9664\u4e86\u5728\u7ec8\u7aef\u4e2d\u624b\u52a8\u8f93\u5165\u547d\u4ee4\u7684\u6b65\u9aa4\n* \u811a\u672c\u51fd\u6570\u5747\u79fb\u5230\u4e86Automator.py\n* \u4fee\u6539\u534f\u8bae\u4e3aGPL 3.0\n\n2020/6/21  By:CyiceK\n\n- \u6dfb\u52a0\u81ea\u52a8\u514d\u8d39\u626d\u86cb\u529f\u80fd\n- \u6dfb\u52a0\u5237\u7ecf\u9a8c\u5173(1-1)\u529f\u80fd\n\n2020/6/20\n\n* \u589e\u52a0\u4e86\u4f9b\u5927\u53f7\u4f7f\u7528\u7684\u81ea\u52a8\u5237\u5b8c\u7b2c\u4e09\u4e2a\u5730\u4e0b\u57ce\u53ca\u63a2\u7d22\u7684\u529f\u80fd\n\n2020/6/20  By:CyiceK\n\n- \u6dfb\u52a0\u5bb6\u56ed\u6536\u53d6\u548c\u516c\u4f1a\u70b9\u8d5e\u529f\u80fd\n- \u66f4\u66ffmaoxian\u6587\u4ef6\u540d\u4e3adixiacheng\n- \u4f18\u5316\u4e00\u5c0f\u70b9\u4ee3\u7801 XD\n\n2020/6/18\n\n- \u4fee\u590d\u4e86issue\u4e2d\u5730\u4e0b\u57ce\u64a4\u9000\u65f6\u53ef\u80fd\u622a\u56fe\u5230\u64a4\u9000\u6309\u94ae\u7684\u95ee\u9898\n- \u4fee\u590d\u4e86\u5730\u4e0b\u57ce\u53cc\u500d\u671f\u95f4\u65e0\u6cd5\u8bc6\u522b\u5730\u4e0b\u57ce\u56fe\u6807\u95ee\u9898\n- \u4fee\u590d\u4e86\u767b\u5f55\u65f6\u53ef\u80fd\u56e0\u63a7\u4ef6\u672a\u80fd\u53ca\u65f6\u5f39\u51fa\u800c\u5931\u8d25\u7684\u95ee\u9898\n- \u6536\u53d6\u793c\u7269\u51fd\u6570\u4f18\u5316\u4e86\u903b\u8f91\uff0c\u53bb\u6389\u4e86\u5168\u90e8\u6536\u53d6\u6309\u94ae\u7684\u9501\u5b9a\n- \u884c\u4f1a\u6350\u8d60\u51fd\u6570\u4f18\u5316\u4e86\u903b\u8f91\uff0c\u73b0\u5728\u5927\u6982\u4e0d\u4f1a\u6350\u8d60\u5931\u8d25\u4e86\n- \u5730\u4e0b\u57ce\u51fd\u6570\u4f18\u5316\u4e86\u903b\u8f91\u589e\u52a0\u4e86\u9c81\u68d2\u6027\uff0c\u52a0\u5165\u8df3\u8fc7\u5267\u60c5/\u9996\u6b21\u8fdb\u5165\u65f6\u5df2\u7ecf\u8fdb\u4e86\u5730\u4e0b\u57ce \u4e24\u79cd\u60c5\u51b5\u7684\u521d\u59cb\u53f7\u7684\u5904\u7406\u6cd5\n"
 },
 {
  "repo": "floe/deepbacksub",
  "language": "C++",
  "readme_contents": "# DeepBackSub\n\n## Virtual Video Device for Background Replacement with Deep Semantic Segmentation\n\n![Screenshots with my stupid grinning face](screenshot.jpg)\n(Credits for the nice backgrounds to [Mary Sabell](https://dribbble.com/shots/4686178-Bauhaus-Poster) and [PhotoFunia](https://photofunia.com/effects/retro-wave))\n\nIn these modern times where everyone is sitting at home and skype-ing/zoom-ing/webrtc-ing all the time, I was a bit annoyed about always showing my messy home office to the world. Skype has a \"blur background\" feature, but that starts to get boring after a while (and it's less private than I would personally like). Zoom has some background substitution thingy built-in, but I'm not touching that software with a bargepole (and that feature is not available on Linux anyway). So I decided to look into how to roll my own implementation without being dependent on any particular video conferencing software to support this.\n\nThis whole shebang involves three main steps with varying difficulty:\n  - find person in video (hard)\n  - replace background (easy)\n  - pipe data to virtual video device (medium)\n\n## Finding person in video\n\n### Attempt 0: Depth camera (Intel Realsense)\n\nI've been working a lot with depth cameras previously, also for background segmentation (see [SurfaceStreams](https://github.com/floe/surface-streams)), so I just grabbed a leftover RealSense camera from the lab and gave it a shot. However, the depth data in a cluttered office environment is quite noisy, and no matter how I tweaked the camera settings, it could not produce any depth data for my hair...? I looked like a medieval monk who had the top of his head chopped off, so ... next.\n\n### Attempt 1: OpenCV BackgroundSubtractor\n\nSee https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html for tutorial.\nShould work OK for mostly static backgrounds and small moving objects, but does not work for a mostly static person in front of a static background. Next.\n\n### Attempt 2: OpenCV Face Detector\n\nSee https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html for tutorial.\nWorks okay-ish, but obviously only detects the face, and not the rest of the person. Also, only roughly matches an ellipse which is looking rater weird in the end. Next.\n\n### Attempt 3: Deep learning!\n\nI've heard good things about this deep learning stuff, so let's try that. I first had to find my way through a pile of frameworks (Keras, Tensorflow, PyTorch, etc.), but after I found a ready-made model for semantic segmentation based on Tensorflow Lite ([DeepLab v3+](https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/default/1)), I settled on that.\n\nI had a look at the corresponding [Python example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py), [C++ example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image), and [Android example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/android), and based on those, I first cobbled together a [Python demo](https://github.com/floe/deepbacksub/blob/master/deepseg.py). That was running at about 2.5 FPS, which is really excruciatingly slow, so I built a [C++ version](https://github.com/floe/deepbacksub/blob/master/deepseg.cc) which manages 10 FPS without too much hand optimization. Good enough.\n\n## Replace Background\n\nThis is basically one line of code with OpenCV: `bg.copyTo(raw,mask);` Told you that's the easy part.\n\n## Virtual Video Device\n\nI'm using [v4l2loopback](https://github.com/umlaeute/v4l2loopback) to pipe the data from my userspace tool into any software that can open a V4L2 device. This isn't too hard because of the nice examples, but there are some catches, most notably color space. It took quite some trial and error to find a common pixel format that's accepted by Firefox, Skype, and guvcview, and that is [YUYV](https://www.linuxtv.org/downloads/v4l-dvb-apis-old/V4L2-PIX-FMT-YUYV.html). Nicely enough, my webcam can output YUYV directly as raw data, so that does save me some colorspace conversions.\n\n## End Result\n\nThe dataflow through the whole program is roughly as follows:\n\n  - init\n    - load background.png, convert to YUYV\n    - load DeepLab v3+ network, initialize TFLite\n    - setup V4L2 Loopback device (w,h,YUYV)\n  - loop\n    - grab raw YUYV image from camera\n    - extract square ROI in center\n      - downscale ROI to 257 x 257 (*)\n      - convert to RGB (*)\n      - run DeepLab v3+\n      - convert result to binary mask for class \"person\"\n      - denoise mask using erode/dilate\n    - upscale mask to raw image size\n    - copy background over raw image with mask (see above)\n    - `write()` data to virtual video device\n\n(*) these are required input parameters for DeepLab v3+\n\n## Requirements\n\nTested with the following dependencies:\n  - Ubuntu 18.04.5, x86-64\n  - Linux kernel 4.15 (stock package)\n  - OpenCV 3.2.0 (stock package)\n  - V4L2-Loopback 0.10.0 (stock package)\n  - Tensorflow Lite 2.1.0 (from [repo](https://github.com/tensorflow/tensorflow/tree/v2.1.0/tensorflow/lite))\n    - Ultra-short build guide for Tensorflow Lite C++ library: clone repo above, then...\n      - run `./tensorflow/lite/tools/make/download_dependencies.sh`\n      - run `./tensorflow/lite/tools/make/build_lib.sh`\n  \nTested with the following software:\n  - Firefox \n    - 74.0.1 (works)\n    - 76.0.1 (works)\n  - Skype \n    - 8.58.0.93 (works)\n    - 8.60.0.76 (works)\n  - guvcview 2.0.5 (works with parameter `-c read`)\n  - Microsoft Teams 1.3.00.5153 (works)\n  - Chrome 81.0.4044.138 (works)\n  - Zoom 5.0.403652.0509 (works - yes, I'm a hypocrite, I tested it with Zoom after all :-)\n  \n## Usage\n\nFirst, load the v4l2loopback module (extra settings needed to make Chrome work):\n```\nsudo modprobe v4l2loopback devices=1 max_buffers=2 exclusive_caps=1 card_label=\"VirtualCam\"\n```\nThen, run deepseg (-d for debug, -c for capture device, -v for virtual device):\n```\n./deepseg -d -c /dev/video0 -v /dev/video1\n```\n\n## Limitations/Extensions\n\nAs usual: pull requests welcome.\n  - The project name isn't catchy enough. Help me find a nice [backronym](https://en.wikipedia.org/wiki/Backronym).\n  - Resolution is currently hardcoded to 640x480 (lowest common denominator).\n  - Only works with Linux, because that's what I use.\n  - Needs a webcam that can produce raw YUYV data (but extending to the common YUV420 format should be trivial)\n  - CPU hog: maxes out two cores on my 2.7 GHz i5 machine for just VGA @ 10 FPS.\n  - Uses stock Deeplab v3+ network. Maybe re-training with only \"person\" and \"background\" classes could improve performance?\n\n## Fixed\n  \n  - Should probably do a erosion (+ dilation?) operation on the mask.\n  - Background image size needs to match camera resolution (see issue #1).\n\n## Other links\n\nFirefox preferred formats: https://dxr.mozilla.org/mozilla-central/source/media/webrtc/trunk/webrtc/modules/video_capture/linux/video_capture_linux.cc#142-159\n"
 },
 {
  "repo": "amarlearning/Finger-Detection-and-Tracking",
  "language": "Python",
  "readme_contents": "# Finger Detection and Tracking\n\n[![HitCount](http://hits.dwyl.com/amarlearning/opencv.svg)](http://hits.dwyl.com/amarlearning/opencv)\n[![Issues](https://camo.githubusercontent.com/926d8ca67df15de5bd1abac234c0603d94f66c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174)](https://github.com/amarlearning/Finger-Detection-and-Tracking/issues)\n[![Say Thanks!](https://img.shields.io/badge/SayThanks.io-%E2%98%BC-1EAEDB.svg)](mailto:amar.om1994@gmail.com)\n\n> Tracking the movement of a finger is an important feature of many computer vision applications. In this application, A histogram based approach is used to separate out the hand from the background frame. Thresholding and Filtering techniques are used for background cancellation to obtain optimum results.\n\n## Tutorial\n[Finger Detection and Tracking using OpenCV and Python](https://dev.to/amarlearning/finger-detection-and-tracking-using-opencv-and-python-586m)\n\n## How to run the code\nGo to the Finger Detection and Tracking directory.\nThen, \n\n1. Run the code with command `python FingerDetection.py`\n2. Put your parm over the green squares\n3. Then, press `z` key to start tracking\n4. Finish program with `Esc` key.\n\n## Demo\n[![Demo GitHub Sectory](https://cdn-images-1.medium.com/max/1040/1*ngmwjSu1gt4mmqhm9JJcpg.jpeg)](https://www.youtube.com/watch?v=P3dUePye_-k)\n\n## Stargazers over time\n\n[![Stargazers over time](https://starcharts.herokuapp.com/amarlearning/Finger-Detection-and-Tracking.svg)](https://starcharts.herokuapp.com/amarlearning/Finger-Detection-and-Tracking)\n\n## Issues\n\nYou can report the bugs at the [issue tracker](https://github.com/amarlearning/Finger-Detection-and-Tracking/issues)\n\n**OR**\n\nYou can [tweet me](https://twitter.com/iamarpandey) if you can't get it to work. In fact, you should tweet me anyway.\n\n***\n\n## License\n\nBuilt with \u2665 by Amar Prakash Pandey([@amarlearning](http://github.com/amarlearning)) under BSD 2-Clause \"Simplified\" License\n\nYou can find a copy of the License at [BSD 2-Clause \"Simplified\" License](https://raw.githubusercontent.com/amarlearning/Finger-Detection-and-Tracking/master/LICENSE)\n"
 },
 {
  "repo": "Aditya-Khadilkar/Face-tracking-with-Anime-characters",
  "language": "Python",
  "readme_contents": "# Face-tracking-with-Anime-characters\nHello! I have made a Python project where YURI from the game doki doki literature club accesses the webcam and stares directly into the player's soul. Hope you enjoy!\n\n![yuri](assets/demo.gif)\n\nRequirements:\n - Python 3.5 or above\n - OpenCV\n - Numpy\n - PyGame\n\nClone this repository and run yuriblush.py\nThe pictures given below are saved in the assets folder.(you can also use your own waifus if you want)\nRun the code and have fun with your waifu ... weeb\n\nMake your own:\n\nyou will need only 2 images\n1) character without eyes\n![yuriwoeyes](assets/yuri_no_eyes.bmp)\n2) eyes\n![eyes](assets/only_eyes.png)\n just use those images instead and you are good to go!\n\n<h1>Version 2: Yuri now has proximity </h1>\n<p>\nIf you move your face close to the webcam, she blushes!\n<br>\nJust run the yuriblush.py file and you're good!\n</p>\n\n![yuri cute](assets/demo_blush.gif)\n\n<h1>Featured in articles:</h1>\n<h3><a href = \"https://analyticsindiamag.com/top-7-anime-based-open-source-projects/\">analyticsindiamag</a><br>\n<a href = \"https://medium.mybridge.co/36-amazing-python-open-source-projects-v-2019-2fe058d79450\">Medium</a></h3>\n"
 },
 {
  "repo": "naokishibuya/car-finding-lane-lines",
  "language": "Jupyter Notebook",
  "readme_contents": "\n# Finding Lane Lines on the Road\n***\n\n[![png](images/dark-shades.png)](https://youtu.be/HTPEWC-fjCQ)\n[Video Link](https://youtu.be/HTPEWC-fjCQ)\n\n[Digest on Medium](https://medium.com/@naokishibuya/finding-lane-lines-on-the-road-30cf016a1165#.en7kaxeq4)\n\nIn this project, I used Python and OpenCV to find lane lines in the road images.  \n\nThe following techniques are used:\n\n- Color Selection\n- Canny Edge Detection\n- Region of Interest Selection\n- Hough Transform Line Detection\n\nFinally, I applied all the techniques to process video clips to find lane lines in them.\n\n## How to Reproduce the results\n\nTo set up the conda environment, execute the following:\n\n```python\nconda env create -f environment.yaml\n```\n\nThen, activate the environment:\n\n```python\nconda activate car-finding-lane-lines\n```\n\nStart the jupyter notebook:\n\n```bash\njupyter notebook Finding_Lane_Lines_on_the_Road.ipynb\n```\n\nExecute all the cells in the jupyter notebook.  The output videos are generated in the `output_videos` folder.\n\n---\n\nThe following explains what the codes in the notebook do.\n\n## Test Images\n\nLet's load and examine the test images.\n\n![png](images/output_6_0.png)\n\n\nLines are in white or yellow.  A white lane is a series of alternating dots and short lines, which we need to detect as one line.\n\n## Color Selection\n\n### RGB Color Space\n\nThe images are loaded in RGB color space.  Let's try selecting only yellow and white colors in the images using the RGB channels.\n\nReference: [RGB Color Code Chart](http://www.rapidtables.com/web/color/RGB_Color.htm)\n\n\n```python\n# image is expected be in RGB color space\ndef select_rgb_white_yellow(image): \n    # white color mask\n    lower = np.uint8([200, 200, 200])\n    upper = np.uint8([255, 255, 255])\n    white_mask = cv2.inRange(image, lower, upper)\n    # yellow color mask\n    lower = np.uint8([190, 190,   0])\n    upper = np.uint8([255, 255, 255])\n    yellow_mask = cv2.inRange(image, lower, upper)\n    # combine the mask\n    mask = cv2.bitwise_or(white_mask, yellow_mask)\n    masked = cv2.bitwise_and(image, image, mask = mask)\n    return masked\n```\n\n![png](images/output_9_0.png)\n\n\nIt looks pretty good except the two in which the yellow lines are not clear due to the dark shade from the tree on the left.\n\n### HSL and HSV Color Space\n\nUsing `cv2.cvtColor`, we can convert RGB image into different color space.  For example, [HSL and HSV color space](https://en.wikipedia.org/wiki/HSL_and_HSV).\n\n<img src='images/hsl-hsv.png' width='50%'>\n\nImage Source: [https://commons.wikimedia.org/wiki/File:Hsl-hsv_models.svg](https://commons.wikimedia.org/wiki/File:Hsl-hsv_models.svg)\n\n### HSV Color Space\n\nHow does it look when RGB images are converted into HSV color space?\n\n\n```python\ndef convert_hsv(image):\n    return cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n```\n\n![png](images/output_13_0.png)\n\nThe yellow lines are very clear including the ones under the shades but the white lines are less clear.\n\n### HSL Color Space\n\nHow does it look like when images are converted from RGB to HSL color space?\n\n```python\ndef convert_hls(image):\n    return cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n```\n\n![png](images/output_16_0.png)\n\n\nBoth the white and yellow lines are clearly recognizable.  Also, the yellow lines under the shades are clearly shown.\n\nLet's build a filter to select those white and yellow lines.  I want to select particular range of each channels (Hue, Saturation and Light).\n\n- Use `cv2.inRange` to filter the white color and the yellow color seperately.  \n  The function returns 255 when the filter conditon is satisfied.  Otherwise, it returns 0.\n- Use `cv2.bitwise_or` to combine these two binary masks.  \n  The combined mask returns 255 when either white or yellow color is detected.\n- Use `cv2.bitwise_and` to apply the combined mask onto the original RGB image\n\n\n```python\ndef select_white_yellow(image):\n    converted = convert_hls(image)\n    # white color mask\n    lower = np.uint8([  0, 200,   0])\n    upper = np.uint8([255, 255, 255])\n    white_mask = cv2.inRange(converted, lower, upper)\n    # yellow color mask\n    lower = np.uint8([ 10,   0, 100])\n    upper = np.uint8([ 40, 255, 255])\n    yellow_mask = cv2.inRange(converted, lower, upper)\n    # combine the mask\n    mask = cv2.bitwise_or(white_mask, yellow_mask)\n    return cv2.bitwise_and(image, image, mask = mask)\n\nwhite_yellow_images = list(map(select_white_yellow, test_images))\n```\n\n![png](images/output_19_0.png)\n\n\nFor the white color, \n- I chose high **Light** value.\n- I did not filter **Hue**, **Saturation** values.\n  \nFor the yellow color, \n- I chose **Hue** around 30 to choose yellow color.\n- I chose relatively high **Saturation** to exclude yellow hills\n\nThe combined mask filters the yellow and white lines very clearly.\n\n## Canny Edge Detection\n\nThe Canny edge detector was developed by John F. Canny in 1986.  \n\nWe want to detect edges in order to find straight lines especially lane lines.  For this, \n\n- use `cv2.cvtColor` to convert images into gray scale\n- use `cv2.GaussianBlur` to smooth out rough edges \n- use `cv2.Canny` to find edges\n\nLet's take a look at each step in details.\n\nNote: [Canny Edge Detection Wikipedia](https://en.wikipedia.org/wiki/Canny_edge_detector) has a good description in good details.\n\n\n### Gray Scaling\n\nThe images should be converted into gray scaled ones in order to detect shapes (edges) in the images.  This is because the Canny edge detection measures the magnitude of pixel intensity changes or gradients (more on this later).\n\nHere, I'm converting the white and yellow line images from the above into gray scale for edge detection.\n\n\n```python\ndef convert_gray_scale(image):\n    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\ngray_images = list(map(convert_gray_scale, white_yellow_images))\n```\n\n![png](images/output_23_0.png)\n\n\n### Gaussian Smoothing (Gaussian Blur)\n\nWhen there is an edge (i.e. a line), the pixel intensity changes rapidly (i.e. from 0 to 255) which we want to detect.  But before doing so, we should make the edges smoother.  As you can see,  the above images have many rough edges which causes many noisy edges to be detected.\n\nI use `cv2.GaussianBlur` to smooth out edges.\n\n- [Gaussian Filter OpenCV Theory](http://docs.opencv.org/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.html#gaussian-filter)\n- [cv2.GaussianBlur OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur)\n\n\n```python\ndef apply_smoothing(image, kernel_size=15):\n    \"\"\"\n    kernel_size must be postivie and odd\n    \"\"\"\n    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n```\n\nThe GaussianBlur takes a `kernel_size` parameter which you'll need to play with to find one that works best.  I tried 3, 5, 9, 11, 15, 17 (they must be positive and odd) and check the edge detection (see the next section) result.  The bigger the `kernel_size` value is,  the more blurry the image becomes.  \n\nThe bigger `kearnel_size` value requires more time to process. It is not noticeable with the test images but we should keep that in mind (later we'll be processing video clips).  So, we should prefer smaller values if the effect is similar.\n\n\n```python\nblurred_images = list(map(lambda image: apply_smoothing(image), gray_images))\n```\n\n![png](images/output_27_0.png)\n\n\n### Edge Detection\n\n`cv2.Canny` takes two threshold values which requires some explanation.\n\nWikipedia says:\n\n> it is essential to filter out the edge pixel with the weak gradient value and preserve the edge with the high gradient value. Thus two threshold values are set to clarify the different types of edge pixels, one is called high threshold value and the other is called the low threshold value. If the edge pixel\u2019s gradient value is higher than the high threshold value, they are marked as strong edge pixels. If the edge pixel\u2019s gradient value is smaller than the high threshold value and larger than the low threshold value, they are marked as weak edge pixels. If the pixel value is smaller than the low threshold value, they will be suppressed. \n\nAccording to the OpenCV documentation, the double thresholds are used as follows:\n\n- If a pixel gradient is higher than the upper threshold, the pixel is accepted as an edge\n- If a pixel gradient value is below the lower threshold, then it is rejected.\n- If the pixel gradient is between the two thresholds, then it will be accepted only if it is connected to a pixel that is above the upper threshold.\n- Canny recommended a upper:lower ratio between 2:1 and 3:1.\n\nThese two threshold values are empirically determined.  Basically, you will need to define them by trials and errors.\n\nI first set the `low_threshold` to zero and then adjust the `high_threshold`.   If `high_threshold` is too high, you find no edges.  If `high_threshold` is too low, you find too many edges.  Once you find a good `high_threshold`, adjust the `low_threshold` to discard the weak edges (noises) connected to the strong edges.\n\n- [Canny Edge Detection OpenCV Theory](http://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)\n- [cv2.Canny OpenCV API Reference](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)\n\n\n```python\ndef detect_edges(image, low_threshold=50, high_threshold=150):\n    return cv2.Canny(image, low_threshold, high_threshold)\n\nedge_images = list(map(lambda image: detect_edges(image), blurred_images))\n```\n\n![png](images/output_29_0.png)\n\n\n## Region of Interest Selection\n\nWhen finding lane lines, we don't need to check the sky and the hills.  \n\nRoughly speaking, we are interested in the area surrounded by the red lines below:\n\n<img src='images/region-of-interest.png' width='50%'/>\n\nSo, we exclude outside the region of interest by apply a mask.\n\n- [cv2.fillPoly OpenCV API Reference](http://docs.opencv.org/modules/core/doc/drawing_functions.html#fillpoly)\n\n\n```python\ndef filter_region(image, vertices):\n    \"\"\"\n    Create the mask using the vertices and apply it to the input image\n    \"\"\"\n    mask = np.zeros_like(image)\n    if len(mask.shape)==2:\n        cv2.fillPoly(mask, vertices, 255)\n    else:\n        cv2.fillPoly(mask, vertices, (255,)*mask.shape[2]) # in case, the input image has a channel dimension        \n    return cv2.bitwise_and(image, mask)\n\n    \ndef select_region(image):\n    \"\"\"\n    It keeps the region surrounded by the `vertices` (i.e. polygon).  Other area is set to 0 (black).\n    \"\"\"\n    # first, define the polygon by vertices\n    rows, cols = image.shape[:2]\n    bottom_left  = [cols*0.1, rows*0.95]\n    top_left     = [cols*0.4, rows*0.6]\n    bottom_right = [cols*0.9, rows*0.95]\n    top_right    = [cols*0.6, rows*0.6] \n    # the vertices are an array of polygons (i.e array of arrays) and the data type must be integer\n    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n    return filter_region(image, vertices)\n\n\n# images showing the region of interest only\nroi_images = list(map(select_region, edge_images))\n```\n\n![png](images/output_31_0.png)\n\n\nNow we have lane lines but we need to recognize them as lines.  Especially, two lines: the left lane and the right lane.\n\n## Hough Transform Line Detection\n\nI'm using `cv2.HoughLinesP` to detect lines in the edge images.\n\nThere are several parameters you'll need to tweak and tune:\n\n- rho: Distance resolution of the accumulator in pixels.\n- theta: Angle resolution of the accumulator in radians.\n- threshold: Accumulator threshold parameter. Only those lines are returned that get enough votes (> `threshold`).\n- minLineLength: Minimum line length. Line segments shorter than that are rejected.\n- maxLineGap: Maximum allowed gap between points on the same line to link them.\n\nMore details can be found:\n\n- [Hough Line Transform OpenCV Theory](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html)\n- [cv.HoughLinesP OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/feature_detection.html?highlight=houghlinesp#houghlinesp)\n\n\n```python\ndef hough_lines(image):\n    \"\"\"\n    `image` should be the output of a Canny transform.\n    \n    Returns hough lines (not the image with lines)\n    \"\"\"\n    return cv2.HoughLinesP(image, rho=1, theta=np.pi/180, threshold=20, minLineLength=20, maxLineGap=300)\n\n\nlist_of_lines = list(map(hough_lines, roi_images))\n```\n\n`list_of_lines` contains a list of lines detected.  With the above parameters, approximately 5-15 lines are detected for each image.\n\nLet's draw the lines onto the original images.  \n\n![png](images/output_36_0.png)\n\n\n### Averaging and Extrapolating Lines\n\nThere are multiple lines detected for a lane line.  We should come up with an averaged line for that.\n\nAlso, some lane lines are only partially recognized.  We should extrapolate the line to cover full lane line length.\n\nWe want two lane lines: one for the left and the other for the right.  The left lane should have a positive slope, and the right lane should have a negative slope.  Therefore, we'll collect positive slope lines and negative slope lines separately and take averages.\n\nNote: in the image, `y` coordinate is reversed.  The higher `y` value is actually lower in the image.  Therefore, the slope is negative for the left lane, and the slope is positive for the right lane.\n\n\n```python\ndef average_slope_intercept(lines):\n    left_lines    = [] # (slope, intercept)\n    left_weights  = [] # (length,)\n    right_lines   = [] # (slope, intercept)\n    right_weights = [] # (length,)\n    \n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            if x2==x1:\n                continue # ignore a vertical line\n            slope = (y2-y1)/(x2-x1)\n            intercept = y1 - slope*x1\n            length = np.sqrt((y2-y1)**2+(x2-x1)**2)\n            if slope < 0: # y is reversed in image\n                left_lines.append((slope, intercept))\n                left_weights.append((length))\n            else:\n                right_lines.append((slope, intercept))\n                right_weights.append((length))\n    \n    # add more weight to longer lines    \n    left_lane  = np.dot(left_weights,  left_lines) /np.sum(left_weights)  if len(left_weights) >0 else None\n    right_lane = np.dot(right_weights, right_lines)/np.sum(right_weights) if len(right_weights)>0 else None\n    \n    return left_lane, right_lane # (slope, intercept), (slope, intercept)\n```\n\nUsing the above `average_lines` function, we can calculate average slope and intercept for the left and right lanes of each image.  \n\nLet's draw the lanes.  I need to convert the slope and intercept into pixel points.\n\n\n```python\ndef make_line_points(y1, y2, line):\n    \"\"\"\n    Convert a line represented in slope and intercept into pixel points\n    \"\"\"\n    if line is None:\n        return None\n    \n    slope, intercept = line\n    \n    # make sure everything is integer as cv2.line requires it\n    x1 = int((y1 - intercept)/slope)\n    x2 = int((y2 - intercept)/slope)\n    y1 = int(y1)\n    y2 = int(y2)\n    \n    return ((x1, y1), (x2, y2))\n```\n\nOur `draw_lines` except a list of lines as the second parameter.  Each line is a list of 4 values (x1, y1, x2, y2).  The data type needs to be integer for `cv2.line` to work without throwing an error.\n\n\n```python\ndef lane_lines(image, lines):\n    left_lane, right_lane = average_slope_intercept(lines)\n    \n    y1 = image.shape[0] # bottom of the image\n    y2 = y1*0.6         # slightly lower than the middle\n\n    left_line  = make_line_points(y1, y2, left_lane)\n    right_line = make_line_points(y1, y2, right_lane)\n    \n    return left_line, right_line\n\n    \ndef draw_lane_lines(image, lines, color=[255, 0, 0], thickness=20):\n    # make a separate image to draw lines and combine with the orignal later\n    line_image = np.zeros_like(image)\n    for line in lines:\n        if line is not None:\n            cv2.line(line_image, *line,  color, thickness)\n    # image1 * \u03b1 + image2 * \u03b2 + \u03bb\n    # image1 and image2 must be the same shape.\n    return cv2.addWeighted(image, 1.0, line_image, 0.95, 0.0)\n             \n    \nlane_images = []\nfor image, lines in zip(test_images, list_of_lines):\n    lane_images.append(draw_lane_lines(image, lane_lines(image, lines)))\n```\n\n\n![png](images/output_42_0.png)\n\n\n## Video Clips\n\nI'm drawing lanes on video clips.\n\n\n```python\nfrom collections import deque\n\nQUEUE_LENGTH=50\n\nclass LaneDetector:\n    def __init__(self):\n        self.left_lines  = deque(maxlen=QUEUE_LENGTH)\n        self.right_lines = deque(maxlen=QUEUE_LENGTH)\n\n    def process(self, image):\n        white_yellow = select_white_yellow(image)\n        gray         = convert_gray_scale(white_yellow)\n        smooth_gray  = apply_smoothing(gray)\n        edges        = detect_edges(smooth_gray)\n        regions      = select_region(edges)\n        lines        = hough_lines(regions)\n        left_line, right_line = lane_lines(image, lines)\n\n        def mean_line(line, lines):\n            if line is not None:\n                lines.append(line)\n\n            if len(lines)>0:\n                line = np.mean(lines, axis=0, dtype=np.int32)\n                line = tuple(map(tuple, line)) # make sure it's tuples not numpy array for cv2.line to work\n            return line\n\n        left_line  = mean_line(left_line,  self.left_lines)\n        right_line = mean_line(right_line, self.right_lines)\n\n        return draw_lane_lines(image, (left_line, right_line))\n```\n\nLet's try the one with the solid white lane on the right first.\n\n```python\ndef process_video(video_input, video_output):\n    detector = LaneDetector()\n\n    clip = VideoFileClip(os.path.join('test_videos', video_input))\n    processed = clip.fl_image(detector.process)\n    processed.write_videofile(os.path.join('output_videos', video_output), audio=False)\n```\n\nThe video inputs are in test_videos folder.  The video outputs are generated in output_videos folder.\n\n```python\n%time process_video('solidWhiteRight.mp4', 'white.mp4')    \n\n%time process_video('solidYellowLeft.mp4', 'yellow.mp4')\n\n%time process_video('challenge.mp4', 'extra.mp4')\n```\n\n- [White Lanes Video](https://youtu.be/lc1QNDvjReA)\n- [Yellow Lanes Video](https://youtu.be/lvLLalGfy9M)\n- [Dark Shades Video](https://youtu.be/HTPEWC-fjCQ)\n\n## Conclusion\n\nThe project was successful in that the video images clearly show the lane lines are detected properly and lines are very smoothly handled.\n\nIt only detects the straight lane lines.  It is an advanced topic to handle curved lanes (or the curvature of lanes).  We'll need to use perspective transformation and also poly fitting lane lines rather than fitting to straight lines.\n\nHaving said that, the lanes near the car are mostly straight in the images.  The curvature appears at further distance unless it's a steep curve.  So, this basic lane finding technique is still very useful.\n\nAnother thing is that it won't work for steep (up or down) roads because the region of interest mask is assumed from the center of the image.\n\nFor steep roads, we first need to detect the horizontal line (between the sky and the earth) so that we can tell up to where the lines should extend.\n"
 },
 {
  "repo": "WatershedArts/Footfall",
  "language": "C++",
  "readme_contents": "## Footfall\n\n### Introduction\n\nFootfall is a camera based people counting system that uses a Raspberry Pi and Pi Cam and openFrameworks. There is more information about the system on our [blog](http://blogs.wcode.org/2015/04/footfall-a-camera-based-people-counting-system-for-under-60/).\n\n<p align=\"center\">\n  <img src=\"./images/FootfallGif.gif\">\n</p>\n\n\n**Important**: The source code provides a barebones system that **will require** customisation for each installation. \n\nThis means tweaking variables which are explained in the **[Configuration.md](./docs/configuration.md)**.\n\nBuilt using **openFrameworks 0.9.8**.\n\n### How to Customise\nInside the bin/data folder of the Footfall App there is a config.json file. This contains all of the relevant variables you will need to change.\n\nPlease see the [Configuration](./docs/config.md) section.\n\n### Documentation\nFind attached the relevant readmes to help build Footfall system.\n\n* [Raspberry Pi](./docs/rpi.md)\n* [Compiling](./docs/compiling.md)\n* [Configuration](./docs/configuration.md)\n* [Running](./docs/running.md)\n* [Server](./docs/server.md)\n* [Troubleshooting](./docs/troubleshooting.md)\n\n### Thanks\nThanks to Kyle McDonald for [ofxCv](http://github.com/kylemcdonald/ofxCv) and George Profenza for [ofxCvPiCam] (https://github.com/orgicus/ofxCvPiCam).\n\n### Warning\nThe original software was intended for sole use within **[Watershed](http://www.watershed.co.uk)**, therefore some of the source code has been altered for public use and differs slightly to our systems. For example our system generated event tags showing screening in conjunction with the total number of people in Watershed, to do this we had to pre-populate some timestamps and may cause an issue if the system is ran past a certain time.\n\n## Fixes\nWill update soon.\n\n\n## Updates\n#### Update 1\n\nI have added some new documentation about how to create the server side system  database etc... There is also a new guide about how to setup the raspberry pi as a server.\n\nI will be updating this repo with more detailed instructions and improved tracking over the coming weeks.\n\n#### Update 2\n\n##### 15/09/16\nI will updating the project to of_0.9.3 over the next coming weeks this should resolve some of the issues people have been having with ofxCv | ofXML.\n\n#### Update 3\n\n##### 13/02/17\nI will updating the project to of_0.9.8 soon. There will also only be one version of Footfall that should work across all Pi Platforms.\n\nIn the mean time. If you are coming up against compiler errors for ofxCvPiCam and ofxCv, try moving the ofxCvPiCam libs then, download the latest ofxCv repo. If you are using an earlier version oF ie less than 0.9.8. checkout the VS2012-stable branch. If you are using 0.9.8 use the origin/stable branch this will negate the glm:: errors. \n"
 },
 {
  "repo": "jbohnslav/opencv_transforms",
  "language": "Python",
  "readme_contents": "# opencv_transforms\r\n\r\nThis repository is intended as a faster drop-in replacement for [Pytorch's Torchvision augmentations](https://github.com/pytorch/vision/). This repo uses OpenCV for fast image augmentation for PyTorch computer vision pipelines. I wrote this code because the Pillow-based Torchvision transforms was starving my GPU due to slow image augmentation.\r\n\r\n## Requirements\r\n* A working installation of OpenCV. **Tested with OpenCV version 3.4.1, 4.1.0**\r\n* Tested on Windows 10 and Ubuntu 18.04. There is evidence that OpenCV doesn't work well with multithreading on Linux / MacOS, for example `num_workers >0` in a pytorch `DataLoader`. I haven't run into this issue yet. \r\n\r\n## Installation\r\nopencv_transforms is now a pip package! Simply use\r\n* `pip install opencv_transforms`\r\n\r\n## Usage\r\n**Breaking change! Please note the import syntax!** \r\n* `from opencv_transforms import transforms`\r\n* From here, almost everything should work exactly as the original `transforms`.\r\n#### Example: Image resizing \r\n```python\r\nimport numpy as np\r\nimage = np.random.randint(low=0, high=255, size=(1024, 2048, 3))\r\nresize = transforms.Resize(size=(256,256))\r\nimage = resize(image)\r\n```\r\nShould be 1.5 to 10 times faster than PIL. See benchmarks\r\n\r\n## Performance\r\n* Most transformations are between 1.5X and ~4X faster in OpenCV. Large image resizes are up to 10 times faster in OpenCV.\r\n* To reproduce the following benchmarks, download the [Cityscapes dataset](https://www.cityscapes-dataset.com/). \r\n* An example benchmarking file can be found in the notebook **bencharming_v2.ipynb** I wrapped the Cityscapes default directories with a HDF5 file for even faster reading. \r\n\r\n![resize](benchmarks/benchmarking_Resize.png)\r\n![random crop](benchmarks/benchmarking_Random_crop_quarter_size.png)\r\n![change brightness](benchmarks/benchmarking_Color_brightness_only.png)\r\n![change brightness and contrast](benchmarks/benchmarking_Color_constrast_and_brightness.png)\r\n![change contrast only](benchmarks/benchmarking_Color_contrast_only.png)\r\n![random horizontal flips](benchmarks/benchmarking_Random_horizontal_flip.png)\r\n\r\nThe changes start to add up when you compose multiple transformations together.\r\n![composed transformations](benchmarks/benchmarking_Resize_flip_brightness_contrast_rotate.png)\r\n\r\n## TODO\r\n- [x] Initial commit with all currently implemented torchvision transforms\r\n- [x] Cityscapes benchmarks\r\n- [ ] Make the `resample` flag on `RandomRotation`, `RandomAffine` actually do something\r\n- [ ] Speed up augmentation in saturation and hue. Currently, fastest way is to convert to a PIL image, perform same augmentation as Torchvision, then convert back to np.ndarray\r\n"
 },
 {
  "repo": "mgdm/OpenCV-for-PHP",
  "language": "C++",
  "readme_contents": "# OpenCV for PHP\n\nThis is a PHP extension wrapping the OpenCV library for image processing. It\nlets you use the OpenCV library for image recognition and modification tasks.\n\nIt requires PHP 5.3, and OpenCV 2.0 or above.\n"
 },
 {
  "repo": "YuanhaoGong/CurvatureFilter",
  "language": "C++",
  "readme_contents": "### Curvature filters are efficient solvers for variational models.\nThese curvature filters are developed by Yuanhao Gong during his PhD. MC filter and TV filter are exactly the same as described in the paper. But the GC filter is slightly modified. Please cite following papers if you use curvature filter in your work. Thank you!\n\n:books: **<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835193\" target=\"_blank\">The Paper</a>**. The general theory is in Chapter **Six** of **<a href=\"http://e-collection.library.ethz.ch/eserv/eth:47737/eth-47737-02.pdf\" target=\"_blank\">PhD thesis</a>** (downloaded **17,000+**)\n\n:closed_book: Presentation of Gaussian Curvature Filter: **<a href=\"http://www.slideshare.net/YuanhaoGong/a-fast-implicit-gaussian-curvature-filter\" target=\" blank\">LinkedIn</a>**, **<a href=\"https://www.dropbox.com/s/ax73park0popi4x/GCFilter_small.pdf?dl=0\" target=\"_blank\">Dropbox</a>** or **<a href=\"https://pan.baidu.com/s/1geS2EXH\" target=\"_blank\">Baidu</a>**. \n\n:blue_book: Poster of Bernstein Filter can be found **[here](images/BernsteinFilter.pdf)**. \n\n:gift: a short introduction in Chinese (\u4e2d\u6587): **<a href=\"https://zhuanlan.zhihu.com/p/22971865\" target=\"_blank\">Zhihu(Editors' Choice)</a>** or this **<a href=\"http://www.zhihu.com/question/35499791\" target=\"_blank\">Zhihu</a>**\n\n:trophy: **source code** in **C++** and **Java** can also be found from **<a href=\"http://mosaic.mpi-cbg.de/?q=downloads/curvaturefilters\" target=\"_blank\">MOSAIC group</a>**\n\n:bell: The kernels summary and one example how to get the kernel can be found **[here](images/CF_Kernels.pdf)**.\n\n:e-mail: gongyuanhao@gmail.com or join the **<a href=\"https://groups.google.com/forum/?hl=en#!forum/curvaturefilter\" target=\"_blank\">Curvature Filter Forum</a>**\n***\n```text\n@ARTICLE{gong:cf, \n    author={Yuanhao Gong and Ivo F. Sbalzarini}, \n    journal={IEEE Transactions on Image Processing}, \n    title={Curvature filters efficiently reduce certain variational energies}, \n    year={2017}, \n    volume={26}, \n    number={4}, \n    pages={1786-1798}, \n    doi={10.1109/TIP.2017.2658954}, \n    ISSN={1057-7149}, \n    month={April},}\n\n@phdthesis{gong:phd, \n    title={Spectrally regularized surfaces}, \n    author={Gong, Yuanhao}, \n    year={2015}, \n    school={ETH Zurich, Nr. 22616},\n    note={http://dx.doi.org/10.3929/ethz-a-010438292}}\n\t\n@article{gong:gc,\n    Author = {Yuanhao Gong and Ivo F. Sbalzarini},\n    Journal = {Intl. Conf. Image Proc. (ICIP)},\n    Month = {September},\n    Pages = {534--538},\n    Title = {Local weighted {G}aussian curvature for image processing},\n    Year = {2013}}\n```\n***\n## Curvature filters' philosophy \n\nTraditional solvers, such as gradient descent or Euler Lagrange Euqation, start at the total energy and use diffusion scheme to carry out the minimization. When the initial condition is the original image, the data fitting energy always increases while the regularization energy always reduces during the optimization, as illustrated in the below figure. Thus, regularization energy must be the dominant part since the total energy has to decrease. \n\nTherefore, **Curvature filters focus on minimizing the regularization term,** whose minimizers are already known. For example, if the regularization is Gaussian curvature, the developable surfaces minimize this energy. Therefore, in curvature filter, developable surfaces are used to approximate the data. **As long as the decreased amount in the regularization part is larger than the increased amount in the data fitting energy, the total energy is reduced.**\n\n![image](images/phs.PNG)\n\n***\n## Features\n| Theoretical  | Practical |\n| ------------- |:-------------:|\n| **Generality**: handle arbitrary data fitting term (BlackBox) ![ image ](images/box.png) | **Efficient**: three or four order of magnitude faster than traditional solvers ![ image ](images/fast.jpg) |\n| **Convergence**: theoretically guaranteed ![ image ](images/theory.png) | **Implementation**: 40 lines in Matlab and 100 lines in C++ ![ image ](images/easy.png) |\n\n***\n## Faster and Faster \n| Filter       | Bilateral Filter | Guided Filter | Guided Filter | MC Filter | GC Filter | Bernstein Filter |\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| Lang.      | C++ | Matlab | C++ | C++ | C++| C++|\n| MilliSec.      | 103 | 514 | 130 | 8 (or **327 MPixels/sec**) | 11| 7|\n\nRunning time with 10 iterations on 512X512 Lena image. Matlab version is R2015a and GCC version is 5.1. All tests are on a Thinkpad T410 with i7-620M core CPU (2.6GHz). We take the time for 100 iterations and divide it by 10. On average, curvature filters take 1 millisecond per iteration.\n\nOn my new taptop(Thinkpad T470p, NVIDIA GeForce 940MX, 384 CUDA cores), GPU version of MC filter can achieve **2500 MPixels/Second** with shared memory and single precision. \n\nOn the TITAN Xp card, MC filter can achieve **33.2 Giga Pixels/Second** with shared memory and single precision. On the Tesla K40c card (2880 cores), MC filter can achieve **8090 MPixels/Second** with shared memory and single precision. \n\n***\n## Example Applications\n### 1) Denoising\n![image](images/denoise.PNG)\nThe noise free test image can be downloaded **[here](images/developable.png)**\n### 2) Only minimize the regularization \nGC = Gaussian Curvature, MC = Mean Curvature, TV = Total Variation\n![image](images/curvatureFilters.png)\n### 3) Minimize a variational model, showing the line profile\nWe show three lines' profiles during minimizing a mean curvature regularized model (MC filter used). \n\n| ![ image](images/Lena_three_lines.png)      | ![image ](images/MC_line1_small.gif) |\n| ------------- |:-------------:|\n| ![image ](images/MC_line2_small.gif)      | ![image ](images/MC_line3_small.gif) |\n### 4) Cartoon Texture Decomposition\n![image](images/decomposition.png)\n### 5) Registration\nfrom left to right: original reference image, distorted source image, registered results by TV filter, MC filter and GC filter.\n![image](images/lena_circ.png)\n***\n## On Triangular Meshes (preliminary results, p.195 in the thesis)\noriginal mesh (left) and processed mesh (right), the energy profile is shown in the middle.\n![image](images/GC_mesh.jpg)\n***\n## FAQ:\n1) Why dual mesh (DM) structure is needed?\n\nThere are two reasons. First, these four sets guarantee the convergence. Second, \nwe can use the updated neighbors for current position. Therefore, it is more computational efficient.\n\n2) What is the difference between these three filters?\n\nIn general, GC filter is better in preserving details, compared with the other two. And\nTV filter is better in removing noise as well as details. MC filter is between these two.\n\nThese three filters are correspond to three types of variational models. User should decide\nwhich prior is to be assumed about the ground truth. \n\n3) What is the difference between split and nosplit scheme?\n\nIn general, splitting the image into four sets and looping on them is computational faster.\nHowever, in some cases like deconvolution, we need to merge the four sets after every iteration.\nSo, it is better do nosplit scheme.\n\nThese two lead to exactly the same result. The split code is just more cache friendly.\n"
 },
 {
  "repo": "leblancfg/autocrop",
  "language": "Python",
  "readme_contents": "# autocrop\n\n[![Travis Status](https://travis-ci.org/leblancfg/autocrop.svg?branch=master)](https://travis-ci.org/leblancfg/autocrop) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/y2iqfj2vgt6pofn3/branch/master?svg=true)](https://ci.appveyor.com/project/leblancfg/autocrop/branch/master) [![codecov](https://codecov.io/gh/leblancfg/autocrop/branch/master/graph/badge.svg)](https://codecov.io/gh/leblancfg/autocrop) [![Documentation](https://img.shields.io/badge/docs-passing-success.svg)](https://leblancfg.com/autocrop) [![PyPI version](https://badge.fury.io/py/autocrop.svg)](https://badge.fury.io/py/autocrop) [![Downloads](https://pepy.tech/badge/autocrop)](https://pepy.tech/project/autocrop) [![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/leblancfg/autocrop.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/leblancfg/autocrop/context:python)\n\n<p align=\"center\"><img title=\"obama_crop\" src=\"https://cloud.githubusercontent.com/assets/15659410/10975709/3e38de48-83b6-11e5-8885-d95da758ca17.png\"></p>\n\nPerfect for profile picture processing for your website or batch work for ID cards, autocrop will output images centered around the biggest face detected.\n\n## Installation\nSimple!\n\n~~~sh\npip install autocrop\n~~~\n\n## Use\nAutocrop can be used [from the command line](#from-the-command-line) or directly [from Python API](#from-python).\n\n### From Python\nImport the `Cropper` class, set some parameters (optional), and start cropping.\n\nThe `crop` method accepts filepaths or `np.ndarray`, and returns Numpy arrays. These are easily handled with [PIL](https://pillow.readthedocs.io/) or [Matplotlib](https://matplotlib.org/).\n\n~~~python\nfrom PIL import Image\nfrom autocrop import Cropper\n\ncropper = Cropper()\n\n# Get a Numpy array of the cropped image\ncropped_array = cropper.crop('portrait.png')\n\n# Save the cropped image with PIL\ncropped_image = Image.fromarray(cropped_array)\ncropped_image.save('cropped.png')\n~~~\n\nFurther examples and use cases are found in the [accompanying Jupyter Notebook](https://github.com/leblancfg/autocrop/blob/master/tests/visual_tests.ipynb).\n\n### From the command line\n\n\tusage: [-h] [-o OUTPUT] [-i INPUT] [-w WIDTH] [-H HEIGHT] [-v]\n\n\tAutomatically crops faces from batches of pictures\n\n\toptional arguments:\n\t  -h, --help\n\t  \t\tShow this help message and exit\n\t  -o, --output, -p, --path\n\t\t\tFolder where cropped images will be placed.\n\t\t\tDefault: current working directory\n\t  -r, --reject\n\t\t\tFolder where images without detected faces will be placed.\n\t\t\tDefault: same as output directory\n\t  -i, --input\n\t\t\tFolder where images to crop are located.\n\t\t\tDefault: current working directory\n\t  -w, --width\n\t\t\tWidth of cropped files in px. Default=500\n\t  -H, --height\n\t\t\tHeight of cropped files in px. Default=500\n\t  --facePercent\n\t  \t\tZoom factor. Percentage of face height to image height.\n\t  -v, --version\n\t  \t\tShow program's version number and exit\n\n#### Examples\n\n* Crop every image in the `pics` folder, resize them to 400 px squares, and output them in the `crop` directory:\n\t- `autocrop -i pics -o crop -w 400 -H 400`.\n\t- Images where a face can't be detected will be left in `crop`.\n* Same as above, but output the images with undetected faces to the `reject` folder:\n\t- `autocrop -i pics -o crop -r reject -w 400 -H 400`.\n\t\nIf no output folder is added, asks for confirmation and destructively crops images in-place.\n\n\n## Supported file types\n\nThe following file types are supported:\n\n- EPS files (`.eps`)\n- GIF files (`.gif`) (only the first frame of an animated GIF is used)\n- JPEG 2000 files (`.j2k`, `.j2p`, `.jp2`, `.jpx`)\n- JPEG files (`.jpeg`, `.jpg`, `.jpe`)\n- LabEye IM files (`.im`)\n- macOS ICNS files (`.icns`)\n- Microsoft Paint bitmap files (`.msp`)\n- PCX files (`.pcx`)\n- Portable Network Graphics (`.png`)\n- Portable Pixmap files (`.pbm`, `.pgm`, `.ppm`)\n- SGI files (`.sgi`)\n- SPIDER files (`.spi`)\n- TGA files (`.tga`)\n- TIFF files (`.tif`, `.tiff`)\n- WebP (`.webp`)\n- Windows bitmap files (`.bmp`, `.dib`)\n- Windows ICO files (`.ico`)\n- X bitmap files (`.xbm`)\n\n\n### Gotchas\nAutocrop uses OpenCV to perform face detection, which is installed through binary [wheels](http://pythonwheels.com/). If you *already* have OpenCV 3+ installed, you may wish to uninstall the additional OpenCV installation: `pip uninstall opencv-python`.\n\n### Installing directly\nIn some cases, you may wish the package directly, instead of through [PyPI](https://pypi.python.org/pypi):\n\n~~~\ncd ~\ngit clone https://github.com/leblancfg/autocrop\ncd autocrop\npip install .\n~~~\n\n### conda\nDevelopment of a `conda-forge` package for the [Anaconda Python distribution](https://www.anaconda.com/download/) is also currently slated for development. Please leave feedback on [issue #7](https://github.com/leblancfg/autocrop/issues/7) if you are insterested in helping out.\n\n## Requirements\nBest practice for your projects is of course to [use virtual environments](http://docs.python-guide.org/en/latest/dev/virtualenvs/). At the very least, you will need to [have pip installed](https://pip.pypa.io/en/stable/installing/).\n\nAutocrop is currently being tested on:\n\n* Python 3.6+\n* OS:\n    - Linux\n    - macOS\n    - Windows\n\n## More Info\nCheck out:\n\n* http://docs.opencv.org/master/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0\n* http://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html#gsc.tab=0\n\nAdapted from:\n\n* http://photo.stackexchange.com/questions/60411/how-can-i-batch-crop-based-on-face-location\n\n## Contributing\n\nAlthough autocrop is essentially a CLI wrapper around a single OpenCV function, it is actively developed. It has active users throughout the world.\n\nIf you would like to contribute, please consult the [contribution docs](CONTRIBUTING.md).\n"
 },
 {
  "repo": "joaopedronardari/OpenCV-AndroidSamples",
  "language": "Java",
  "readme_contents": "# OpenCV-AndroidSamples\nOpenCV samples for Android from OpenCV SDK using Android Studio and Gradle System\n\n[Demo Available on Google Play](https://play.google.com/store/apps/details?id=com.jnardari.opencv_androidsamples)\n\n## Integrated Samples\n- Tutorial 1 Camera Preview\n- Tutorial 2 Mixed Processing (uses JNI)\n- Tutorial 3 Camera Control\n- Image Manipulations\n- Face Detection (uses JNI)\n- Color Blob Detection\n- Puzzle 15\n"
 },
 {
  "repo": "Betterming/opencv_exercises",
  "language": "Python",
  "readme_contents": "# opencv_exercises\n## \u89c6\u9891\u5730\u5740\uff1ahttps://www.bilibili.com/video/av24998616\n### \u4ee3\u7801\u4e3b\u8981\u662f\u8ddf\u7740\u8d3e\u5fd7\u521a\u8001\u5e08\u4e00\u884c\u4e00\u884c\u6572\u51fa\u6765\u7684\u3002\u7edd\u5927\u90e8\u5206API\u90fd\u67e5\u4e86\u8d44\u6599\u3001\u52a0\u4e86\u6ce8\u91ca\uff0c\u57fa\u672c\u4e0a\u6bcf\u884c\u8981\u6ce8\u91ca\u7684\u4ee3\u7801\u90fd\u6ce8\u91ca\u4e86\u3002\n\n#### \u4e0b\u9762\u662f\u4e00\u4e9b\u95ee\u9898\u4e0e\u89e3\u7b54\uff0c\u5927\u5bb6\u60f3\u8981\u8865\u5145\u6216\u8005\u6709\u4ec0\u4e48\u7591\u95ee\u53ef\u4ee5\u5728Github\u4e0a\u53d1issue\u6216\u8005b\u7ad9\u53d1\u6d88\u606f\u7ed9\u6211\n\n### \u7591\u95ee\u4e0e\u89e3\u7b54\uff1a\n0. github\u4ee3\u7801\u4e0b\u8f7d\u62a5\u9519\u6216\u8005\u4e0b\u8f7d\u540e\u89e3\u538b\u62a5\u9519\n    - \u89e3\u7b54\uff1a\u53ef\u4ee5\u5728\u9879\u76ee\"Clone or download\"\u4e2d\u9009\u62e9https\u65b9\u5f0f\uff0c\u7528\u672c\u5730git\u5de5\u5177\u4e0b\u8f7d\uff1agit clone https://github.com/Betterming/opencv_exercises.git\n    - \u65b9\u4fbf\u5927\u5bb6\u4e0b\u8f7d\u4f7f\u7528\uff0c\u63d0\u4f9b\u672c\u9879\u76ee\u538b\u7f29\u5305\u94fe\u63a5\uff1a[opencv-exercises](https://cloud.189.cn/t/ZvENb2bE7BRf) \uff08\u8bbf\u95ee\u7801\uff1amy7k\uff09\n1. \u627e\u4e0d\u5230\u5305\uff1aModuleNotFoundError\uff1aNo module named 'cv2'\n    - \u89e3\u51b3\uff1a\u9996\u5148\u8981\u5b89\u88c5opencv\u5305  pip install opencv-python\uff0c\u82e5\u8fd8\u6ca1\u6709\u89e3\u51b3\uff0c\u9700\u8981\u5728pycharm\u4e2d\u5f15\u5165\u89e3\u91ca\u5668\u73af\u5883\uff0csetting->Project Interpreter \u70b9\u51fbProject Interpreter\u53f3\u4fa7\u952f\u9f7f\u9009\u62e9python\u73af\u5883\uff0c\u53ef\u80fd\u9700\u8981\u91cd\u542fpycharm\n2. \u62a5\u9519\uff1aerror: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow' \n    - \u89e3\u51b3\uff1a \u8fd9\u79cd\u95ee\u9898\u4e00\u822c\u662f\u56e0\u4e3a\u56fe\u7247/\u89c6\u9891\u7684\u8def\u5f84\u6709\u95ee\u9898\uff0c\u8def\u5f84\u505a\u597d\u4e0d\u80fd\u6709\u4e2d\u6587\uff0c\u6ce8\u610f\u4e0d\u540c\u7cfb\u7edf\u4e4b\u95f4\u8def\u5f84\u53ef\u80fd\u8868\u793a\u4e0d\u4e00\u6837\uff0c\u53ef\u4ee5\u5728\u8def\u5f84\u5b57\u7b26\u4e32\u524d\u9762\u52a0\u4e00\u4e2a\u5b57\u7b26r\n3. \u9a8c\u8bc1\u7801\u90a3\u8282\u62a5\u9519\uff1araise TesseractNotFoundError() pytesseract.pytesseract.TesseractNotFoundError: tesseract is not installed or it's not in your path\n    - \u89e3\u7b54\uff1a\u4e0d\u540c\u7cfb\u7edf\u91c7\u7528\u4e0d\u540c\u7b56\u7565\uff1a\n    ```cmd\n        On Linux\n            sudo apt update\n            sudo apt install tesseract-ocr\n            sudo apt install libtesseract-dev\n        On Mac\n            brew install tesseract\n        On Windows\n            \u5148\u4e0b\u8f7dtesseract\u5305\uff1ahttps://github.com/UB-Mannheim/tesseract/wiki. \n            \u7136\u540e\u4fee\u6539\u6e90\u7801pytesseract.py\u4e2dtesseract_cmd\u6307\u5411\u7684\u8def\u5f84\uff1atesseract_cmd = 'C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe'\n    ```\n\n\n# \u76ee\u5f55\n1. \u6982\u8ff0\u4e0e\u73af\u5883  tutorial_1_demo\n2. \u56fe\u50cf\u548c\u89c6\u9891\u8bfb\u53d6\u4e0e\u4fdd\u5b58  tutorial_1_RW\n3. \u7ed8\u56fe\u51fd\u6570  tutorial_1_draw\n4. \u628a\u9f20\u6807\u5f53\u753b\u7b14  tutorial_1_set_mouse\n5. \u7528\u6ed1\u52a8\u6761\u505a\u8c03\u8272\u677f  tutorial_1_tracebar\n6. numpy\u5728\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u57fa\u672c\u4f7f\u7528  tutorial_2_numpy\n7. \u989c\u8272\u7a7a\u95f4  tutorial_3_colorspace\n8. \u50cf\u7d20\u8fd0\u7b97  tutorial_4_Arithmetic\n9. \u56fe\u50cfroi\u4e0e\u6cdb\u6d2a\u586b\u5145  tutorial_5_roi\n10. \u51e0\u4f55\u53d8\u6362  tutorial_5_perspective_transform\n11. \u6a21\u7cca\u64cd\u4f5c  tutorial_6_7_blur\n12. \u8fb9\u7f18\u4fdd\u7559\u6ee4\u6ce2  tutorial_8_EPF\n13. \u76f4\u65b9\u56fe  tutorial_9_10_histogram\n14. \u76f4\u65b9\u56fe\u53cd\u5411\u6295\u5f71  tutorial_11_backprojection\n15. \u6a21\u677f\u5339\u914d  tutorial_12_template\n16. \u56fe\u50cf\u4e8c\u503c\u5316  tutorial_13_14_threshold\n17. \u56fe\u50cf\u91d1\u5b57\u5854  tutorial_15_pyramid\n18. \u56fe\u50cf\u68af\u5ea6  tutorial_16_grad\n19. Canny\u8fb9\u7f18\u68c0\u6d4b  tutorial_17_canny\n20. \u76f4\u7ebf\u68c0\u6d4b\u548c\u5706\u68c0\u6d4b  tutorial_18_19_Hough\n21. \u8f6e\u5ed3\u53d1\u73b0  tutorial_20_contours\n22. \u5bf9\u8c61\u6d4b\u91cf  tutorial_21_measure\n23. \u81a8\u80c0\u4e0e\u8150\u8680  tutorial_22_erode_dilate\n24. \u5f00\u95ed\u64cd\u4f5c  tutorial_23_24_morphology\n25. \u5206\u6c34\u5cad\u7b97\u6cd5  tutorial_25_watershed\n26. \u4eba\u8138\u68c0\u6d4b  tutorial_26_face_detection\n27. \u6570\u5b57\u9a8c\u8bc1\u7801\u8bc6\u522b  tutorial_27_recognization\n"
 },
 {
  "repo": "sarathknv/adversarial-examples-pytorch",
  "language": "Python",
  "readme_contents": "# Implementation of Papers on Adversarial Examples\nImplementation of papers with real-time visualizations and parameter control.\n\n## Dependencies  \n* Python3  \n* PyTorch (built from source)\n* OpenCV  \n* NumPy \n* SciPy\n* TensorBoard\n\n## Contents\n* [Adversarial perturbations are not random noise. DNNs are robust to such random perturbations.](random_perturbations/)\n* [Fast Gradient Sign Method (FGSM)](fgsm/)\n* [Basic Iterative Method](iterative/)\n* [One Pixel Attack](one_pixel_attack)\n* [AdvGAN - Generating Adversarial Examples with Adversarial Networks](adv_gan/)\n* [Spatially Transformed Adversarial Examples](spatially_transformed/)\n* [Generative Adversarial Trainer](gat/)\n\n\n\n\n\n------------------------------------------------\n# Random Perturbations  \n\nFrom one of the first papers on Adversarial examples - [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572),\n> The direction of perturbation, rather than the specific point in space, matters most. Space is\nnot full of pockets of adversarial examples that finely tile the reals like the rational numbers.  \n\nThis project examines this idea by testing the robustness of a DNN to randomly generated perturbations.\n\n## Usage\n```bash\n$ python3 explore_space.py --img images/horse.png\n```\n\n\n## Demo\n ![fgsm.gif](random_perturbations/images/horse_explore_demo.gif)  \n\nThis code adds to the input image (`img`) a randomly generated perturbation (`vec1`) which is subjected to a max norm constraint `eps`. This adversarial image lies on a hypercube centerd around the original image. To explore a region (a hypersphere) around the adversarial image (`img + vec1`), we add to it another perturbation (`vec2`) which is constrained by L<sub>2</sub> norm `rad`.  \nPressing keys `e` and `r` generates new `vec1` and `vec2` respectively.  \n\n\n\n\n## Random Perturbations   \n \n The classifier is robust to these random perturbations even though they have severely degraded the image. Perturbations are clearly noticeable and have significantly higher max norm.  \n \n | ![horse_explore](random_perturbations/images/horse_explore_single.gif) | ![automobile_explore](random_perturbations/images/automobile_explore.gif) | ![truck_explore](random_perturbations/images/truck_explore.gif) |  \n |:------------------------------------------:|:-----------------------:|:-----------:|  \n |             **horse**                      |      **automobile**     |: **truck** :|  \n \n In above images, there is no change in class labels and very small drops in probability.\n\n## FGSM Perturbations  \nA properly directed perturbation with max norm as low as 3, which is almost imperceptible, can fool the classifier.    \n\n | ![horse_scaled](random_perturbations/images/horse_scaled.png) | ![horse_adversarial](random_perturbations/images/horse_fgsm.png) | ![perturbation](random_perturbations/images/horse_fgsm_pert.png) |\n |:---------:|:--------------------:|:--------------------------:|\n | **horse** |  predicted - **dog** | perturbation **(eps = 6)** |  \n \n\n\n</br>  \n</br>\n\n# Fast Gradient Sign Method (FGSM)\n\n[Paper](https://arxiv.org/abs/1412.6572)  \n\n## Usage\n\n* **Run the script**\n```bash\n$ python3 fgsm_mnist.py --img one.jpg --gpu\n```  \n\n```bash\n$ python3 fgsm_imagenet.py --img goldfish.jpg --model resnet18 --gpu\n```  \n\n```fgsm_mnsit.py``` - for attack on custom model trained on MNIST whose weights are ```9920.pth.tar```.  \n```fgsm_imagenet``` - for pretrained imagenet models - resnet18, resnet50 etc.\n\n\n* **Control keys**  \n  - use trackbar to change `epsilon` (max norm)  \n  - `esc` - close  \n  - `s` - save perturbation and adversarial image  \n\n\n## Demo    \n![fgsm.gif](fgsm/images/demo/fgsm.gif) \n\n\n## Results  \n#### MNIST\n| Adversarial Image | Perturbation | \n|:----:|:----:|   \n| <img src=\"fgsm/images/results/adv_4.png\" width=\"84\"> | <img src=\"fgsm/images/results/perturbation_4_38.png\" width=\"84\"> |  \n| Pred: 4 | eps: 38 |  \n| <img src=\"fgsm/images/results/adv_7.png\" width=\"84\"> | <img src=\"fgsm/images/results/perturbation_7_60.png\" width=\"84\"> |  \n| Pred: 7 | eps: 60 |   \n| <img src=\"fgsm/images/results/adv_8(2).png\" width=\"84\"> | <img src=\"fgsm/images/results/perturbation_8(2)_42.png\" width=\"84\"> |  \n| Pred: 8 | eps: 42 |  \n| <img src=\"fgsm/images/results/adv_8.png\" width=\"84\"> | <img src=\"fgsm/images/results/perturbation_8_12.png\" width=\"84\"> |  \n| Pred: 8 | eps: 12 |    \n| <img src=\"fgsm/images/results/adv_9.png\" width=\"84\"> | <img src=\"fgsm/images/results/perturbation_9_17.png\" width=\"84\"> |  \n| Pred: 9 | eps: 17 |    \n\n\n\n\n</br>  \n</br>\n  \n# Basic Iterative Method (Targeted and Untargeted)\nPaper: [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)  \n\n\n## Usage  \n* **Run the script**\n```bash\n$ python3 iterative.py --img images/goldfish.jpg --model resnet18 --target 4\n# If argument 'target' is not specified, it is untargeted attack\n```\n* **Control keys**  \n  - use trackbar to change `epsilon` (max norm of perturbation) and `iter` (number of iterations)  \n  - `esc` close  and `space` to pause\n  - `s` save perturbation and adversarial image  \n\n\n## Demo \n![iterative.gif](iterative/images/demo/iterative.gif)\n\n\n</br>  \n</br>\n\n# One Pixel Attack for Fooling Deep Neural Networks\n\n[Paper](https://arxiv.org/abs/1710.08864)  \n\n> Existence of single pixel adversarial perturbations suggest that the assumption made in [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf) that small additive perturbation on the values of many dimensions will accumulate and cause huge change to the output, might not be necessary for explaining why natural images are sensitive to small perturbations. \n\n\n## Usage\n\n```bash\n$ python3 one_pixel.py --img airplane.jpg --d 3 --iters 600 --popsize 10\n```  \n`d` is number of pixels to change (**L<sub>0</sub>** norm)  \n`iters` and `popsize` are paprameters for [Differential Evolution](https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/)  \n\n## Results  \n\nAttacks are typically successful for images with low confidence. For successful attacks on high confidence images increase `d`, i.e., number of pixels to perturb.\n\n| ![airplane](one_pixel_attack/images/airplane_bird_8075.png) | ![bird](one_pixel_attack/images/bird_deer_8933.png) | ![cat](one_pixel_attack/images/cat_frog_8000.png)  |         ![frog](one_pixel_attack/images/frog_bird_6866.png) |  ![horse](one_pixel_attack/images/horse_deer_9406.png)  |\n|:------------------------------------------:|:----------------------------------:|:---------------------------------:|:-----------------------------------------:|:--------------------------------------:|  \n| **bird [0.8075]**                   |               **deer [0.8933]**           |  **frog [0.8000]**                |                        **bird [0.6866]**   |       **deer [0.9406]**                |\n\n\n\n\n\n\n\n</br>  \n</br>\n\n# AdvGAN - Generating Adversarial Examples with Adversarial Networks\n\n[Paper](https://arxiv.org/pdf/1801.02610.pdf) | IJCAI 2018  \n\n\n## Usage\n\n#### Inference\n```bash\n$ python3 advgan.py --img images/0.jpg --target 4 --model Model_C --bound 0.3\n```  \nEach of these settings has a separate Generator trained. This code loads appropriate trained model from ```saved/``` directory based on given arguments. As of now there are 22 Generators for different targets, different bounds (0.2 and 0.3) and target models (only ```Model_C``` for now).\n\n\n#### Training AdvGAN (Untargeted)\n```bash\n$ python3 train_advgan.py --model Model_C --gpu\n```  \n#### Training AdvGAN (Targeted)\n```bash\n$ python3 train_advgan.py --model Model_C --target 4 --thres 0.3 --gpu\n# thres: Perturbation bound \n```  \nUse ```--help``` for other arguments available (```epochs```, ```batch_size```, ```lr``` etc.)\n\n\n#### Training Target Models (Models A, B and C)\n```bash\n$ python3 train_target_models.py --model Model_C\n```  \n\nFor TensorBoard visualization,\n```bash\n$ python3 generators.py\n$ python3 discriminators.py\n```  \n\nThis code supports only MNIST dataset for now. Same notations as in paper are followed (mostly).\n\n\n\n## Results \nThere are few changes that have been made for model to work.\n* Generator in paper has ```ReLU``` on the last layer. If input data is normalized to [-1 1] there wouldn't be any perturbation in the negative region. As expected accuracies were poor (~10% Untargeted). So ```ReLU``` was removed. Also, data normalization had significat effect on performance. With [-1 1] accuracies were around 70%. But with [0 1] normalization accuracies were ~99%.\n* Perturbations (```pert```) and adversarial images (```x + pert```) were clipped. It's not converging otherwise.\n\nThese results are for the following settings.\n* Dataset - MNIST\n* Data normalization - [0 1]\n* thres (perturbation bound) - 0.3 and 0.2\n* No ```ReLU``` at the end in Generator\n* Epochs - 15\n* Batch Size - 128\n* LR Scheduler - ```step_size``` 5, ```gamma``` 0.1 and initial ```lr``` - 0.001\n\n\n| Target     |Acc [thres: 0.3]  | Acc [thres: 0.2] |\n|:----------:|:---------:|:---------:|\n| Untargeted | 0.9921    | 0.8966    |    \n| 0          | 0.9643    | 0.4330    |\n| 1          | 0.9822    | 0.4749    |  \n| 2          | 0.9961    | 0.8499    |\n| 3          | 0.9939    | 0.8696    |  \n| 4          | 0.9833    | 0.6293    |\n| 5          | 0.9918    | 0.7968    |  \n| 6          | 0.9584    | 0.4652    |\n| 7          | 0.9899    | 0.6866    |  \n| 8          | 0.9943    | 0.8430    |\n| 9          | 0.9922    | 0.7610    |  \n\n\n\n#### Untargeted\n| <img src=\"adv_gan/images/results/untargeted_0_9.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_1_3.png\" width=\"84\"> |<img src=\"adv_gan/images/results/untargeted_2_8.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_3_8.png\" width=\"84\"> |  <img src=\"adv_gan/images/results/untargeted_4_4.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_5_3.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_6_8.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_7_3.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_8_3.png\" width=\"84\"> | <img src=\"adv_gan/images/results/untargeted_9_8.png\" width=\"84\"> | \n|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|Pred: 9|Pred: 3|Pred: 8|Pred: 8|Pred: 4|Pred: 3|Pred: 8|Pred: 3|Pred: 3|Pred: 8|\n\n\n#### Targeted\n| Target: 0 | Target: 1 | Target: 2 | Target: 3 | Target: 4 | Target: 5 | Target: 6 | Target: 7 | Target: 8 | Target: 9 |  \n|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n| <img src=\"adv_gan/images/results/targeted_0_0_0.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_1_1.png\" width=\"84\"> |<img src=\"adv_gan/images/results/targeted_0_2_2.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_3_3.png\" width=\"84\"> |  <img src=\"adv_gan/images/results/targeted_0_4_4.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_5_5.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_6_6.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_7_7.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_8_8.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_0_9_9.png\" width=\"84\"> |\n|Pred: 0|Pred: 1|Pred: 2|Pred: 3|Pred: 4|Pred: 5|Pred: 6|Pred: 7|Pred: 8|Pred: 9|\n| <img src=\"adv_gan/images/results/targeted_1_0_0.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_1_1.png\" width=\"84\"> |<img src=\"adv_gan/images/results/targeted_1_2_2.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_3_3.png\" width=\"84\"> |  <img src=\"adv_gan/images/results/targeted_1_4_4.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_5_5.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_6_6.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_7_7.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_8_8.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_1_9_9.png\" width=\"84\"> |\n|Pred: 0|Pred: 1|Pred: 2|Pred: 3|Pred: 4|Pred: 5|Pred: 6|Pred: 7|Pred: 8|Pred: 9|\n| <img src=\"adv_gan/images/results/targeted_9_0_0.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_1_1.png\" width=\"84\"> |<img src=\"adv_gan/images/results/targeted_9_2_2.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_3_3.png\" width=\"84\"> |  <img src=\"adv_gan/images/results/targeted_9_4_4.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_5_5.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_6_6.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_7_7.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_8_8.png\" width=\"84\"> | <img src=\"adv_gan/images/results/targeted_9_9_9.png\" width=\"84\"> |\n|Pred: 0|Pred: 1|Pred: 2|Pred: 3|Pred: 4|Pred: 5|Pred: 6|Pred: 7|Pred: 8|Pred: 9|\n\n    \n\n\n\n</br>  \n</br>\n\n# Spatially Transformed Adversarial Examples\n[Paper](https://arxiv.org/abs/1801.02612) | ICLR 2018  \nRefer [View Synthesis by Appearance Flow](https://people.eecs.berkeley.edu/~tinghuiz/papers/eccv16_appflow.pdf) for clarity.\n\n\n## Usage\n```bash\n$ python3 stadv.py --img images/1.jpg --target 7\n```  \nRequires OpenCV for real-time visualization.  \n\n\n## Demo\n![0_1](spatially_transformed/images/demo/0_1.gif) ![1_2](spatially_transformed/images/demo/1_2.gif) ![2_3](spatially_transformed/images/demo/2_3.gif) ![3_4](spatially_transformed/images/demo/3_4.gif) ![4_5](spatially_transformed/images/demo/4_5.gif) ![5_6](spatially_transformed/images/demo/5_6.gif) ![6_7](spatially_transformed/images/demo/6_7.gif) ![7_8](spatially_transformed/images/demo/7_8.gif) ![8_9](spatially_transformed/images/demo/8_9.gif) ![9_0](spatially_transformed/images/demo/9_0.gif)\n\n## Results  \n#### MNIST\nColumn index is target label and ground truth images are along diagonal. \n  \n  \n![tile](spatially_transformed/images/tile.png?raw=true)\n\n</br>  \n</br>\n\n\n\n\n"
 },
 {
  "repo": "inspurer/WorkAttendanceSystem",
  "language": "Python",
  "readme_contents": "\n## \u9879\u76ee\u53ca\u4f5c\u8005\u8bf4\u660e\n\n \n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/dlib--green.svg\"></a>\n  <img src=\"https://img.shields.io/badge/opencv--red.svg\"></a>\n  <img src=\"https://img.shields.io/badge/sqlite3--blue.svg\"></a>\n  <img src=\"https://img.shields.io/badge/numpy--yellow.svg\"></a>\n </p>\n \n \n \u4e00\u4e2a\u57fa\u4e8e Opencv\u3001dlib \u4eba\u8138\u8bc6\u522b\u7684\u5458\u5de5\u8003\u52e4\u7cfb\u7edf\uff0c\u4f5c\u8005\u67d0\u53cc\u4e00\u6d41A\u7c7b\u5927\u5b66\u91cc\u7684~~\u4e00\u6d41~~\u5b66\u751f\uff0c\u5199\u4e8e2018/09/\uff0cPython \u5b66\u4e60\u671f\u95f4\u3002  \n \n \n|\u4f5c\u8005|[inspurer](https://inspurer.github.io/2018/06/07/%E6%9C%88%E5%B0%8F%E6%B0%B4%E9%95%BF%E7%9A%84%E7%94%B1%E6%9D%A5/#more)|\n|:---:|:---:|\n|QQ\u4ea4\u6d41\u7fa4|[861016679](https://jq.qq.com/?_wv=1027&k=5Js6sKS)|\n|\u4e2a\u4eba\u535a\u5ba2|[https://inspurer.github.io/](https://inspurer.github.io/)|\n\n## \u5f00\u6e90\u7248\u53ca\u5546\u4e1a\u7248\u8bf4\u660e\n\n\n||\u5f00\u6e90\u7248|\u5546\u4e1a\u7248|\n|:---:|:---:|:---:|\n|\u4eba\u8138\u5f55\u5165|\u652f\u6301|\u652f\u6301|\n|\u4e0a\u73ed\u5237\u8138\u7b7e\u5230|\u652f\u6301|\u652f\u6301|\n|\u4e0b\u73ed\u5237\u8138\u7b7e\u5230|\u4e0d\u652f\u6301|\u652f\u6301|\n|\u81ea\u5b9a\u4e49\u7b7e\u5230\u65f6\u95f4|\u4e0d\u652f\u6301|\u652f\u6301|\n|\u8003\u52e4\u65e5\u5fd7\u5c55\u793a\uff08\u7535\u5b50\u8868\u683c\uff09|\u652f\u6301|\u652f\u6301|\n|\u8003\u52e4\u65e5\u5fd7\u5c55\u793a\uff08\u7f51\u9875\u8868\u683c\uff09|\u4e0d\u652f\u6301|\u652f\u6301|\n|\u8003\u52e4\u65e5\u5fd7\u81ea\u5b9a\u4e49\u5bfc\u51fa|\u4e0d\u652f\u6301|\u652f\u6301|\n\n>\u5546\u4e1a\u7248\u6e90\u4ee3\u7801\u548c exe \u5747\u672a\u516c\u5f00\uff0c\u5982\u9700\u5546\u4e1a\u7248\u5728**\u5fae\u4fe1\u516c\u4f17\u53f7\uff1a\u6708\u5c0f\u6c34\u957f**\u540e\u53f0\u79c1\u6233\u6211\uff0c\u4f38\u624b\u515a\u52ff\u6270\u3002\n\n\n------------------------------------------------------------------------------------\n\n## QuickStart\n\n\n<ol>\n <li>IDE\uff1aPycharm</li>\n <li>\u73af\u5883\uff1aPython 3.5+</li>\n <li>\u7b2c\u4e09\u65b9\u5305\uff1apip install -r requirements.txt</li>\n</ol>\n  \n# V1.0\n\n\n## \u9879\u76ee\u7ed3\u6784    \n\n~~mainui.py\u662f\u4e3b\u754c\u9762\uff0c\u8c03\u7528face_img_register.py\u548cface_recognize_punchcard.py\u3002\nface_img_register.py\u4e3b\u8981\u5b9e\u73b0\u5f55\u5165\u4eba\u8138\u4fe1\u606f\u529f\u80fd\uff0cface_recognize_punchcard.py\u5b9e\u73b0\u5237\u8138\u8003\u52e4\u3002\nface_feature_storage.py\u662f\u5728\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6587\u4ef6\uff0c\u53ef\u65e0\u89c6 \u3002\nface_recognize_punchcard_lib.py\u548cface_recognize_punchcard.py\u672c\u8d28\u4e0a\u5dee\u4e0d\u591a\uff0c\u4f46\u662f\u524d\u8005\u662f\u7ed9face_img_register.py\u5e93\u4f9d\u8d56\u3002    \n\u9632\u6b62\u5f55\u5165\u4e24\u4e2a\u540c\u6837\u7684\u4eba\u8138\u5efa\u4e0d\u540c\u6570\u636e\u5e93\u7684\u98ce\u9669.~~\n\n## \u8fd0\u884c\u6548\u679c   \n### 1. \u4e3b\u754c\u9762   \n\n\n![](pictures/1.png)   \n\n\n### 2. \u4eba\u8138\u5f55\u5165   \n\n\n![](pictures/2.png)    \n\n\n### 3. \u5237\u8138\u8003\u52e4   \n\n>\u56fe\u7247\u6d89\u53ca\u4e2a\u4eba\u9690\u79c1\uff0c\u5df2\u88ab\u6211\u5220\u9664\u3002\n\n## \u66f4\u65b0     \n### 2018/9/23\u66f4\u65b0\n\n>mainui.py-->myapp.py   \n>face_recognize_punchcard_lib.py\u7b49\u9e21\u808b\u6587\u4ef6\u653e\u5230useless\u6587\u4ef6\u5939\u91cc    \n>\u8fd0\u884c\u6548\u7387\u663e\u8457\u63d0\u9ad8   \n\n### 2018/9/25\u66f4\u65b0    \n\n>\u89e3\u51b3\u540c\u6b65\u6027\u95ee\u9898\uff0c\u65b0\u5f55\u5165\u7684\u4eba\u8138\u80fd\u7acb\u5373\u88ab\u8bc6\u522b    \n>\u4ee3\u7801\u7684\u8fd0\u884c\u901f\u5ea6\u5c11\u8bb8\u4e0b\u964d    \n----------------------------------------------------------------------------------------------\n\n# V2.0\n\n## \u91cd\u6784\u4ee3\u7801   \n\n>\u8001\u5e08\u770b\u4e86\u6211\u7684V1.0\u7248\u672c\u540e\u7ed9\u7684\u5efa\u8bae\uff0c\n\n>\u7b2c\u4e00\uff0c\u4e0d\u80fd\u6709\u592a\u591a\u5f39\u7a97\u754c\u9762\uff0c\u4e00\u4e9b\u64cd\u4f5c\u5e94\u8be5\u653e\u5230\u5de5\u5177\u680f\u4e2d\uff0c\u800c\u4e0d\u662f\u5f39\u7a97\u5b9e\u73b0\uff1b\n\n>\u7b2c\u4e8c\uff0c\u6570\u636e\u4fdd\u5b58\u5728csv\u6587\u4ef6\u5bb9\u6613\u88ab\u7a83\u8bfb\uff0c\u5e94\u8be5\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u91cc\uff1b\n\n>\u7b2c\u4e09\uff0c\u663e\u793a\u8003\u52e4\u65e5\u5fd7\u65f6\u5e94\u8be5\u7531\u81ea\u5df1\u7684\u7535\u5b50\u8868\u683c\u5b9e\u73b0\u800c\u4e0d\u662f\u76f4\u63a5\u8c03\u7528excel\u3002\n\n>\u57fa\u4e8e\u4ee5\u4e0a\u7684\u95ee\u9898\u548c\u7b97\u6cd5\u4f18\u5316\u7684\u9700\u8981\uff0c\u6211\u628a\u4ee3\u7801\u4f18\u5316\u6210\u4e86V2.0\uff0c\u540c\u65f6\u6240\u4ee5\u7684\u4ee3\u7801\u5408\u5728\u4e00\u4e2aWorkAttwndanceSystem.py\u6587\u4ef6\u91cc\uff0c\u5c31\u4eba\u8138\u8bc6\u522b\u8fd9\u90e8\u5206\u4ee3\u7801\u800c\u8a00\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u590d\u7528\u5ea6\n\n## 1. \u5168\u65b0\u8bbe\u8ba1\u7684UI,\u66f4\u4eba\u6027\u5316\u7684\u64cd\u4f5c    \n![](pictures/4.png)![](pictures/5.png)\n    \n## 2. \u6570\u636e\u66f4\u52a0\u5b89\u5168\uff0c\u7528\u6237\u4e0d\u53ef\u89c1\n>\u4eba\u8138\u6570\u636e\u548c\u7b7e\u5230\u65e5\u5fd7\u5168\u90e8\u4fdd\u5b58\u5728inspurer.db\u6570\u636e\u5e93\u6587\u4ef6\u91cc\uff0c\u66f4\u52a0\u5b89\u5168\uff1b\u800c\u4e14\u5bf9\u4eba\u8138\u6570\u636e\u8fdb\u884c\u4e86\u538b\u7f29\uff0c\u66f4\u52a0\u5c0f\u5de7\u3002   \n## 3. \u6ce8\u610f\u4e8b\u9879   \n<ol>\n<li>\u6253\u5f00\u6444\u50cf\u5934\u65f6\u8bf7\u5de6\u53f3\u6643\u52a8\u4e00\u4e0b\u4eba\u8138\uff0c\u786e\u4fdd\u4eba\u8138\u8bc6\u522b\u5f00\u59cb.</li>\n<li>\u4eba\u8138\u8bc6\u522b\u65f6\u505a\u4e86\u62d2\u7edd\u5904\u7406\uff0c\u591a\u5f20\u4eba\u8138\u65f6\uff0c\u53ea\u53d6\u8ddd\u79bb\u5c4f\u5e55\u6700\u8fd1\u7684\u4eba\u8138.</li>\n<li>`\u65b0\u5efa\u5f55\u5165`\u65f6\u4f1a\u81ea\u52a8\u5f55\u5165\u5341\u5f20\u4eba\u8138\u7167\u7247\uff0c\u4e5f\u53ef\u624b\u52a8\u70b9\u51fb`\u5b8c\u6210\u5f55\u5165`\u7acb\u5373\u5b8c\u6210\uff0c\u4e4b\u540e\u5c31\u4f1a\u8ba1\u7b97\u4eba\u8138\u6570\u636e\u5e76\u5b58\u50a8\u5230\u6570\u636e\u5e93\u4e2d,\u5de6\u8fb9\u7684\u4fe1\u606f\u680f\u4f1a\u6709\u76f8\u5e94\u7684\u4fe1\u606f\u6253\u5370.</li?\n<li>`\u5f00\u59cb\u7b7e\u5230`\u540e\uff0c\u5982\u4e0d\u70b9\u51fb`\u7ed3\u675f\u7b7e\u5230`,\u5c31\u4f1a\u4e00\u76f4\u5bf9\u5c4f\u5e55\u524d\u7684\u4eba\u8138\u8fdb\u884c\u7b7e\u5230\uff0c\u7b7e\u5230\u72b6\u6001\u5206\u4e09\u79cd,\u672a\u8bc6\u522b\u7684\u4eba\u8138,\u7b7e\u5230\u6210\u529f,\u7b7e\u5230\u6210\u529f\u4f46\u662f\u8fdf\u5230\u4e86\uff089.00\u540e\u7b7e\u5230\uff09,\u91cd\u590d\u7b7e\u5230(\u6b64\u65f6\u7b7e\u5230\u65e5\u5fd7\u4e0d\u4f1a\u5199\u5165\u5230\u6570\u636e\u5e93.</li>         \n<li>\u4e3a\u786e\u4fdd\u7a0b\u5e8f\u7a33\u5b9a\u6027\uff0c\u6bcf\u4e00\u4e2a\u83dc\u5355\u91cc\u7684\u6309\u94ae\u5c3d\u91cf\u6210\u5bf9\u64cd\u4f5c.</li>  \n</ol>\n\n-----------------------------------------------------------------------------------\n\n# \u540e\u8bdd\n\n## git clone\u4e0b\u8f7d\u4ed3\u5e93\u901f\u5ea6\u8fc7\u6162\u7684\u95ee\u9898\u8bf7\u53c2\u8003:[CSDN\u535a\u5ba2](https://blog.csdn.net/ygdxt/article/details/82825013)\n\n## \u6709\u5173\u4ee3\u7801\u7684\u8be6\u7ec6\u89e3\u91ca\u548c\u8bbe\u8ba1\u5b9e\u73b0\u8fc7\u7a0b\u8bf7\u53c2\u8003:[\u5fae\u4fe1\u516c\u4f17\u53f7:\u6708\u5c0f\u6c34\u957f](https://mp.weixin.qq.com/s/6BxBQoSwzhI6WooKMuTkNA)  \n\n## V2.0\u7248\u672c\u5df2\u6253\u5305,\u53ef\u76f4\u63a5\u4e0b\u8f7d\u8fd0\u884c,\u6587\u4ef6\u8fc7\u5927\uff0c\u53ea\u4e0a\u4f20\u767e\u5ea6\u4e91(\u6ce8\u610f\u8fd9\u662f exe \u4e0d\u662f\u6e90\u4ee3\u7801\uff01\uff01\uff01\uff09  \n\n[\u94fe\u63a5\uff1ahttps://pan.baidu.com/s/1aIA6AhTB8LVESSAN5jgDjQ \u63d0\u53d6\u7801\uff1al7or](https://pan.baidu.com/s/1aIA6AhTB8LVESSAN5jgDjQ) \n\n## \u66f4\u65b0\u8ba1\u5212\n\n<ol>\n <li>\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\uff0c\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002</li>\n <li>\u91c7\u7528 PyQt5 \u91cd\u6784\u6574\u4e2a\u754c\u9762\u3002</li>\n <li>\u6539\u7528 Java \u8bed\u8a00\uff0c\u79fb\u690d\u5230 Android \u5e73\u53f0\u4e0a\u3002</li>\n</ol>\n\n## \u8d5e\u8d4f  \n![](https://github.com/inspurer/WorkAttendanceSystem/blob/master/pictures/TIM%E5%9B%BE%E7%89%8720181208222337.png)  \n\n"
 },
 {
  "repo": "vRallev/OpenCV",
  "language": "Java",
  "readme_contents": "# Deprecated\n### This project is not maintained anymore.\n\n\n~~OpenCV Java project for Android~~\n===================================\n\n~~The OpenCV library for Android as Gradle project.~~\n\nAll credit goes to http://opencv.org/ I've took their library and exported it as `.aar` library. Now you can easily use OpenCV in a Gradle / Android Studio project.\n\nUsage Maven Repo\n----------------\n\nI've uploaded the `.aar` in my maven repository. You only need to add following lines to your `build.gradle` to add the dependency:\n```groovy\nrepositories {\n    maven {\n        url 'https://raw.github.com/vRallev/mvn-repo/master/'\n    }\n}\n\ndependencies {\n    compile 'org.opencv:opencv-android:2.4.8'\n}\n```\n\nCompiling the library\n---------------------\n\nYou can also clone the library and add it your local maven repository.\n \n 1. Clone the repository.\n 2. In the root project folder (`opencv-library`) run `gradle uploadArchives`.\n 3. Add the same dependency as above:\n\n```groovy\nrepositories {\n    mavenLocal()\n}\n\ndependencies {\n    compile 'org.opencv:opencv-android:2.4.8'\n}\n``` \n\nLicense\n-------\n\n\tBy downloading, copying, installing or using the software you agree to this license.\n\tIf you do not agree to this license, do not download, install,\n\tcopy or use the software.\n\n\n\t                          License Agreement\n\t               For Open Source Computer Vision Library\n\t                       (3-clause BSD License)\n\n\tCopyright (C) 2000-2015, Intel Corporation, all rights reserved.\n\tCopyright (C) 2009-2011, Willow Garage Inc., all rights reserved.\n\tCopyright (C) 2009-2015, NVIDIA Corporation, all rights reserved.\n\tCopyright (C) 2010-2013, Advanced Micro Devices, Inc., all rights reserved.\n\tCopyright (C) 2015, OpenCV Foundation, all rights reserved.\n\tCopyright (C) 2015, Itseez Inc., all rights reserved.\n\tThird party copyrights are property of their respective owners.\n\n\tRedistribution and use in source and binary forms, with or without modification,\n\tare permitted provided that the following conditions are met:\n\n\t  * Redistributions of source code must retain the above copyright notice,\n\t    this list of conditions and the following disclaimer.\n\n\t  * Redistributions in binary form must reproduce the above copyright notice,\n\t    this list of conditions and the following disclaimer in the documentation\n\t    and/or other materials provided with the distribution.\n\n\t  * Neither the names of the copyright holders nor the names of the contributors\n\t    may be used to endorse or promote products derived from this software\n\t    without specific prior written permission.\n\n\tThis software is provided by the copyright holders and contributors \"as is\" and\n\tany express or implied warranties, including, but not limited to, the implied\n\twarranties of merchantability and fitness for a particular purpose are disclaimed.\n\tIn no event shall copyright holders or contributors be liable for any direct,\n\tindirect, incidental, special, exemplary, or consequential damages\n\t(including, but not limited to, procurement of substitute goods or services;\n\tloss of use, data, or profits; or business interruption) however caused\n\tand on any theory of liability, whether in contract, strict liability,\n\tor tort (including negligence or otherwise) arising in any way out of\n\tthe use of this software, even if advised of the possibility of such damage."
 },
 {
  "repo": "aws-samples/amazon-rekognition-video-analyzer",
  "language": "JavaScript",
  "readme_contents": "Create a Serverless Pipeline for Video Frame Analysis and Alerting\r\n========\r\n\r\n## Introduction\r\nImagine being able to capture live video streams, identify objects using deep learning, and then trigger actions or notifications based on the identified objects -- all with low latency and without a single server to manage.\r\n\r\nThis is exactly what this project is going to help you accomplish with AWS. You will be able to setup and run a live video capture, analysis, and alerting solution prototype.\r\n\r\nThe prototype was conceived to address a specific use case, which is alerting based on a live video feed from an IP security camera. At a high level, the solution works as follows. A camera surveils a particular area, streaming video over the network to a video capture client. The client samples video frames and sends them over to AWS, where they are analyzed and stored along with metadata. If certain objects are detected in the analyzed video frames, SMS alerts are sent out. Once a person receives an SMS alert, they will likely want to know what caused it. For that, sampled video frames can be monitored with low latency using a web-based user interface.\r\n\r\nHere's the prototype's conceptual architecture:\r\n\r\n![Architecture](doc/serverless_pipeline_arch_2.png)\r\n\r\nLet's go through the steps necessary to get this prototype up and running. If you are starting from scratch and are not familiar with Python, completing all steps can take a few hours.\r\n\r\n## Preparing your development environment\r\nHere\u2019s a high-level checklist of what you need to do to setup your development environment.\r\n\r\n1. Sign up for an AWS account if you haven't already and create an Administrator User. The steps are published [here](http://docs.aws.amazon.com/lambda/latest/dg/setting-up.html).\r\n\r\n2. Ensure that you have Python 2.7+ and Pip on your machine. Instructions for that varies based on your operating system and OS version.\r\n\r\n3. Create a Python [virtual environment](https://virtualenv.pypa.io/en/stable/) for the project with Virtualenv. This helps keep project\u2019s python dependencies neatly isolated from your Operating System\u2019s default python installation. **Once you\u2019ve created a virtual python environment, activate it before moving on with the following steps**.\r\n\r\n4. Use Pip to [install AWS CLI](http://docs.aws.amazon.com/cli/latest/userguide/installing.html). [Configure](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) the AWS CLI. It is recommended that the access keys you configure are associated with an IAM User who has full access to the following:\r\n - Amazon S3\r\n - Amazon DynamoDB\r\n - Amazon Kinesis\r\n - AWS Lambda\r\n - Amazon CloudWatch and CloudWatch Logs\r\n - AWS CloudFormation\r\n - Amazon Rekognition\r\n - Amazon SNS\r\n - Amazon API Gateway\r\n - Creating IAM Roles\r\n\r\n The IAM User can be the Administrator User you created in Step 1.\r\n\r\n5. Make sure you choose a region where all of the above services are available. Regions us-east-1 (N. Virginia), us-west-2 (Oregon), and eu-west-1 (Ireland) fulfill this criterion. Visit [this page](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/) to learn more about service availability in AWS regions.\r\n\r\n6. Use Pip to install [Open CV](https://github.com/opencv/opencv) 3 python dependencies and then compile, build, and install Open CV 3 (required by Video Cap clients). You can follow [this guide](http://www.pyimagesearch.com/2016/11/28/macos-install-opencv-3-and-python-2-7/) to get Open CV 3 up and running on OS X Sierra with Python 2.7. There's [another guide](http://www.pyimagesearch.com/2016/12/05/macos-install-opencv-3-and-python-3-5/) for Open CV 3 and Python 3.5 on OS X Sierra. Other guides exist as well for Windows and Raspberry Pi.\r\n\r\n6. Use Pip to install [Boto3](http://boto3.readthedocs.io/en/latest/). Boto is the Amazon Web Services (AWS) SDK for Python, which allows Python developers to write software that makes use of Amazon services like S3 and EC2. Boto provides an easy to use, object-oriented API as well as low-level direct access to AWS services.\r\n\r\n7. Use Pip to install [Pynt](https://github.com/rags/pynt). Pynt enables you to write project build scripts in Python.\r\n\r\n8. Clone this GitHub repository. Choose a directory path for your project that does not contain spaces (I'll refer to the full path to this directory as _\\<path-to-project-dir\\>_).\r\n\r\n9. Use Pip to install [pytz](http://pytz.sourceforge.net/). Pytz is needed for timezone calculations. Use the following commands:\r\n\r\n```bash\r\npip install pytz # Install pytz in your virtual python env\r\n\r\npip install pytz -t <path-to-project-dir>/lambda/imageprocessor/ # Install pytz to be packaged and deployed with the Image Processor lambda function\r\n```\r\n\r\nFinally, obtain an IP camera. If you don\u2019t have an IP camera, you can use your smartphone with an IP camera app. This is useful in case you want to test things out before investing in an IP camera. Also, you can simply use your laptop\u2019s built-in camera or a connected USB camera. If you use an IP camera, make sure your camera is connected to the same Local Area Network as the Video Capture client.\r\n\r\n## Configuring the project\r\n\r\nIn this section, I list every configuration file, parameters within it, and parameter default values. The build commands detailed later extract the majority of their parameters from these configuration files. Also, the prototype's two AWS Lambda functions - Image Processor and Frame Fetcher - extract parameters at runtime from `imageprocessor-params.json` and `framefetcher-params.json` respectively.\r\n\r\n>**NOTE: Do not remove any of the attributes already specified in these files.**\r\n\r\n\r\n\r\n> **NOTE: You must set the value of any parameter that has the tag NO-DEFAULT** \r\n\r\n### config/global-params.json\r\n\r\nSpecifies \u201cglobal\u201d build configuration parameters. It is read by multiple build scripts.\r\n\r\n```json\r\n{\r\n    \"StackName\" : \"video-analyzer-stack\"\r\n}\r\n```\r\nParameters:\r\n\r\n* `StackName` - The name of the stack to be created in your AWS account.\r\n\r\n### config/cfn-params.json\r\nSpecifies and overrides default values of AWS CloudFormation parameters defined in the template (located at aws-infra/aws-infra-cfn.yaml). This file is read by a number of build scripts, including ```createstack```, ```deploylambda```, and ```webui```.\r\n\r\n```json\r\n{\r\n    \"SourceS3BucketParameter\" : \"<NO-DEFAULT>\",\r\n    \"ImageProcessorSourceS3KeyParameter\" : \"src/lambda_imageprocessor.zip\",\r\n    \"FrameFetcherSourceS3KeyParameter\" : \"src/lambda_framefetcher.zip\",\r\n\r\n    \"FrameS3BucketNameParameter\" : \"<NO-DEFAULT>\",\r\n\r\n    \"FrameFetcherApiResourcePathPart\" : \"enrichedframe\",\r\n    \"ApiGatewayRestApiNameParameter\" : \"VidAnalyzerRestApi\",\r\n    \"ApiGatewayStageNameParameter\": \"development\",\r\n    \"ApiGatewayUsagePlanNameParameter\" : \"development-plan\"\r\n}\r\n```\r\nParameters:\r\n\r\n* `SourceS3BucketParameter` - The Amazon S3 bucket to which your AWS Lambda function packages (.zip files) will be deployed. If a bucket with such a name does not exist, the `deploylambda` build command will create it for you with appropriate permissions. AWS CloudFormation will access this bucket to retrieve the .zip files for Image Processor and Frame Fetcher AWS Lambda functions.\r\n\r\n* `ImageProcessorSourceS3KeyParameter` - The Amazon S3 key under which the Image Processor function .zip file will be stored.\r\n\r\n* `FrameFetcherSourceS3KeyParameter` - The Amazon S3 key under which the Frame Fetcher function .zip file will be stored.\r\n\r\n* `FrameS3BucketNameParameter` - The Amazon S3 bucket that will be used for storing video frame images. **There must not be an existing S3 bucket with the same name.**\r\n\r\n* `FrameFetcherApiResourcePathPart` - The name of the Frame Fetcher API resource path part in the API Gateway URL.\r\n\r\n* `ApiGatewayRestApiNameParameter` - The name of the API Gateway REST API to be created by AWS CloudFormation.\r\n\r\n* `ApiGatewayStageNameParameter` - The name of the API Gateway stage to be created by AWS CloudFormation.\r\n\r\n* `ApiGatewayUsagePlanNameParameter` - The name of the API Gateway usage plan to be created by AWS CloudFormation.\r\n\r\n\r\n### config/imageprocessor-params.json\r\nSpecifies configuration parameters to be used at run-time by the Image Processor lambda function. This file is packaged along with the Image Processor lambda function code in a single .zip file using the `packagelambda` build script.\r\n\r\n```json\r\n{\r\n\t\"s3_bucket\" : \"<NO-DEFAULT>\",\r\n\t\"s3_key_frames_root\" : \"frames/\",\r\n\r\n\t\"ddb_table\" : \"EnrichedFrame\",\r\n\r\n\t\"rekog_max_labels\" : 123,\r\n    \"rekog_min_conf\" : 50.0,\r\n\r\n\t\"label_watch_list\" : [\"Human\", \"Pet\", \"Bag\", \"Toy\"],\r\n\t\"label_watch_min_conf\" : 90.0,\r\n\t\"label_watch_phone_num\" : \"\",\r\n\t\"label_watch_sns_topic_arn\" : \"\",\r\n\t\"timezone\" : \"US/Eastern\"\r\n}\r\n```\r\n\r\n* `s3_bucket` - The Amazon S3 bucket in which Image Processor will store captured video frame images. The value specified here _must_ match the value specified for the `FrameS3BucketNameParameter` parameter in the `cfn-params.json` file.\r\n\r\n* `s3_key_frames_root` - The Amazon S3 key prefix that will be prepended to the keys of all stored video frame images.\r\n\r\n* `ddb_table` - The Amazon DynamoDB table in which Image Processor will store video frame metadata. The default value,`EnrichedFrame`, matches the default value of the AWS CloudFormation template parameter `DDBTableNameParameter` in the `aws-infra/aws-infra-cfn.yaml` template file.\r\n\r\n* `rekog_max_labels` - The maximum number of labels that Amazon Rekognition can return to Image Processor.\r\n\r\n* `rekog_min_conf` - The minimum confidence required for a label identified by Amazon Rekognition. Any labels with confidence below this value will not be returned to Image Processor.\r\n\r\n* `label_watch_list` - A list of labels for to watch out for. If any of the labels specified in this parameter are returned by Amazon Rekognition, an SMS alert will be sent via Amazon SNS. The label's confidence must exceed `label_watch_min_conf`.\r\n\r\n* `label_watch_min_conf` - The minimum confidence required for a label to trigger a Watch List alert.\r\n\r\n* `label_watch_phone_num` - The mobile phone number to which a Watch List SMS alert will be sent. Does not have a default value. **You must configure a valid phone number adhering to the E.164 format (e.g. +1404XXXYYYY) for the Watch List feature to become active.**\r\n\r\n* `label_watch_sns_topic_arn` - The SNS topic ARN to which you want Watch List alert messages to be sent. The alert message contains a notification text in addition to a JSON formatted list of Watch List labels found. This can be used to publish alerts to any SNS subscribers, such as Amazon SQS queues.\r\n\r\n* `timezone` - The timezone used to report time and date in SMS alerts. By default, it is \"US/Eastern\". See this list of [country codes, names, continents, capitals, and pytz timezones](https://gist.github.com/pamelafox/986163)).\r\n\r\n### config/framefetcher-params.json\r\nSpecifies configuration parameters to be used at run-time by the Frame Fetcher lambda function. This file is packaged along with the Frame Fetcher lambda function code in a single .zip file using the ```packagelambda``` build script.\r\n\r\n```json\r\n{\r\n    \"s3_pre_signed_url_expiry\" : 1800,\r\n\r\n    \"ddb_table\" : \"EnrichedFrame\",\r\n    \"ddb_gsi_name\" : \"processed_year_month-processed_timestamp-index\",\r\n\r\n    \"fetch_horizon_hrs\" : 24,\r\n    \"fetch_limit\" : 3\r\n}\r\n```\r\n\r\n* `s3_pre_signed_url_expiry` - Frame Fetcher returns video frame metadata. Along with the returned metadata, Frame Fetcher generates and returns a pre-signed URL for every video frame. Using a pre-signed URL, a client (such as the Web UI) can securely access the JPEG image associated with a particular frame. By default, the pre-signed URLs expire in 30 minutes.\r\n\r\n* `ddb_table` - The Amazon DynamoDB table from which Frame Fetcher will fetch video frame metadata. The default value,`EnrichedFrame`, matches the default value of the AWS CloudFormation template parameter `DDBTableNameParameter` in the `aws-infra/aws-infra-cfn.yaml` template file.\r\n\r\n* `ddb_gsi_name` - The name of the Amazon DynamoDB Global Secondary Index that Frame Fetcher will use to query frame metadata. The default value matches the default value of the AWS CloudFormation template parameter `DDBGlobalSecondaryIndexNameParameter` in the `aws-infra/aws-infra-cfn.yaml` template file.\r\n\r\n* `fetch_horizon_hrs` - Frame Fetcher will exclude any video frames that were ingested prior to the point in the past represented by (time now - `fetch_horizon_hrs`).\r\n\r\n* `fetch_limit` - The maximum number of video frame metadata items that Frame Fetcher will retrieve from Amazon DynamoDB.\r\n\r\n## Building the prototype\r\nCommon interactions with the project have been simplified for you. Using pynt, the following tasks are automated with simple commands: \r\n\r\n- Creating, deleting, and updating the AWS infrastructure stack with AWS CloudFormation\r\n- Packaging lambda code into .zip files and deploying them into an Amazon S3 bucket\r\n- Running the video capture client to stream from a built-in laptop webcam or a USB camera\r\n- Running the video capture client to stream from an IP camera (MJPEG stream)\r\n- Build a simple web user interface (Web UI)\r\n- Run a lightweight local HTTP server to serve Web UI for development and demo purposes\r\n\r\nFor a list of all available tasks, enter the following command in the root directory of this project:\r\n\r\n```bash\r\npynt -l\r\n```\r\n\r\nThe output represents the list of build commands available to you:\r\n\r\n![pynt -l output](doc/pynt%20dash%20l.png)\r\n\r\nBuild commands are implemented as python scripts in the file ```build.py```. The scripts use the AWS Python SDK (Boto) under the hood. They are documented in the following section.\r\n\r\n>Prior to using these build commands, you must configure the project. Configuration parameters are split across JSON-formatted files located under the config/ directory. Configuration parameters are described in detail in an earlier section.\r\n\r\n\r\n## Build commands\r\n\r\nThis section describes important build commands and how to use them. If you want to use these commands right away to build the prototype, you may skip to the section titled _\"Deploy and run the prototype\"_.\r\n\r\n### The `packagelambda` build command\r\n\r\nRun this command to package the prototype's AWS Lambda functions and their dependencies (Image Processor and Frame Fetcher) into separate .zip packages (one per function). The deployment packages are created under the `build/` directory.\r\n\r\n```bash\r\npynt packagelambda # Package both functions and their dependencies into zip files.\r\n\r\npynt packagelambda[framefetcher] # Package only Frame Fetcher.\r\n```\r\n\r\nCurrently, only Image Processor requires an external dependency, [pytz](http://pytz.sourceforge.net/). If you add features to Image Processor or Frame Fetcher that require external dependencies, you should install the dependencies using Pip by issuing the following command.\r\n\r\n```bash\r\npip install <module-name> -t <path-to-project-dir>/lambda/<lambda-function-dir>\r\n```\r\nFor example, let's say you want to perform image processing in the Image Processor Lambda function. You may decide on using the [Pillow](http://pillow.readthedocs.io/en/3.0.x/index.html) image processing library. To ensure Pillow is packaged with your Lambda function in one .zip file, issue the following command:\r\n\r\n```bash\r\npip install Pillow -t <path-to-project-dir>/lambda/imageprocessor #Install Pillow dependency\r\n```\r\n\r\nYou can find more details on installing AWS Lambda dependencies [here](http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html).\r\n\r\n### The `deploylambda` build command\r\n\r\nRun this command before you run `createstack`. The ```deploylambda``` command uploads Image Processor and Frame Fetcher .zip packages to Amazon S3 for pickup by AWS CloudFormation while creating the prototype's stack. This command will parse the deployment Amazon S3 bucket name and keys names from the cfn-params.json file. If the bucket does not exist, the script will create it. This bucket must be in the same AWS region as the AWS CloudFormation stack, or else the stack creation will fail. Without parameters, the command will deploy the .zip packages of both Image Processor and Frame Fetcher. You can specify either \u201cimageprocessor\u201d or \u201cframefetcher\u201d as a parameter between square brackets to deploy an individual function.\r\n\r\nHere are sample command invocations.\r\n\r\n```bash\r\npynt deploylambda # Deploy both functions to Amazon S3.\r\n\r\npynt deploylambda[framefetcher] # Deploy only Frame Fetcher to Amazon S3.\r\n```\r\n\r\n### The `createstack` build command\r\nThe createstack command creates the prototype's AWS CloudFormation stack behind the scenes by invoking the `create_stack()` API. The AWS CloudFormation template used is located at aws-infra/aws-infra-cfn.yaml under the project\u2019s root directory. The prototype's stack requires a number of parameters to be successfully created. The createstack script reads parameters from both global-params.json and cfn-params.json configuration files. The script then passes those parameters to the `create_stack()` call.\r\n\r\nNote that you must, first, package and deploy Image Processor and Frame Fetcher functions to Amazon S3 using the `packagelambda` and `deploylambda` commands (documented later in this guid) for the AWS CloudFormation stack creation to succeed.\r\n\r\nYou can issue the command as follows:\r\n\r\n```bash\r\npynt createstack\r\n```\r\n\r\nStack creation should take only a couple of minutes. At any time, you can check on the prototype's stack status either through the AWS CloudFormation console or by issuing the following command.\r\n\r\n```bash\r\npynt stackstatus\r\n```\r\n\r\nCongratulations! You\u2019ve just created the prototype's entire architecture in your AWS account.\r\n\r\n\r\n### The `deletestack` build command\r\n\r\nThe `deletestack` command, once issued, does a few things. \r\nFirst, it empties the Amazon S3 bucket used to store video frame images. Next, it calls the AWS CloudFormation delete_stack() API to delete the prototype's stack from your account. Finally, it removes any unneeded resources not deleted by the stack (for example, the prototype's API Gateway Usage Plan resource).\r\n\r\nYou can issue the `deletestack` command as follows.\r\n\r\n```bash\r\npynt deletestack\r\n```\r\n\r\nAs with `createstack`, you can monitor the progress of stack deletion using the `stackstatus` build command.\r\n\r\n### The `deletedata` build command\r\n\r\nThe `deletedata` command, once issued, empties the Amazon S3 bucket used to store video frame images. Next, it also deletes all items in the DynamoDB table used to store frame metadata.\r\n\r\nUse this command to clear all previously ingested video frames and associated metadata. The command will ask for confirmation [Y/N] before proceeding with deletion.\r\n\r\nYou can issue the `deletedata` command as follows.\r\n\r\n```bash\r\npynt deletedata\r\n```\r\n\r\n### The `stackstatus` build command\r\n\r\nThe `stackstatus` command will query AWS CloudFormation for the status of the prototype's stack. This command is most useful for quickly checking that the prototype is up and running (i.e. status is \"CREATE\\_COMPLETE\" or \"UPDATE\\_COMPLETE\") and ready to serve requests from the Web UI.\r\n\r\nYou can issue the command as follows.\r\n\r\n\r\n```bash\r\npynt stackstatus # Get the prototype's Stack Status\r\n```\r\n\r\n\r\n### The `webui` build command\r\n\r\nRun this command when the prototype's stack has been created (using `createstack`). The webui command \u201cbuilds\u201d the Web UI through which you can monitor incoming captured video frames. First, the script copies the webui/ directory verbatim into the project\u2019s build/ directory. Next, the script generates an apigw.js file which contains the API Gateway base URL and the API key to be used by Web UI for invoking the Fetch Frames function deployed in AWS Lambda. This file is created in the Web UI build directory.\r\n\r\nYou can issue the Web UI build command as follows.\r\n\r\n```bash\r\npynt webui\r\n```\r\n\r\n### The `webuiserver` build command\r\n\r\nThe webuiserver command starts a local, lightweight, Python-based HTTP server on your machine to serve Web UI from the build/web-ui/ directory. Use this command to serve the prototype's Web UI for development and demonstration purposes. You can specify the server\u2019s port as pynt task parameter, between square brackets.\r\n\r\nHere\u2019s sample invocation of the command.\r\n\r\n```bash\r\npynt webuiserver # Starts lightweight HTTP Server on port 8080.\r\n```\r\n\r\n### The `videocaptureip` and `videocapture` build commands\r\n\r\nThe videocaptureip command fires up the MJPEG-based video capture client (source code under the client/ directory). This command accepts, as parameters, an MJPEG stream URL and an optional frame capture rate. The capture rate is defined as 1 every X number of frames. Captured frames are packaged, serialized, and sent to the Kinesis Frame Stream. The video capture client for IP cameras uses Open CV 3 to do simple image processing operations on captured frame images \u2013 mainly image rotation.\r\n\r\nHere\u2019s a sample command invocation.\r\n\r\n```bash\r\npynt videocaptureip[\"http://192.168.0.2/video\",20] # Captures 1 frame every 20.\r\n```\r\n\r\nOn the other hand, the videocapture command (without the trailing 'ip'), fires up a video capture client that captures frames from a camera attached to the machine on which it runs. If you run this command on your laptop, for instance, the client will attempt to access its built-in video camera. This video capture client relies on Open CV 3 to capture video from physically connected cameras. Captured frames are packaged, serialized, and sent to the Kinesis Frame Stream.\r\n\r\nHere\u2019s a sample invocation.\r\n\r\n```bash\r\npynt videocapture[20] # Captures one frame every 20.\r\n```\r\n\r\n## Deploy and run the prototype\r\nIn this section, we are going use project's build commands to deploy and run the prototype in your AWS account. We\u2019ll use the commands to create the prototype's AWS CloudFormation stack, build and serve the Web UI, and run the Video Cap client.\r\n\r\n* Prepare your development environment, and ensure configuration parameters are set as you wish.\r\n\r\n* On your machine, in a command line terminal change into the root directory of the project. Activate your virtual Python environment. Then, enter the following commands:\r\n\r\n```bash\r\n$ pynt packagelambda #First, package code & configuration files into .zip files\r\n\r\n#Command output without errors\r\n\r\n$ pynt deploylambda #Second, deploy your lambda code to Amazon S3\r\n\r\n#Command output without errors\r\n\r\n$ pynt createstack #Now, create the prototype's CloudFormation stack\r\n\r\n#Command output without errors\r\n\r\n$ pynt webui #Build the Web UI\r\n\r\n#Command output without errors\r\n```\r\n\r\n* On your machine, in a separate command line terminal:\r\n\r\n```bash\r\n$ pynt webuiserver #Start the Web UI server on port 8080 by default\r\n```\r\n\r\n* In your browser, access http://localhost:8080 to access the prototype's Web UI. You should see a screen similar to this:\r\n\r\n![Empty Web UI](doc/webui-empty.png)\r\n\r\n* Now turn on your IP camera or launch the app on your smartphone. Ensure that your camera is accepting connections for streaming MJPEG video over HTTP, and identify the local URL for accessing that stream.\r\n\r\n* Then, in a terminal window at the root directory of the project, issue this command:\r\n\r\n```bash\r\n$ pynt videocaptureip[\"<your-ip-cam-mjpeg-url>\",<capture-rate>]\r\n```\r\n* Or, if you don\u2019t have an IP camera and would like to use a built-in camera:\r\n\r\n```bash\r\n$ pynt videocapture[<frame-capture-rate>]\r\n```\r\n\r\n* Few seconds after you execute this step, the dashed area in the Web UI will auto-populate with captured frames, side by side with labels recognized in them.\r\n\r\n## When you are done\r\nAfter you are done experimenting with the prototype, perform the following steps to avoid unwanted costs.\r\n\r\n* Terminate video capture client(s) (press Ctrl+C in command line terminal where you got it running)\r\n* Close all open Web UI browser windows or tabs.\r\n* Execute the ```pynt deletestack``` command (see docs above)\r\n* After you run ```deletestack```, visit the AWS CloudFormation console to double-check the stack is deleted.\r\n* Ensure that Amazon S3 buckets and objects within them are deleted.\r\n\r\nRemember, you can always setup the entire prototype again with a few simple commands.\r\n\r\n# License\r\nLicensed under the Amazon Software License.\r\n\r\nA copy of the License is located at\r\n\r\n[http://aws.amazon.com/asl/](http://aws.amazon.com/asl/)\r\n\r\n# The AWS CloudFormation Stack (optional read)\r\n\r\nLet\u2019s quickly go through the stack that AWS CloudFormation sets up in your account based on the template. AWS CloudFormation uses as much parallelism as possible while creating resources. As a result, some resources may be created in an order different than what I\u2019m going to describe here.\r\n\r\nFirst, AWS CloudFormation creates the IAM roles necessary to allow AWS services to interact with one another. This includes the following.\r\n\r\n* _ImageProcessorLambdaExecutionRole_ \u2013 a role to be assumed by the Image Processor lambda function. It allows full access to Amazon DynamoDB, Amazon S3, Amazon SNS, and AWS CloudWatch Logs. The role also allows read-only access to Amazon Kinesis and Amazon Rekognition. For simplicity, only managed AWS role permission policies are used.\r\n\r\n* _FrameFetcherLambdaExecutionRole_ \u2013 a role to be assumed by the Frame Fetcher lambda function. It allows full access to Amazon S3, Amazon DynamoDB, and AWS CloudWatch Logs. For simplicity, only managed AWS permission policies are used.\r\nIn parallel, AWS CloudFormation creates the Amazon S3 bucket to be used to store the captured video frame images. It also creates the Kinesis Frame Stream to receive captured video frame images from the Video Cap client.\r\n\r\nNext, the Image Processor lambda function is created in addition to an AWS Lambda Event Source Mapping to allow Amazon Kinesis to trigger Image Processor once new captured video frames are available. \r\n\r\nThe Frame Fetcher lambda function is also created. Frame Fetcher is a simple lambda function that responds to a GET request by returning the latest list of frames, in descending order by processing timestamp, up to a configurable number of hours, called the \u201cfetch horizon\u201d (check the framefetcher-params.json file for more run-time configuration parameters). Necessary AWS Lambda Permissions are also created to permit Amazon API Gateway to invoke the Frame Fetcher lambda function.\r\n\r\nAWS CloudFormation also creates the DynamoDB table where Enriched Frame metadata is stored by the Image Processor lambda function as described in the architecture overview section of this post. A Global Secondary Index (GSI) is also created; to be used by the Frame Fetcher lambda function in fetching Enriched Frame metadata in descending order by time of capture.\r\n\r\nFinally, AWS CloudFormation creates the Amazon API Gateway resources necessary to allow the Web UI to securely invoke the Frame Fetcher lambda function with a GET request to a public API Gateway URL.\r\n\r\nThe following API Gateway resources are created.\r\n\r\n* REST API named \u201cRtRekogRestAPI\u201d by default.\r\n\r\n* An API Gateway resource with a path part set to \u201cenrichedframe\u201d by default.\r\n\r\n* A GET API Gateway method associated with the \u201cenrichedframe\u201d resource. This method is configured with Lambda proxy integration with the Frame Fetcher lambda function (learn more about AWS API Gateway proxy integration here). The method is also configured such that an API key is required.\r\n\r\n* An OPTIONS API Gateway method associated with the \u201cenrichedframe\u201d resource. This method\u2019s purpose is to enable Cross-Origin Resource Sharing (CORS). Enabling CORS allows the Web UI to make Ajax requests to the Frame Fetcher API Gateway URL. Note that the Frame Fetcher lambda function must, itself, also return the Access-Control-Allow-Origin CORS header in its HTTP response.\r\n\r\n* A \u201cdevelopment\u201d API Gateway deployment to allow the invocation of the prototype's API over the Internet.\r\n\r\n* A \u201cdevelopment\u201d API Gateway stage for the API deployment along with an API Gateway usage plan named \u201cdevelopment-plan\u201d by default.\r\n\r\n* An API Gateway API key, name \u201cDevApiKey\u201d by default. The key is associated with the \u201cdevelopment\u201d stage and \u201cdevelopment-plan\u201d usage plan.\r\n\r\nAll defaults can be overridden in the cfn-params.json configuration file. That\u2019s it for the prototype's AWS CloudFormation stack! **This stack was designed primarily for development/demo purposes, especially how the Amazon API Gateway resources are set up.**\r\n\r\n# FAQ\r\n\r\n> **Q: Why is this project titled \"amazon-rekognition-video-analyzer\" despite the security-focused use case?** \r\n\r\n> **A:** Although this prototype was conceived to address the security monitoring and alerting use case, you can use the prototype's architecture and code as a starting point to address a wide variety of use cases involving low-latency analysis of live video frames with Amazon Rekognition. \r\n"
 },
 {
  "repo": "leadrien/opencv_native_androidstudio",
  "language": "Java",
  "readme_contents": "Native OpenCV with Android Studio\n=================================\n\nThis application is a sample Android Studio project with native OpenCV.\n\nIt gets the camera frames, make JNI calls with its gray matrices references as parameters, add some random noise to the images from a C++ method, and render the generated frames.\n\nIt works with Android Studio 3+\n\nLast included OpenCV version: 3.4\n\n\n\nUsage\n-----\n\nHere is how to use this project to run native OpenCV code.\n\n* Make sure you have Android SDK up to date, with NDK installed and CMake\n* Download latest OpenCV SDK for Android from OpenCV.org and decompress the zip file.\n* Clone this project\n* Create a symlink named `jniLibs` in `app/src/main` that points to `YOUR_OPENCV_SDK/sdk/native/libs`\n* In `app/CMakeLists.txt` change line 9 to points to `YOUR_OPENCV_SDK/sdk/native/jni/include`\n* Sync gradle\n* Run the application\n\n\nHow to create the native OpenCV project from scratch\n----------------------------------------------------\n\nHere is how I made this project. If you simply want to run openCV with NDK support, use the previous **Usage** chapter. If you want to build the full project from scratch, you can follow the steps in this chapter.\n\n* Make sure you have Android SDK up to date, with NDK installed\n* Download latest OpenCV SDK for Android from OpenCV.org and decompress the zip file.\n\n* Create a new Android Studio project\n  * Check Include C++ Support\n  * Choose empty Activity\n  * In C++ Support, you can check -fexceptions and -frtti\n\n* Import OpenCV library module\n  * New -> Import Module\n  * Choose the `YOUR_OPENCV_SDK/sdk/java` folder\n  * Unckeck replace jar, unckeck replace lib, unckeck create gradle-style\n\n* Set the OpenCV library module up to fit your SDK\n\n  Edit `openCVLibrary/build.gradle` to fit your SDK:\n\n  ```\n    compileSdkVersion 27\n    defaultConfig {\n        minSdkVersion 19\n        targetSdkVersion 27\n    }\n  ```\n\n* Add OpenCV module dependency in your app module\n\n  File -> Project structure -> Module app -> Dependencies tab -> New module dependency -> choose OpenCV library module\n\n\n* Make a symlink named `jniLibs` in `app/src/main` that points to `YOUR_OPENCV_SDK/sdk/native/libs`\n\n* Set the app build.gradle\n  * Add abiFilters\n    ```\n        externalNativeBuild {\n            cmake {\n                cppFlags \"-frtti -fexceptions\"\n                abiFilters 'x86', 'x86_64', 'armeabi-v7a', 'arm64-v8a'\n            }\n        }\n    ```\n\n  * Add openCV jniLibs directory to point to the symlink\n    ```\n    sourceSets {\n        main {\n            jniLibs.srcDirs = ['src/main/jniLibs']\n        }\n    }\n    ```\n  See [build.gradle](app/build.gradle)\n\n* Configure the `CMakeLists.txt` file\n  * After the `cmake_minimum_required`, add\n\n    ```\n    include_directories(YOUR_OPENCV_SDK/sdk/native/jni/include)\n    add_library( lib_opencv SHARED IMPORTED )\n    set_target_properties(lib_opencv PROPERTIES IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/src/main/jniLibs/${ANDROID_ABI}/libopencv_java3.so)\n    ```\n\n  * At the end of the file add `lib_opencv` to the `target_link_libraries` list\n\n\n* Grant camera permission\n  * Add the following lines in the `AndroidManifest.xml` file\n\n    ```\n    <uses-permission android:name=\"android.permission.CAMERA\"/>\n    <uses-feature android:name=\"android.hardware.camera\"/>\n    <uses-feature android:name=\"android.hardware.camera.autofocus\"/>\n    <uses-feature android:name=\"android.hardware.camera.front\"/>\n    <uses-feature android:name=\"android.hardware.camera.front.autofocus\"/>\n    ```\n\n* Add OpenCV code\n  * [MainActivity.java](app/src/main/java/ch/hepia/iti/opencvnativeandroidstudio/MainActivity.java)\n  * [activity_main.xml](app/src/main/res/layout/activity_main.xml)\n  * [native-lib.cpp](app/src/main/cpp/native-lib.cpp)\n\n"
 },
 {
  "repo": "php-opencv/php-opencv-examples",
  "language": "PHP",
  "readme_contents": "#### Examples\n- [detect face by cascade classifier](https://github.com/php-opencv/php-opencv-examples/blob/master/detect_face_by_cascade_classifier.php)\n\n![detect_face_by_cascade_classifier.jpg](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/detect_face_by_cascade_classifier.jpg)\n\n- [detect face by pretrained caffe model res10_300x300_ssd by ddn module](https://github.com/php-opencv/php-opencv-examples/blob/master/detect_face_by_dnn_ssd.php)\n\n![detect_face_by_dnn_ssd.jpg](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/detect_face_by_dnn_ssd.jpg)\n\n- [detect facemarks by LBF algorithm](https://github.com/php-opencv/php-opencv-examples/blob/master/detect_facemarks_by_lbf.php)\n\n![detect_facemarks_by_lbf.jpg](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/detect_facemarks_by_lbf.jpg)\n\n- [recognize face by LBPH algorithm](https://github.com/php-opencv/php-opencv-examples/blob/master/recognize_face_by_lbph.php)\n\n![recognize_face_by_lbph.jpg](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/recognize_face_by_lbph.jpg)\n\n- [upscale image x2 by waifu2x](https://github.com/php-opencv/php-opencv-examples/blob/master/upscale_image_x2_by_dnn_waifu2x.php)\n\n![upscale_image_x2_by_dnn_waifu2x.png](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/images/icon_64x64.png) ![icon_64x64.png](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/upscale_image_x2_by_dnn_waifu2x.png)\n\n- [classify image by pretrained caffe model mobilenet](https://github.com/php-opencv/php-opencv-examples/blob/master/classify_image_by_dnn_mobilenet.php)\n\n![cat.jpg](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/images/cat.jpg)\nResults: 87%: Egyptian cat, 4%: tabby, tabby cat, 2%: tiger cat\n\n- [detect objects by pretrained tensorflow model mobilenet](https://github.com/php-opencv/php-opencv-examples/blob/master/detect_objects_by_dnn_mobilenet.php)\n\n![detect_objects_by_dnn_mobilenet.png](https://raw.githubusercontent.com/php-opencv/php-opencv-examples/master/results/detect_objects_by_dnn_mobilenet.png)\n\n#### Helper for autocomplete and highlighting in your IDE\n- [phpdoc file](https://github.com/php-opencv/php-opencv-examples/blob/master/phpdoc.php)\n- [php-opencv-ide-helper](https://github.com/hihozhou/php-opencv-ide-helper)\n\n#### Installation\n- [opencv](https://github.com/php-opencv/php-opencv/wiki/Installation) on ubuntu/centos\n- [opencv-examples](https://github.com/php-opencv/php-opencv-examples/wiki/installation-via-docker) via docker\n\n#### Requirements\n- PHP 7.0 / 7.1 / 7.2\n- OpenCV 4.0.0+\n- [php-opencv](https://github.com/php-opencv/php-opencv)\n\n#### Thanks\n- author of examples and contributor of library -> https://github.com/morozovsk\n- send your PR for library there -> https://github.com/php-opencv/php-opencv\n- author of library -> https://github.com/hihozhou\n"
 },
 {
  "repo": "bytedeco/javacv-examples",
  "language": "Scala",
  "readme_contents": "JavaCV-Examples\n===============\n\nThis project contains examples of using [JavaCV](https://github.com/bytedeco/javacv) \nand other library wrappers from [javacpp-presets](https://github.com/bytedeco/javacpp-presets) project.\n\n* [OpenCV_Cookbook](OpenCV_Cookbook) - JavaCV versions of the examples presented in the Robert Lagani\u00e8re's book\n\"OpenCV Computer Vision Application Programming Cookbook\".\nThe original examples in the Cookbook are in C++, here they are translated to use JavaCV API.\n* [flandmark-demo](flandmark-demo) - example of using [flandmark](https://github.com/uricamic/flandmark) library.\n* [FlyCapture2-demo](FlyCapture2-demo) - examples of using \n[JVM wrapper](https://github.com/bytedeco/javacpp-presets/tree/master/flycapture) for \nFlir/Point Grey [FlyCapture SDK](https://www.ptgrey.com/flycapture-sdk).\n* [Spinnaker-demo](Spinnaker-demo) - examples of using \n[JVM wrapper](https://github.com/bytedeco/javacpp-presets/tree/master/spinnaker) for \nFlir/Point Grey [Spinnaker SDK](https://www.ptgrey.com/spinnaker-sdk).\n\n\n\n"
 },
 {
  "repo": "bsdnoobz/opencv-code",
  "language": "C++",
  "readme_contents": "OpenCV Code\n===========\n\n---\n\nC++ and Python source code extracted from the tutorials at http://opencv-code.com.\n\n"
 },
 {
  "repo": "moneyDboat/wechat_jump_jump",
  "language": "Python",
  "readme_contents": "## \u524d\u8a00\n\u6700\u8fd1\u5fae\u4fe1\u5c0f\u6e38\u620f\u8df3\u4e00\u8df3\u5927\u70ed\uff0c\u81ea\u5df1\u4e5f\u662f\u4e2d\u6bd2\u9887\u4e45\uff0c\u65e0\u5948\u624b\u6b8b\u6700\u9ad8\u5206\u53ea\u62ff\u5230200\u5206\u3002\u65e0\u610f\u95f4\u770b\u5230[\u6559\u4f60\u7528Python\u6765\u73a9\u5fae\u4fe1\u8df3\u4e00\u8df3](https://zhuanlan.zhihu.com/p/32452473)\u4e00\u6587\uff0c\u5728\u7535\u8111\u4e0a\u5229\u7528adb\u9a71\u52a8\u5de5\u5177\u64cd\u4f5c\u624b\u673a\uff0c\u8be6\u7ec6\u7684\u4ecb\u7ecd\u4ee5\u53ca\u5982\u4f55\u5b89\u88c5adb\u9a71\u52a8\u53ef\u4ee5\u53bb\u770b\u8fd9\u7bc7\u6587\u7ae0\uff0c\u8fd9\u91cc\u5c31\u4e0d\u518d\u4ecb\u7ecd\u4e86\u3002\u4f46\u662f\u539f\u6587\u6bcf\u6b21\u8df3\u8dc3\u9700\u8981\u624b\u52a8\u70b9\u51fb\uff0c\u4e8e\u662f\u60f3\u5c1d\u8bd5\u5229\u7528\u56fe\u50cf\u5904\u7406\u7684\u65b9\u6cd5\u81ea\u52a8\u5316\u3002  \n\u6700\u91cd\u8981\u7684\u4e0d\u662f\u6700\u7ec8\u5237\u7684\u5206\u6570\uff0c\u800c\u662f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u8fc7\u7a0b\u3002\u82b1\u4e86\u4e00\u4e2a\u4e0b\u5348\u5c1d\u8bd5\u5404\u79cd\u65b9\u6cd5\uff0c\u6700\u7ec8\u91c7\u7528opencv\u7684\u6a21\u677f\u5339\u914d+\u8fb9\u7f18\u68c0\u6d4b\uff0c\u65b9\u6cd5\u5f88\u7b80\u5355\u4f46\u6548\u679c\u5f88\u597d\u3002  \n\u672c\u6587\u4e3b\u8981\u5206\u4eab\u5982\u4f55\u7528Opencv\u5bf9\u6e38\u620f\u622a\u56fe\u8fdb\u884c\u68c0\u6d4b\uff0c\u81ea\u52a8\u627e\u5230\u5c0f\u4eba\u548c\u8df3\u8dc3\u76ee\u6807\u70b9\u7684\u4f4d\u7f6e\uff0c\u8ba1\u7b97\u8df3\u8dc3\u8ddd\u79bb\uff0c\u4ece\u800c\u8ba9\u7535\u8111\u5e2e\u4f60\u73a9\u8df3\u4e00\u8df3\u6e38\u620f\uff01\n\u672c\u6587\u7684\u4ee3\u7801\u89c1https://github.com/moneyDboat/wechat_jump_jump\uff0c\u6b22\u8fcefork\u548cstar\uff5e\n\n## \u4e3b\u8981\u4f7f\u7528\u7684Python\u5e93\u53ca\u5bf9\u5e94\u7248\u672c\uff1a\npython 3.6  \nopencv-python 3.3.0  \nnumpy 1.13.3  \n\n## Opencv  \n\u9996\u5148\u4ecb\u7ecd\u4e0bopencv\uff0c\u662f\u4e00\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u672c\u6587\u5c06\u7528\u5230opencv\u91cc\u7684\u6a21\u677f\u5339\u914d\u548c\u8fb9\u7f18\u68c0\u6d4b\u529f\u80fd\u3002  \n\n### \u6a21\u677f\u5339\u914d\n\u6a21\u677f\u5339\u914d\u662f\u5728\u4e00\u5e45\u56fe\u50cf\u4e2d\u5bfb\u627e\u4e00\u4e2a\u7279\u5b9a\u76ee\u6807\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u539f\u7406\u975e\u5e38\u7b80\u5355\uff0c\u904d\u5386\u56fe\u50cf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684\u4f4d\u7f6e\uff0c\u6bd4\u8f83\u5404\u5904\u4e0e\u6a21\u677f\u662f\u5426\u201c\u76f8\u4f3c\u201d\uff0c\u5f53\u76f8\u4f3c\u5ea6\u8db3\u591f\u9ad8\u65f6\uff0c\u5c31\u8ba4\u4e3a\u627e\u5230\u4e86\u6211\u4eec\u7684\u76ee\u6807\u3002  \n\u4f8b\u5982\u63d0\u4f9b\u5c0f\u4eba\u7684\u6a21\u677f\u56fe\u7247\n![\u8fd9\u91cc\u5199\u56fe\u7247\u63cf\u8ff0](http://img.blog.csdn.net/20171231132928712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW9uZXlkYm9hdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n```\nimport cv2\nimport numpy as np\n\n# imread()\u51fd\u6570\u8bfb\u53d6\u76ee\u6807\u56fe\u7247\u548c\u6a21\u677f\nimg_rgb = cv2.imread(\"0.png\", 0)\ntemplate = cv2.imread('temp1.jpg', 0) \n\n# matchTemplate \u51fd\u6570\uff1a\u5728\u6a21\u677f\u548c\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u5bfb\u627e\u5339\u914d,\u83b7\u5f97\u5339\u914d\u7ed3\u679c\u56fe\u50cf \n# minMaxLoc \u51fd\u6570\uff1a\u5728\u7ed9\u5b9a\u7684\u77e9\u9635\u4e2d\u5bfb\u627e\u6700\u5927\u548c\u6700\u5c0f\u503c\uff0c\u5e76\u7ed9\u51fa\u5b83\u4eec\u7684\u4f4d\u7f6e\nres = cv2.matchTemplate(img_rgb,template,cv2.TM_CCOEFF_NORMED)\nmin_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)\n```\n\u4f7f\u7528OpenCV\u7684matchTemplate\u51fd\u6570\uff0c\u5c31\u80fd\u627e\u5230\u4e2d\u5c0f\u4eba\u7684\u4f4d\u7f6e\u3002\u5c0f\u4eba\u7684\u68c0\u6d4b\u6548\u679c\u975e\u5e38\u597d\uff0c\u6bcf\u6b21\u90fd\u80fd\u8bc6\u522b\u5f97\u5f88\u7cbe\u786e\u3002\n![\u8fd9\u91cc\u5199\u56fe\u7247\u63cf\u8ff0](http://img.blog.csdn.net/20171231133114181?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW9uZXlkYm9hdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\u89c2\u5bdf\u5230\u5c0f\u4eba\u8df3\u5230\u7269\u5757\u4e2d\u5fc3\u4e4b\u540e\uff0c\u4e0b\u4e00\u4e2a\u7269\u5757\u4e2d\u5fc3\u5c31\u4f1a\u51fa\u73b0\u767d\u8272\u5c0f\u5706\u70b9\uff0c\u540c\u6837\u53ef\u4ee5\u5339\u914d\u56fe\u4e2d\u767d\u8272\u5c0f\u5706\u70b9\uff0c\u4ece\u800c\u83b7\u5f97\u8df3\u8dc3\u76ee\u6807\u70b9\u7684\u5750\u6807\uff0c\u8ba1\u7b97\u8df3\u8dc3\u7684\u8ddd\u79bb\u3002\n![\u8fd9\u91cc\u5199\u56fe\u7247\u63cf\u8ff0](http://img.blog.csdn.net/20171231133244302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW9uZXlkYm9hdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\u4f46\u662f\u53ea\u5339\u914d\u5c0f\u5706\u70b9\u83b7\u5f97\u8df3\u8dc3\u76ee\u6807\u4f4d\u7f6e\u4f1a\u51fa\u73b0\u95ee\u9898\uff0c\u56e0\u4e3a\u6709\u4e9b\u7269\u5757\u672c\u8eab\u5c31\u662f\u767d\u8272\u7684\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5931\u8d25\uff0c\u6240\u4ee5\u6211\u4eec\u5728\u68c0\u6d4b\u5931\u8d25\uff08\u6a21\u677f\u5339\u914d\u7684\u76f8\u4f3c\u5ea6\u5f88\u4f4e\uff09\u7684\u60c5\u51b5\u4e0b\u91c7\u7528\u8fb9\u7f18\u68c0\u6d4b\u3002\n\n### \u8fb9\u7f18\u68c0\u6d4b\n\u8fb9\u7f18\u68c0\u6d4b\u987e\u540d\u601d\u4e49\u5c31\u662f\u68c0\u6d4b\u56fe\u7247\u4e2d\u7684\u8fb9\u7f18\uff0c\u4f7f\u7528opencv\u4e2d\u7684cv2.Canny\u51fd\u6570\u3002\n\u8df3\u4e00\u8df3\u7684\u753b\u9762\u5f88\u7b80\u6d01\uff0c\u6240\u4ee5\u8fb9\u7f18\u68c0\u6d4b\u7684\u6548\u679c\u5f88\u597d\u3002\u68c0\u6d4b\u51fa\u8fb9\u7f18\u540e\uff0c\u4ece\u4e0a\u81f3\u4e0b\u626b\u63cf\u56fe\u7247\u5c31\u80fd\u627e\u5230\u4e0b\u4e00\u4e2a\u7269\u5757\u7684\u5927\u81f4\u4f4d\u7f6e\u3002\n```\nimg = cv2.imread('1.png', 0)\n\n# \u5148\u505a\u9ad8\u65af\u6a21\u7cca\u80fd\u591f\u63d0\u9ad8\u8fb9\u7f18\u68c0\u6d4b\u7684\u6548\u679c\nimg = cv2.GaussianBlur(img,(5,5),0)  \ncanny = cv2.Canny(img, 1, 10) \n```\n![\u8fd9\u91cc\u5199\u56fe\u7247\u63cf\u8ff0](http://img.blog.csdn.net/20171231133343730?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW9uZXlkYm9hdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n# \u603b\u7ed3\n\u4ee5\u4e0a\u5c31\u662f\u7528OpenCV\u8ba9\u7535\u8111\u5e2e\u4f60\u73a9\u8df3\u4e00\u8df3\u7684\u6574\u4f53\u601d\u8def\uff0c\u8fd8\u6709\u5f88\u591a\u7ec6\u8282\u4e4b\u540e\u518d\u8865\u5145\uff0c\u5177\u4f53\u7684\u6d41\u7a0b\u89c1https://github.com/moneyDboat/wechat_jump_jump\u4e2d\u7684play.py\u6587\u4ef6\uff0c\u6211\u5df2\u7ecf\u5c3d\u529b\u5c06\u4ee3\u7801\u6ce8\u91ca\u5199\u5f97\u8be6\u5c3d\u3002  \n\u7535\u8111\u4e0a\u5b89\u88c5\u597dadb\u9a71\u52a8\u548c\u76f8\u5173\u7684Python\u5e93\uff0c\u624b\u673a\u901a\u8fc7\u6570\u636e\u7ebf\u8fde\u63a5\u7535\u8111\uff0c\u8fd0\u884cplay.py\uff0c\u63a5\u4e0b\u6765\u4f60\u5c31\u53ef\u4ee5\u5237\u5237\u5267\u5403\u5403\u96f6\u98df\uff0c\u7136\u540e\u8ba9\u7535\u8111\u5e2e\u4f60\u5237\u5206\u5566\uff5e  \n\u8fd9\u662f\u6211\u81ea\u5df1\u7684\u7ed3\u679c\u622a\u56fe\uff0c\u81ea\u52a8\u5237\u52301000\u5206\u4ee5\u4e0a\u662f\u6ca1\u6709\u95ee\u9898\u7684\u3002  \n![\u8fd9\u91cc\u5199\u56fe\u7247\u63cf\u8ff0](http://img.blog.csdn.net/20171231133441199?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbW9uZXlkYm9hdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\u8fd8\u6709\u5f88\u591a\u4e0d\u5b8c\u5584\u7684\u5730\u65b9\uff0c\u4f8b\u5982\u5c4f\u5e55\u5206\u8fa8\u7387\u9002\u914d\u7b49\uff0c\u5982\u679c\u6709\u4ec0\u4e48\u66f4\u597d\u7684\u60f3\u6cd5\u548c\u5efa\u8bae\uff0c\u6b22\u8fce\u8bc4\u8bba\u5171\u540c\u63a2\u8ba8\uff5e\uff5e\n"
 },
 {
  "repo": "aptogo/OpenCVForiPhone",
  "language": "C++",
  "readme_contents": "Computer vision with iOS: Building an OpenCV framework\n------------------------------------------------------\n\nThis is the source code that accompanies our [article][1] on how to build an \nOpenCV framework for iOS devices. The framework can be added to your projects by \ndragging and dropping and supports video capture using the OpenCV highgui module. \n\n[1]: http://aptogo.co.uk/2011/09/opencv-framework-for-ios/\n\n13/02/12 - Updated for iOS 5, armv6 support dropped. \n           OpenCV.framework updated to trunk r7286.   Aziz Baibabaev (Abai)"
 },
 {
  "repo": "murtazahassan/Learn-OpenCV-in-3-hours",
  "language": "Python",
  "readme_contents": "# LEARN OPENCV IN 3 HOURS USING PYTHON - INCLUDING EXAMPLE PROJECTS\n[![Watch Video](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Resources/Thumbnail.jpg)](https://youtu.be/WQeoO7MI0Bs)\n\n\n# What will you learn ?\n</br>\n\n|Virtual Painter|Document Scanner|Number Plate Detector|\n|:----:|:----:|:----:|\n|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project1.gif\" width=\"300\" height=\"200\" />|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project2.gif\" width=\"300\" height=\"200\" />|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project3.gif\" width=\"300\" height=\"200\" /> </br>|\n|Contours Detection|Color Detection|Face Detection|\n|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Marker.gif\" width=\"300\" height=\"200\" />|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter7.gif\" width=\"300\" height=\"200\" />|<img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter9.gif\" width=\"300\" height=\"200\" /> </br>|\n\n\n\n# Prerequisites\n</br>\n\n|Index|Topic|Image|Video|Description|\n|:----:|:----:|:----:|:----:|:----:|\n|1|Introduction to Images| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Introduction.gif\" width=\"300\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=137)     | Basic concept of Images. The RGB Channels and Gray Scale Images.  </br> |\n|2|Installations| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Installations.gif\" width=\"300\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=277)     | Installation process of Pyhton, OpenCV and the Pycharm IDE  </br> |\n\n\n\n\n# Chapters\n\n|Chapter|Topic|Image|Video|Description|\n|:----:|:----:|:----:|:----:|:----:|\n|1| [How to Read Image Video-Webcam](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter1.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter1.gif\" width=\"360\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=549) | Learn how to read images videos and webcam.  </br> |\n|2| [5 Must Know OpenCV Basic Functions](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter2.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter2.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=1021) |     5 Must know opencv functions for beginners. Gray Scale, Blur, Edge Detection, Dialation and Erosion.  </br> |\n|3| [How to Crop and Resize Images](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter3.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter3.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=1651) |   How to crop and resize and iamge. Resize could be used to scale up or scale down an image where cropping can be used to get a part of the image. </br> |\n|4| [How to Draw Shapes and Text](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter4.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter4.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=2052) |      Learn to create blank images along with how to draw Lines, rectangles, circles and custom text.   </br> |\n|5| [Warp Prespective/BirdView](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter5.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter5.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=2699) |      Learn how to creat a warp prespective of a selected area of an image using fixed points.   </br> |\n|6| [Joining Multiple Images to Display](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter6.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter6.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=3004) |      Join multiple images together as one image for easy visualization of the workflow. Learn how to do it for smaller noumber of images and how it could be scaled up to have several images in the same image.   </br> |\n|7| [Color Detection](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter7.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter7.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=3374)       |How to detect any color in an image using the HSV space with the help of opencv Trackbars.  </br> |\n|8| [Contour/Shape Detection](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter8.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter8.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=4537)       |How to detect shapes of objects by finding their contours. Contours are basically outline that bound the shape or form of an object. So we will be detecting multiple shapes and how many corners points each shape has along with its area .   </br> |\n|9| [Face Detection](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/chapter9.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/Chapter9.gif\" width=\"300\" height=\"150\" /> |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=6031)       |How to detect faces in realtime using Viola Jones method.   </br> |\n|___|___________________|______________________________|__________| ____________________________\n\n\n# Projects\n</br>\n\n\n|Project|Topic|Image|Video|Description|\n|:----:|:----:|:----:|:----:|:----:|\n|1| [Virtual Paint](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/project1.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project1.gif\" width=\"450\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=6363)     | Detecting Color and using colored marker to draw virtually. </br> |\n|2| [Documnet Scanner](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/project2.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project2.gif\" width=\"450\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=8145)     |  In this project we are going to create a simple document scanner using opencv. We will learn how to run this in real time</br> |\n|3|[Number Plate Detector](https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/project3.py)| <img src=\"https://github.com/murtazahassan/Learn-OpenCV-in-3-hours/blob/master/Tumbnails/project3.gif\" width=\"450\" height=\"150\" />  |[Watch Now](https://youtu.be/WQeoO7MI0Bs?t=10594)     | In this project we will detect number plates on cars in realtime and save them with a click of a button. </br> |\n\n\n# Notes\n</br>\n\n|Index|Comment|\n|:---|:---|\n|1.|[Recommended IDE: PyCharm Community edition](https://www.jetbrains.com/pycharm/download/)|\n|2.|[Python: 3.7.6](https://www.python.org/downloads/release/python-376/)|\n"
 },
 {
  "repo": "its-pointless/gcc_termux",
  "language": "C",
  "readme_contents": "\nFrom now on im using apt for updates\nadd \ndeb https://its-pointless.github.io/files/24  termux extras\n(use https://its-pointless.github.io/files/21 if you are still using android 5 or 6)\nfile with .list suffix in $PREFIX/etc/apt/sources.list.d/\ngpg key is https://its-pointless.github.io/pointless.gpg\nif not installed install gnupg\napt-get install gnupg\napt-key add pointless.gpg will add the key to apt\napt-get update\n\nor use https://its-pointless.github.io/setup-pointless-repo.sh\n\nuse the commands setupclang setupclang and setupgcc-7 to switch compilers\nnothing complex just moving symlinks around\n\n03/04/2019\nr-cran-tidyverse should work consistently except for those using android-5 due to linking issues\naround librcon.so. Above android-5 the library can see all symbols linked in android-5 that the linker\nwon't let the library see symbols in libR.so since its not directly linked. I am not sure right now \nthe correct way to deal with it. But changing $PREFIX/lib/R/etc/Makeconf to add -LR to ldflags and \nreinstalling readr package will work for now.\n\n15/01/2019\nns_parse.h for a few missing reolver libs stuff missing  in android 5 and 6 so msmtp works. Havent tested \nas its a 20 minute cut and paste job from musl.\nalso opencv python3 doesn;t work as is but is easy enough to fix \nhttps://github.com/its-pointless/its-pointless.github.io/issues/18 \nwill do proper fixes at some point this week.\n\n17/12/2018\nadded gnat for x86_64 and aarch64, gprbuild and xmlada packages available as well\n\n27/09/2018\n\nAdded portaudio and elfutils \nFor port audio to work without issue it requires pulseaudio. It also needs pulseaudio\nsample rate set to PROPERTY_OUTPUT_SAMPLE_RATE or latency is too high\nthis value can be obtained via termux-audio-info\nAny issues with this please mention...\n\n14/08/2018\n\njulia updated to 1.0.0\ninstalling packages that link to libs is a bit tricky and requires\nwiriting deps.jl yourself so they point to installed termux libs.\nEven if you can compile the libs using julia, without setting \nLD_LIBRARY_PATH to where julia installed the libs it won't use them\ncausing error. \n\n\n30/07/2018\nfixed a few annoyances with gcc-8 \nOz now becomes Os automatically \nand few warnings that llvm-config add won't cause errors since they are \nonly implemented in clang \n\n30/03/2018\n\njulia is compiling on all archs but must be done on device. see my android-termux branch \nof julia. Also added an arm build for testing...\n\n\n\n20/03/2018\n\nTESTING openblas for x86_64 atom....\n\n\n7/02/2018\n\nadded povray because ... i don't know.\n\n\n10/01/2018\n\nupdated rustc and rustc-nightlies\nfixed issues on x86_64 gcc\nadded racket for arm and i686\n\n\n5/01/2018\nchanged gcc-7 sources to linaro ... because why not? going to add source so \neveryone can compile and won't rely on me providing binaries... at all. \n\nadded gnatmake for arm ( other archs are not likely to come since it requires\na sigtramp implentation that is absent on android)\n\nadded racket ... likely buggy for arm and i686. not working yet on 64 bit archs\n\nalso adding libboostpython libs which is a pull request for termux/termux-packages \n\n23/12/2017\nupdated ecl arm and julia for x86_64. both should now work again\n\n20/12/2017\n\nupdated cargo for all but i686\nupdated rustc-nightly for and added for i686. not on x86_64...\nif i have time i will work on it. Ecl should now compile maxima\ntechnically there aren't any \"major\" dependencies missing for sagemath \nwould have to built on device though...\n\n\n13/12/2017\nadded ecl rustc nightly for arm and aarch64. \nadded the cross_config stuff for ecl as well.\n\n28/11/2017\nadded libgomp for gcc-7 in 7.2.0 and ndk 4.9.x flavours\nThanks to @arietal libcairo now works in R. \nalso added julia lang for i686 x86_64. \nI have no idea how long or if ever arm and aarch64 will take. \n\n19/11/2017\nr-cran package 3.4.2-1 uploading, should be working again... enabled openmp because i can.\npforth 32 bit fix upload for testing.\nshould get  updated scipy and numpy at some point hopefully soon\n\n11/11/2017 \nrustc now working for x86_64 and i686. The bug with cargo on arm is also fixed.\nAs libgit2 needed to be compiled with -no-integrated-as the fix is statically linked.\n\n\n\n08/10/2017\nadded python 2 versions numpy and scipy\n\n\n01/10/2017\nguile 2.0 and 2.2 added\ni686 version for 2.0 has threads disabled due to crashing half the time.\n2.2 has no such issue as far as I can tell but the build process for 2.2 builds\nneeds some work before a pull request to main repo.\n\n\n12/09/2017\nrustc is broken if you upgraded libllvm to 5.0 \nto rememdy that install newly added package libllvm-4so should be working from there\n\n\n\n08/09/2017\n\nadded fixedshe package and command executing it will start a new shell with\nLD_PRELOAD=$PREFIX/lib/libandroid-fixshebang.so $shell\nthis will cause commands /bin/sh and /usr/bin/env to be redirected making shebangs with those \ncommands work. to change default shell to do this the command fixedshe chsh will work as chsh does.\n\nDoing this will enable  R and Octave library installs to work correctly first time much more often\nthings like fixedshe octave also work\n\n\n27/08/2017\n\nadded openldap f0r testing need feedback \n\nupdated gcc to 7.2 \n\nadded more stuff to x86_64 ans i686\n\n\n\n08/08/2017\nadding i686 and x86_64  stuff \ncargo and rusttc \noctave and R\netc\nalso some experimental nodejs 8.2.1 builds ...\n\n\n\n29/07/2017\n\nto get cargo for arm working add this to ~/.gitconfig \n\n[url \"git://github.com/rust-lang/crates.io-index\"]\n        insteadOf = https://github.com/rust-lang/crates.io-index\n\n\n25/07/2017\noctave is fixed and back up on repo as well as gmic arm.\n\n24/07/2017\noctave isn't working right now ... \n\n23/07/2017\na working gmic binoary for arm required some magic getting it...\n\n\n22/07/2017\n\ngmic is not working for arm and it appears to be stubborn about it.\nsorry about that. \n\n\n17/07/2017\n\nGot almost everything working with the transision to libc++_shared. Updating the apt\nrepo shortyly there are a few packages there that are not not mentioned here... \n\n\n\n\n\n\n\n27/05/2017\nadded rustc and cargo for arm and aarch64 from @vishalbiswas\na bit buggy for now \n\n\n24/05/2017\n\nAdded quantlib. Will be adding R stuff shortly. Mpd-ext is also in repo extended for modpluglib and\nlibgme. So game emulation music plays. Haven't tested that yet should work.  \n\n\n\njust added imgflo ... i haven't tested it if someoen wants to inform me how its screwed \ni would be thankful.\nupdated now with termux-packages pull requests as well\n\nelectrum egg added ...\nupdates\nadded boost ledger r-cran \nthe mpv-ext is just mpv with caca enabled and alpine has been updated.\n\n\nOctave seems to be working fairly well...\nreport bugs???\nIts less annoying to compile than R so i might fix things that are really broken. \nThe scripts to compile fortran stuff needs a bit of work to be more user friendly \nand less of a hack. \n\ntermux gcc resurrected purely because scipy needs it. \nDon't annoy fornwall if you find a bug since its \nentirely unsupported. \nThis is a compiler of last resort hecause you \nneed to compile fortran or something needs gcc and\nonly gcc.\n\n\nFor scipy to work you need blaslib(openblas, reference blas +lapack or atlas) libgfortran\nnumpy and scipy.\n\nIf you want to do this yourself its not hard.\nhttps://github.com/xianyi/OpenBLAS/wiki/How-to-build-OpenBLAS-for-Android\nI compiled arm and arm64 tool chain for linux x86_64 (the .tar.bz2 file)\nso you can extract over the android ndk tool chain for ndk-13 and when termux creates tool chain it should\npull stuff in. \nhttps://github.com/xianyi/OpenBLAS/wiki/How-to-build-OpenBLAS-for-Android\n\nThe default makefile for openblas uses hardfp and termux uses softfp so it won't work\non termux. Don't use it on arm use either lapack or atlas. Atlas will likely be faster\nthan lapack. \n\nLDFLAGS when compiling scipy and numpy for distribution is important. You can compile on android 6.0 and it might not \nwork on 5.1 because the linker is different. \n"
 },
 {
  "repo": "ithewei/hplayer",
  "language": "C++",
  "readme_contents": "# \u591a\u753b\u9762\u64ad\u653e\u5668\n\n## \u9700\u6c42\u5206\u6790\n\n- \u505a\u4e00\u4e2a\u7c7b\u4f3cVLC\u7684\u64ad\u653e\u5668\uff0c\u80fd\u64ad\u653e\u6587\u4ef6\u6e90\u3001\u7f51\u7edc\u6e90\u3001\u8bbe\u5907\u6355\u83b7\u6e90\uff1b\n- \u754c\u9762\u8981\u6c42\u591a\u753b\u9762\u76d1\u63a7\u7f51\u683c\uff0c\u53ef\u81ea\u7531\u5207\u6362\u591a\u753b\u9762\u98ce\u683c\uff0c\u652f\u6301\u62d6\u62fd\u4e0e\u5408\u5e76\uff1b\n\n## \u6982\u8981\u8bbe\u8ba1\n\n- \u4f7f\u7528Qt\u5b9e\u73b0\u754c\u9762\uff1b\n- \u4f7f\u7528FFmpeg\u62c9\u6d41\uff0c\u7f16\u89e3\u7801\uff0c\u8f6c\u7801\uff1b\n- \u4f7f\u7528OpenCV\u5904\u7406\u56fe\u7247\uff1b\n- \u4f7f\u7528OpenGL\u6e32\u67d3\u89c6\u9891\u5e27\uff1b\n\n## \u8be6\u7ec6\u8bbe\u8ba1\n\n**\u754c\u9762\u8bbe\u8ba1**\n\n![](hplayer.png)\n\n**\u591a\u753b\u9762\u6548\u679c\u56fe**\n\n![](hplayer4.png)\n\n![](hplayer25.png)\n\n## \u540e\u671f\u8ba1\u5212\n\n- \u6dfb\u52a0\u663e\u793a\u5668\u6355\u83b7\u6e90\uff1b\n- \u6dfb\u52a0\u56fe\u7247\u3001\u6587\u5b57\u3001\u65f6\u95f4\u53e0\u52a0\u529f\u80fd\uff1b\n- \u6dfb\u52a0\u591a\u753b\u9762\u5408\u6210\u529f\u80fd\uff1b\n- \u6dfb\u52a0\u63a8\u6d41\u3001\u5f55\u5236\u529f\u80fd\uff1b\n- \u6dfb\u52a0\u4eba\u8138\u68c0\u6d4b\u4e0e\u8bc6\u522b\u529f\u80fd\uff1b\n- \u6dfb\u52a0\u7f8e\u989c\u529f\u80fd\uff1b\n\n## Submodule\n```\ngit clone --recurse-submodules https://github.com/ithewei/hplayer.git\n```\nor\n```\ngit clone https://github.com/ithewei/hplayer.git\ngit submodule update --init\n```\n\n## Mirror\n```\nhttps://gitee.com/ithewei/hplayer.git\n```\n\n## Build\n\nsee BUILD.md\n\n## \u9879\u76ee\u535a\u5ba2\n\nhttps://hewei.blog.csdn.net/article/category/9275796\n"
 },
 {
  "repo": "janza/docker-python3-opencv",
  "language": "Dockerfile",
  "readme_contents": "[![Docker Automated buil](https://img.shields.io/docker/automated/jjanzic/docker-python3-opencv.svg)]()\n\nDocker image with python 3.7 and opencv 4.1.0\n\nUsage:\n\n    docker run -it jjanzic/docker-python3-opencv python\n    >>> import cv2\n\nImage tagged with `:contrib` contains docker image built with [contrib modules](https://github.com/opencv/opencv_contrib/)\n\nList of available docker tags:\n\n- `opencv-4.1.0` (`latest` branch)\n- `contrib-opencv-4.1.0` (`opencv_contrib` branch)\n- `opencv-4.0.1`\n- `contrib-opencv-4.0.1`\n- `opencv-4.0.0`\n- `contrib-opencv-4.0.0`\n- `opencv-3.4.2`\n- `contrib-opencv-3.4.2`\n- `opencv-3.4.1`\n- `contrib-opencv-3.4.1`\n- `opencv-3.4.0`\n- `contrib-opencv-3.4.0`\n- `opencv-3.3.0`\n- `contrib-opencv-3.3.0`\n- `opencv-3.2.0`\n- `contrib-opencv-3.2.0`\n"
 },
 {
  "repo": "ZQPei/Sorting_Visualization",
  "language": "Python",
  "readme_contents": "![](img/sort.jpg)\n\n# Sorting Visualization and Audibilization\n\n![](<https://img.shields.io/badge/python3-passing-brightgreen.svg>)\n![](<https://img.shields.io/badge/Video%20record-support-brightgreen.svg>)\n![](<https://img.shields.io/badge/Sound-support-brightgreen.svg>)\n![](<https://img.shields.io/badge/Sparse%20data-support-brightgreen.svg>)\n![](<https://img.shields.io/badge/Resampling%20data-support-brightgreen.svg>)\n\n## Latest Update2 (05/04/2019)\n\n- **Great News:** Now we can get the Voice of Sorting Algorithm simultaneously, and this is so funny that we should all have a try!!!\n\n## Latest Update\n\n- Add three more sorting algorithms: `CombSort`, `RadixSort`,  `MonkeySort`\n- Now you can record the whole sorting procedure to *avi videos.\n- Adding `-r` in command line can get you Re-sampled data.\n- Adding `-s` in command line can get you Sparse data.\n\n## Introduction\n\nThis repository is a demo of visualizing 12 types of Sorting Algorithms. It aims to make Sorting Algorithms easier for programmers to understand. Also, you can see the difference of Time Complexity between different sorting algorithms.\n\n| Sorting Algorithm | AverageTime Complexity | Bad Time Complexity | Stability |\n| ----------------- | ---------------------- | ------------------- | --------- |\n| Bubble Sort       | O(N^2)                 | O(N^2)              | YES       |\n| Insertion Sort    | O(N^2)                 | O(N^2)              | YES       |\n| Shell Sort        | O(N^5/4)               | O(N^2)              | NO        |\n| Selection Sort    | O(N^2)                 | O(n^2)              | NO        |\n| Heap Sort         | O(NlogN)               | O(NlogN)            | NO        |\n| Merge Sort        | O(NlogN)               | O(NlogN)            | YES       |\n| Quick Sort        | O(NlogN)               | O(N^2)              | NO        |\n| Bucket Sort       | O(N)                   | O(N)                | YES       |\n| Cycle Sort        | O(N)                   | O(N^2)              | NO        |\n| Comb Sort         | O(N^2)                 | O(N^2)              | NO        |\n| Radix Sort        | O(N)                   | O(N)                | YES       |\n| Monkey Sort       | O(N!)                  | O(N!)               | YES       |\n\n## Demos\n\n|                            |                            |                         |\n| -------------------------- | -------------------------- | ----------------------- |\n| ![](img/BubbleSort.gif)    | ![](img/InsertionSort.gif) | ![](img/ShellSort.gif)  |\n| ![](img/SelectionSort.gif) | ![](img/HeapSort.gif)      | ![](img/MergeSort.gif)  |\n| ![](img/QuickSort.gif)     | ![](img/BucketSort.gif)    | ![](img/CycleSort.gif)  |\n| ![](img/CombSort.gif)      | ![](img/RadixSort.gif)     | ![](img/MonkeySort.gif) |\n\n## Dependencies\n\n- python3.x\n- cv2\n- numpy\n- pygame\n\n## Quick Start\n\n0. Check all dependencies installed\n\n      This command can help you install all the dependent packages\n\n      `pip install -r requirements.txt`\n\n1. Clone this repository\n\n   `git clone git@github.com:ZQPei/Sort_Visualization.git`\n\n2. Start\n\n   `python main.py -l 512 -t BubbleSort`\n\n   - `-l` `--length`: Array Length\n   - `-t` `--sort-type`: Sorting Type. Default type is BubbleSort\n     - BubbleSort\n     - InsertionSort\n     - ShellSort\n     - SelectionSort\n     - HeapSort\n     - MergeSort\n     - QuickSort\n     - BucketSort\n     - CycleSort\n     - CombSort\n     - RadixSort(LSD)\n     - MonkeySort\n   - `-i` `--interval`: Time Interval of next frame\n   - `-r` `--resample`: Get Resampled Array\n   - `-s` `--sparse`: Sparse Array\n   - `-n` `--no-record`: Don't record to *.avi video!\n   - `--silent`: No voice output\n   - `--sound-interval`: Time of sound\n\n\n### May you have fun!\n\n"
 },
 {
  "repo": "PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA",
  "language": "C++",
  "readme_contents": "# Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA\nHands-On GPU Accelerated Computer Vision with OpenCV and CUDA, published by Packt\n\n<a href=\"https://www.packtpub.com/application-development/hands-gpu-accelerated-computer-vision-opencv-and-cuda?utm_source=github&utm_medium=repository&utm_campaign=9781789348293 \"><img src=\"https://d255esdrn735hr.cloudfront.net/sites/default/files/imagecache/ppv4_main_book_cover/cover%20-%20Copy_10995.png\" alt=\"Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA\" height=\"256px\" align=\"right\"></a>\n\nThis is the code repository for [Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA](https://www.packtpub.com/application-development/hands-gpu-accelerated-computer-vision-opencv-and-cuda?utm_source=github&utm_medium=repository&utm_campaign=9781789348293 ), published by Packt.\n\n**Effective techniques for processing complex image data in real time using GPUs\t**\n\n## What is this book about?\nComputer vision has been revolutionizing a wide range of industries, and OpenCV is the most widely chosen tool for computer vision with its ability to work in multiple programming languages. Nowadays, in computer vision, there is a need to process large images in real time, which is difficult to handle for OpenCV on its own. This is where CUDA comes into the picture, allowing OpenCV to leverage powerful NVDIA GPUs. This book provides a detailed overview of integrating OpenCV with CUDA for practical applications.\n\nThis book covers the following exciting features:\nUnderstand how to access GPU device properties and capabilities from CUDA programs\n<ul>\n    <li>Learn how to accelerate searching and sorting algorithms</li>\n    \n<li>Detect shapes such as lines and circles in images</li>\n\n<li>Explore object tracking and detection with algorithms</li>\n\n<li>Process videos using different video analysis techniques in Jetson TX1</li>\n\n<li>Access GPU device properties from the PyCUDA program</li>\n\n<li>Understand how kernel execution works </li>\n\nIf you feel this book is for you, get your [copy](https://www.amazon.com/dp/1789348293) today!\n\n<a href=\"https://www.packtpub.com/?utm_source=github&utm_medium=banner&utm_campaign=GitHubBanner\"><img src=\"https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png\" \nalt=\"https://www.packtpub.com/\" border=\"5\" /></a>\n\n## Instructions and Navigations\nAll of the code is organized into folders. For example, Chapter02.\n\nThe code will look like the following:\n```\nwhile (tid < N)\n    {\n       d_c[tid] = d_a[tid] + d_b[tid];\n       tid += blockDim.x * gridDim.x;\n    }\n```\n\n**Following is what you need for this book:**\nThis book is a go-to guide for you if you are a developer working with OpenCV and want to learn how to process more complex image data by exploiting GPU processing. A thorough understanding of computer vision concepts and programming languages such as C++ or Python is expected.\n\nWith the following software and hardware list you can run all code files present in the book (Chapter 1-12).\n### Software and Hardware List\n| Chapter | Software required | OS required |\n| -------- | ------------------------------------ | ----------------------------------- |\n| 1-4 | CUDA Toolkit X.X, Microsoft Visual Studio Community Edition, Nsight | Windows, Mac OS X, and Linux (Any) |\n| 5-8 | OpenCV Library | Windows, Mac OS X, and Linux (Any) |\n| 10-12 | Anaconda Python, PyCUDA | Windows, Mac OS X, and Linux (Any) |\n\n\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. [Click here to download it](https://www.packtpub.com/sites/default/files/downloads/978-1-78934-829-3_ColorImages.pdf).\n\nVisit the following link to check out videos of the code being run: http://bit.ly/2PZOYcH\n\n### Related products\n* OpenCV 3 Computer Vision with Python Cookbook [[Packt]](https://www.packtpub.com/application-development/opencv-3-computer-vision-python-cookbook?utm_source=github&utm_medium=repository&utm_campaign=) [[Amazon]](https://www.amazon.com/dp/1788474449)\n\n* Computer Vision with OpenCV 3 and Qt5 [[Packt]](https://www.packtpub.com/application-development/computer-vision-opencv-3-and-qt5?utm_source=github&utm_medium=repository&utm_campaign=9781788472395 ) [[Amazon]](https://www.amazon.com/dp/178847239X)\n\n\n## Get to Know the Author\n**Bhaumik Vaidya**\nBhaumik Vaidya is an experienced computer vision engineer and mentor. He has worked extensively on OpenCV Library in solving computer vision problems. He is a University gold medalist in masters and is now doing a PhD in the acceleration of computer vision algorithms built using OpenCV and deep learning libraries on GPUs. He has a background in teaching and has guided many projects in computer vision and VLSI(Very-large-scale integration). He has worked in the VLSI domain previously as an ASIC verification engineer, so he has very good knowledge of hardware architectures also. He has published many research papers in reputable journals to his credit. He, along with his PhD mentor, has also received an NVIDIA Jetson TX1 embedded development platform as a research grant from NVIDIA.\n\n\n\n### Suggestions and Feedback\n[Click here](https://docs.google.com/forms/d/e/1FAIpQLSdy7dATC6QmEL81FIUuymZ0Wy9vH1jHkvpY57OiMeKGqib_Ow/viewform) if you have any feedback or suggestions.\n\n\n"
 },
 {
  "repo": "gustavz/realtime_object_detection",
  "language": "Python",
  "readme_contents": "# realtime_object_detection\nRealtime Object Detection based on Tensorflow's [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) and [DeepLab Project](https://github.com/tensorflow/models/tree/master/research/deeplab)\n<img src=\"test_images/rod.png\" width=\"75.0%\">\n> Version 1: use branch [r1.0](https://github.com/GustavZ/realtime_object_detection/tree/r1.0) for the original repo that was focused on high performance inference of `ssd_mobilenet` <br />\n(*x10 Performance Increase on Nvidia Jetson TX2*)\n\n> Version 2: use branch [Master](https://github.com/GustavZ/realtime_object_detection/tree/master) or to be additionally able to run and test Mask-Detection Models, KCF-Tracking and DeepLab Models (*merge of the repo [realtime_segmenation](https://github.com/GustavZ/realtime_segmenation)*)\n\n> ROS Support: To use this Repo as ROS-Package including detection and segmentation ROS-Nodes use branch [ros](https://github.com/GustavZ/realtime_object_detection/tree/ros).\nAlternativley use the repo [objectdetection_ros](https://github.com/GustavZ/objectdetection_ros)\n\n## About the Project\nThe Idea was to create a scaleable realtime-capable object detection pipeline that runs on various systems. <br />\nPlug and play, ready to use without deep previous knowledge.<br /> <br />\nThe project includes following work:\n- optional download of tensorflow pretrained models\n- do Inference with OpenCV, either through video input or on selected test_images. <br />\nsupported Models are all `research/object_detection` as well as `research/deeplab` models\n- enjoy this project's own `ssd_mobilenet` speed hack, which splits the model in a mutlithreaded cpu and gpu session. <br />\nResults in up to x10 performance increase depending on the running system <br />\n\u21d2 which makes it (one of) the fastest inference piplines out there\n- run statistic tests on sets of images and get statistical information like mean and median fps, std dev and much more\n- create `timeline` files measuring the exact time consumption of each operation in your model\n- inspect, summarize, quantize, transform and benchmark models with the provided `scripts/`\n- Use this Repo as ROS Package. the detection subscirbes a ROS Image topic and publishes the detection as ROS Node.\n\n\n## Inference:  \n- create a copy of `config.sample.yml` named `config.yml` and only change configurations inside this file <br />\nFor example: If you are not interested in visualization: set `VISUALIZE` to `False`, <br />\nor if you want to switch off the speed hack set `SPLIT_MODEL` to `False`, <br />\n- to be able to use KCF_Tracking inside `scripts/` run `bash build_kcf.sh` to build it and set `USE_TRACKER` to `True` to use it <br />\n(currently only works for pure object detection models without `SPLIT_MODEL`)\n- new class (Model,Config,Visualizer) structure. Simply create your own test file with:\n    ```\n    from rod.model import ObjectDetectionModel, DeepLabModel\n    from rod.config import Config\n\n    model_type = 'od'                                              #or 'dl'\n    input_type = 'video'                                           #or 'image'\n    config = Config(model_type)\n    model = ObjectDetectionModel(config).prepare_model(input_type) #or DeepLabModel\n    model.run()\n    ```\n- Alternativley run `python` + `objectdetection_video.py` or `objectdetection_image.py` or `deeplab_video.py` or `deeplab_image.py` or `allmodels_image.py`\n\n\n## Scripts:\nTo make use of the tools provided inside `scripts/` follow this guide: <br />\n- first change all paths and variables inside `config_tools.sh` to your needs / according to your system\n- When using **the first time** run: `source config_tools.sh` and in the same terminal **run only once** `source build_tools.sh` to build the tools. this will take a while. <br />\n- For all following uses first run: `source config_tools.sh`(due to the exported variables) and after that you are able to run the wanted scripts **always from the same terminal** with `source script.sh`.\n- All scripts log the terminal output to `test_results/`\n\n\n## Setup:\nUse the following setup for best and verified performance\n- Ubuntu 16.04\n- Python 2.7\n- Tensorflow 1.4\n([this repo](https://github.com/peterlee0127/tensorflow-nvJetson) provides pre-build tf wheel files for jetson tx2)\n- OpenCV 3.3.1\n> Note: tensorflow v1.7.0 seems to have massive performance issues (try to use other versions)\n\n## Current max Performance on `ssd_mobilenet`:\n- Dell XPS 15 with i7 @ 2.80GHZ x8 and GeForce GTX 1050 4GB:  **100 FPS**\n- Nvidia Jetson Tx2 with Tegra 8GB:                           **30 FPS**\n\n\n\n## Related Work:\n- [objectdetection_ros](https://github.com/GustavZ/objectdetection_ros): This Repository as ROS Package ready to use\n- [test_models](https://github.com/GustavZ/test_models): A repo for models i am currently working on for benchmark tests\n- [deeptraining_hands](https://github.com/GustavZ/deeptraining_hands): A repo for setting up the [ego](http://vision.soic.indiana.edu/projects/egohands/)- and [oxford](http://www.robots.ox.ac.uk/~vgg/data/hands/) hands-datasets.<br />\nIt also contains several scripts to convert various annotation formats to be able to train Networks on different deep learning frameworks <br />\ncurrently supports `.xml`, `.mat`, `.csv`, `.record`, `.txt` annotations\n- [yolo_for_tf_od_api](https://github.com/GustavZ/yolo_for_tf_od_api): A repo to be able to include Yolo V2 in tf's object detection api\n- [realtime_segmenation](https://github.com/GustavZ/realtime_segmenation): This repo was merged into v2.0\n- [Mobile_Mask_RCNN](https://github.com/GustavZ/Mobile_Mask_RCNN): a Keras Model for training Mask R-CNN for mobile deployment\n- [tf_training](https://github.com/GustavZ/tf_training): Train Mobile Mask R-CNN Models on AWS Cloud\n- [tf_models](https://github.com/GustavZ/tf_models): My `tensorflow/models` fork which includes `yolov2` and `mask_rcnn_mobilenet_v1_coco`\n- [eetfm_automation](https://github.com/GustavZ/eetfm_automation): Export and Evaluation of TensorFlow Models Automation based on the Object Detection API\n"
 },
 {
  "repo": "mbeyeler/opencv-python-blueprints",
  "language": "Python",
  "readme_contents": "# OpenCV with Python Blueprints\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.154060.svg)](https://doi.org/10.5281/zenodo.154060)\n[![Google group](https://img.shields.io/badge/Google-Discussion%20group-lightgrey.svg)](https://groups.google.com/d/forum/opencv-python-blueprints)\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](http://www.gnu.org/licenses/gpl-3.0)\n\nThis repository contains all up-to-date source code for the following book:\n\n<img src=\"https://2.bp.blogspot.com/-0kv2Un_wtT4/VlX2XOazp3I/AAAAAAAAACE/bmZ6AsPfRKY8D6Btr10SObc6QiD8Hi0bQ/s200/2690OS_OpenCV%2Bwith%2BPython%2BBlueprints_.jpg\" align=\"left\" style=\"width: 220px; margin-right: 5px\"/>\nMichael Beyeler <br/>\n<a href=\"http://www.amazon.com/OpenCV-Python-Blueprints-Michael-Beyeler/dp/1785282697\"><b>OpenCV with Python Blueprints: Design and develop advanced computer vision projects using OpenCV with Python</b></a>\n\nPackt Publishing Ltd., London, England <br/>\nPaperback: 230 pages <br/>\nISBN 978-178528269-0\n<br clear=\"both\"/>\n\nThis book demonstrates how to develop a series of intermediate to advanced projects using OpenCV and Python,\nrather than teaching the core concepts of OpenCV in theoretical lessons. Instead, the working projects\ndeveloped in this book teach the reader how to apply their theoretical knowledge to topics such as\nimage manipulation, augmented reality, object tracking, 3D scene reconstruction, statistical learning,\nand object categorization.\n\nBy the end of this book, readers will be OpenCV experts whose newly gained experience allows them to develop their own advanced computer vision applications.\n\nIf you use either book or code in a scholarly publication, please cite as:\n> M. Beyeler, (2015). OpenCV with Python Blueprints: Design and develop advanced computer vision projects using OpenCV with Python. Packt Publishing Ltd., London, England, 230 pages, ISBN 978-\n178528269-0.\n\nOr use the following bibtex:\n```\n@book{OpenCVWithPythonBlueprints,\n\ttitle = {{OpenCV with Python Blueprints}},\n\tsubtitle = {Design and develop advanced computer vision projects using {OpenCV} with {Python}},\n\tauthor = {Michael Beyeler},\n\tyear = {2015},\n\tpages = {230},\n\tpublisher = {Packt Publishing Ltd.},\n\tisbn = {978-178528269-0}\n}\n```\n\nScholarly work referencing this book:\n- B Zhang et al. (2018). Automatic matching of construction onsite resources under camera views. *Automation in Construction*.\n- A Jakubovi\u0107 & J Velagi\u0107 (2018). Image Feature Matching and Object Detection Using Brute-Force Matchers. *International Symposium ELMAR*.\n- B Zhang et al. (2018). Multi-View Matching for Onsite Construction Resources with Combinatorial Optimization. *International Symposium on Automation and Robotics in Construction (ISARC)* 35:1-7.\n- LA Marcomini (2018). Identifica\u00e7\u00e3o autom\u00e1tica do comportamento do tr\u00e1fego a partir de imagens de v\u00eddeo. *Escola de Engenharia de S\u00e3o Carlos*, Master's Thesis.\n- G Laica et al. (2018). Dise\u00f1o y construcci\u00f3n de un andador inteligente para el desplazamiento aut\u00f3nomo de los adultos mayores con visi\u00f3n reducida y problemas de movilidad del hogar de vida \"Luis Maldonado Tamayo\" mediante la investigaci\u00f3n de t\u00e9cnicas de visi\u00f3n artificial. *Departamento de Ciencias de la Energ\u00eda y Mec\u00e1nica, Universidad de las Fuerzas Armadas ESPE*, Master's Thesis.\n- I Huitzil-Velasco et al. (2017). Test of a Myo Armband. *Revista de Ciencias Ambientales y Recursos Naturales* 3(10): 48-56.\n- Y G\u00fc\u00e7l\u00fct\u00fcrk et al. (2016). Convolutional sketch inversion. *European Conference on Computer Vision (ECCV)* 810-824.\n\n\nAll code was tested with OpenCV 2.4.9 and Python 2.7 on Ubuntu 14.04 and Windows 8.1, and is available from:\nhttps://github.com/mbeyeler/opencv-python-blueprints.\n\nFor questions, discussions, and more detailed help please refer to the Google group:\nhttps://groups.google.com/d/forum/opencv-python-blueprints\n\n\n## Critical Reception\n\n<img src=\"https://3.bp.blogspot.com/-m8yl8xCrM3Q/V9yFYMAj3YI/AAAAAAAAAq8/5IzGqAeUp9cCwq13j1EL7aunfUvvre5bQCLcB/s640/opencv-python-blueprints-amazon-new.png\" style=\"width: 70%; margin-left: 15%\"/>\n\nWhat readers on Amazon have to say:\n\n> The author does a great job explaining the concepts needed to understand what's happening in the application without \n> the need of going into too many details. <br/>\n&ndash; [Sebastian Montabone](http://www.samontab.com)\n\n> Excellent book to build practical OpenCV projects! I'm still relatively new to OpenCV, but all examples are well \n> laid out and easy to follow. The author does a good job explaining the concepts in detail and shows how they apply \n> in real life. As a professional programmer, I especially love that you can just fork the code from GitHub and follow \n> along. Strongly recommend to readers with basic knowledge of computer vision, machine learning, and Python!\n&ndash; Amazon Customer\n\n> Usually I'm not a big fan of technical books because they are too dull, but this one is written in an engaging \n> manner with a few dry jokes here and there. Can only recommend! <br/>\n&ndash; lakesouth\n\n\n\n## Who This Book Is for\nAs part of Packt's Blueprints series, this book is for intermediate users of OpenCV who aim to master their skills\nby developing advanced practical applications. You should already have some\nexperience of building simple applications, and you are expected to be familiar with\nOpenCV's concepts and Python libraries. Basic knowledge of Python programming\nis expected and assumed.\n\nBy the end of this book, you will be an OpenCV expert, and your newly gained\nexperience will allow you to develop your own advanced computer vision\napplications.\n\n\n\n## Software Requirements\nAll projects can run on Windows, Mac, or Linux, and require the following software packages:\n* OpenCV 2.4.9 or later: Recent 32-bit and 64-bit versions as well as installation instructions are available at\nhttp://opencv.org/downloads.html. Platform-specific installation instructions can be found at\nhttp://docs.opencv.org/doc/tutorials/introduction/table_of_content_introduction/table_of_content_introduction.html.\n* Python 2.7 or later: Recent 32-bit and 64-bit installers are available at https://www.python.org/downloads. The\ninstallation instructions can be found at https://wiki.python.org/moin/BeginnersGuide/Download.\n* NumPy 1.9.2 or later: This package for scientific computing officially comes in 32-bit format only, and can be\nobtained from http://www.scipy.org/scipylib/download.html. The installation instructions can be found at \nhttp://www.scipy.org/scipylib/building/index.html#building.\n\nIn addition, some chapters require the following free Python modules:\n* wxPython 2.8 or later (Chapters 1 to 4, 7): This GUI programming toolkit can be obtained from\n  http://www.wxpython.org/download.php.\n  Its installation instructions are given at http://wxpython.org/builddoc.php.\n  If you are using Max OS 10.11 (El Capitan), try:\n  \n  ```\n  $ sudo pip install --upgrade --trusted-host wxpython.org --pre -f http://wxpython.org/Phoenix/snapshot-builds/ wxPython_Phoenix\n  ```\n  \n  See [this bug](https://github.com/mbeyeler/opencv-python-blueprints/issues/9) for context.\n  Thanks to @KaroAntonio for the fix!\n* matplotlib 1.4.3 or later (Chapters 4 to 7): This 2D plotting library can be obtained from\n  http://matplotlib.org/downloads.html. Its installation instructions can be found by going to\n  http://matplotlib.org/faq/installing_faq.html#how-to-install.\n* SciPy 0.16.0 or later (Chapter 1): This scientific Python library officially comes in 32-bit only, and can be\n  obtained from http://www.scipy.org/scipylib/download.html. The installation instructions can be found at\n  http://www.scipy.org/scipylib/building/index.html#building.\n* libfreenect 0.5.2 or later (Chapter 2): The libfreenect module by the OpenKinect project (http://www.openkinect.org)\n  provides drivers and libraries for the Microsoft Kinect hardware, and can be obtained from\n  https://github.com/OpenKinect/libfreenect. Its installation instructions can be found at\n  http://openkinect.org/wiki/Getting_Started.\n\nFurthermore, the use of iPython (http://ipython.org/install.html) is highly recommended as it provides a flexible,\ninteractive console interface.\n\n## License\nThe software is released under the GNU General Public License (GPL), which is the most commonly used free software\nlicense according to Wikipedia. GPL allows for commercial use, distribution, modification, patent use, and private use.\n\nThe GPL is a copyleft license, which means that derived works can only be distributed under the same license terms.\nFor more information, please see the license file.\n"
 },
 {
  "repo": "IBM/powerai-counting-cars",
  "language": "Jupyter Notebook",
  "readme_contents": "# Detect, track, and count cars in a video\n\n![video-to-gif](doc/source/images/output-video-as-gif.gif)\n\n**Note: This repo has been updated to use Maximo Visual Inspection (formerly known as PowerAI Vision). Everything is the same except for the name itself.**\n\nWhether you are counting cars on a road or products on a conveyer belt, there are many use cases for computer vision with video. With video as input, automatic labeling can be used to create a better classifier with less manual effort. This code pattern shows you how to create and use a classifier to identify objects in motion and then track the objects and count them as they enter designated regions of interest.\n\nIn this code pattern, we will create a video car counter using Maximo Visual Inspection, OpenCV and a Jupyter Notebook. We'll use a little manual labeling and a lot of automatic labeling to train an object classifier to recognize cars on a highway. We'll load another car video into a Jupyter Notebook where we'll process the individual frames and annotate the video.\n\nWe'll use our deployed model for inference to detect cars on a sample of the frames at a regular interval. We'll use OpenCV to track the cars from frame to frame in between inference. In addition to counting the cars as they are detected, we'll also count them as they cross a \"finish line\" for each lane and show cars per second.\n\nCredit goes to Michael Hollinger for his initial notebook counting objects with PowerAI Vision.\n\nWhen the reader has completed this code pattern, they will understand how to:\n\n* Use automatic labeling to create an object detection classifier from a video\n* Process frames of a video using a Jupyter Notebook, OpenCV, and Maximo Visual Inspection (formerly known as PowerAI Vision)\n* Detect objects in video frames with Maximo Visual Inspection\n* Track objects from frame to frame with OpenCV\n* Count objects in motion as they enter a region of interest\n* Annotate a video with bounding boxes, labels and statistics\n\n![architecture](doc/source/images/architecture.png)\n\n## Flow\n\n1. Upload a video using the Maximo Visual Inspection web UI.\n2. Use automatic labeling and train a model.\n3. Deploy the model to create a Maximo Visual Inspection (formerly known as PowerAI Vision) inference API.\n4. Use a Jupyter Notebook to detect, track, and count cars in a video.\n\n## Included components\n\n* [IBM Maximo Visual Inspection](https://www.ibm.com/products/ibm-maximo-visual-inspection): Rapidly unleash the power of computer vision for inspection automation without deep learning expertise.\n* [Jupyter Notebook](https://jupyter.org/): An open source web application that allows you to create and share documents that contain live code, equations, visualizations, and explanatory text.\n* [OpenCV](https://opencv.org): Open source computer vision library.\n\n## Featured technologies\n\n* [Artificial Intelligence](https://developer.ibm.com/technologies/artificial-intelligence/): Artificial intelligence can be applied to disparate solution spaces to deliver disruptive technologies.\n* [Cloud](https://developer.ibm.com/depmodels/cloud/): Accessing computer and information technology resources through the Internet.\n* [Data Science](https://developer.ibm.com/technologies/data-science/): Systems and scientific methods to analyze structured and unstructured data in order to extract knowledge and insights.\n* [Mobile](https://mobilefirstplatform.ibmcloud.com/): Systems of engagement are increasingly using mobile technology as the platform for delivery.\n* [Python](https://www.python.org/): Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n## Watch the Video\n\n[![video](https://img.youtube.com/vi/19vaot75JCY/0.jpg)](https://youtu.be/19vaot75JCY)\n\n## Prerequisites\n\n### PowerAI Vision\n\nThis code pattern requires Maximo Visual Inspection (formerly known as PowerAI Vision).\n\nGo [here](https://www.ibm.com/support/pages/ibm-maximo-visual-inspection)\nto learn more about trial access (Scroll down to the `Give it a try` section).\n\n*This code pattern has been updated with screenshots and instructions for PowerAI Vision 1.1.3, but the instructions are the same for IBM Maximo Visual Insights.*\n\n### Jupyter Notebooks\n\nThe code included in this code pattern runs in a Jupyter Notebook. The notebook itself does not require Maximo Visual Inspection or Power Systems (only access to the deployed API). To run the Jupyter Notebook locally, install it using Anaconda.  The installation instructions are [here](https://jupyter.readthedocs.io/en/latest/install.html).\n\n## Steps\n\n1. [Create a dataset in Maximo Visual Inspection](#1-create-a-dataset-in-maximo-visual-inspection)\n2. [Train and deploy](#2-train-and-deploy)\n3. [Automatic labeling](#3-automatic-labeling)\n4. [Train and deploy](#4-train-and-deploy)\n5. [Run the notebook](#5-run-the-notebook)\n6. [Create the annotated video](#6-create-the-annotated-video)\n\n> Hint: If you need a shortcut, you can import the dataset from `data/examples/dataset_auto_labeled.zip`, train and deploy that dataset, and then run the notebook (but you'll get more out of this if you go through all the steps).\n\n### 1. Create a dataset in Maximo Visual Inspection\n\nTo create a new dataset for object detection training from a video, use the Maximo Visual Inspection  UI and start with a small manually annotated dataset (we'll expand on it with automatic labeling later).\n\n* Download the video to use to train the dataset from [here](https://raw.githubusercontent.com/IBM/powerai-counting-cars/master/data/training_video.mp4). Use the `Download` button to create `training_video.mp4` in your browser's Downloads folder.\n\n* Click on the `Data Sets` tab.\n* Click on the `Create new data set` card.\n* Provide a data set name.\n* Click the `Create` button.\n* Click on the newly created data set card.\n\n  ![add_dataset](doc/source/images/video_data_platform_dataset.png)\n\n* Click on `Import files` and open your downloaded `training_video.mp4` file.\n\n* Select the new uncategorized card and click the `Label objects` button.\n\n  ![add_dataset](doc/source/images/manage_videos.png)\n\n* Click on `Auto capture`, set the capture interval to 5 seconds, and click the `Auto capture` button.\n\n  ![add_label](doc/source/images/add_label.png)\n\n  > Tips!  Click on `How to label` for more detailed labeling tips.\n\n* Create a new object label for the data set by clicking `+ Add new` by the `Objects` list. For example, add a `car` object label.\n\n* Label the objects in the frames by following these steps.\n  * Select a frame in the carousel.\n  * Select the correct object label.\n  * Choose `Box` from the bottom left.\n  * Draw a box around each car in each captured frame.\n\n  ![manual_tagging](doc/source/images/manual_tagging.png)\n\n### 2. Train and deploy\n\n#### Train the model\n\n* Go back to the data set page and click `Train model`.\n* Select `Object detection`.\n* Review the `Advanced` settings. You can train faster (with less accuracy) by reducing the max iterations.\n* Click `Train`.\n\n#### Deploy the model\n\n* Go to the `Models` tab.\n* Click the `Deploy model` button.\n* Use the `Deployed Models` tab to see the status.\n* When the status is ready, click on the deployed model to get the API endpoint.\n\n### 3. Automatic labeling\n\nWe use the first deployed model that you trained with manually annotated frames and use inference to automatically label more cars in your training video.\n\n* Go back to the data set.\n* Select your video card.\n* Click the `Label objects` button.\n* Click on `Auto label`.\n* Enter 1 for `Capture Interval (Seconds)`.\n* Select your deployed model.\n* Click the `Auto label` button.\n\nFrames are captured at the specified interval and labels are added by using the specified trained model. By default, the automatically added labels are light red.\nAfter processing, you can manually add labels to the frames that have been auto labeled and you can manipulate (move, resize) the labels that were automatically generated. If a frame with automatically generated labels is edited, all labels on the frame are converted to manual labels.\n\n### 4. Train and deploy\n\nRepeat the above train and deploy process with the newest dataset which was enhanced with automatic labeling.\n\n* Train model\n* Deploy model\n\nThis dataset has many more frames and labeled objects. It will create a much more accurate model.\n\n### 5. Run the notebook\n\nThe code included in this code pattern runs in a Jupyter Notebook. After you configure the URL of your deployed model in the notebook, you can just run it, read it, and watch the results.\n\n* Start your Jupyter Notebooks. Starting in your `powerai-counting-cars` cloned repo directory will help you find the notebook and the output as described below. Jupyter Notebooks will open in your browser.\n\n   ```bash\n   cd powerai-counting-cars\n   jupyter notebook\n   ```\n\n* Navigate to the `notebooks` directory and open the notebook file named `counting_cars.ipynb` by clicking on it.\n\n  ![open_notebook](doc/source/images/open_notebook.png)\n\n* Edit the cell below **Required setup!** to replace the URL with the API endpoint of your deployed model. Use the copy button to capture the whole URL.\n\n  ![required_setup](doc/source/images/required_setup.png)\n\n* Use the drop-down menu `Cell > Run All` to run the notebook, or run the cells one at a time top-down using the play button.\n\n  ![run_all](doc/source/images/run_all.png)\n\n* As the cells run, watch the output for results or errors. A running cell will have a label like `In [*]`. A completed cell will have a run sequence number instead of the asterisk.\n\n* The **Test the API on a single frame** cell will demonstrate that you have correctly deployed your inference API. It should output JSON that includes classified cars. A portion of the output would look something like this:\n\n    ```json\n      \"classified\": [\n        {\n          \"confidence\": 0.9997443556785583,\n          \"ymax\": 370,\n          \"label\": \"car\",\n          \"xmax\": 516,\n          \"xmin\": 365,\n          \"ymin\": 240\n        }\n      ]\n    ```\n\n* The **Get object detection results for sampled frames** cell runs inference on a sampling of the video frames. The output will show a progress counter like this:\n\n  ![inference_progress](doc/source/images/inference_progress.png)\n\n* The **Inference, tracking, and annotation** cell processes every frame and has a similar progress counter. You can also preview the annotated frames as they are created in the `output` directory.\n\n* The **Play the annotated frames in the notebook** cell displays the annotated frames in a loop to demonstrate the new video after they are all created. The notebook animation is usually slower than the video.\n\n### 6. Create the annotated video\n\nYou can create an MP4 video from the annotated frames if you have a working installation of [ffmpeg](https://www.ffmpeg.org/). The command is commented out as the last cell of the notebook. You can run it from there, or use the script in `tools/create_video.sh`. The script takes the output directory (with the annotated frames) as an argument like this:\n\n```bash\ncd powerai-counting-cars\n./tools/create_video.sh notebooks/output\n```\n\n> Note: There is also a tool to create a gif from the video. We used that to show the sample output below.\n\n## Sample output\n\nAs the notebook cells run, check for errors and watch the progress indicators. After the video has been annotated, the frames will play (like a video) in the notebook. The notebook playback is usually slow. If you used `ffmpeg` to create an annotated video, you can play it back at full speed.\n\nExample annotated video: [here](https://ibm.box.com/v/powerai-vision-counted-cars)\n\nExample notebook with static output: [here](data/examples/example_notebook.ipynb)\n\nExample compressed and converted to gif:\n\n![video-to-gif](doc/source/images/output-video-as-gif.gif)\n\n## Troubleshooting\n\n* Stopped adding cars.\n\n  > If you are using a trial environment, your model deployment may be limited to 1 hour. Simply deploy the model again and run the notebook over (or from where the errors started). Using cached results allows the notebook to continue where it left off.\n\n## Links\n\n* [Maximo Visual Inspection Learning Path](https://developer.ibm.com/technologies/vision/series/learning-path-powerai-vision/): From computer vision basics to creating your own apps.\n* [Maximo Visual Inspection Object Detection](https://github.com/IBM/powerai-vision-object-detection): use Maximo Visual Insights Object Detection to detect and label objects, within an image, based on customized training.\n* [Computer vision](https://en.wikipedia.org/wiki/Computer_vision): Read about computer vision on Wikipedia.\n* [Object detection](https://en.wikipedia.org/wiki/Object_detection): Read about object detection on Wikipedia.\n* [Artificial intelligence](https://www.entrepreneur.com/article/283990): Can artificial intelligence identify pictures better than humans?\n* [Artificial intelligence and machine learning](https://developer.ibm.com/technologies/artificial-intelligence/): Build artificial intelligence functions into your app.\n\n## Learn more\n\n* **Artificial intelligence code patterns**: Enjoyed this code pattern? Check out our other [AI code patterns](https://developer.ibm.com/technologies/artificial-intelligence/).\n* **AI and data code pattern playlist**: Bookmark our [playlist](https://www.youtube.com/playlist?list=PLzUbsvIyrNfknNewObx5N7uGZ5FKH0Fde) with all of our code pattern videos\n* **PowerAI**: Get started or get scaling, faster, with a software distribution for machine learning running on the Enterprise Platform for AI: [IBM Power Systems](https://www.ibm.com/us-en/marketplace/deep-learning-platform)\n\n## License\n\nThis code pattern is licensed under the Apache License, Version 2. Separate third-party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1](https://developercertificate.org/) and the [Apache License, Version 2](https://www.apache.org/licenses/LICENSE-2.0.txt).\n"
 },
 {
  "repo": "arunponnusamy/object-detection-opencv",
  "language": "Python",
  "readme_contents": "# Object detection using deep learning with OpenCV and Python \n\nOpenCV `dnn` module supports running inference on pre-trained deep learning models from popular frameworks like Caffe, Torch and TensorFlow. \n\nWhen it comes to object detection, popular detection frameworks are\n * YOLO\n * SSD\n * Faster R-CNN\n \n Support for running YOLO/DarkNet has been added to OpenCV dnn module recently. \n \n ## Dependencies\n  * opencv\n  * numpy\n  \n`pip install numpy opencv-python`\n\n**Note: Compatability with Python 2.x is not officially tested.**\n\n ## YOLO (You Only Look Once)\n \n Download the pre-trained YOLO v3 weights file from this [link](https://pjreddie.com/media/files/yolov3.weights) and place it in the current directory or you can directly download to the current directory in terminal using\n \n `$ wget https://pjreddie.com/media/files/yolov3.weights`\n \n Provided all the files are in the current directory, below command will apply object detection on the input image `dog.jpg`.\n \n `$ python yolo_opencv.py --image dog.jpg --config yolov3.cfg --weights yolov3.weights --classes yolov3.txt`\n \n \n **Command format** \n \n _$ python yolo_opencv.py --image /path/to/input/image --config /path/to/config/file --weights /path/to/weights/file --classes /path/to/classes/file_\n \n Checkout the [blog post](http://www.arunponnusamy.com/yolo-object-detection-opencv-python.html) to learn more.\n \n ### sample output :\n ![](object-detection.jpg)\n \nCheckout the object detection implementation available in [cvlib](http:cvlib.net) which enables detecting common objects in the context through a single function call `detect_common_objects()`.\n \n \n (_SSD and Faster R-CNN examples will be added soon_)\n"
 },
 {
  "repo": "davidstutz/superpixel-benchmark",
  "language": "C++",
  "readme_contents": "# Superpixels: An Evaluation of the State-of-the-Art\n\n[![Build Status](https://travis-ci.org/davidstutz/superpixel-benchmark.svg?branch=master)](https://travis-ci.org/davidstutz/superpixel-benchmark)\n\nThis repository contains the source code used for evaluation in [1], a large-scale \ncomparison of state-of-the-art superpixel algorithms.\n\n**[ArXiv](https://arxiv.org/abs/1612.01601) | \n[Project Page](http://davidstutz.de/projects/superpixel-benchmark/) | \n[Datasets](https://github.com/davidstutz/superpixel-benchmark-data) | \n[Doxygen Documentation](https://davidstutz.github.io/superpixel-benchmark/)**\n\nThis repository subsumes earlier work on comparing superpixel algorithms:\n[davidstutz/gcpr2015-superpixels](https://github.com/davidstutz/gcpr2015-superpixels), \n[davidstutz/superpixels-revisited](https://github.com/davidstutz/superpixels-revisited).\n\n**Please cite the following work if you use this benchmark or the provided tools or implementations:**\n\n    [1] D. Stutz, A. Hermans, B. Leibe.\n        Superpixels: An Evaluation of the State-of-the-Art.\n        Computer Vision and Image Understanding, 2018.\n\nAlso make also sure to cite additional papers when using datasets or superpixel algorithms.\n\n**Updates:**\n\n* An implementation of the average metrics, i.e. Average Boundary Recall (called\nAverage Miss Rate in the updated paper), Average Undersegmentation Error\nand Average Explained Variation (called Average Unexplained Variation in the updated paper)\nis provided in `lib_eval/evaluation.h` and an easy-to-use command line tool is provided,\nsee `eval_average_cli` and the corresponding documentation and examples in\n[Executables](docs/EXECUTABLES.md) and [Examples](docs/EXAMPLE.md) respectively.\n* As of Mar 29, 2017 the paper was accepted for publication at \n[CVIU](https://www.journals.elsevier.com/computer-vision-and-image-understanding/).\n* The converted (i.e. pre-processed) NYUV2, SBD and SUNRGBD datasets are now available\nin the [data repository](https://github.com/davidstutz/superpixel-benchmark-data).\n* The source code of MSS has been added.\n* The source code of PF and SEAW has been added.\n* Doxygen documentation is now available [here](http://davidstutz.github.io/superpixel-benchmark/).\n* The presented paper was in preparation for a longer period of time \u2014 \n  some recent superpixel algorithms are not included in the comparison. These include \n  [SCSP](https://github.com/freifeld/fastSCSP) and [LRW](https://github.com/shenjianbing/lrw14).\n\n## Table of Contents\n\n* [Introduction](#introduction)\n* [Algorithms](#algorithms)\n    * [Submission](#submission)\n* [Documentation](docs/README.md)\n    * [Datasets](docs/DATASETS.md)\n    * [Algorithms](docs/ALGORITHMS.md)\n        * [Submission](docs/SUBMISSION.md)\n    * [Benchmark](docs/BENCHMARK.md)\n    * [Building](docs/BUILDING.md)\n        * [Building CIS](docs/BUILDING_CIS.md)\n    * [Executables](docs/EXECUTABLES.md)\n    * [Examples](docs/EXAMPLES.md)\n    * [Parameters](docs/PARAMETERS.md)\n    * [Results](docs/RESULTS.md)\n        * [Data](docs/DATA.md)\n* [License](#license)\n\n## Introduction\n\nSuperpixels group pixels similar in color and other low-level properties.\nIn this respect, superpixels address two problems inherent to the processing of \ndigital images: firstly, pixels are merely a result of discretization; \nand secondly, the high number of pixels in large images prevents many algorithms\nfrom being computationally feasible. Superpixels were introduced as more natural \nentities - grouping pixels which perceptually belong together while heavily reducing\nthe number of primitives.\n\nThis repository can be understood as supplementary material for an extensive \nevaluation of 28 algorithms on 5 datasets regarding visual quality, performance,\nruntime, implementation details and robustness - as presented in [1]. To ensure \na fair comparison, parameters have been optimized on separate training sets; as \nthe number of generated superpixels heavily influences parameter optimization, \nwe additionally enforced connectivity. Furthermore, to evaluate superpixel algorithms \nindependent of the number of superpixels, we propose to integrate over commonly \nused metrics such as Boundary Recall, Undersegmentation Error and Explained Variation. \nFinally, we present a ranking of the superpixel algorithms considering multiple \nmetrics and independent of the number of generated superpixels, as shown below.\n\n![Algorithm ranking.](RANKING.png?raw=true \"Algorithm ranking.\")\n\nThe table shows the average ranks across the 5 datasets, taking into account Average\nBoundary Recall (ARec) and Average Undersegmentation Error (AUE) - lower is better \nin both cases, see [Benchmark](docs/BENCHMARK.md).\nThe confusion matrix shows the rank distribution of the algorithms across the datasets.\n\n## Algorithms\n\nThe following algorithms were evaluated in [1], and most of them are included in\nthis repository:\n\nIncluded                                   | Algorithm    | Reference\n-------------------------------------------|--------------|-----------\n:ballot_box_with_check:                    | CCS          | [Ref. & Web](http://www.emrahtasli.com/research/spextraction/)\n[Instructions](docs/BUILDING_CIS.md)       | CIS          | [Ref.](http://www.csd.uwo.ca/~olga/Papers/eccv2010final.pdf) & [Web](http://www.csd.uwo.ca/faculty/olga/)\n:ballot_box_with_check:                    | CRS          | [Ref.](http://link.springer.com/chapter/10.1007%2F978-3-642-40395-8_21#page-1) & [Web](http://www.vsi.cs.uni-frankfurt.de/research/superpixel-segmentation/)\n:ballot_box_with_check:                    | CW           | [Ref.](https://www.tu-chemnitz.de/etit/proaut/rsrc/cws_pSLIC_ICPR.pdf) & [Web](https://www.tu-chemnitz.de/etit/proaut/forschung/cv/segmentation.html.en)\n:ballot_box_with_check:                    | DASP         | [Ref.](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460572) & [Web](https://github.com/Danvil/dasp)\n:ballot_box_with_check:                    | EAMS         | [Ref.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.8.5341&rep=rep1&type=pdf), [Ref.](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=977560), [Ref.](https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf) & [Web](http://coewww.rutgers.edu/riul/research/code/EDISON/)\n:ballot_box_with_check:                    | ERS          | [Ref.](http://www.merl.com/publications/docs/TR2011-035.pdf) & [Web](http://mingyuliu.net/)\n:ballot_box_with_check:                    | FH           | [Ref.](http://www.cs.cornell.edu/~dph/papers/seg-ijcv.pdf) & [Web](https://cs.brown.edu/~pff/segment/index.html)\n:ballot_box_with_check:                    | MSS          | [Ref.](http://avestia.com/MVML2014_Proceedings/papers/67.pdf)\n:ballot_box_with_check:                    | PB           | [Ref.](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126393&tag=1) & [Web](http://yuhang.rsise.anu.edu.au/yuhang/misc.html)\n:ballot_box_with_check:                    | preSLIC      | [Ref.](https://www.tu-chemnitz.de/etit/proaut/rsrc/cws_pSLIC_ICPR.pdf) & [Web](https://www.tu-chemnitz.de/etit/proaut/forschung/cv/segmentation.html.en)\n:ballot_box_with_check:                    | reSEEDS      | [Web](http://davidstutz.de/projects/superpixelsseeds/)\n:ballot_box_with_check:                    | SEAW         | [Ref.](http://patrec.cs.tu-dortmund.de/pubs/papers/Strassburg2015-OIS) & [Web](https://github.com/JohannStrassburg/InfluenceSegImageParsingCode)\n:ballot_box_with_check:                    | SEEDS        | [Ref.](http://arxiv.org/pdf/1309.3848v1.pdf) & [Web](http://www.mvdblive.org/seeds/)\n:ballot_box_with_check:                    | SLIC         | [Ref.](http://www.kev-smith.com/papers/SLIC_Superpixels.pdf) & [Web](http://ivrl.epfl.ch/research/superpixels)\n:ballot_box_with_check:                    | TP           | [Ref.](http://www.cs.toronto.edu/~babalex/09.pami.turbopixels.pdf) & [Web](http://www.cs.toronto.edu/~babalex/research.html)\n:ballot_box_with_check:                    | TPS          | [Ref.](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6298495) & [Web](http://hzfu.github.io/subpage/codes.html)\n:ballot_box_with_check:                    | vlSLIC       | [Web](http://www.vlfeat.org/overview/slic.html)\n:ballot_box_with_check:                    | W            | [Web](http://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=watershed#watershed)\n:ballot_box_with_check:                    | WP           | [Ref.](http://cmm.ensmp.fr/~machairas/waterpixels.html) & [Web](http://cmm.ensmp.fr/~machairas/waterpixels.html)\n:ballot_box_with_check:                    | PF           | [Ref.](http://users.dickinson.edu/~jmac/publications/fast-superpixels-WMVC09.pdf) & [Web](http://users.dickinson.edu/~jmac/publications/PathFinder.zip)\n:ballot_box_with_check:                    | LSC          | [Ref.](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Superpixel_Segmentation_Using_2015_CVPR_paper.pdf) & [Web](http://jschenthu.weebly.com/projects.html)\n:ballot_box_with_check:                    | RW           | [Ref.](http://cns.bu.edu/~lgrady/grady2004multilabel.pdf) & [Web](http://cns.bu.edu/~lgrady/software.html)\n:ballot_box_with_check:                    | QS           | [Ref.](http://vision.cs.ucla.edu/papers/vedaldiS08quick.pdf) & [Web](http://www.vlfeat.org/overview/quickshift.html)\n:ballot_box_with_check:                    | NC           | [Ref.](http://ttic.uchicago.edu/~xren/publication/xren_iccv03_discrim.pdf) & [Web](http://www.cs.sfu.ca/~mori/research/superpixels)\n:ballot_box_with_check:                    | VCCS         | [Ref.](http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Papon_Voxel_Cloud_Connectivity_2013_CVPR_paper.pdf) & [Web](http://pointclouds.org/documentation/tutorials/supervoxel_clustering.php)\n:ballot_box_with_check:                    | POISE        | [Ref.](http://web.engr.oregonstate.edu/~lif/Middle_Child_ICCV15.pdf) & [Web](http://rehg.org/poise/)\n:ballot_box_with_check:                    | VC           | [Ref.](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6186738) & [Web](http://www-personal.umich.edu/~jwangumi/software.html)\n:ballot_box_with_check:                    | ETPS         | [Ref.](http://www.cs.toronto.edu/~yaojian/cvpr15.pdf) & [Web](https://bitbucket.org/mboben/spixel)\n:ballot_box_with_check:                    | ERGC         | [Ref.](https://hal.archives-ouvertes.fr/hal-00945893/document), [Ref.](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025886) & [Web](https://sites.google.com/site/pierrebuyssens/code/ergc)\n\n### Submission\n\nTo keep the benchmark alive, we encourage authors to make their implementations\npublicly available and integrate them into this benchmark. We are happy to help with the\nintegration and update the results published in [1] and on the \n[project page](http://davidstutz.de/projects/superpixel-benchmark/).\nAlso see the [Documentation](docs/README.md) for details.\n\n## License\n\nLicenses for source code corresponding to:\n\nD. Stutz, A. Hermans, B. Leibe. **Superpixels: An Evaluation of the State-of-the-Art.** Computer Vision and Image Understanding, 2018.\n\t\t\nNote that the source code/data is based on other projects for which separate licenses apply, see:\n\n* [Algorithms](docs/ALGORITHMS.md)\n* [Datasets](docs/DATASETS.md)\n\nCopyright (c) 2016-2018 David Stutz, RWTH Aachen University\n\n**Please read carefully the following terms and conditions and any accompanying documentation before you download and/or use this software and associated documentation files (the \"Software\").**\n\nThe authors hereby grant you a non-exclusive, non-transferable, free of charge right to copy, modify, merge, publish, distribute, and sublicense the Software for the sole purpose of performing non-commercial scientific research, non-commercial education, or non-commercial artistic projects.\n\nAny other use, in particular any use for commercial purposes, is prohibited. This includes, without limitation, incorporation in a commercial product, use in a commercial service, or production of other artefacts for commercial purposes.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nYou understand and agree that the authors are under no obligation to provide either maintenance services, update services, notices of latent defects, or corrections of defects with regard to the Software. The authors nevertheless reserve the right to update, modify, or discontinue the Software at any time.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. You agree to cite the corresponding papers (see above) in documents and papers that report on research using the Software.\n"
 },
 {
  "repo": "mihaibujanca/dynamicfusion",
  "language": "C++",
  "readme_contents": "DynamicFusion\n============\nImplementation of [Newcombe et al. 2015 DynamicFusion paper](http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdf).\n\n#### This project is still in active development and does not yet reproduce the results of the paper accurately.\n\nThe code is based on this [KinectFusion implemenation](https://github.com/Nerei/kinfu_remake)\n\n## Building instructions:\n\n### Ubuntu 16.04\nClone dynamicfusion and dependencies. \n```\ngit clone https://github.com/mihaibujanca/dynamicfusion --recursive\n```\n\nInstall NVIDIA drivers.\n- Enable NVidia drivers (Search / Additional Drivers) selecting:\n\t\"Using NVIDIA binary driver - version 375.66 from nvidia-375 (proprietary, tested)\"\n\t\"Using processor microcode firmware for Intel CPUs from intel-microcode (proprietary)\"\n- Restart pc to complete installation\n\nAlternatively a good tutorial with some common issues covered can be found [here](\n              https://askubuntu.com/a/61433/167689).\n\nFor fresh installs (this assumes you cloned your project in your home directory!):\n```\nchmod +x build.sh\n./build.sh\n```\n\nIf you are not on a fresh install, check `build.sh` for building instructions and dependencies.\n\nIf you want to build the tests as well, set `-DBUILD_TESTS=ON`.\\\nTo save frames showing the reconstruction progress, pass `-DSAVE_RECONSTRUCTION_FRAMES=ON`. The frames will be saved in `<project_root>/output`\n\nTo build documentation, go to the project root directory and execute\n```\ndoxygen -g\ndoxygen Doxyfile\n```\n\n\n### Running\n```\n./download_data \n./build/bin/dynamicfusion data/umbrella\n```\n\n### Windows\nDependencies:\n* CUDA 5.0 or higher\n* OpenCV 2.4.8 or higher (modules opencv_core, opencv_highgui, opencv_calib3d, opencv_imgproc, opencv_viz). Make sure that WITH_VTK flag is enabled in CMake during OpenCV configuration.\n* Boost (libraries system, filesystem and program options. Only used in the demo. Tested with [1.64.0](http://www.boost.org/users/history/version_1_64_0.html))\n* Ceres solver (Tested with version [1.13.0](http://ceres-solver.org/ceres-solver-1.13.0.tar.gz))\n\nImplicit dependency (needed by opencv_viz):\n* VTK 5.8.0 or higher\n* SuiteSparse, BLAS and LAPACK for ceres\nOptional dependencies:\n* GTest for testing\n* Doxygen for documentation\n* OpenNI v1.5.4 for getting input straight from a kinect device.\n\n[Install NVIDIA drivers](https://www.geforce.com/drivers) and [CUDA](https://developer.nvidia.com/cuda-downloads)\n* [Install LAPACK](http://icl.cs.utk.edu/lapack-for-windows/lapack/).\n* [Install VTK](http://www.vtk.org/download/) (download and build from source)\n* [Install OpenCV](http://docs.opencv.org/3.2.0/d3/d52/tutorial_windows_install.html).  \n* [Install Boost](http://www.boost.org/users/download/)\n\n \nOptionals:\n* [Doxygen](http://www.stack.nl/~dimitri/doxygen/download.html)\n* [GTest](https://github.com/google/googletest) \n* [OpenNI]( http://pointclouds.org/downloads/windows.html)\n\n[Download the dataset](http://lgdv.cs.fau.de/uploads/publications/data/innmann2016deform/umbrella_data.zip).\\\nCreate a `data` folder inside the project root directory. \\\nUnzip the archive into `data` and remove any files that are not .png. \\\nInside `data`, create directories `color` and `depth`, and move color and depth frames to their corresponding folders.\n\nTo use with .oni captures or straight from a kinect device, use `./build/bin/dynamicfusion_kinect <path-to-oni>` or `./build/bin/dynamicfusion_kinect <device_id>` \n\n---\nNote: currently, the frame rate is too low (10s / frame) to be able to cope with live inputs, so it is advisable that you capture your input first.\n\n## References\n[DynamicFusion project page](http://grail.cs.washington.edu/projects/dynamicfusion/)\n\n```\n@InProceedings{Newcombe_2015_CVPR,\nauthor = {Newcombe, Richard A. and Fox, Dieter and Seitz, Steven M.},\ntitle = {DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2015}\n}\n```\n\nThe example dataset is taken from the [VolumeDeform project](http://lgdv.cs.fau.de/publications/publication/Pub.2016.tech.IMMD.IMMD9.volume_6/).\n```\n@inbook{innmann2016volume,\nauthor = \"Innmann, Matthias and Zollh{\\\"o}fer, Michael and Nie{\\ss}ner, Matthias and Theobalt, Christian \n         and Stamminger, Marc\",\neditor = \"Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max\",\ntitle = \"VolumeDeform: Real-Time Volumetric Non-rigid Reconstruction\",\nbookTitle = \"Computer Vision -- ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\n            October 11-14, 2016, Proceedings, Part VIII\",\nyear = \"2016\",\npublisher = \"Springer International Publishing\",\naddress = \"Cham\",\npages = \"362--379\",\nisbn = \"978-3-319-46484-8\",\ndoi = \"10.1007/978-3-319-46484-8_22\",\nurl = \"http://dx.doi.org/10.1007/978-3-319-46484-8_22\"\n}\n```\n"
 },
 {
  "repo": "jetsonhacks/buildOpenCVTX2",
  "language": "Shell",
  "readme_contents": "# buildOpenCVTX2\nBuild and install OpenCV for the NVIDIA Jetson TX2\n\nThese scripts build OpenCV version 3.4 for the NVIDIA Jetson TX2 Development Kit. Please see Releases/Tags for earlier versions.\n\nOpenCV is a rich environment which can be configured in many different ways. You should configure OpenCV for your needs, by modifying the build file \"buildOpenCV.sh\". Note that selecting different options in OpenCV may also have additional library requirements which are not included in these scripts. Please read the notes below for other important points before installing.\n\nThe buildOpenCV script has two optional command line parameters:\n\n<ul>\n<li>-s | --sourcedir   Directory in which to place the opencv sources (default $HOME)</li>\n<li>-i | --installdir  Directory in which to install opencv libraries (default /usr/local)</li>\n</ul>\n\nTo run the the build file:\n\n$ ./buildOpenCV.sh -s &lt;file directory&gt;\n\nThis example will build OpenCV in the given file directory and install OpenCV in the /usr/local directory.\n\nThe folder ~/opencv and (optional) ~/opencv_extras contain the source, build and extra data files. If you wish to remove them after installation, a convenience script is provided:\n\n$ ./removeOpenCVSources.sh -d &lt;file directory&gt;\n\nwhere the &lt;file directory&gt; contains the OpenCV source.\n\nThe folder ~/opencv and ~/opencv_extras contain the source, build and extra data files. If you wish to remove them after installation, a convenience script is provided:\n\n$ ./removeOpenCVSources.sh\n\n<h3>Packaging</h3>\nAn alternative build script, buildAndPackageOpenCV.sh , will build the OpenCV package as described above and the build .deb files using the standard OpenCV mechanism defined using the CPACK_BINARY_DEB=ON in the OpenCV Make file. See the script.\n\nThe buildAndPackageOpenCV script has two optional command line parameters:\n\n<ul>\n<li>-s | --sourcedir   Directory in which to place the opencv sources (default $HOME)</li>\n<li>-i | --installdir  Directory in which to install opencv libraries (default /usr/local)</li>\n</ul>\n\nTo run the the build file:\n\n$ ./buildAndPackageOpenCV.sh -s &lt;file directory&gt;\n\nThis example will build OpenCV in the given file directory and install OpenCV in the /usr/local directory.\n\nThe corresponding .deb files will be in the &lt;file directory&gt;/opencv/build directory in .deb file and compressed forms. \n\n<h4>Installing .deb files</h4>\n\nTo install .deb files:\n\nSwitch to the directory where the .deb files are located. Then:\n\n<blockquote>\n$ sudo dpkg -i OpenCV-&lt;OpenCV Version info&gt;-aarch64-libs.deb\n\n<em>For example: $ sudo dpkg -i OpenCV-3.4.1-1-g75a2577-aarch64-libs.deb</em> \n\n$ sudo apt-get install -f\n\n$ sudo dpkg -i OpenCV-&lt;OpenCV Version info&gt;-aarch64-dev.deb \n\n$ sudo dpkg -i OpenCV-&lt;OpenCV Version info&gt;-aarch64-python.deb </blockquote>\n\nThe libraries will be installed in /usr/lib\n\nBinaries are in /usr/bin\n\nopencv.pc is in /usr/lib/pkgconfig\n\n<strong>Package Notes: </strong>\n<ul><li>The build process default installation is in /usr/local\nNote that the .deb file install into /usr</li>\n<li>After installation, the dpkg/apt name does not include version information, e.g. the name is opencv-libs</li>\n</ul>\n\n## Notes\nThere may be issues if different version of OpenCV are installed. JetPack normally installs OpenCV in the /usr folder. You will need to consider if this is appropriate for your application. It is important to realize that many packages may rely on OpenCV. The standard installation by JetPack places the OpenCV libraries in the /usr directory. \n\nYou may consider removing OpenCV installed by JetPack before performing this script installation:\n\n$ sudo apt-get purge libopencv*\n\nWith this script release, the script now installs OpenCV in /usr/local. Earlier versions of this script installed in /usr. You may have to set your include and libraries and/or PYTHONPATH to point to the new version. See the Examples folder. Alternatively, you may want to change the script to install into the /usr directory.\n\nThe Jetson is an aarch64 machine, which means that the OpenCV configuration variable ENABLE_NEON is ignored. The compiler includes NEON support for all machines with aarch64 architecture.\n\nThese scripts rely on OpenCV finding the correct CUDA version, instead of setting it manually.\n\nSpecial thanks to Daniel (Github user @dkoguciuk) for script cleanup.\n\n\n## References\n\nMost of this information is derived from:\n\nhttp://docs.opencv.org/3.2.0/d6/d15/tutorial_building_tegra_cuda.html\n\nhttps://devtalk.nvidia.com/default/topic/965134/opencv-3-1-compilation-on-tx1-lets-collect-the-quot-definitive-quot-cmake-settings-/?offset=3\n\n## Release Notes\nJune 2018\n* L4T 28.2\n* CUDA 9\n* OpenCV 3.4\n* Added command line arguments to set source and installation directories\n* Add a script to build OpenCV .deb packages.\n* Add upstream patch for C library compilation issues\n\nMay 2018\n* L4T 28.2\n* CUDA 9\n* OpenCV 3.4\n* OpenGL support added to build script\n* Fast Math support (cuBLAS) added\n* Supports both Python 2 and Python 3\n* Canny Detection example supports built-in camera and USB camera. See the Examples folder\n\nSeptember 2017\n* L4T 28.1\n* CUDA 8\n* OpenCV 3.3\n* GStreamer support added to build script\n* GStreamer OpenCV examples using the Jetson onboard camera \n\nApril 2017\n* Initial Release\n* L4T 27.1\n* OpenCV 3.2\n\n## License\nMIT License\n\nCopyright (c) 2017-2018 Jetsonhacks\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n \n"
 },
 {
  "repo": "genicam/harvesters",
  "language": "Python",
  "readme_contents": ".. figure:: https://user-images.githubusercontent.com/8652625/40595190-1e16e90e-626e-11e8-9dc7-207d691c6d6d.jpg\n    :align: center\n    :alt: The Harvesters\n\n    Pieter Bruegel the Elder, The Harvesters, 1565, (c) The Metropolitan Museum of Art\n\n.. image:: https://readthedocs.org/projects/harvesters/badge/?version=latest\n    :target: https://harvesters.readthedocs.io/en/latest/?badge=latest\n\n.. image:: https://img.shields.io/pypi/v/harvesters.svg\n    :target: https://pypi.org/project/harvesters\n\n.. image:: https://zenodo.org/badge/133908095.svg\n   :target: https://zenodo.org/badge/latestdoi/133908095\n\n----\n\n############\nIntroduction\n############\n\nHello everyone. I'm Kazunari, the author of Harvester and a technical contributor to GenICam.\n\nSince I opened the source code of Harvester in May 2018, so many people tried out that and gave me very positive feedback that improves Harvester. The number of feedback and the number of people who I discussed with was much more than I expected what I had in the beginning and it truly was one of the exciting experiences I ever had in my professional career.\n\nThe original motivation that drove me to develop Harvester was like this: Until I got the idea of Harvester, I had to learn and adapt to popular proprietary 3rd party image acquisition/processing libraries from scratch every time even though I just wanted to acquire an image. To be honest, I had felt it's a bit annoying. A straightforward solution to the issue was to have a unified image acquisition library. In addition, as a guy who likes to work with Python, I wanted to make it a Python module to boost productivity.\n\nI believe that Harvester can help you to concentrate on image processing that you really have to be responsible for. Of course, you would have to implement the algorithm again using other sophisticated and powerful proprietary libraries to optimize the performance in reality. However, (this is very important so let me stress this) Harvester is just a productive sandbox that encapsulates the image acquisition process and does not intend to take the place of such superior image processing libraries because they were designed for exactly that purpose. Harvester just wants to shorten the time you spend to realize your brilliant ideas as much as possible. With this meaning, Harvester can be considered as a tool that helps you to quickly iterate the prototyping process. Note that it is vital for designers because the prototyping process defines the basic quality of the final product. Fortunately, Harvester has got firm support from the high-quality GenTL Producers on the market. It means you can smoothly start working with any GenICam compliant cameras that you may want to work with.\n\nSo, everyone, there's no worry anymore. Keep having fun working. Harvester has got your back.\n\nGreetings, Kazunari Kudo.\n\n**************************\nHarvester as Python Module\n**************************\n\nTechincally speaking, Harvester consists of two Python modules, Harvester Core and Harvester GUI, and technically speaking, each library is responsible for the following tasks:\n\n- Harvester Core: Image acquisition & remote device manipulation\n- Harvester GUI: Image data visualization\n\nHarvester consumes image acquisition libraries, so-called GenTL Producers. Just grabbing a GenTL Producer and GenICam compliant machine vision cameras, then Harvester will supply you the acquired image data as `numpy <http://www.numpy.org>`_ array to make your image processing task productive.\n\nYou can freely use, modify, distribute Harvester under `Apache License-2.0 <https://www.apache.org/licenses/LICENSE-2.0>`_ without worrying about the use of your software: personal, internal or commercial.\n\nCurrently, Harvester is being developed by the motivated contributors from all over the world.\n\n***********************\nWhere Is The Name From?\n***********************\n\nHarvester's name is from the great Flemish painter, Pieter Bruegel the Elder's painting so-called \"The Harvesters\". You can see the painting in the top of this page. Harvesters harvest a crop every season that has been fully grown and the harvested crop is passed to the consumers. On the other hand, image acquisition libraries acquire images as their crop and the images are passed to the following processes. We found the similarity between them and decided to name our library Harvester.\n\nApart from anything else, we love its peaceful and friendly name.\n\n----\n\n.. contents:: Table of Contents\n    :depth: 2\n\n**Disclaimer**: All external pictures should have associated credits. If there are missing credits, please tell us, we will correct it. Similarly, all excerpts should be sourced. If not, this is an error and we will correct it as soon as you tell us.\n\n----\n\n******************\nDevelopment Status\n******************\n\nThe Harvester project has started since April 2018 and it's still under development as of October 2018 but many developers and researchers over the world have already confirmed that it is actually usable with the popular GenTL Producers and GenICam compliant cameras from the following companies. We have realized the progress had been brought by all interested people's positive expectation in the machine vision market and we strongly believe it will sustain to the following years. Of course, we will never forget the importance of volunteer companies which provided us their products to test Harvester. Thank you very much!\n\nNote that we as the committee have not prepared any formal certification procedure for Harvester. The following results were presented by Harvester users who confirmed Harvester worked for their use cases. It is true that the following information does not cover whole domain but as a fact it is helpful sometimes at least.\n\n.. list-table::\n    :header-rows: 1\n    :align: center\n\n    - - Company Name\n      - GenTL Producer for CoaXPress\n      - GenTL Producer for GigE Vision\n      - GenTL Producer for USB3 Vision\n      - GenICam compliant cameras\n    - - `Active Silicon <https://www.activesilicon.com/>`_\n      - Worked\n      - Not applicable\n      - Not applicable\n      - Not applicable\n    - - `Adimec <https://www.adimec.com/>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `Allied Vision <https://www.alliedvision.com/en/digital-industrial-camera-solutions.html>`_\n      - Not tested\n      - Not tested\n      - Not tested\n      - Worked\n    - - `Automation Technology <https://www.automationtechnology.de/cms/en/>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `Basler <https://www.baslerweb.com/>`_\n      - Not applicable\n      - Not applicable\n      - Worked\n      - Worked\n    - - `Baumer Optronic <https://www.baumer.com/se/en/>`_\n      - Not applicable\n      - Worked\n      - Worked\n      - Worked\n    - - `CREVIS <http://www.crevis.co.kr/eng/main/main.php>`_\n      - Not applicable\n      - Not tested\n      - Not applicable\n      - Worked\n    - - `CRITICAL LINK <https://www.criticallink.com>`_\n      - Not applicable\n      - Not applicable\n      - Worked\n      - Worked\n    - - `DAHENG VISION <http://en.daheng-image.com/main.html>`_\n      - Not applicable\n      - Worked\n      - Worked\n      - Worked\n    - - `Euresys <https://www.euresys.com/Homepage>`_\n      - Worked\n      - Not tested\n      - Not tested\n      - Not applicable\n    - - `FLIR <https://www.flir.com>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `Gardasoft <http://www.gardasoft.com>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `JAI <https://www.jai.com>`_\n      - Not tested\n      - Worked\n      - Worked\n      - Worked\n    - - `The IMAGING SOURCE <https://www.theimagingsource.com/>`_\n      - Not tested\n      - Not tested\n      - Not tested\n      - Worked\n    - - `Lucid Vision Labs <https://thinklucid.com>`_\n      - Not applicable\n      - Worked\n      - Not applicable\n      - Worked\n    - - `MACNICA Inc. <https://www.macnica.co.jp/en/top>`_\n      - Not tested\n      - Not tested\n      - Worked\n      - Worked\n    - - `MATRIX VISION GmbH <https://www.matrix-vision.com/home-en.html>`_\n      - Not applicable\n      - Worked\n      - Worked\n      - Worked\n    - - `Matrox Imaging <https://matrox.com/en/>`_\n      - Worked\n      - Not applicable\n      - Not applicable\n      - Not applicable\n    - - `OMRON SENTECH <https://sentech.co.jp/en/>`_\n      - Not tested\n      - Not tested\n      - Worked\n      - Worked\n    - - `PCO <https://www.pco-imaging.com/>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Not tested\n    - - `Roboception <https://roboception.com/en/>`_\n      - Not applicable\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `SICK <https://www.sick.com/ag/en/>`_\n      - Not applicable\n      - Worked\n      - Not applicable\n      - Worked\n    - - `Silicon Software <https://silicon.software/>`_\n      - Not tested\n      - Not tested\n      - Not tested\n      - Not applicable\n    - - `STEMMER IMAGING <https://www.stemmer-imaging.com/en/>`_\n      - Not tested\n      - Worked\n      - Worked\n      - Not applicable\n    - - `Teledyne DALSA <http://www.teledynedalsa.com/en/products/imaging/cameras/>`_\n      - Not tested\n      - Not applicable\n      - Not applicable\n      - Worked\n    - - `Vieworks <http://www.vieworks.com/eng/main.html>`_\n      - Not tested\n      - Not applicable\n      - Not applicable\n      - Not tested\n    - - `XIMEA <https://www.ximea.com/>`_\n      - Not tested\n      - Not tested\n      - Not tested\n      - Not tested\n\nPlease don't hesitate to tell us if you have tested Harvester with your GenTL Producer or GenICam compliant device. We will add your company/organization name to the list.\n\n***********\nNeed a GUI?\n***********\n\nWould you like to have a GUI? Harvester has a sister project that is called **Harvester GUI**. Oops, there's no punch line on its name! Please take a look its source repository if you are interested in it:\n\nhttps://github.com/genicam/harvesters_gui\n\n.. image:: https://user-images.githubusercontent.com/8652625/43035346-c84fe404-8d28-11e8-815f-2df66cbbc6d0.png\n    :align: center\n    :alt: Image data visualizer\n\n*************\nAnnouncements\n*************\n\n- **Version 1.2.8**: Resolves issue `#191 <https://github.com/genicam/harvesters/issues/191>`_.\n- **Version 1.2.7**: Resolves issues `#167 <https://github.com/genicam/harvesters/issues/167>`_, `#181 <https://github.com/genicam/harvesters/issues/181>`_, `#183 <https://github.com/genicam/harvesters/issues/183>`_, `#184 <https://github.com/genicam/harvesters/issues/184>`_, `#185 <https://github.com/genicam/harvesters/issues/185>`_, and `#188 <https://github.com/genicam/harvesters/issues/188>`_.\n- **Version 1.2.6**: Reverted the change made for version 1.2.5.\n- **Version 1.2.5**: Resolves issue `#180 <https://github.com/genicam/harvesters/issues/180>`_.\n- **Version 1.2.4**: Resolves issues `#125 <https://github.com/genicam/harvesters/issues/125>`_, `#169 <https://github.com/genicam/harvesters/issues/169>`_, `#172 <https://github.com/genicam/harvesters/issues/172>`_, `#175 <https://github.com/genicam/harvesters/issues/175>`_, and `#178 <https://github.com/genicam/harvesters/issues/178>`_.\n- **Version 1.2.3**: Resolves issue `#165 <https://github.com/genicam/harvesters/issues/165>`_.\n- **Version 1.2.2**: Resolves issue `#146 <https://github.com/genicam/harvesters/issues/146>`_; please let me know if it breaks something on your side. I will revert the change as soon as possible.\n- **Version 1.2.1**: Resolves issues `#145 <https://github.com/genicam/harvesters/issues/145>`_, `#155 <https://github.com/genicam/harvesters/issues/155>`_, `#157 <https://github.com/genicam/harvesters/issues/157>`_, and `#159 <https://github.com/genicam/harvesters/issues/159>`_.\n- **Version 1.2.0**: Resolves issues `#127 <https://github.com/genicam/harvesters/issues/127>`_, `#131 <https://github.com/genicam/harvesters/issues/131>`_, `#141 <https://github.com/genicam/harvesters/issues/141>`_, and `#142 <https://github.com/genicam/harvesters/issues/142>`_. The fix for ticket #131 improves the performance of both stability and capable acquisition rate of the image acquisition.\n- **Version 1.1.1**: Resolves issue `#126 <https://github.com/genicam/harvesters/issues/126>`_.\n- **Version 1.1.0**: Resolves issue `#120 <https://github.com/genicam/harvesters/issues/120>`_.\n- **Version 1.0.5**: Resolves issue `#124 <https://github.com/genicam/harvesters/issues/124>`_.\n- **Version 1.0.4**: Partly resolves issue `#121 <https://github.com/genicam/harvesters/issues/121>`_.\n\nOther older releases should be found at `Milestones page <https://github.com/genicam/harvesters/milestones>`_ on GitHub.\n\n################\nOnline Resources\n################\n\n****************\nAsking Questions\n****************\n\nWe have prepared an FAQ page. Perhaps your issue could be resolved just reading through it:\n\nhttps://github.com/genicam/harvesters/wiki/FAQ\n\nIf any article was not mentioning about the issue you are facing, please try to visit the following page and check if there's a ticket that is relevant to the issue. If nothing has been mentioned yet, feel free to create an issue ticket so that we can support you:\n\nhttps://github.com/genicam/harvesters/issues\n\n***************\nImportant Links\n***************\n\n.. list-table::\n\n    - - Documentation\n      - https://harvesters.readthedocs.io/en/latest/\n    - - Digital Object Identifier\n      - https://zenodo.org/record/3554804#.Xd4HSi2B01I\n    - - EMVA website\n      - https://www.emva.org/standards-technology/genicam/genicam-downloads/\n    - - Harvester GUI\n      - https://github.com/genicam/harvesters_gui\n    - - Issue tracker\n      - https://github.com/genicam/harvesters/issues\n    - - PyPI\n      - https://pypi.org/project/harvesters/\n    - - Source repository\n      - https://github.com/genicam/harvesters\n\n***************\nGenTL Producers\n***************\n\nAs of today, we have tested Harvester with the following GenTL Producers and it definitely is the shortest way to get one from the following list to get Harvester working with tangible machine vision cameras:\n\n.. list-table::\n    :header-rows: 1\n    :align: center\n\n    - - Company Name\n      - SDK Name\n      - Camera Manufacturer Free\n    - - Basler AG\n      - `Pylon <https://www.baslerweb.com/en/products/software/basler-pylon-camera-software-suite/>`_\n      - No\n    - - Baumer Optronic\n      - `Baumer GAPI SDK <https://www.baumer.com/ae/en/product-overview/image-processing-identification/software/baumer-gapi-sdk/c/14174>`_\n      - Yes for GEV and No for U3V\n    - - DAHENG VISION\n      - `MER Galaxy View <http://en.daheng-image.com/products_list/&pmcId=a1dda1e7-5d40-4538-9572-f4234be49c9c.html>`_\n      - No\n    - - JAI\n      - `JAI SDK <https://www.jai.com/support-software/jai-software>`_\n      - Yes\n    - - MATRIX VISION GmbH\n      - `mvIMPACT Acquire <http://static.matrix-vision.com/mvIMPACT_Acquire/>`_\n      - Yes\n    - - OMRON SENTECH\n      - `SentechSDK <https://sentech.co.jp/en/data/>`_\n      - No\n    - - STEMMER IMAGING\n      - `Common Vision Blox <https://www.commonvisionblox.com/en/cvb-download/>`_\n      - Yes\n\nYou might be able to directly download one at their website but please note that perhaps some of them could require you to register your information to get one. In addition, some GenTL Producers might block you to connect other competitors' cameras.\n\n###########\nTerminology\n###########\n\nBefore start talking about the detail, let's take a look at some important terminologies that frequently appear in this document. These terminologies are listed as follows:\n\n* **The GenApi-Python Binding**: A Python module that communicates with the GenICam reference implementation.\n\n* **A GenTL Producer**: A library that has C interface and offers consumers a way to communicate with cameras over physical transport layer dependent technology hiding the detail from the consumer.\n\n* **The GenTL-Python Binding**: A Python module that communicates with GenTL Producers.\n\n* **Harvester**: A Python module that consists of Harvester Core and Harvester GUI.\n\n* **Harvester Core**: A part of Harvester that works as an image acquisition engine.\n\n* **Harvester GUI**: A part of Harvester that works as a graphical user interface of Harvester Core.\n\n* **A GenICam compliant device**: It's typically a camera. Just involving the GenICam reference implementation, it offers consumers a way to dynamically configure/control the target remote devices.\n\nThe following diagram shows the hierarchy and relationship of the relevant modules:\n\n.. figure:: https://user-images.githubusercontent.com/8652625/48105146-a3b0e700-e279-11e8-8a3f-f94372aeff37.png\n    :align: center\n    :alt: Module hierarchy\n\n###############\nGetting Started\n###############\n\nIn this section, we will learn how to instruct procedures to get Harvester work.\n\n*******************\nSystem Requirements\n*******************\n\nThe following software modules are required to get Harvester working:\n\n* Either of Python 3.4, 3.5, 3.6, or 3.7 (**Only 64bit versions** are supported as of October 2018.)\n\nIn addition, please note that we don't supported Cygwin on Windows. This restriction is coming from a fact that the GenICam reference implementation has not supported it.\n\nIn addition, you will need the following items to let Harvester make something meaningful:\n\n* GenTL Producers\n* GenICam compliant machine vision cameras\n\nHarvester has been confirmed it works with the following 64-bit operating systems:\n\n* Fedora 27\n* macOS 10.13\n* Red Hat Enterprise Linux Workstation 7.4\n* Ubuntu 14.04\n* Windows 7\n* Windows 10\n\nNote that it's just a snapshot at a moment. If you are curious to know the reality, just make a try because Harvester is for free!\n\n*****************\nInstalling Python\n*****************\n\nFirst, let's install Python. There are several options for you but I would like to introduce you Anaconda here. You can download Anaconda from the following URL:\n\nhttps://www.anaconda.com/download/\n\nFor Windows, please find a 64-Bit graphical installer that fits your machine and download it. The installation process is straightforward but it could be a bad idea to add the Anaconda Python executable directory to the ``PATH`` environment variable because it means your system begins to use your Anaconda Python instead of the system Python that had been already installed before you installed Anaconda Python.\n\nTo not letting Anaconda Python interfere in your system Python, not adding Anaconda Python to the ``PATH`` and you should always launch ``Anaconda Prompt`` in the ``Anaconda3 (64-bit)`` folder from the Windows's start menu. It will automatically kick up the Anaconda Python so that you can immediately use the functionality that Anaconda provides you.\n\nOn Linux machines, you can make it with the following steps. First, please type the following command. Invoking that command, you will be able to use the ``conda`` command which allows you to activate an environment; note that the following code has been modified for my setup on a macOS machine:\n\n.. code-block:: shell\n\n    $ echo \". /Users/kznr/anaconda3/etc/profile.d/conda.sh\" >> ~/.bash_profile\n\nThen activate the root environment:\n\n.. code-block:: shell\n\n    $ conda activate\n\nNow you can start working for installing Harvester.\n\nCreating an Environment\n=======================\n\nAfter installing a Python, let's create an isolated environment where does not interfere in your system. An environment is very helpful for developers because everything will be okay just deleting the environment if you completely corrupted it by accident. Please imagine a case where you corrupt the system-wide Python. It's obviously a nightmare and it will enforce you to spend some days to recover it so it is very recommended to work in an isolated environment when you need to develop something.\n\nAssume we have added the Anaconda Python executable directory to the ``PATH`` environment variable. To create an environment on a UNIX system, please type the following command; we name the environment ``genicam``:\n\n.. code-block:: shell\n\n    $ conda create -n genicam python=3.6\n\nWe have created an environment ``genicam`` with Python ``3.6``. If you prefer to install another version, just change the version number above.\n\nAfter that, we activate the environment to work with Harvester. To activate the environment, type the following command:\n\n.. code-block:: shell\n\n    $ conda activate genicam\n\nIf it works well then you will be able to find ``genicam`` in the shell prompt as follows:\n\n.. code-block:: shell\n\n    (genicam) kznr@Kazunaris-MacBook:~%\n\nThen let's check the version number of Python. To check the version number of Python, type the following command:\n\n.. code-block:: shell\n\n    $ python --version\n\nYou should be able to see the expected version number in its return as follows:\n\n.. code-block:: shell\n\n    Python 3.6.5 :: Anaconda, Inc.\n\nFinally, to deactivate the environment, type the following command:\n\n.. code-block:: shell\n\n    $ conda deactivate\n\nIt's so easy.\n\n***************************\nInstalling a GenTL Producer\n***************************\n\nNow we install a GenTL Producer that works with Harvester. Harvester can't acquire images without it.\n\nToday, many camera manufacturers and software vendors all over the world provide GenTL Producers to support image acquisition using GenICam compliant cameras. However, you should note that some GenTL Producers may block cameras from other competitors. Though it's perfectly legal but we recommend you here to use a GenTL Producer from MATRIX VISION as a one of reliable GenTL Producer for this tutorial because it doesn't block cameras from other competitors. However, please respect their license and give them feedback immediately if you find something to be reported or something that you appreciate. As an open source activity, we would like to pay our best respect to their attitude and their products.\n\nYou can get their SDK from the following URL; please download ``mvIMPACT_Acquire`` and install it.\n\nhttp://static.matrix-vision.com/mvIMPACT_Acquire/2.29.0/\n\nOnce you installed their SDK, you can find the appropriate GenTL Producer just grepping ``*.cti``. Note that Harvester supports only 64-bit version of GenTL Producers as of November 2018.\n\nThis is just for your information but you can find the list of other reliable GenTL Producers `here <https://github.com/genicam/harvesters#gentl-producers>`_.\n\n*************************\nInstalling Harvester Core\n*************************\n\nBefore installing Harvester, let's make sure that you are working in the environment that you created in `the previous chapter <https://github.com/genicam/harvesters#id18>`_.\n\nAfter that, you can install Harvester via PyPI invoking the following command; note that the package name is ``harvesters`` but not ``harvester``; unfortunately, the latter word had been reserved by another project:\n\n.. code-block:: shell\n\n    $ pip install harvesters\n\nFor people who those have already installed it:\n\n.. code-block:: shell\n\n    $ pip install --upgrade harvesters\n\nOr more simply:\n\n.. code-block:: shell\n\n    $ pip install -U harvesters\n\nPerhaps ``pip`` could install cached package. If you want to install the newly dowloaded package, you should invoke the following command:\n\n.. code-block:: shell\n\n    $ pip install -U --no-cache-dir harvesters\n\nThese commands will automatically install the required modules such as ``numpy`` or ``genicam2`` (the Python Binding for the GenICam GenApi & the GenTL Producers) if the module has not yet installed on your environment.\n\nGetting back to the original topic, you could install the latest development version it using ``setup.py`` cloning Harvester from GitHub:\n\n.. code-block:: shell\n\n    $ git clone https://github.com/genicam/harvesters.git && cd harvesters && python setup.py install\n\n#######################\nWorking with Harveseter\n#######################\n\nHarvester Core is an image acquisition engine. No GUI. You can use it as an image acquisition library which acquires images from GenTL Producers through the GenTL-Python Binding and controls the target remote device (it's typically a camera) through the GenApi-Python Binding.\n\nHarvester Core works as a minimalistic front-end for image acquisition. Just importing it from your Python script, you should immediately be able to set images on your table.\n\nYou'll be able to download the these language binding runtime libraries from the `EMVA website <https://www.emva.org/standards-technology/genicam/genicam-downloads/>`_, however, it's not available as of May 2018, because they have not officially released yet. Fortunately they are in the final reviewing process so hopefully they'll be released by the end of 2018.\n\nIf you don't have to care about the display rate for visualizing acquired images, the combination of Harvester Core and `Matplotlib <https://matplotlib.org>`_ might be a realistic option for that purpose.\n\n*********************************\nTasks Harvester Core Does for You\n*********************************\n\nThe main features of Harvester Core are listed as follows:\n\n* Image acquisition through GenTL Producers\n* Multiple loading of GenTL Producers in a single Python script\n* GenICam feature node manipulation of the target remote device\n\nNote that the second item implies you can involve multiple types of transport layers in your Python script. Each transport layer has own advantages and disadvantages and you should choose appropriate transport layers following your application's requirement. You just need to acquire images for some purposes and the GenTL Producers deliver the images somehow. It truly is the great benefit of the GenTL Standard! And of course, not only GenTL Producers but Harvester Core offer you a way to manipulate multiple remote devices in a single Python script with an intuitive manner.\n\nOn the other hand, Harvester Core could be considered as a simplified version of the GenTL-Python Binding; actually, Harvester Core hides it in its back and shows only intuitive interfaces to its clients. Harvester Core just offers you a relationship between you and a remote device. Nothing more. We say it again, just you and a remote device. If you need to manipulate more relevant GenTL modules or have to achieve something over a hardcore way, then you should directly work with the GenTL-Python Binding.\n\n*************************\nHarvester Core on IPython\n*************************\n\nThe following code block shows Harvester Core is running on IPython. An acquired image is delivered as the payload of a buffer and the buffer can be fetched by calling the ``fetch_buffer`` method of the ``ImageAcquirer`` class. Once you get an image you should be able to immediately start image processing. If you're running on the Jupyter notebook, you should be able to visualize the image data using Matplotlib. This step should be helpful to check what's going on your trial in the image processing flow.\n\n.. code-block:: python\n\n    (genicam) kznr@Kazunaris-MacBook:~% ipython\n    Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29)\n    Type 'copyright', 'credits' or 'license' for more information\n    IPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help.\n\n    In [1]: from harvesters.core import Harvester\n\n    In [2]: import numpy as np  # This is just for a demonstration.\n\n    In [3]: h = Harvester()\n\n    In [4]: h.add_file('/Users/kznr/dev/genicam/bin/Maci64_x64/TLSimu.cti')\n\n    In [5]: h.update()\n\n    In [6]: len(h.device_info_list)\n    Out[6]: 4\n\n    In [7]: h.device_info_list[0]\n    Out[7]: (id_='TLSimuMono', vendor='EMVA_D', model='TLSimuMono', tl_type='Custom', user_defined_name='Center', serial_number='SN_InterfaceA_0', version='1.2.3')\n\n    In [8]: ia = h.create_image_acquirer(0)\n\n    In [9]: ia.remote_device.node_map.Width.value = 8\n\n    In [10]: ia.remote_device.node_map.Height.value = 8\n\n    In [11]: ia.remote_device.node_map.PixelFormat.value = 'Mono8'\n\n    In [12]: ia.start_acquisition()\n\n    In [13]: with ia.fetch_buffer() as buffer:\n        ...:     # Let's create an alias of the 2D image component:\n        ...:     component = buffer.payload.components[0]\n        ...:\n        ...:     # Note that the number of components can be vary. If your\n        ...:     # target remote device transmits a multi-part information, then\n        ...:     # you'd get two or more components in the payload. However, now\n        ...:     # we're working with a remote device that transmits only a 2D image.\n        ...:     # So we manipulate only index 0 of the list object, components.\n        ...:\n        ...:     # Let's see the acquired data in 1D:\n        ...:     _1d = component.data\n        ...:     print('1D: {0}'.format(_1d))\n        ...:\n        ...:     # Reshape the NumPy array into a 2D array:\n        ...:     _2d = component.data.reshape(\n        ...:         component.height, component.width\n        ...:     )\n        ...:     print('2D: {0}'.format(_2d))\n        ...:\n        ...:     # Here are some trivial calculations:\n        ...:     print(\n        ...:         'AVE: {0}, MIN: {1}, MAX: {2}'.format(\n        ...:             np.average(_2d), _2d.min(), _2d.max()\n        ...:         )\n        ...:     )\n        ...:\n    1D: [123 124 125 126 127 128 129 130 124 125 126 127 128 129 130 131 125 126\n     127 128 129 130 131 132 126 127 128 129 130 131 132 133 127 128 129 130\n     131 132 133 134 128 129 130 131 132 133 134 135 129 130 131 132 133 134\n     135 136 130 131 132 133 134 135 136 137]\n    2D: [[123 124 125 126 127 128 129 130]\n     [124 125 126 127 128 129 130 131]\n     [125 126 127 128 129 130 131 132]\n     [126 127 128 129 130 131 132 133]\n     [127 128 129 130 131 132 133 134]\n     [128 129 130 131 132 133 134 135]\n     [129 130 131 132 133 134 135 136]\n     [130 131 132 133 134 135 136 137]]\n    AVE: 130.0, MIN: 123, MAX: 137\n\n    In [14]: ia.stop_acquisition()\n\n    In [15]: ia.destroy()\n\n    In [16]: h.reset()\n\n    In [17]: quit\n    (genicam) kznr@Kazunaris-MacBook:~%\n\n######################\nThe Harvester Workflow\n######################\n\n****************\nAcquiring Images\n****************\n\nFirst, let's import Harvester:\n\n.. code-block:: python\n\n    from harvesters.core import Harvester\n\nThen instantiate a Harvester object; we're going to use ``h`` that stands for\nHarvester as its identifier.\n\n.. code-block:: python\n\n    h = Harvester()\n\nAnd load a CTI file; loading a CTI file, you can communicate with the GenTL\nProducer:\n\n.. code-block:: python\n\n    # ATTENTION! Please use the CTI file in the original location!\n\n    # Why? Visit https://github.com/genicam/harvesters/wiki/FAQ and\n    # read \"I pointed out a CTI file but Harvester says the image doesn't\n    # exist (Part 2).\"\n\n    h.add_file('path/to/gentl_producer.cti')\n\nNote that you can add **one or more CTI files** on a single Harvester Core object. To add another CTI file, just repeat calling ``add_file`` method passing another target CTI file:\n\n.. code-block:: python\n\n    h.add_file('path/to/another_gentl_producer.cti')\n\nAnd the following code will let you know the CTI files that have been loaded\non the Harvester object:\n\n.. code-block:: python\n\n    h.files\n\nIn a contrary sense, you can remove a specific CTI file that you have added with the following code:\n\n.. code-block:: python\n\n    h.remove_file('path/to/gentl_producer.cti')\n\nAnd now you have to update the list of remote devices; it fills up your device\ninformation list and you'll select a remote device to control from the list:\n\n.. code-block:: python\n\n    h.update()\n\nThe following code will let you know the remote devices that you can control:\n\n.. code-block:: python\n\n    h.device_info_list\n\nOur friendly GenTL Producer, so called TLSimu, gives you the following information:\n\n.. code-block:: python\n\n    [(unique_id='TLSimuMono', vendor='EMVA_D', model='TLSimuMono', tl_type='Custom', user_defined_name='Center', serial_number='SN_InterfaceA_0', version='1.2.3'),\n     (unique_id='TLSimuColor', vendor='EMVA_D', model='TLSimuColor', tl_type='Custom', user_defined_name='Center', serial_number='SN_InterfaceA_1', version='1.2.3'),\n     (unique_id='TLSimuMono', vendor='EMVA_D', model='TLSimuMono', tl_type='Custom', user_defined_name='Center', serial_number='SN_InterfaceB_0', version='1.2.3'),\n     (unique_id='TLSimuColor', vendor='EMVA_D', model='TLSimuColor', tl_type='Custom', user_defined_name='Center', serial_number='SN_InterfaceB_1', version='1.2.3')]\n\nAnd you create an image acquirer object specifying a target remote device. The image acquirer does the image acquisition task for you. In the following example it's trying to create an acquirer object of the first candidate remote device in the device information list:\n\n.. code-block:: python\n\n    ia = h.create_image_acquirer(0)\n\nOr equivalently:\n\n.. code-block:: python\n\n    ia = h.create_image_acquirer(list_index=0)\n\nYou can connect the same remote device passing more unique information to the method. In the following case, we specify a serial number of the target remote device:\n\n.. code-block:: python\n\n    ia = h.create_image_acquirer(serial_number='SN_InterfaceA_0')\n\nYou can specify a target remote device using properties that are provided through the ``device_info_list`` property of the ``Harvester`` class object. Note that it is invalid if the specifiers gives you two ore more remote devices. Please specify sufficient information so that the combination gives you a unique target remote device.\n\nWe named the image acquirer object ``ia`` in the above example but in a practical occasion, you may give it a purpose oriented name like ``ia_face_detection``. Note that a camera itself does NOT acquirer/receive images but it just transmits them. In a machine vision application, there should be two roles at least: One transmits images and the other acquires them. The ``ImageAcquirer`` class objects play the latter role and it holds a camera as the ``remote_device`` object, the source of images.\n\nAnyway, then now we start image acquisition:\n\n.. code-block:: python\n\n    ia.start_acquisition()\n\nOnce you started image acquisition, you should definitely want to get an image. Images are delivered to the acquirer allocated buffers. To fetch a buffer that has been filled up with an image, you can have 2 options; the first option is to use the ``with`` statement:\n\n.. code-block:: python\n\n    with ia.fetch_buffer() as buffer:\n        # Work with the Buffer object. It consists of everything you need.\n        print(buffer)\n        # The buffer will automatically be queued.\n\nHaving that code, the fetched buffer is automatically queued once the code step out from the scope of the ``with`` statement. It's prevents you to forget queueing it by accident. The other option is to manually queue the fetched buffer by yourself:\n\n.. code-block:: python\n\n    buffer = ia.fetch_buffer()\n    print(buffer)\n    # Don't forget to queue the buffer.\n    buffer.queue()\n\nIn this option, again, please do not forget that you have to queue the buffer by yourself. If you forget queueing it, then you'll lose a buffer that could be used for image acquisition. Everything is up to your design, so please choose an appropriate way for you. In addition, once you queued the buffer, the Buffer object will be obsolete. There's nothing to do with it.\n\nOkay, then you would stop image acquisition with the following code:\n\n.. code-block:: python\n\n    ia.stop_acquisition()\n\nAnd the following code disconnects the connecting remote device from the image acquirer; you'll have to create an image acquirer object again when you have to work with a remote device:\n\n.. code-block:: python\n\n    ia.destroy()\n\nIf you finished working with the ``Harvester`` object, then release the acquired resources calling the ``reset`` method:\n\n.. code-block:: python\n\n    h.reset()\n\nNow you can quit the program! Please not that ``Harvester`` and ``ImageAcquirer`` also support the ``with`` statement. So you may write program as follows:\n\n.. code-block:: python\n\n    with Harvester() as h:\n        with h.create_image_acquirer(0) as ia:\n            # Work, work, and work with the ia object.\n            # The ia object will automatically call the destroy method\n            # once it goes out of the block.\n\n        # The h object will automatically call the reset method\n        # once it goes out of the block.\n\nThis way prevents you forget to release the acquired external resources. If this notation doesn't block your use case then you should rely on the ``with`` statement.\n\n***********************************\nReshaping a NumPy Array as an Image\n***********************************\n\nWe have learned how to acquire images from a target remote device through an ``ImageAcquirer`` class object. In this section, we will learn how to reshape the acquired image into another that can be used by your application.\n\nFirst, you should know that Harvester Core returns you an image as a 1D NumPy array.\n\n.. code-block:: python\n\n    buffer = ia.fetch_buffer()\n    _1d = buffer.payload.components[0].data\n\nPerhaps you may expect to have it as a 2D array but Harvester Core doesn't in reality because if Harvester Core provides an image as a specific shape, then it could limit your algorithm that you can apply to get the image that fits to your expected shape. Instead, Harvester Core provides you an image as a 1D array and also provides you required information that you would need while you're reshaping the original array to another.\n\nThe following code is an except from Harvester GUI that reshapes the source 1D array to another to draw it on the VisPy canvas. VisPy canvas takes ``content`` as an image to draw:\n\n.. code-block:: python\n\n    from harvesters.util.pfnc import mono_location_formats, \\\n        rgb_formats, bgr_formats, \\\n        rgba_formats, bgra_formats\n\n    payload = buffer.payload\n    component = payload.components[0]\n    width = component.width\n    height = component.height\n    data_format = component.data_format\n\n    # Reshape the image so that it can be drawn on the VisPy canvas:\n    if data_format in mono_location_formats:\n        content = component.data.reshape(height, width)\n    else:\n        # The image requires you to reshape it to draw it on the\n        # canvas:\n        if data_format in rgb_formats or \\\n                data_format in rgba_formats or \\\n                data_format in bgr_formats or \\\n                data_format in bgra_formats:\n            #\n            content = component.data.reshape(\n                height, width,\n                int(component.num_components_per_pixel)  # Set of R, G, B, and Alpha\n            )\n            #\n            if data_format in bgr_formats:\n                # Swap every R and B:\n                content = content[:, :, ::-1]\n        else:\n            return\n\nNote that ``component.num_components_per_pixel`` returns a ``float`` so please don't forget to cast it when you pass it to the ``reshape`` method of NumPy array. If you try to set a ``float`` then the method will refuse it.\n\nIt's not always but sometimes you may have to handle image formats that require you to newly create another image calculating each pixel component value referring to the pixel location. To help such calculation, ``Component2DImage`` class provides the ``represent_pixel_location`` method to tell you the 2D pixel location that corresponds to the pixel format. The pixel location is defined by Pixel Format Naming Convention, PFNC in short. The array that is returned by the method is a 2D NumPy array and it corresponds to the model that is defined by PFNC.\n\n.. code-block:: python\n\n    pixel_location = component.represent_pixel_location()\n\nThe 2D array you get from the method is equivalent to the definition that is given by PFNC. The following screenshot is an excerpt from the PFNC 2.1:\n\n.. image:: https://user-images.githubusercontent.com/8652625/47624017-dad91700-db5a-11e8-9f87-6f383c0c6627.png\n    :align: center\n    :alt: The definition of the pixel location of LMN422 formats\n\nFor example, if you acquired a YCbCr422_8 format image, then the first and the second rows of ``pixel_location`` would look as follows; ``L`` is used to denote the 1st component, ``M`` is for the 2nd, and ``N`` is for the 3rd, and they correspond to ``Y``, ``Cb``, and ``Cr`` respectively; in the following description, for a given pixel, the first index represents the row number and the second index represents the column number and note that the following index notation is based on one but not zero though you will use the zero based notation in your Python script:\n\n.. code-block:: python\n\n    [Y11, Cb11, Y12, Cr11, Y13, Cb13, Y14, Cr13, ...]\n    [Y21, Cb21, Y22, Cr21, Y23, Cb23, Y24, Cr23, ...]\n\nHaving that pixel location, you should be able to convert the color space of each row from YCbCr to RGB.\n\n.. code-block:: python\n\n    import numpy as np\n    # Create the output array that has been filled up with zeros.\n    rgb_2d = np.zeros(shape=(height, width, 3), dtype='uint8')\n    # Calculate each pixel component using pixel_location.\n    # Calculation block follows:\n    #     ...\n\nFor example, if you have an 8 bits YCbCr709 image, then you can get the RGB values of the first pixel calculating the following formula:\n\n.. image:: https://user-images.githubusercontent.com/8652625/47624981-298bae80-db65-11e8-8f78-53b188f22f53.png\n    :align: center\n    :alt: \\begin{align*} R_{11} &= 1.16438 (Y_{11} - 16) &                           & + 1.79274 (Cr_{11} - 128) \\\\G_{11} &= 1.16438 (Y_{11} - 16) & - 0.21325 (Cb_{11} - 128) & - 0.53291 (Cr_{11} - 128) \\\\B_{11} &= 1.16438 (Y_{11} - 16) & - 0.21240 (Cb_{11} - 128) \\\\\\end{align*}\n\nSimilarly, you can get the RGB values of the second pixel calculating the following formula:\n\n.. image:: https://user-images.githubusercontent.com/8652625/47625009-6657a580-db65-11e8-900d-f84f70e055a5.png\n    :align: center\n    :alt: \\begin{align*} R_{12} &= 1.16438 (Y_{12} - 16) &                           & + 1.79274 (Cr_{11} - 128) \\\\G_{12} &= 1.16438 (Y_{12} - 16) & - 0.21325 (Cb_{11} - 128) & - 0.53291 (Cr_{11} - 128) \\\\B_{11} &= 1.16438 (Y_{11} - 16) & - 0.21240 (Cb_{11} - 128) \\\\\\end{align*}\n\nOnce you finished filling up each pixel with a set of RGB values, then you'll be able to handle it as a RGB image but not a YCbCr image.\n\nYou can download the standard document of PFNC at the `EMVA website <https://www.emva.org/standards-technology/genicam/genicam-downloads/>`_.\n\n**********************************\nManipulating GenICam Feature Nodes\n**********************************\n\nProbably almost of the Harvester users would be interested in manipulating GenIcam feature nodes through Harvester. Let's assume that we are going to control a GenICam feature node called ``Foo``.\n\nTo get the value of ``Foo``, we code as follows:\n\n.. code-block:: python\n\n    a = ia.remote_device.node_map.Foo.value\n\nOn the other hand, if ``Foo`` is an Integer node then we code as follows to set a value:\n\n.. code-block:: python\n\n    ia.remote_device.node_map.Foo.value = 42\n\nIf ``Foo`` is a Boolean node, then you code as follows:\n\n.. code-block:: python\n\n    ia.remote_device.node_map.Foo.value = True\n\nOr if ``Foo`` is an Enumeration node, then you code as follows; it also works for a case where Foo is a String node:\n\n.. code-block:: python\n\n    ia.remote_device.node_map.Foo.value = 'Bar'\n\nIf ``Foo`` is a Command node, then you can execute the command with the following\n\n.. code-block:: python\n\n    ia.remote_device.node_map.Foo.execute()\n\nThere you can dive much more deeper in the GenICam GenApi but the description above would be sufficient for a general use.\n\nAh, one more thing. You may want to know the available GenICam feature nodes in the target remote physical device. In such a case, you can probe them calling the ``dir`` function as follows:\n\n.. code-block:: python\n\n    dir(ia.remote_device.node_map)\n\nYou should be able to find (probably) familiar feature names in the output.\n\n########\nAppendix\n########\n\n*********************\nOpen Source Resources\n*********************\n\nHarvester Core uses the following open source libraries/resources:\n\n* Pympler\n\n  | License: `Apache License, Version 2.0 <https://www.apache.org/licenses/LICENSE-2.0.html>`_\n  | Copyright (c) Jean Brouwers, Ludwig Haehne, Robert Schuppenies\n\n  | https://pythonhosted.org/Pympler/\n  | https://github.com/pympler/pympler\n  | https://pypi.org/project/Pympler/\n\n* Versioneer\n\n  | License: `The Creative Commons \"Public Domain Dedication\" license  (CC0-1.0) <https://creativecommons.org/publicdomain/zero/1.0/>`_\n  | Copyright (c) 2018 Brian Warner\n\n  | https://github.com/warner/python-versioneer\n\n***************\nAcknowledgement\n***************\n\nThe initial idea about Harvester suddenly came up to a software engineer, Kazunari Kudo's head in the early April of year 2018 and he immediately decided to bring the first prototype to the International Vision Standards Meeting, IVSM in short, that was going to be held in Frankfurt am Main in the following early May. During the Frankfurt IVSM, interested engineers tried out Harvester and confirmed it really worked using commercial machine vision cameras provided by well-known machine vision camera manufacturers in the world. Having that fact, the attendees of the IVSM warmly welcomed Harvester.\n\nThe following individuals have directly or indirectly contributed to the development activity of Harvester or encouraged the developers by their thoughtful warm words; they are our respectable wonderful colleagues:\n\nRod Barman, Stefan Battmer, David Beek, Jan Becvar, David Bernecker, Chris Beynon, Eric Bourbonnais, Benedikt Busch, George Chamberlain, Thomas Detjen, Friedrich Dierks, Dana Diezemann, Emile Dodin, Reynold Dodson, Sascha Dorenbeck, Jozsa Elod, Erik Eloff, Katie Ensign, Andreas Ertl, James Falconer, Werner Feith, Maciej Gara, Andreas Gau, Sebastien Gendreau, Francois Gobeil, Werner Goeman, Jean-Paul Goglio, Markus Grebing, Eric Gross, Ioannis Hadjicharalambous, Uwe Hagmaier, Tim Handschack, Christopher Hartmann, Reinhard Heister, Gerhard Helfrich, Jochem Herrmann, Heiko Hirschmueller, Tom Hopfner, David Hoese, Karsten Ingeman Christensen, Severi Jaaskelainen, Mattias Johannesson, Mark Jones, Mattias Josefsson, Martin Kersting, Stephan Kieneke, Tom Kirchner, Lutz Koschorreck, Frank Krehl, Maarten Kuijk, Max Larin, Ralf Lay, Min Liu, Sergey Loginonvskikh, Thomas Lueck, Alain Marchand, Rocco Matano, Masahide Matsubara, Stephane Maurice, Robert McCurrach, Mike Miethig, Thies Moeller, Roman Moie, Katsura Muramatsu, Marcel Naggatz, Hartmut Nebelung, Damian Nesbitt, Quang Nhan Nguyen, Klaus-Henning Noffz, Neerav Patel, Jan Pech, Merlin Plock, Joerg Preckwinkel, Benjamin Pussacq, Dave Reaves, Thomas Reuter, Gordon Rice, Andreas Rittinger, Ryan Robe, Nicolas P. Rougier, Felix Ruess, Matthias Schaffland, Michael Schmidt, Jan Scholze, Martin Schwarzbauer, Rupert Stelz, Madhura Suresh, Chendra Hadi Suryanto, Andrew Wei Chuen Tan, Timo Teifel, Albert Theuwissen, Laval Tremblay, Tim Vlaar, Silvio Voitzsch, Stefan Von Weihe, Frederik Voncken, Roman Wagner, Ansger Waschki, Anne Wendel, Michael Williamson, Jean-Michel Wintgens, Manfred Wuetschner, Jang Xu, Christoph Zierl, Sebastian Yap, and Juraj Zopp\n\n******************************\nColumn: Seeing and Recognition\n******************************\n\nThe following is a short column that was written by one of my favorite photographers/philosophers, Katsura Muramatsu. Even though the column was exclusively dedicated to her exhibition called \"Natura naturans\", she generously allowed me to excerpt the column for this Harvester project. It would give us an opportunity to thinking about seeing and recognition we regularly do. To me, at least, they sound like a habit on which we unconsciously premise when we do machine vision: We tend to see everything in a way that we want to interpret. Of course, I would like to take the fact in a positive way though.\n\n*\"When we talk to someone, we implicitly or explicitly try to find a piece of evidence in his/her eyes that he/she is alive. The same situation happens in a case where we face a stuffed animal and the face, or especially the eyes play a much more important role rather than its fur or other parts. When people visit this place, the museum where I took photos of these stuffed animals, they would feel that they are seen by the animals rather than they see the animals even though the animals are not alive anymore. In fact, the eyes of the stuffed animals are made of glass or other materials such as plastic. I frequently ask myself why we felt that we were seen through their eyes made of those materials.\" - Katsura Muramatsu*\n\n.. figure:: https://user-images.githubusercontent.com/8652625/65650928-c261cd00-e047-11e9-9ce3-972461c3e15d.jpg\n    :align: center\n    :alt: Ordo: Eastern Wolf\n\n    Title: \"Ordo: Eastern Wolf\" (2018)\n    \n    \u00a9 Katsura Muramatsu All Rights Reserved\n\n    http://hellerraum.nobody.jp\n"
 },
 {
  "repo": "yunchih/ORB-SLAM2-GPU2016-final",
  "language": "C++",
  "readme_contents": "# ORB-SLAM2-GPU\nThis is a fork of Raul Mur-Artal's [ORB-SLAM2](https://github.com/raulmur/ORB_SLAM2), on which we rewrite hot paths with CUDA. Our optimization enables us to run the algorithm in **real time** on a Nvidia's Jetson TX1.\n\n- [Project presentation website](http://yunchih.github.io/ORB-SLAM2-GPU2016-final/)\n- [Demo video on youtube](https://www.youtube.com/watch?v=p77hLLRfBGQ)\n\n**Following is from the original README of ORB-SLAM2**\n\n## Introduction\n\nORB-SLAM2 is a real-time SLAM library for **Monocular**, **Stereo** and **RGB-D** cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/eval_odometry.php) as stereo or monocular, and in the [TUM dataset](http://vision.in.tum.de/data/datasets/rgbd-dataset) as RGB-D or monocular. We also provide a ROS node to process live monocular or RGB-D streams. **The library can be compiled without ROS**. ORB-SLAM2 provides a GUI to change between a *SLAM Mode* and *Localization Mode*, see section 9 of this document.\n\n#####Videos showing ORB-SLAM2:\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=dF7_I2Lin54\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/dF7_I2Lin54/0.jpg\" \nalt=\"Tsukuba Dataset\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=51NQvg5n-FE\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/51NQvg5n-FE/0.jpg\" \nalt=\"KITTI Dataset\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=LnbAI-o7YHk\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/LnbAI-o7YHk/0.jpg\" \nalt=\"TUM RGBD Dataset\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=MUyNOEICrf8\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/MUyNOEICrf8/0.jpg\" \nalt=\"EuRoC Dataset (V1_02, V1_03)\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=xXt90wZejwk\n\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/xXt90wZejwk/0.jpg\" \nalt=\"EuRoC Dataset (V1_02, V1_03)\" width=\"240\" height=\"180\" border=\"10\" /></a>\n\n**Notice for ORB-SLAM Monocular users:**\nThe monocular capabilities of ORB-SLAM2 compared to [ORB-SLAM Monocular](https://github.com/raulmur/ORB_SLAM) are similar. However in ORB-SLAM2 we apply a full bundle adjustment after a loop closure, the extraction of ORB is slightly different (trying to improve the dispersion on the image) and the tracking is also slightly faster. The GUI of ORB-SLAM2 also provides you new capabilities as the *modes* mentioned above and a reset button. We recommend you to try this new software :)\n\n###Related Publications:\n\n[1] Ra\u00fal Mur-Artal, J. M. M. Montiel and Juan D. Tard\u00f3s. **ORB-SLAM: A Versatile and Accurate Monocular SLAM System**. *IEEE Transactions on Robotics,* vol. 31, no. 5, pp. 1147-1163, 2015. (2015 IEEE Transactions on Robotics **Best Paper Award**). **[PDF](http://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf)**.\n\n[2] Dorian G\u00e1lvez-L\u00f3pez and Juan D. Tard\u00f3s. **Bags of Binary Words for Fast Place Recognition in Image Sequences**. *IEEE Transactions on Robotics,* vol. 28, no. 5, pp.  1188-1197, 2012. **[PDF](http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf)**\n\n#1. License\n\nORB-SLAM2 is released under a [GPLv3 license](https://github.com/raulmur/ORB_SLAM2/blob/master/License-gpl.txt). For a list of all code/library dependencies (and associated licenses), please see [Dependencies.md](https://github.com/raulmur/ORB_SLAM2/blob/master/Dependencies.md).\n\nFor a closed-source version of ORB-SLAM2 for commercial purposes, please contact the authors: orbslam (at) unizar (dot) es.\n\nIf you use ORB-SLAM2 in an academic work, please cite:\n\n    @article{murTRO2015,\n      title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System},\n      author={Mur-Artal, Ra\\'ul, Montiel, J. M. M. and Tard\\'os, Juan D.},\n      journal={IEEE Transactions on Robotics},\n      volume={31},\n      number={5},\n      pages={1147--1163},\n      doi = {10.1109/TRO.2015.2463671},\n      year={2015}\n     }\n\n#2. Prerequisites\nWe have tested the library in **Ubuntu 12.04** and **14.04**, but it should be easy to compile in other platforms. A powerful computer (e.g. i7) will ensure real-time performance and provide more stable and accurate results.\n\n## C++11 or C++0x Compiler\nWe use the new thread and chrono functionalities of C++11.\n\n## Pangolin\nWe use [Pangolin](https://github.com/stevenlovegrove/Pangolin) for visualization and user interface. Dowload and install instructions can be found at: https://github.com/stevenlovegrove/Pangolin.\n\n## OpenCV\nWe use [OpenCV](http://opencv.org) to manipulate images and features. Dowload and install instructions can be found at: http://opencv.org. **Required at leat 2.4.3. Tested with OpenCV 2.4.11**.\n\n## Eigen3\nRequired by g2o (see below). Download and install instructions can be found at: http://eigen.tuxfamily.org. **Required at least 3.1.0**.\n\n## BLAS and LAPACK\n[BLAS](http://www.netlib.org/blas) and [LAPACK](http://www.netlib.org/lapack) libraries are requiered by g2o (see below). On ubuntu:\n```\nsudo apt-get install libblas-dev\nsudo apt-get install liblapack-dev\n```\n\n## DBoW2 and g2o (Included in Thirdparty folder)\nWe use modified versions of the [DBoW2](https://github.com/dorian3d/DBoW2) library to perform place recognition and [g2o](https://github.com/RainerKuemmerle/g2o) library to perform non-linear optimizations. Both modified libraries (which are BSD) are included in the *Thirdparty* folder.\n\n## ROS (optional)\nWe provide some examples to process the live input of a monocular, stereo or RGB-D camera using [ROS](ros.org). Building these examples is optional. In case you want to use ROS, a version Hydro or newer is needed.\n\n#3. Building ORB-SLAM2 library and TUM/KITTI examples\n\nClone the repository:\n```\ngit clone https://github.com/raulmur/ORB_SLAM2.git ORB_SLAM2\n```\n\nWe provide a script `build.sh` to build the *Thirdparty* libraries and *ORB-SLAM2*. Please make sure you have installed all required dependencies (see section 2). Execute:\n```\ncd ORB_SLAM2\nchmod +x build.sh\n./build.sh\n```\n\nThis will create **libORB_SLAM2.so**  at *lib* folder and the executables **mono_tum**, **mono_kitti**, **rgbd_tum**, **stereo_kitti** in *Examples* folder.\n\n#4. Monocular Examples\n\n## TUM Dataset\n\n1. Download a sequence from http://vision.in.tum.de/data/datasets/rgbd-dataset/download and uncompress it.\n\n2. Execute the following command. Change `TUMX.yaml` to TUM1.yaml,TUM2.yaml or TUM3.yaml for freiburg1, freiburg2 and freiburg3 sequences respectively. Change `PATH_TO_SEQUENCE_FOLDER`to the uncompressed sequence folder.\n```\n./Examples/Monocular/mono_tum Vocabulary/ORBvoc.txt Examples/Monocular/TUMX.yaml PATH_TO_SEQUENCE_FOLDER\n```\n\n## KITTI Dataset  \n\n1. Download the dataset (grayscale images) from http://www.cvlibs.net/datasets/kitti/eval_odometry.php \n\n2. Execute the following command. Change `KITTIX.yaml`by KITTI00-02.yaml, KITTI03.yaml or KITTI04-12.yaml for sequence 0 to 2, 3, and 4 to 12 respectively. Change `PATH_TO_DATASET_FOLDER` to the uncompressed dataset folder. Change `SEQUENCE_NUMBER` to 00, 01, 02,.., 11. \n```\n./Examples/Monocular/mono_kitti Vocabulary/ORBvoc.txt Examples/Monocular/KITTIX.yaml PATH_TO_DATASET_FOLDER/dataset/sequences/SEQUENCE_NUMBER\n```\n\n#5. Stereo Example\n\n## KITTI Dataset\n\n1. Download the dataset (grayscale images) from http://www.cvlibs.net/datasets/kitti/eval_odometry.php \n\n2. Execute the following command. Change `KITTIX.yaml`to KITTI00-02.yaml, KITTI03.yaml or KITTI04-12.yaml for sequence 0 to 2, 3, and 4 to 12 respectively. Change `PATH_TO_DATASET_FOLDER` to the uncompressed dataset folder. Change `SEQUENCE_NUMBER` to 00, 01, 02,.., 11. \n```\n./Examples/Stereo/stereo_kitti Vocabulary/ORBvoc.txt Examples/Stereo/KITTIX.yaml PATH_TO_DATASET_FOLDER/dataset/sequences/SEQUENCE_NUMBER\n```\n\n#6. RGB-D Example\n\n## TUM Dataset\n\n1. Download a sequence from http://vision.in.tum.de/data/datasets/rgbd-dataset/download and uncompress it.\n\n2. Associate RGB images and depth images using the python script [associate.py](http://vision.in.tum.de/data/datasets/rgbd-dataset/tools). We already provide associations for some of the sequences in *Examples/RGB-D/associations/*. You can generate your own associations file executing:\n\n  ```\n  python associate.py PATH_TO_SEQUENCE/rgb.txt PATH_TO_SEQUENCE/depth.txt > associations.txt\n  ```\n\n3. Execute the following command. Change `TUMX.yaml` to TUM1.yaml,TUM2.yaml or TUM3.yaml for freiburg1, freiburg2 and freiburg3 sequences respectively. Change `PATH_TO_SEQUENCE_FOLDER`to the uncompressed sequence folder. Change `ASSOCIATIONS_FILE` to the path to the corresponding associations file.\n\n  ```\n  ./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt Examples/RGB-D/TUMX.yaml PATH_TO_SEQUENCE_FOLDER ASSOCIATIONS_FILE\n  ```\n\n#7. ROS Examples\n\n### Building the nodes for mono, stereo and RGB-D\n1. Add the path including *Examples/ROS/ORB_SLAM2* to the ROS_PACKAGE_PATH environment variable. Open .bashrc file and add at the end the following line. Replace PATH by the folder where you cloned ORB_SLAM2:\n\n  ```\n  export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:PATH/ORB_SLAM2/Examples/ROS\n  ```\n  \n2. Go to *Examples/ROS/ORB_SLAM2* folder and execute:\n\n  ```\n  mkdir build\n  cd build\n  cmake .. -DROS_BUILD_TYPE=Release\n  make -j\n  ```\n  \n### Running Monocular Node\nFor a monocular input from topic `/camera/image_raw` run node ORB_SLAM2/Mono. You will need to provide the vocabulary file and a settings file. See the monocular examples above.\n\n  ```\n  rosrun ORB_SLAM2 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n  ```\n  \n### Running Stereo Node\nFor a stereo input from topic `/camera/left/image_raw` and `/camera/right/image_raw` run node ORB_SLAM2/Stereo. You will need to provide the vocabulary file and a settings file. If you **provide rectification matrices** (see Examples/Stereo/EuRoC.yaml example), the node will recitify the images online, **otherwise images must be pre-rectified**.\n\n  ```\n  rosrun ORB_SLAM2 Stereo PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION\n  ```\n  \n**Example**: Download a rosbag (e.g. V1_01_easy.bag) from the EuRoC dataset (http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets). Open 3 tabs on the terminal and run the following command at each tab:\n  ```\n  roscore\n  ```\n  \n  ```\n  rosrun ORB_SLAM2 Stereo Vocabulary/ORBvoc.txt Examples/Stereo/EuRoC.yaml true\n  ```\n  \n  ```\n  rosbag play --pause V1_01_easy.bag /cam0/image_raw:=/camera/left/image_raw /cam1/image_raw:=/camera/right/image_raw\n  ```\n  \nOnce ORB-SLAM2 has loaded the vocabulary, press space in the rosbag tab. Enjoy!. Note: a powerful computer is required to run the most exigent sequences of this dataset.\n\n### Running RGB_D Node\nFor an RGB-D input from topics `/camera/rgb/image_raw` and `/camera/depth_registered/image_raw`, run node ORB_SLAM2/RGBD. You will need to provide the vocabulary file and a settings file. See the RGB-D example above.\n\n  ```\n  rosrun ORB_SLAM2 RGBD PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n  ```\n  \n#8. Processing your own sequences\nYou will need to create a settings file with the calibration of your camera. See the settings file provided for the TUM and KITTI datasets for monocular, stereo and RGB-D cameras. We use the calibration model of OpenCV. See the examples to learn how to create a program that makes use of the ORB-SLAM2 library and how to pass images to the SLAM system. Stereo input must be synchronized and rectified. RGB-D input must be synchronized and depth registered.\n\n#9. SLAM and Localization Modes\nYou can change between the *SLAM* and *Localization mode* using the GUI of the map viewer.\n\n### SLAM Mode\nThis is the default mode. The system runs in parallal three threads: Tracking, Local Mapping and Loop Closing. The system localizes the camera, builds new map and tries to close loops.\n\n### Localization Mode\nThis mode can be used when you have a good map of your working area. In this mode the Local Mapping and Loop Closing are deactivated. The system localizes the camera in the map (which is no longer updated), using relocalization if needed. \n\n"
 },
 {
  "repo": "cedricve/raspicam",
  "language": "C++",
  "readme_contents": "\nThis library allows to use the Raspberry Pi Camera. \n\n* Main features:\n - Provides  class RaspiCam for easy and full control of the camera\n - Provides class  RaspiCam_Cv for easy control of the camera with OpenCV.\n - Provides class  RaspiCam_Still and RaspiCam_Still_Cv for controlling the camera in still mode\n - Easy compilation/installation using cmake.\n - No need to install development file of userland. Implementation is hidden.\n - Many examples \n\n* ChangeLog\n0.1.3\n  - Native support for BGR and RGB in opencv classes. No need to do conversion anymore.\n0.1.2\n  - Solved deadlock error in grab\n0.1.1\n  - Moved to c++11  mutex  and condition_variables. Bug fixed that caused random dead lock condition in grab()\n0.1.0\n  - Bug fixed in release for RapiCam and RaspiCam_Cv\n0.0.7\n  - Added classes  RaspiCam_Still and RaspiCam_Still_Cv for still camera mode\n0.0.6\n  - Bug ins cv camera corrected\n0.0.5 \n  - getImageBuffeSize change by getImageBufferSize (sorry)\n  - Change in capture format. Now, it is able to capture in RGB at high speed. \n  - The second parameter of retrieve is now useless. Format must be specified in Raspicam::(set/get)Format before opening the camera and can not be change during operation.\n  - RaspiCam_Cv captures in BGR, which is obtained by converting from RGB. Therefore,  performance drops to half repect to the RaspiCam in RGB mode when using 1280x960.\n0.0.4\n  - Added shutter speed camera control\n  - OpenCv set/get params are now scaled to [0,100]\n  - Added more command line options in test programs\n0.0.3\n  - Fixed error in color conversion (rgb and bgr were swapped)\n  - Added command line options in raspicam_test to adjust exposure\n  - Changes in RaspiCam_Cv so that exposure can be adjusted. Very simply.\n0.0.2\n - Decoupled opening from the start of capture in RaspiCam if desired. RapiCam::open and RaspiCam::startCapture\n - Added  function RaspiCam::getId and RaspiCam_Cv::getId\n - Added a new way to convert yuv2rgb which is a bit faster.Thanks to Stefan Gufman (gruffypuffy at gmail dot com)\n - Added command line option -test_speed to utils programs (do not save images to memory)\n - Removed useless code in private_impl\n \n0.0.1\nInitial libary\n\n\n* Compiling\n\nDownload the file to your raspberry. Then, uncompress the file and compile\n\ntar xvzf raspicamxx.tgz\ncd raspicamxx\nmkdir build\ncd build\ncmake ..\n\nAt this point you'll see something like \n-- CREATE OPENCV MODULE=1\n-- CMAKE_INSTALL_PREFIX=/usr/local\n-- REQUIRED_LIBRARIES=/opt/vc/lib/libmmal_core.so;/opt/vc/lib/libmmal_util.so;/opt/vc/lib/libmmal.so\n-- Change a value with: cmake -D<Variable>=<Value>\n-- \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/pi/raspicam/trunk/build\n\nIf OpenCV development files are installed in your system, then  you see\n-- CREATE OPENCV MODULE=1\notherwise this option will be 0 and the opencv module of the library will not be compiled.\n\nFinally compile and install\nmake\nsudo make install\n\n\nAfter that, you have the programs raspicam_test  and raspicam_cv_test (if opencv was enabled).\nRun the first program to check that compilation is ok.\n\nYou can check that the library has installed the header files under /usr/local/lib/raspicam , and the libraries in\n/usr/local/lib/libraspicam.so and /usr/local/lib/libraspicam_cv.so (if opencv support enabled)\n\n* Using it in your projects\n\nWe provide a simple example to use the library. Create a directory for our own project. \n\nFirst create a file with the name simpletest_raspicam.cpp and add the following code\n\n/**\n*/\n#include <ctime>\n#include <unistd.h>\n#include <fstream>\n#include <iostream>\n#include <raspicam/raspicam.h>\nusing namespace std;\n\nint main ( int argc,char **argv ) {\n    raspicam::RaspiCam Camera; //Cmaera object\n    //Open camera \n    cout<<\"Opening Camera...\"<<endl;\n    if ( !Camera.open()) {cerr<<\"Error opening camera\"<<endl;return -1;}\n    //wait a while until camera stabilizes\n    cout<<\"Sleeping for 3 secs\"<<endl;\n    sleep(3);\n    //capture\n    Camera.grab();\n    //allocate memory\n    unsigned char *data=new unsigned char[  Camera.getImageTypeSize ( raspicam::RASPICAM_FORMAT_RGB )];\n    //extract the image in rgb format\n    Camera.retrieve ( data,raspicam::RASPICAM_FORMAT_RGB );//get camera image\n    //save\n    std::ofstream outFile ( \"raspicam_image.ppm\",std::ios::binary );\n    outFile<<\"P6\\n\"<<Camera.getWidth() <<\" \"<<Camera.getHeight() <<\" 255\\n\";\n    outFile.write ( ( char* ) data, Camera.getImageTypeSize ( raspicam::RASPICAM_FORMAT_RGB ) );\n    cout<<\"Image saved at raspicam_image.ppm\"<<endl;\n    //free resrources    \n    delete data;\n    return 0;\n}\n//\n\nNow, create a file named CMakeLists.txt and add:\n#####################################\ncmake_minimum_required (VERSION 2.8) \nproject (raspicam_test)\nset (CMAKE_MODULE_PATH \"/usr/local/lib/cmake/${CMAKE_MODULE_PATH}\")\nfind_package(raspicam REQUIRED)\nadd_executable (simpletest_raspicam simpletest_raspicam.cpp)  \ntarget_link_libraries (simpletest_raspicam ${raspicam_LIBS})\n#####################################\n\nFinally, create,compile and execute\nmkdir build\ncd build\ncmake ..\nmake\n./simpletest_raspicam\n\nA more complete sample project is provided in SourceForge.\n\n* OpenCV Interface\n\nIf the OpenCV is found when compiling the library, the libraspicam_cv.so module is created and the RaspiCam_Cv class available.\nTake a look at the examples in utils to see how to use the class. In addition, we show here how you can use the RaspiCam_Cv in your own project using cmake.\n\n\nFirst create a file with the name simpletest_raspicam_cv.cpp and add the following code\n\n#include <ctime>\n#include <iostream>\n#include <raspicam/raspicam_cv.h>\nusing namespace std; \n\nint main ( int argc,char **argv ) {\n   \n    time_t timer_begin,timer_end;\n    raspicam::RaspiCam_Cv Camera;\n    cv::Mat image;\n    int nCount=100;\n    //set camera params\n    Camera.set( CV_CAP_PROP_FORMAT, CV_8UC1 );\n    //Open camera\n    cout<<\"Opening Camera...\"<<endl;\n    if (!Camera.open()) {cerr<<\"Error opening the camera\"<<endl;return -1;}\n    //Start capture\n    cout<<\"Capturing \"<<nCount<<\" frames ....\"<<endl;\n    time ( &timer_begin );\n    for ( int i=0; i<nCount; i++ ) {\n        Camera.grab();\n        Camera.retrieve ( image);\n        if ( i%5==0 )  cout<<\"\\r captured \"<<i<<\" images\"<<std::flush;\n    }\n    cout<<\"Stop camera...\"<<endl;\n    Camera.release();\n    //show time statistics\n    time ( &timer_end ); /* get current time; same as: timer = time(NULL)  */\n    double secondsElapsed = difftime ( timer_end,timer_begin );\n    cout<< secondsElapsed<<\" seconds for \"<< nCount<<\"  frames : FPS = \"<<  ( float ) ( ( float ) ( nCount ) /secondsElapsed ) <<endl;\n    //save image \n    cv::imwrite(\"raspicam_cv_image.jpg\",image);\n    cout<<\"Image saved at raspicam_cv_image.jpg\"<<endl;\n}\n\n\n\nNow, create a file named CMakeLists.txt and add:\n#####################################\ncmake_minimum_required (VERSION 2.8) \nproject (raspicam_test)\nset (CMAKE_MODULE_PATH \"/usr/local/lib/cmake/${CMAKE_MODULE_PATH}\")\nfind_package(raspicam REQUIRED)\nfind_package(OpenCV)\nIF  ( OpenCV_FOUND AND raspicam_CV_FOUND)\n\tMESSAGE(STATUS \"COMPILING OPENCV TESTS\")\n\tadd_executable (simpletest_raspicam_cv simpletest_raspicam_cv.cpp)  \n\ttarget_link_libraries (simpletest_raspicam_cv ${raspicam_CV_LIBS})\nELSE()\n\tMESSAGE(FATAL_ERROR \"OPENCV NOT FOUND IN YOUR SYSTEM\")\nENDIF()\n#####################################\n\nFinally, create,compile and execute\nmkdir build\ncd build\ncmake ..\nmake\n./simpletest_raspicam_cv\n\n"
 },
 {
  "repo": "bakwc/PornDetector",
  "language": "Python",
  "readme_contents": "# PornDetector\nTwo python porn images (nudity) detectors.\n\nFirst one (pcr.py) use scikit-learn and opencv. I was able to get ~85% accuracy on markup with 1500 positive and 1500 negative samples. It use two machine-learned classifiers - one of them use HSV colors histogram, and another use SIFT descriptors.\n\nSecond one (nnpcr.py) uses tensorflow neural network. I was able to get ~90% accuracy on the same markup. It use 4 convolutional (3x3 filters) combined with max_pool (2x2) layers, one 1024 fully connected layer and a softmax classifier at the end.\n\n### Requirements of opencv & sklearn detector\n- python 2.7\n- scikit-learn 0.15\n- opencv 2.4 (build it from sources, cause it [missing SIFT](http://stackoverflow.com/questions/18561910/opencv-python-cant-use-surf-sift) by default)\n\n### Requirements of tensorlflow detector\n- python 2.7\n- opencv 2.4 (you can take binary from repository)\n- latest tensorflow\n\nThis is my configuration, may be it can work with another library versions.\n\n### Usage of opencv & sklearn detector\n- Url prediction demo: `./pcr.py url http://example.com/img.jpg`\n- Code usage:\n```python\nfrom pcr import PCR\nmodel = PCR()\nmodel.loadModel('model.bin')\npredictions = model.predict(['image1.jpg', 'image2.jpg', 'image3.jpg'])\nprint predictions\n```\n\n### Usage of tensorlflow detector\n- Url prediction demo: `./nnpcr.py url http://example.com/img.jpg`\n- Code usage:\n```python\nfrom nnpcr import NNPCR\nmodel = NNPCR()\nmodel.loadModel('nnmodel.bin')\npredictions = model.predict(['image1.jpg', 'image2.jpg', 'image3.jpg'])\nprint predictions\n```\n\n### Train model\n- create directory 1 (with non-porn images), 2 (with porn images), cache (empty)\n- Run `./pcr.py train` (to train opencv & sklearn) or `./nnpcr.py train` (for tensorflow one).\n\nAfter train finish you will see accuracy and you will get \"model.bin\" file with your trained model. Now you can use it to detect porn (see functions predictTest and predictUrl). I added a sample model (model.bin) - you can test it without training your own model, but I recomend you to gather some huge collection of images (eg, 50K) for best results.\n\n### License\nPublic domain (but it may use some patented algorithms, eg. SIFT - so you should check license of all used libraries).\n"
 },
 {
  "repo": "cxcxcxcx/imgwarp-opencv",
  "language": "C++",
  "readme_contents": "# imgwarp-opencv\nWarps an image by a few control points. For C++ and OpenCV 2.0+. \n\nCurrently includes Moving Least Square warping (for sparse control points, see reference), and piece-wise affine warping (for dense control points only).\n\nA JavaScript version is [here](http://chenxing.name/fun/imgwarp-js/).\n\n| ![](https://raw.githubusercontent.com/cxcxcxcx/imgwarp-opencv/master/data/imgs/tux.jpg) | ![](https://github.com/cxcxcxcx/imgwarp-opencv/blob/wiki/imgs/tux_sim.png?raw=true) |\n|:--------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|\n| Linux Tux | Warped Tux |\n\n| ![](https://github.com/cxcxcxcx/imgwarp-opencv/blob/wiki/imgs/monalisa_ori.jpg) | ![](https://github.com/cxcxcxcx/imgwarp-opencv/blob/wiki/imgs/monalisa_warped.png) |\n|:--------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|\n| Image from Schaefer's paper | Warped as Schaefer did |\n\nThe demo application:\n\n![](https://github.com/cxcxcxcx/imgwarp-opencv/blob/wiki/imgs/ui.png)\n\nPapers implemented:\nImage Deformation Using Moving Least Squares\nSchaefer S., McPhail T. and Warren J.\nACM SIGGRAPH 2006, pages 533-540\n\n\nAutomatically exported from code.google.com/p/imgwarp-opencv\n(Deprecated) This is used in [an interactive face deforming project](http://code.google.com/p/faceworkshop/).\n"
 },
 {
  "repo": "LiuXiaolong19920720/Add-Christmas-Hat",
  "language": "Python",
  "readme_contents": "# Add-Christmas-Hat\n\n#### Add Christmas hat on one's head based on OpneCV and Dlib\n#### You can download a trained facial shape predictor from [http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2](http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2)\n---\n\n<div>\n<img src=\"https://github.com/LiuXiaolong19920720/Add-Christmas-Hat/blob/master/output.jpg\" width=\"70%\">\n</div>\n\n---\n"
 },
 {
  "repo": "EvilPort2/Simple-OpenCV-Calculator",
  "language": "Python",
  "readme_contents": "# Simple-OpenCV-Calculator\nA gesture controlled calculator.\n\n## Note\n<b>This project is longer actively maintained. This project is now merged with Sign-Language. See the README of <a href=\"https://github.com/evilport2/sign-language\">Sign-Language</a></b>.\n\n## Outcome\nCheck it out here in this <a href =\"https://youtu.be/Q5oeA-ebL7c\">video</a>\n\n## Requirements\n1. OpenCV\n2. Numpy\n3. Keras with Tesorflow as backend\n\n### Creating a gesture\n  1. First set your hand histogram. You do not need to do it again if you have already done it. But you do need to do it if the lighting conditions change. To do so type the command given below and follow the instructions below.\n    \n    python set_hand_hist.py\n\n  * A windows \"Set hand histogram\" will appear.\n  * \"Set hand histogram\" will have 50 squares (5x10).\n  * Put your hand in those squares.\n  * Press 'c'. 2 other windows will appear. \"res\" and \"Thresh\".\n  * On pressing 'c' only the parts of the image which has your skin color should appear on the \"res\" window. White patches corresponding to this should appear on the \"Thresh\" window. \n  * In case you are not successful then move your hand a little bit and press 'c' again. Repeat this until you get a good histogram.\n  * After you get a good histogram press 's' to save the histogram. All the windows close.\n\n  2. The next thing you need to do is create your gestures. That is done by the command given below. On starting executing this program, you will have to enter the gesture number and gesture name/text. Then an OpenCV window called \"Capturing gestures\" which will appear. In the webcam feed you will see a green window (inside which you will have to do your gesture) and a counter that counts the number of pictures stored.\n\n    python create_gestures.py    \n3. Press 'c' when you are ready with your gesture. Capturing gesture will begin after a few seconds. Move your hand a little bit here and there. You can pause capturing by pressing 'c' and resume it by pressing 'c'. Capturing resumes after a few secondAfter the counter reaches 1200 the window will close automatically.\n  4. When you are done adding new gestures run the load_images.py file once. You do not need to run this file again until and unless you add a new gesture.\n    \n    python load_images.py\n\n### Displaying all gestures\n  1. To see all the gestures that are stored in 'gestures/' folder run this command\n    \n    python display_all_gestures.py\n\n### Training a model\n  1. So training can be done with Keras. To train using Keras then use the cnn_keras.py file.\n  \n    python cnn_keras.py\n2. If you use Keras you will have the model in the root directory by the name cnn_keras2.h5.\n\nYou do not need to retrain your model every time. In case you added or removed a gesture then you need to retrain it.\n\n### Running the calculator\n0. Make sure you have already run the set_hand_hist.py file to set the histogram.\n  1. Run the hand-calculator.py file using the command below\n\n    python3 hand-calculator.py\n    \n2. This version uses numbers in American Sign Language.\n3. To confirm a digit make sure you keep the same gesture for 20 frames. On successful confirmation, the number will appear in the vertical center of the black part of the window.\n4. To confirm a number make a fist and keep in the green box for 25 frames. You will get used to the timing :P.\n5. You can have any number of digits for both first number and second number.\n6. Currently there are 10 operators.\n7. During operator selection, 1 means '+', 2 means '-', 3 means '\\*', 4 means '/', 5 means '%', 6 means '\\*\\*', 7 means '>>' or right shift operator, 8 means '<<' or left shift operator, 9 means '&' or bitwise AND and 0 means '|' or bitwise OR.\n\n\n## Got a question?\nIf you have any questions that are bothering you please contact me on my <a href = \"http://www.facebook.com/dibakar.saha.750\">facebook profile</a>. Just do not ask me questions like where do I live, who do I work for etc. Also no questions like what does this line do. If you think a line is redundant or can be removed to make the program better then you can obviously ask me or make a pull request.\n"
 },
 {
  "repo": "adityaarora1/LiveEdgeDetection",
  "language": "Java",
  "readme_contents": "[![Release](https://jitpack.io/v/adityaarora1/LiveEdgeDetection.svg)](https://jitpack.io/#adityaarora1/LiveEdgeDetection)\n\n# LiveEdgeDetection\n\nLiveEdgeDetection is an Android document detection library built on top of OpenCV. It scans documents from camera live mode and allows you to adjust crop using the detected 4 edges and performs perspective transformation of the cropped image.\n\n**It works best with a dark background.**\n\n# JavaDocs\nYou can browse the [JavaDocs for the latest release](https://adityaarora1.github.io/LiveEdgeDetection/docs)\n\n# Download apk\nTry the [sample app](https://drive.google.com/file/d/1sO26O4-1-2XAX16czREx7SiiCD8-4ecU/view?usp=sharing)\n\n# Screenshots\n\n![Use darker bg](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/use_darker_bg.png)\n![Move closer](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/move_closer.png)\n![Move away](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/move_away.png)\n![Adjust angle](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/adjust_angle.png)\n![Hold still](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/hold_still.png)\n![Adjust crop](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/adjust_crop.png)\n![Result](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/cropped.png)\n\n# Integrating into your project\nThis library is available in [JitPack.io](https://jitpack.io/) repository.\nTo use it, make sure to add the below inside root build.gradle file\n\n```\nallprojects {\n    repositories {\n        mavenCentral()\n        maven { url \"https://jitpack.io\" }\n    }\n}\n```\n\nand add the repository's url to the app's build.gradle file.\n\n```\ndependencies {\n   compile 'com.github.adityaarora1:LiveEdgeDetection:1.0.6'\n   \n    // Other dependencies your app might use\n}\n```\n# Usage\nOut of the box it uses OpenCV.\n\n1. Start **startActivityForResult** from your activity\n```\nstartActivityForResult(new Intent(this, ScanActivity.class), REQUEST_CODE);\n```\n2. Get a file path for cropped image on **onActivityResult**\n```\nString filePath = data.getExtras().getString(ScanConstants.SCANNED_RESULT);\nBitmap baseBitmap = ScanUtils.decodeBitmapFromFile(filePath, ScanConstants.IMAGE_NAME);\n```\n3. Display the image using **TouchImageView**\n```\n<com.adityaarora.liveedgedetection.view.TouchImageView\n        android:id=\"@+id/scanned_image\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:scaleType=\"center\" />\n```\n\n# Help\n**Versioning policy**\n\nWe use [Semantic Versioning 2.0.0](https://semver.org/) as our versioning policy.\n\n**Bugs, Feature requests**\n\nFound a bug? Something that's missing? Feedback is an important part of improving the project, so please [open an issue](https://github.com/adityaarora1/LiveEdgeDetection/issues).\n\n**Code**\n\nFork this project and start working on your own feature branch. When you're done, send a Pull Request to have your suggested changes merged into the master branch by the project's collaborators. Read more about the [GitHub flow](https://guides.github.com/introduction/flow/).\n\n# License\n```\nCopyright 2018 Aditya Arora.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n# Donation\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/adityaarora1)  [![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.me/adityaarora1)\n\n![Paypal](https://github.com/adityaarora1/LiveEdgeDetection/blob/master/paypal_qr.gif)\n\n"
 },
 {
  "repo": "brakmic/OpenCV",
  "language": "C#",
  "readme_contents": "# OpenCV Demos\n\n**Character Recognition**\n\n*Environment Setup*\n\n* Download & Install <a href=\"http://opencv.org/downloads.html\" target=\"_blank\">OpenCV 3.0.0</a><br/>\n* Download & Install EmguCV, the OpenCV wrapper libs for .NET, from <a href=\"http://sourceforge.net/projects/emgucv/files/\" target=\"_blank\">here</a>.<br/>\n* Set a system-wide variable pointing to the EmguCV install dir, for example <br/><br/>``` EMGU_ROOT=C:\\bin\\Emgu```<br/><br/>\nYou can do this via **System Preferences/Advanced Settings**<br/><br/>\n* Expand the variable *PATH*, which is in the same settings window, with <br/><br/>```%EMGU_ROOT%\\bin\\x64```<br/>\n<br/>This entry is needed because the app will automatically search for **cvextern.dll** located in this directory.<br/><br/>\n* Open the solution file and start the app (Notice: *this is an x64 app*)<br/>\n* Load one of the available images from the **Assets** folder (these are simple invoice documents)<br/>\n* Click on Analyze and wait for the OCR-task to complete. <br/>\n\n*Example*\n\n<img src=\"http://fs2.directupload.net/images/150825/jli4zbww.png\" width=\"800\" height=\"650\">\n\n*There's a second app which is currently in development. No real OCR there, only a few tests with loading images and (de)serializing them.*\n\n<img src=\"http://fs5.directupload.net/images/160125/abupzd6q.png\" />\n\n*License*\n\n\n<a href=\"https://github.com/brakmic/OpenCV/blob/master/LICENSE\">MIT</a>\n\n\n\n"
 },
 {
  "repo": "atilimcetin/global-matting",
  "language": "C++",
  "readme_contents": "# Global Matting\n\nThis project is built by reproducing the global matting algorithm in the CVPR 2011 paper: \n\nHe, Kaiming, et al. \"A global sampling method for alpha matting.\" In CVPR\u201911, pages 2049\u20132056, 2011.\n\n\n## Benchmark\n\nAfter evaluating the results on the [alpha matting evaluation website](http://alphamatting.com/), this implementation (with matting Laplacian as post-processing) ranks 5th in SAD and 6th in MSE (the original implementation ranks 10th in SAD and 9th in MSE). Running time is less than 1 seconds for an 800x600 image.  Hence, this implementation is one of the highest ranked among the fast matting methods.\n\n\n## Example\n\n### Code\n\n```c++\n#include \"globalmatting.h\"\n\n// you can get the guided filter implementation\n// from https://github.com/atilimcetin/guided-filter\n#include \"guidedfilter.h\"\n\nint main()\n{\n    cv::Mat image = cv::imread(\"GT04-image.png\", CV_LOAD_IMAGE_COLOR);\n    cv::Mat trimap = cv::imread(\"GT04-trimap.png\", CV_LOAD_IMAGE_GRAYSCALE);\n\n    // (optional) exploit the affinity of neighboring pixels to reduce the \n    // size of the unknown region. please refer to the paper\n    // 'Shared Sampling for Real-Time Alpha Matting'.\n    expansionOfKnownRegions(image, trimap, 9);\n\n    cv::Mat foreground, alpha;\n    globalMatting(image, trimap, foreground, alpha);\n\n    // filter the result with fast guided filter\n    alpha = guidedFilter(image, alpha, 10, 1e-5);\n    for (int x = 0; x < trimap.cols; ++x)\n        for (int y = 0; y < trimap.rows; ++y)\n        {\n            if (trimap.at<uchar>(y, x) == 0)\n                alpha.at<uchar>(y, x) = 0;\n            else if (trimap.at<uchar>(y, x) == 255)\n                alpha.at<uchar>(y, x) = 255;\n        }\n\n    cv::imwrite(\"GT04-alpha.png\", alpha);\n\n    return 0;\n}\n```\n\n### Result\n\n[![Image](http://atilimcetin.com/global-matting/GT04-image_small.png)](http://atilimcetin.com/global-matting/GT04-image.png)\n[![Trimap](http://atilimcetin.com/global-matting/GT04-trimap_small.png)](http://atilimcetin.com/global-matting/GT04-trimap.png)\n[![Alpha](http://atilimcetin.com/global-matting/GT04-alpha_small.png)](http://atilimcetin.com/global-matting/GT04-alpha.png)\n\n\n## License\n\nMIT License.\n\n\n"
 },
 {
  "repo": "icsfy/Pedestrian_Detection",
  "language": "C++",
  "readme_contents": "# HOG+SVM\u8fdb\u884c\u884c\u4eba\u68c0\u6d4b\n> \u8fd0\u884c\u73af\u5883\u4e3a\uff1aUbuntu 16.04 && OpenCV 2.4.13  \n> \u4ee3\u7801\u53c2\u8003\u81ea\uff1ahttp://blog.csdn.net/masibuaa/article/details/16105073  \n> INRIA Person\u6570\u636e\u5e93\u4e0b\u8f7d\uff1a\n\u3010[\u76f8\u5173\u8bf4\u660e](http://pascal.inrialpes.fr/data/human/)\u3011  \n>\u3010\u76f4\u63a5\u4e0b\u8f7d\u5730\u5740\uff08970M\uff09\u3011ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar\n\n## \u5f00\u59cb\u524d\u7684\u51c6\u5907\u5de5\u4f5c\n\u5f00\u59cb\u524d\u5efa\u7acb\u6587\u4ef6\u5939\u7528\u4e8e\u5b58\u50a8\u6b63\u8d1f\u6837\u672c\u548cHardExample\uff0c\u6b63\u6837\u672c\u56fe\u7247\u76f4\u63a5\u590d\u5236INRIA\u4e2d\u7684\u6b63\u6837\u672c\u56fe\u7247\uff0c\u8d1f\u6837\u672c\u56fe\u7247\u901a\u8fc7\u88c1\u526a\u5f97\u5230\u3002\n```shell\n$ mkdir -p dataset/pos dataset/neg dataset/HardExample\n$ cp INRIAPerson/96X160H96/Train/pos/* dataset/pos/\n```\n## \u7f16\u8bd1\u51fa\u53ef\u6267\u884c\u6587\u4ef6\n```shell\n$ cmake .\n$ make\n```\n> \u4e5f\u53ef\u4ee5\u5728\u547d\u4ee4\u884c\u4f7f\u7528`g++`\u7f16\u8bd1\u51fa\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u4f8b\u5982\n```shell\n$ g++ -o CropImage crop_image.cpp $(pkg-config opencv --cflags --libs)\n$ g++ -o SvmTrainUseHog main.cpp $(pkg-config opencv --cflags --libs)\n$ g++ -o GetHardExample find_save_HardExample.cpp $(pkg-config opencv --cflags --libs)\n$ g++ -o ImageDetect image_detect.cpp $(pkg-config opencv --cflags --libs)\n$ g++ -o VideoDetect video_detect.cpp $(pkg-config opencv --cflags --libs)\n$ g++ -o PeopleDetect peopledetect.cpp $(pkg-config opencv --cflags --libs)\n```\n\n## \u7b2c\u4e00\u6b65\uff0c\u88c1\u526a\u51fa\u8d1f\u6837\u672c\u56fe\u7247\nINRIA\u4e2d\u67091218\u5f20\u8d1f\u6837\u672c\u56fe\u7247\uff0c`CropImage`\u4ece\u6bcf\u4e00\u5f20\u56fe\u7247\u4e2d\u968f\u673a\u88c1\u526a\u51fa10\u5f20\u5927\u5c0f\u4e3a64x128\u7684\u56fe\u7247\uff0c\u6700\u7ec8\u603b\u5171\u4f1a\u5f97\u523012180\u5f20\u56fe\u7247\uff0c\u5b58\u50a8\u5728dataset/neg\u6587\u4ef6\u5939\u4e2d\u3002\u4e0a\u9762\u5df2\u7ecf\u7f16\u8bd1\u51fa\u4e86\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u76f4\u63a5\u901a\u8fc7`CropImage`\u88c1\u526a\u51fa\u8d1f\u6837\u672c\u56fe\u7247\u3002\n```shell\n$ ./CropImage\n```\n## \u7b2c\u4e8c\u6b65\uff0c\u4f7f\u7528\u6b63\u8d1f\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\n\u5148\u4fee\u6539 `dataset.h` \u91cc\u9762\u53c2\u6570\uff0c\u5c06 `TRAIN` \u7531 `false` \u6539\u4e3a `true` , \u4ee5\u8fdb\u884c\u8bad\u7ec3\uff0c\u53c2\u6570\u4fee\u6539\u540e\u9700\u901a\u8fc7 `make` \u91cd\u65b0\u7f16\u8bd1\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u7136\u540e\u901a\u8fc7 `SvmTrainUseHog` \u5f00\u59cb\u8bad\u7ec3\u3002\n```shell\n$ make\n$ ./SvmTrainUseHog\n```\n \u5230\u8fd9\u91cc\u5df2\u7ecf\u5f97\u5230\u4e86 `SVM_HOG.xml` \u53ef\u4ee5\u7528\u6765\u8fdb\u884c\u68c0\u6d4b\uff0c\u4f46\u662f\u68c0\u6d4b\u6548\u679c\u4e0d\u592a\u597d\uff0c\u6240\u4ee5\u4e0b\u9762\u52a0\u5165 HardExample \u6765\u8fdb\u884c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u3002\n\n## \u7b2c\u4e09\u6b65\uff0c\u5f97\u5230HardExample\n\u901a\u8fc7 `GetHardExample` \u4ece INRIA \u539f\u59cb\u7684\u8d1f\u6837\u672c\u56fe\u7247\u4e2d\u68c0\u6d4b\u51fa HardExample \uff0c\u56fe\u7247\u4f1a\u4fdd\u5b58\u5230 dataset/HardExample\n```shell\n$ ./GetHardExample\n```\n## \u7b2c\u56db\u6b65\uff0c\u5c06\u6b63\u8d1f\u6837\u672c\u548cHardExample\u4e00\u8d77\u91cd\u65b0\u8fdb\u884c\u8bad\u7ec3\n\u5c06 HardExample \u56fe\u7247\u5217\u8868\u5199\u5165\u6587\u4ef6 `HardExample_FromINRIA_NegList.txt` \uff0c\n\u4fee\u6539 `dataset.h` \u91cc\u9762\u7684\u53c2\u6570\uff0c\u5c06 `HardExampleNO` \u7531 `0` \u6539\u4e3a\u7b2c\u4e09\u6b65\u4e2d\u5f97\u5230\u7684\u56fe\u7247\u6570\u76ee\uff0c\u4fee\u6539\u540e\u901a\u8fc7 `make` \u91cd\u65b0\u7f16\u8bd1\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u6700\u540e\u901a\u8fc7 `SvmTrainUseHog` \u91cd\u65b0\u8bad\u7ec3\u3002\n```shell\n$ ls dataset/HardExample/ >HardExample_FromINRIA_NegList.txt\n$ make\n$ ./SvmTrainUseHog\n```\n## \u81f3\u6b64\u8bad\u7ec3\u5b8c\u6210\u3002\n\u68c0\u6d4b\u793a\u4f8b\u56fe\u7247\uff1a  \n![\u68c0\u6d4b\u793a\u4f8b\u56fe\u7247](https://github.com/icsfy/Pedestrian_Detection/raw/master/ImgProcessed.jpg)\n## \u5176\u5b83\u8bf4\u660e\n* `SVM_HOG.xml`\u4e3a\u6700\u7ec8\u8bad\u7ec3\u597d\u7684SVM\u5206\u7c7b\u5668\n* `ImageDetect`\u53ef\u5bf9\u56fe\u7247\u8fdb\u884c\u68c0\u6d4b\n* `VideoDetect`\u53ef\u5bf9\u89c6\u9891\u8fdb\u884c\u68c0\u6d4b\n* `PeopleDetect`\u4e3aOpenCV\u9ed8\u8ba4\u53c2\u6570\u7684\u884c\u4eba\u68c0\u6d4b\u7a0b\u5e8f\n\n## [More](https://github.com/icsfy/Pedestrian_Detection/blob/master/MORE.md)\n"
 },
 {
  "repo": "codingforentrepreneurs/OpenCV-Python-Series",
  "language": "Python",
  "readme_contents": "# OpenCV & Python Tutorial Series\n\nRelated code for the series on [YouTube](https://www.youtube.com/playlist?list=PLEsfXFp6DpzRyxnU-vfs3vk-61Wpt7bOS)\n\nNew to OpenCV? Try this: https://youtu.be/YY9f-6u2Q_c\n\n### Learn OpenCV & Python Playlist: [https://www.youtube.com/playlist?list=PLEsfXFp6DpzRyxnU-vfs3vk-61Wpt7bOS](https://www.youtube.com/playlist?list=PLEsfXFp6DpzRyxnU-vfs3vk-61Wpt7bOS)\n\n### Install OpenCV for Python on your system:\n- Mac: https://youtu.be/iluST-V757A\n- Windows: https://youtu.be/Fcc_jemaoNU\n- Linux with Pi Awesome: https://kirr.co/sx77b7"
 },
 {
  "repo": "Xingtao/FFdynamic",
  "language": "C++",
  "readme_contents": "[![Build Status](https://www.travis-ci.org/Xingtao/FFdynamic.svg?branch=master)](https://travis-ci.org/Xingtao/FFdynamic)\n\n### FFdynamic - Extend FFmpeg with run time control and dynamic audio/video composition\n\nThis project shipped with two parts: **FFdynamic** library and applications build on **FFdynamic**\n\n------------\n### Contents\n- [An application *Interactive Live*](#an-application-interactive-live)\n- [*Dynamic Detect* example](#dynamic-detect-example)\n- [FFdynamic library Overview](#ffdynamic-library-overview)\n- [Getting start with a simple application *Transcoding*](#getting-start-with-a-simple-application-transcoding)\n- [Write a plugin component](#write-a-plugin-component)\n- [Installation](#installation)\n- [Contribution](#contribution)\n\n-----------\n## An application *Interactive Live*\n\n**Interactive Live** (ial for short) is an application based on FFdynamic.  \nIal does multiple video and audio input mixing, then streams it out. It could be run on phones or cloud servers.  \n\n**ial** gives flexiable control over the mixing process (dynamical layout change, backgroup picture change, mute/unmute, etc..), shown in the following gif:\n\n![Layout auto change or set as request](asset/layoutChange.gif)\n#### *Layout auto change or set to certain pattern during mixing by request*\nThis picture shows, 1. auto layout change when a new stream joining in (from 2 cells to 3 cells); 2. layout changes to 4 and 9 cells by http requeset. Changes are quite smooth, without any frozen or stuck, due to audio/video sync message communication mechnism.\n\n#### For more details(capacities, http apis, etc..), please refer to [the application](apps/interactiveLive/README.md)\n\n-----------\n## *Dynamic Detect* example\n\n**Dynamic Detect** is a playgroud one can change object detector types at run time while reading online video streams or local files. Those detectors are loaded via OpenCV api. Models of *darknet* yolo3, *caffe* vgg-ssd, and *tensorflow* mobilenet-ssd (all in coco dataset) are tested. Here is an output stream gif, which run 2 detecors in parallle, draw boxes and texts when they locate interested objects.\n\n![draw_detection](asset/dynaDetect.gif)\n\n#### For more details, please refer to [the application](apps/dynaDnnDetect/README.md) and its unit [test](modules/moduleTest/testCvDnnDetect.cpp)\n\n-----------------\n\n### `FFdynamic library Overview`\n\n* **Extending**: FFdynamic extends FFmpeg in the manner of doing video/audio process **compositionally** and each component's state can be **dynamically** changed on the fly.\n\n- **compositional**\n  _FFdynamic_ is structured in a modular way. It takes each component (demux, decode, filter, encode, muxer, and cutomized component) as a building block and they can be combined together as desired at creationg time or run time.  \n  For instance, if we are developing a dehaze algorithm and would like to know how good the dehazed algorithm visually (in compare to original one). FFdynamic provides facilities that allow one to easily realize following composition:\n\n```\nDemux |-> Audio Decode -> |-> Audio Encode ------------------------------------------> |\n      |                                                                                | -> Muxer\n      |                   |-> Dehaze Filter -> |                                       |\n      |-> Video Decode -> |                    | Mix original and dehazed ->| Encode ->|\n                          | -----------------> |\n```\n  As shown, after demux the input stream, we do video decode which will output to two components: 'Dehaze Filter' component and 'mix video' component; after dehaze, its image also output to 'mix video' component, in there we mix original and dehazed image into one. The whole example is [here](#write-a-plugin-component). \n  Normally, one can freely combine components as long as the input data can be processed.\n\n* **on the fly**\n  _FFdynamic_ has a runtime event dispatch module, which can pass request to the component needs dynamical state change. For instance, one could set dynamical 'Key Frame' request to video encoder or 'mute' one audio stream.  \n  _FFdynamic_ also has a runtime components pub-sub module, which each component can subscribe interesed events from other components. For instance, one video encoder in a live show is willing to know the region of people faces in the incoming image', so that it could set more bitrate to this region. We can do this by subscribe events to a face detecting component and get published event with ROI.\n\n- **customization**\n   One can define their own components, for instance\n   - a RTP demuxer with private fields\n   - an object detection module\n   - a packet sending control module\n   \n   \n   Those components are plugins. Once they are done, they can be composed with other components. \n\nIn short, *FFdynamic* is a scaffold allows develop complex audio/video application in a higher and flexiable manner.   \nIt is suitable for two kind of applications:\n* real time audio/video process: live broadcast, video conference backend, transcoding, etc.. with run time control;\n* develop new video/audio process algorithm which needs video clips as inputs and video clips as outputs, and communication or coorperation needed between video and audio streams;\n\n-----------\n## [Getting start with a simple application *Transcoding*](#getting-start-with-a-simple-application-transcoding)\nDo transcoding in a dozen lines of code, [see here](asset/transcode.md)\n\n-----------\n## [Write a plugin component](#write-a-plugin-component)\n\nHere we take the 'dehaze', mentioned in the 'Overview' part, as the example. We developed a dehaze algorithm and make it a FFdynamic's component. Then mix original and dehazed image together to check the result visually.\n\nRefer to [here](examplePlugin/README.md) for plugin source files.\n\n-----------\n## `Installation`\n\n### Dependency Required\n* FFMpeg, glog, cmake (minimal version 3.2)\n- compiler supports at least c++11\n* boost, protobuf3 (optional, only for the application 'Interactive Live')\n- opencv (optional, if you would like to run plugin example)\n\nprotobuf3 is not well supports by some linux distributions' package manager, here is a manually compiling script (sudo required):\n```\nDIR=$(mktemp -d) && cd ${DIR} && \\\ngit clone https://github.com/protocolbuffers/protobuf.git && cd protobuf && \\\ngit submodule update --init --recursive && \\\n./autogen.sh && ./configure && \\\nmake && make check && \\\nsudo make install && sudo ldconfig\n```\n\n#### For Ubuntu / CentOS\nInstall FFmpeg as usal, then  \n\n``` sh\napt install -y cmake3 libgflags-dev libgoogle-glog-dev libboost-all-dev\nor \nyum install -y glog-devel gflags-devel cmake3 boost-devel  \n```\n\n#### For Mac\nInstall FFmpeg as usal, then  \nbrew install cmake glog gflags protobuf boost \n\n### `Docker build`\nTo alleviate the build process, there is a [docker](tools/dockerlize/README.md) with all dependencies installed that you can play with.\n\n### Build library and apps\n\n``` sh\n    Under FFdynamic folder: \n          'sh build.sh'  will build FFdynamic library (need sudo if make install)\n    Under app/interactiveLive folder: \n          'sh build.sh'  will build FFdynamic library and ial program.\n```\n\n#### Optional Dependencies - TODO\n* nvidia driver, cuda tookit, if you prefer using nvidia codec\n\n-----------------\n## `Contribution`\n\nAll contributions are welcome. Some TODOs:\n\n- A webui that can easily operate on Interactive Live application;\n* 'Interactive live' set video cell's border line, border color;\n- Statistics for each component, process time, latency time, detailed info, etc..;\n* An auto data format negotiate module between components;\n- Expose event register to user for easily event/data callback process;\n"
 },
 {
  "repo": "brainhubeu/react-native-opencv-tutorial",
  "language": "Java",
  "readme_contents": "<br/>\n<h1 align=\"center\">\n  react-native-opencv-tutorial\n</h1>\n\n<p align=\"center\">\n  A fully working example of the OpenCV library used together with React Native.\n</p>\n\n<p align=\"center\">\n  <strong>\n    <a href=\"https://brainhub.eu/blog/opencv-react-native-image-processing/\">Blog post</a> | \n    <a href=\"https://brainhub.eu/contact/\">Hire us</a>\n  </strong>\n</p>\n\n<div align=\"center\">\n\n  [![license](https://img.shields.io/badge/License-MIT-green)](https://github.com/brainhubeu/react-native-opencv-tutorial/blob/master/LICENSE.MD)\n  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)\n</div>\n\n## What this tutorial is about\nThis tutorial is how to use React Native together with OpenCV for image processing. This example uses native Java and Objective-C bindings for OpenCV. In this example we use the device's camera to take a photo and detect whether the taken photo is clear or blurred.\n\n## Demo\n\nThe examples below show the situation right after taking a photo. The first one shows what happens if we take a blurry photo and the second one is the situation after we took a clear photo and are able to proceed with it to do whatever we want.\n\n![Blurred photo](./images/blurred_photo.png)\n![Clear photo](./images/clear_photo.png)\n\n## Blog post\n\nhttps://brainhub.eu/blog/opencv-react-native-image-processing/\n\n## Prerequisites\n\n1. XCode\n2. Android Studio\n\n## How to run the project\n\n1. Clone the repository.\n2. `cd cloned/repository/path`\n3. `npm i` or `yarn`\n4. `react-native link`\n5. Run `./downloadAndInsertOpenCV.sh`.\n6. Download manually the Android pack from https://opencv.org/releases.html (version 3.4.1).\n7. Unzip the package.\n8. Import OpenCV to Android Studio, From File -> New -> Import Module, choose sdk/java folder in the unzipped opencv archive.\n9. Update build.gradle under imported OpenCV module to update 4 fields to match your project's `build.gradle`<br/>\n\n\ta) compileSdkVersion<br/>\n\tb) buildToolsVersion<br/>\n\tc) minSdkVersion<br/>\n\td) targetSdkVersion.\n\n10. Add module dependency by Application -> Module Settings, and select the Dependencies tab. Click + icon at bottom, choose Module Dependency and select the imported OpenCV module. For Android Studio v1.2.2, to access to Module Settings : in the project view, right-click the dependent module -> Open Module Settings.\n11. `react-native run-ios` or `react-native run-android`.\n\n### Additional notes\nIn case of any `downloadAndInsertOpenCV.sh ` script related errors, please, check the paths inside this file and change them if they do not match yours.\nIf this script does not run at all since it has no permissions, run `chmod 777 downloadAndInsertOpenCV.sh`.\n\nIf you do not have `React Native` installed, type `npm i -g react-native-cli` in the terminal.\n\n### License\n\nreactNativeOpencvTutorial is copyright \u00a9 2018-2020 [Brainhub](https://brainhub.eu/?utm_source=github) It is free software, and may be redistributed under the terms specified in the [license](LICENSE.MD).\n\n### About\n\nreactNativeOpencvTutorial is maintained by the Brainhub development team. It is funded by Brainhub and the names and logos for Brainhub are trademarks of Brainhub Sp. z o.o.. You can check other open-source projects supported/developed by our teammates here. \n\n[![Brainhub](https://brainhub.eu/brainhub.svg)](https://brainhub.eu/?utm_source=github)\n\nWe love open-source JavaScript software! See our other projects or hire us to build your next web, desktop and mobile application with JavaScript.\n"
 },
 {
  "repo": "jiafeng5513/Evision",
  "language": "C++",
  "readme_contents": "<div align=center><img width=\"100\" height=\"100\" src=\"./src/EvisionSandbox/resource/Evision.ico\"/></div>\n\n<div align=center>Evision \u53cc\u76ee\u89c6\u89c9\u7cfb\u7edf</div>\n<div align=center><a href=\"https://www.bilibili.com/video/av46024738\">\u6f14\u793a\u89c6\u9891</a></div>\n<div align=center></div>\n<div align=center>\u5982\u679c\u60a8\u89c9\u5f97\u6709\u5e2e\u52a9,\u8bf7\u4e3a\u8be5\u9879\u76ee\u70b9star.\u4ee5\u4fbf\u4e8e\u53ca\u65f6\u6536\u5230\u6700\u65b0\u66f4\u65b0.</div>\n\n## \u91cd\u8981\u63d0\u793a:\u5f53\u524dmaster\u5206\u652f\u7684\u76ee\u6807\u68c0\u6d4b\u529f\u80fd\u5904\u4e8e\u7ef4\u62a4\u72b6\u6001,CMake\u4e2d\u7684\u5f00\u5173\u9ed8\u8ba4\u5df2\u7ecf\u5173\u95ed,\u8bf7\u53ca\u65f6\u5173\u6ce8\u6700\u65b0\u8fdb\u5c55.\n\n\nIntroduction:\n=========\n\n1. \u53cc\u76ee\u7cfb\u7edf\u7684\u6807\u5b9a,\u7578\u53d8\u6821\u6b63,\u89c6\u5dee,\u4e09\u7ef4\u91cd\u5efa,\u8ddd\u79bb\u6d4b\u91cf\u7b49.<br>\n2. \u57fa\u4e8eYolo\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b.<br>\n3. ELAS,ADCensus\u89c6\u5dee\u7b97\u6cd5.<br>\n4. \u5173\u4e8e\u53cc\u76ee\u7684\u4e2d\u6587\u8d44\u6599\u91cd\u590d\u5ea6\u592a\u9ad8,\u5e0c\u671b\u5404\u4f4d\u540e\u6765\u8005\u80fd\u591f\u5438\u53d6\u524d\u4eba\u7cbe\u534e,\u6452\u5f03\u524d\u4eba\u7684\u7cdf\u7c95,\u591a\u591a\u81ea\u884c\u63a2\u7d22,\u4e0d\u8981\u6284\u6765\u6284\u53bb<br>\n5. \u5173\u4e8e\u7a0b\u5e8f\u7684BUG,\u4ee5\u53ca\u5176\u4ed6\u56f0\u60d1,\u8bf7\u4f7f\u7528issues,\u6216\u8054\u7cfb\u90ae\u4ef6jiafeng5513@outlook.com<br>\n6. \u8f7b\u91cf\u7248\u6b63\u5728\u5f00\u53d1,\u8bf7\u67e5\u770b\u4f5c\u8005\u7684GitHUb\u4ed3\u5e93\u5e76\u641c\u7d22\"EvisionLight\",\u672a\u6765Evision\u4f1a\u91c7\u7528ImgUI\u4f5c\u4e3a\u754c\u9762,\u4e0d\u518d\u57fa\u4e8eQt,\u4ee5\u63d0\u4f9b\u66f4\u65b9\u4fbf\u7684\u5b89\u88c5\u4f53\u9a8c,\u5e76\u63d0\u9ad8\u8fd0\u884c\u6548\u7387.<br>\n\n\u8d44\u52a9\u4f5c\u8005\n========\n<div align=center><img width=\"500\" height=\"339\" src=\"./doc/pay.png\"/></div>\n\n\n\u76ee\u5f55\n=========\n- [Introduction:](#introduction)\n- [\u8d44\u52a9\u4f5c\u8005](#\u8d44\u52a9\u4f5c\u8005)\n- [\u76ee\u5f55](#\u76ee\u5f55)\n      - [1.Dependencies](#1dependencies)\n      - [2.Directory_specification](#2directory_specification)\n      - [3.build](#3build)\n      - [4.Deprecated_Version](#4deprecated_version)\n      - [5.\u53cc\u76ee\u8bbe\u5907](#5\u53cc\u76ee\u8bbe\u5907)\n      - [6.\u53c2\u8003\u6587\u732e](#6\u53c2\u8003\u6587\u732e)\n      - [7.\u672a\u6765\u8ba1\u5212](#7\u672a\u6765\u8ba1\u5212)\n\n#### 1.Dependencies\n1. Qt :5.13.2.<br>\n2. OpenCV : 4.1.2.<br>\n3. (\u53ef\u9009\u7684)PCL 1.9.1(\u4e0d\u4f7f\u7528PCL,\u5c06\u4e0d\u80fd\u4f7f\u7528Evision3dViz\u548c\u4fdd\u5b58\u70b9\u4e91\u7684\u529f\u80fd).<br>\n4. (\u53ef\u9009\u7684)CUDA 10.2(\u4e0d\u4f7f\u7528CUDA,\u5c06\u65e0\u6cd5\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u6a21\u5757).<br>\n5. (\u53ef\u9009\u7684)NVIDIA\u663e\u5361.<br>\n6. Windows(test pass) or Ubuntu(build pass).<br>\n\n#### 2.Directory_specification\n1. `data`\u6587\u4ef6\u5939\u5b58\u50a8\u6d4b\u8bd5\u7528\u4f8b.<br>\n2. `doc`\u6587\u4ef6\u5939\u4e2d\u5b58\u50a8\u7684\u662f\u6587\u6863\u548c\u6587\u6863\u4e2d\u6240\u7528\u7684\u56fe\u7247\u7b49\u8d44\u6e90,\u63d0\u4f9b\u4e86\u672c\u9879\u76ee\u4f7f\u7528\u7684\u4e00\u4e9b\u7b97\u6cd5\u76f8\u5173\u7684\u8bba\u6587.<br>\n3. `package`\u6587\u4ef6\u5939\u5b58\u50a8\u9879\u76ee\u6240\u9700\u7684\u4f9d\u8d56.<br>\n4. `props`\u4e2d\u5b58\u50a8\u7684\u662f\u5c5e\u6027\u8868<br>\n5. `scripts`\u6587\u4ef6\u5939\u5b58\u50a8\u7f16\u8bd1\u811a\u672c.<br>\n6. `src`\u6587\u4ef6\u5939\u4e2d\u5b58\u653e\u4ee3\u7801\u6587\u4ef6.<br>\n7. `legacy`\u6587\u4ef6\u5939\u4e2d\u5b58\u653e\u7684\u662f\u4e00\u4e9b\u6709\u4e00\u5b9a\u4fdd\u7559\u4ef7\u503c\u7684\u5f03\u7528\u6a21\u5757.<br>\n* `scripts`\u662f\u4f5c\u8005\u5f00\u53d1\u671f\u95f4\u4f7f\u7528\u7684,\u5bf9\u4e8e\u4f7f\u7528\u8005\u6ca1\u6709\u5b9e\u9645\u610f\u4e49.<br>\n* `package`\u4e2d\u542b\u6709pthread\u7684window\u7248\u672c,\u5c06\u4f1a\u5728\u672a\u6765\u6539\u4e3a\u4f7f\u7528CMake\u7ba1\u7406.<br>\n\n#### 3.build\n1. \u7b14\u8005\u4f7f\u7528CMake 3.15\u6784\u5efaVisual Studio\u89e3\u51b3\u65b9\u6848,\u5e76\u4f7f\u7528Visual Studio 2019\u5b8c\u6210\u5f00\u53d1.<br>\n2. \u4f7f\u7528CLion\u7684\u7528\u6237\u9700\u8981\u81ea\u884c\u7f16\u8f91`src/CMakeLists.txt`\u4ee5\u6307\u5b9a\u76f8\u5173\u4f9d\u8d56\u7684\u8def\u5f84.<br>\n3. master\u5206\u652f\u7684\u6700\u65b0\u7248\u4ec5\u4fdd\u8bc1windows\u4e0a\u7f16\u8bd1\u6210\u529f\u5e76\u6b63\u5e38\u8fd0\u884c,\u7406\u8bba\u4e0a\u652f\u6301Linux.<br>\n4. Evision\u76ee\u524d\u7531\u591a\u4e2a\u6a21\u5757\u7ec4\u6210:\n   \n   |      |\u6a21\u5757\u540d|\u529f\u80fd|\u662f\u5426\u5177\u6709UI|\u8f93\u51fa\u76ee\u6807|\n   |:----:|:----:|:----:|:----:|:----:|\n   | 1|EvisionADCensus           |ADCensus\u89c6\u5dee\u7b97\u6cd5  | \u65e0    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 2|EvisionElas               |Elas\u89c6\u5dee\u7b97\u6cd5      | \u65e0    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 3|EvisionPnP                |PnP              | \u65e0    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 4|EvisionObjDetection       |\u76ee\u6807\u68c0\u6d4bUI        | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 5|EvisionObjDetectionEngine |\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5       | \u65e0   |\u52a8\u6001\u94fe\u63a5\u5e93| \n   | 6|EvisionMonocularCalib     |\u5355\u76ee\u6807\u5b9a           | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 7|EvisionPolyTracker        |\u5355\u76ee\u51e0\u4f55\u4f53\u8ffd\u8e2a     | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 8|EvisionCalibrate          |\u53cc\u76ee\u6807\u5b9a          | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   | 9|EvisionDisparity          |\u89c6\u5dee(\u7acb\u4f53\u5339\u914d)    | \u6709   |\u52a8\u6001\u94fe\u63a5\u5e93|\n   |10|EvisionTrace              |\u4ea4\u4e92\u5f0f\u6d4b\u91cf        | \u6709   |\u52a8\u6001\u94fe\u63a5\u5e93|\n   |11|EvisionUndistortion       |\u7578\u53d8\u6821\u6b63          | \u6709   |\u52a8\u6001\u94fe\u63a5\u5e93|  \n   |12|EvisionCamera             |\u5355\u76ee\u548c\u53cc\u76ee\u76f8\u673a\u529f\u80fd | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|   \n   |13|EvisionCloudViewer        |\u4e09\u7ef4\u70b9\u4e91\u67e5\u770b      | \u6709    |\u52a8\u6001\u94fe\u63a5\u5e93|\n   |14|EvisionParamBridge        |\u5916\u90e8\u53c2\u6570\u4f20\u9012       | \u6709   |\u52a8\u6001\u94fe\u63a5\u5e93|  \n   |15|EvisionUtils              |\u901a\u7528\u5de5\u5177\u7c7b         | \u65e0   |\u52a8\u6001\u94fe\u63a5\u5e93|    \n   |16|EvisionSandbox            |\u4e3b\u7a0b\u5e8fUI          | \u6709    |\u53ef\u6267\u884c\u7a0b\u5e8f|\n\n5. build\u65b9\u6848:\n   1. `./src/`\u76ee\u5f55\u4e0b\u9762\u542b\u6709\u7684CMakeLists.txt,Evision\u57fa\u4e8eCMake3.15\u7f16\u5199,\u8bf7\u4f7f\u7528\u7248\u672c\u5927\u4e8e\u7b49\u4e8e3.13.X\u7684CMake.<br>\n   2. Evision\u662f\u53ea\u80fd\u5de5\u4f5c\u572864\u4f4d\u4e0b,\u53e6\u5916\u5982\u679c\u60a8\u4e0d\u662f\u5f88\u719f\u6089CMake,\u63a8\u8350\u4f7f\u7528CMake GUI.\n   3. \u5728CMake GUI\u4e2d\u6253\u5f00ObjectDetection\u548cPointCloudViewer\u7684\u5f00\u5173,\u5c31\u4f1abuild\u76ee\u6807\u68c0\u6d4b\u548c\u70b9\u4e91\u663e\u793a\u6a21\u5757,\u5e76\u4f1a\u8be2\u95ee\u76f8\u5173\u7684\u4f9d\u8d56\u8def\u5f84.<br>\n   4. Evision\u4f9d\u8d56Qt\u548cOpenCV,\u8bf7\u6b63\u786e\u5b89\u88c5\u5e76\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf;\u6b64\u5916\u5982\u679c\u5f00\u542f\u4e86ObjectDetection,\u9700\u8981\u5b89\u88c5CUDA(\u7248\u672c\u4e0d\u4f4e\u4e8e10.0),\u5982\u679c\u5f00\u542f\u4e86PointCloudViewer,\u9700\u8981\u5b89\u88c5PCL\n\n   \n#### 4.Deprecated_Version\n1. MFC\u7248\u672c.[\u6f14\u793a\u89c6\u9891](https://www.bilibili.com/video/av8862669).<br>\n   \u57fa\u4e8e[\u90b9\u5b87\u534e\u8001\u5e08\u7684StereoVision](https://github.com/yuhuazou/StereoVision)\u7f16\u5199.\u5982\u6709\u9700\u8981\u63a8\u8350\u8bbf\u95ee\u90b9\u5b87\u534e\u8001\u5e08\u539f\u7248\u6216\u8005\u5728\u672c\u9879\u76ee\u7684Release\u4e2d\u5bfb\u627e,\u7531\u4e8e\u4f7f\u7528\u7684\u4f9d\u8d56\u8f83\u4e3a\u9648\u65e7,\u5f3a\u70c8\u4e0d\u5efa\u8bae\u7ee7\u7eed\u4f7f\u7528\u6216\u8fdb\u884c\u4e8c\u6b21\u5f00\u53d1.<br>\n2. CvLabMain\u548cCvLabSandbox(C#\u7248\u672c)<br>\n   * \u7528VS\u6253\u5f00`src/CvLib.sln`,\u5c06\u4f1a\u770b\u5230\u4e24\u4e2a\u5de5\u7a0b,\u8be5\u7248\u672c\u5b58\u5728\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u8bb0\u5f55\u5728C#\u5e73\u53f0\u4e0a\u5b9e\u73b0\u76f8\u4f3c\u529f\u80fd\u7684\u65b9\u6cd5,\u5e76\u6ca1\u6709\u5b9e\u73b0Evision\u7684\u5168\u90e8\u529f\u80fd,\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0d\u4f1a\u66f4\u65b0,\u4e0d\u63a8\u8350\u4f7f\u7528.<br>\n    ![image](./doc/cvlib_sln.png)\n   1. CvLabMain\u662f\u7528WPF\u6846\u67b6\u5199\u7684.<br>\n   2. CvLabSandbox\u662f\u7528WinFrom\u5199\u7684.<br>\n   3. Docking\u98ce\u683cMDI\u754c\u9762<br>\n   4. \u4f7f\u7528MVP\u8bbe\u8ba1\u6a21\u5f0f,\u4ea4\u4e92\u57fa\u4e8e\u53cc\u5411\u6570\u636e\u7ed1\u5b9a<br>\n   5. \u8fd0\u884c\u65f6\u4e3a`.NET 4.6.1`,\u4f9d\u8d56\u91c7\u7528Nuget\u4e0b\u8f7d<br>\n   6. packages:<br>\n       >1.AForge.2.2.5<br>\n       >2.AForge.Video.2.2.5<br>\n       >3.AForge.Video.DirectShow.2.2.5<br>\n       >4.EMGU.CV.3.3.0.2824<br>\n       >5.cskin.16.1.14.3<br>\n       >6.WeifenLuo.WinFormsUI.Docking.2.1.0<br>\n       >7.ZedGraph.5.1.5<br> \n\n\n#### 5.\u53cc\u76ee\u8bbe\u5907\n1. \u9700\u8981\u6ce8\u610f\u7684\u662f,\u89c6\u5dee\u6548\u679c,\u70b9\u4e91\u6548\u679c\u548c\u7cbe\u5ea6\u548c\u8bbe\u5907\u5173\u7cfb\u975e\u5e38\u5927,\u56fe\u7247\u7684\u5206\u8fa8\u7387\u8d8a\u9ad8,\u5149\u7167\u6761\u4ef6\u8d8a\u597d,\u7578\u53d8\u8d8a\u5c0f,\u4e00\u81f4\u6027\u8d8a\u597d,\u6700\u7ec8\u6548\u679c\u4e5f\u5c31\u8d8a\u597d.\u6b64\u5916,\u4e24\u4e2a\u76f8\u673a\u7684\u8ddd\u79bb(\u57fa\u7ebf\u957f\u5ea6)\u4f1a\u5f71\u54cd\u7cfb\u7edf\u7684\u6709\u6548\u8303\u56f4,\u4e00\u822c\u6765\u8bb2,\u57fa\u7ebf\u8d8a\u957f\u7684\u53cc\u76ee\u7cfb\u7edf\u8d8a\u5bb9\u6613\u83b7\u53d6\u8fdc\u5904\u76ee\u6807\u7684\u89c6\u5dee,\u5ef6\u957f\u57fa\u7ebf\u80fd\u591f\u4e00\u5b9a\u7a0b\u5ea6\u4e0a(\u56e0\u4e3a\u6709\u6548\u8ddd\u79bb\u548c\u76f8\u673a\u7684\u7126\u6bb5\u4e5f\u6709\u5173)\u5c06\u7cfb\u7edf\u7684\u6709\u6548\u8303\u56f4\u62c9\u5f97\u66f4\u8fdc,\u4f46\u540c\u65f6,\u57fa\u7ebf\u8d8a\u957f,\u76f2\u533a(\u8ddd\u79bb\u76f8\u673a\u8fc7\u8fd1\u7684\u76ee\u6807\u4e0d\u4f1a\u540c\u65f6\u51fa\u73b0\u5728\u4e24\u4e2a\u89c6\u91ce\u4e2d)\u4e5f\u8d8a\u5927.\n2. \u63a8\u8350\u7684\u53cc\u76ee\u7cfb\u7edf:\n   1. ZED/RealSense/MYNTEYE\u5c0f\u89c5\u76f8\u673a.\u8fd9\u662f\u6210\u719f(\u6602\u8d35)\u7684\u5546\u4e1a\u4ea7\u54c1,\u51fa\u5382\u5e26\u6709\u9ad8\u7cbe\u5ea6\u7684\u6807\u5b9a\u6570\u636e\u548c\u529f\u80fd\u5f3a\u5927\u7684SDK,\u800c\u4e14\u8fd8\u5e26\u6709IMU,IR\u4e3b\u52a8\u5149\u5b66\u7b49\u8f85\u52a9\u8bbe\u5907,\u9002\u5408\u505aSLAM,\u7b14\u8005\u8ba4\u4e3a\u8d2d\u4e70\u8fd9\u7c7b\u76f8\u673a\u662f\u6700\u8282\u7ea6\u65f6\u95f4\u6210\u672c\u7684\u65b9\u6cd5.<br>\n   ![image](./doc/cameras.png)\n   1. \u53cc\u76ee\u5f00\u53d1\u677f.\u6dd8\u5b9d\u4e0a\u6709\u5f88\u591a\u8fd9\u7c7b\u4ea7\u54c1,\u4ef7\u683c\u6bd4ZED\u90a3\u7c7b\u4fbf\u5b9c\u5f88\u591a,\u540c\u65f6\u4ed6\u4eec\u5e26\u7684SDK\u4e5f\u8981\u66f4\u7b80\u964b,\u6709\u4e9b\u751a\u81f3\u53ea\u652f\u6301UVC\u534f\u8bae\u6ca1\u6709SDK,\u4f46\u662f\u4ed6\u4eec\u81f3\u5c11\u80fd\u591f\u63a7\u5236\u4e24\u4fa7\u76f8\u673a\u540c\u65f6\u62cd\u7167,\u540c\u65f6\u57fa\u7ebf\u662f\u56fa\u5b9a\u7684,\u80fd\u514d\u53bb\u4e00\u4e9b\u9ebb\u70e6,\u9700\u8981\u6ce8\u610f\u7684\u662f,\u8fd9\u79cd\u5f00\u53d1\u677f\u5927\u591a\u6570\u4f1a\u8f93\u51fa\u4e00\u5f20\u5de6\u53f3\u89c6\u56fe\u62fc\u5728\u4e00\u8d77\u7684\u56fe\u7247<br>\n   2. \u4e24\u4e2a\u5de5\u4e1a\u76f8\u673a\u7ec4\u88c5.\u8fd9\u79cd\u65b9\u6848\u5e76\u4e0d\u4fbf\u5b9c,\u4f46\u662f\u6bd4\u8f83\u81ea\u7531,\u53ef\u4ee5\u81ea\u5df1\u8c03\u6574\u57fa\u7ebf\u548c\u5149\u8f74\u6307\u5411.<br>\n   ![image](./doc/device.png)<br>\n   1. USB\u76f8\u673a\u7ec4\u88c5.\u8fd9\u662f\u6700\u4fbf\u5b9c\u7684\u65b9\u6848,\u53ea\u8981\u4e70\u4e24\u4e2a\u4e00\u6837\u7684USB\u76f8\u673a,\u7136\u540e\u60f3\u529e\u6cd5\u628a\u4ed6\u4eec\u56fa\u5b9a\u8d77\u6765\u5c31\u53ef\u4ee5\u4e86,\u4f46\u662f\u4fbf\u5b9c\u7684USB\u76f8\u673a\u753b\u8d28\u6bd4\u8f83\u6709\u9650,\u566a\u70b9\u6bd4\u8f83\u591a,\u800c\u4e14\u65e0\u6cd5\u63a7\u5236\u4e24\u4e2a\u76f8\u673a\u540c\u65f6\u62cd\u7167,\u518d\u52a0\u4e0a\u6709\u6548\u8ddd\u79bb\u6bd4\u8f83\u6709\u9650,\u4f1a\u5f88\u5927\u7a0b\u5ea6\u4e0a\u9650\u503c\u6548\u679c,\u6b64\u5916,\u7531\u4e8e\u4e24\u4e2a\u76f8\u673a\u56fa\u5b9a\u7684\u4e0d\u7a33\u5b9a\u7b49\u539f\u56e0\u53ef\u80fd\u51fa\u73b0\u79fb\u52a8,\u8fd9\u4f1a\u4f7f\u6807\u5b9a\u5931\u6548,\u6216\u8005\u7531\u4e8e\u6807\u5b9a\u8fc7\u7a0b\u4e2d\u7684\u6ed1\u52a8\u76f4\u63a5\u5bfc\u81f4\u6807\u5b9a\u5931\u8d25.\u4f5c\u8005\u5efa\u8bae,\u5728\u7ecf\u6d4e\u6761\u4ef6\u5141\u8bb8\u7684\u60c5\u51b5\u4e0b,\u5c3d\u91cf\u4e0d\u8981\u91c7\u7528\u8fd9\u79cd\u65b9\u6848<br>\n\n#### 6.\u53c2\u8003\u6587\u732e\n1. [\u76f8\u673a\u6807\u5b9a+\u7578\u53d8\u77eb\u6b63](https://blog.csdn.net/Loser__Wang/article/details/51811347)\n2. [DarkNet](https://github.com/pjreddie/darknet)\n3. [DarkNet_Windows](https://github.com/AlexeyAB/darknet)\n4. [StdoutRedirector](https://github.com/dbzhang800/StdoutRedirector)\n5. [ADCensus\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0](https://wenku.baidu.com/view/3708e0554693daef5ff73d4d.html)\n6. [ADCensus\u9605\u8bfb\u7b14\u8bb02](https://www.cnblogs.com/sinbad360/p/7842009.html)\n7. [\u90b9\u5b87\u534eCSDN](https://blog.csdn.net/chenyusiyuan/article/details/8131496)\n8. [\u6d45\u58a8CSDN](https://blog.csdn.net/poem_qianmo/article/details/19809337)\n9. Mei X, Sun X, Zhou M, et al. On building an accurate stereo matching system on graphics hardware[C]//2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops). IEEE, 2011: 467-474.\n10. Geiger, Andreas, Martin Roser, and Raquel Urtasun. \"Efficient large-scale stereo matching.\" Asian conference on computer vision. Springer, Berlin, Heidelberg, 2010.\n11. Zhang K, Fang Y, Min D, et al. Cross-scale cost aggregation for stereo matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014: 1590-1597.\n12. Martull, Sarah, Martin Peris, and Kazuhiro Fukui. \"Realistic CG stereo image dataset with ground truth disparity maps.\" ICPR workshop TrakMark2012. Vol. 111. No. 430. 2012.\n13. Hirschmuller H. Stereo processing by semiglobal matching and mutual information[J]. IEEE Transactions on pattern analysis and machine intelligence, 2008, 30(2): 328-341.\n14. [\u89c6\u5dee\u7b97\u6cd5](./doc/\u7acb\u4f53\u5339\u914d\u7b97\u6cd5.md)\n15. [warpped libelas with opencv and used pangolin as GUI](https://github.com/HeYijia/stereo_elas)\n\n#### 7.\u672a\u6765\u8ba1\u5212\n1. [\u53c2\u8003](http://blog.csdn.net/hysteric314/article/details/51357318).<br>"
 },
 {
  "repo": "mahaveerverma/hand-gesture-recognition-opencv",
  "language": "Python",
  "readme_contents": "# HAND GESTURE RECOGNITION\n\n### INTRODUCTION\nThis project implements a hand recognition and hand gesture recognition system using OpenCV on Python 2.7. A histogram based approach is used to separate out a hand from the background image. Background cancellation techniques are used to obtain optimum results. The detected hand is then processed and modelled by finding contours and convex hull to recognize finger and palm positions and dimensions. Finally, a gesture object is created from the recognized pattern which is compared to a defined gesture dictionary.\n\n**Platform:** Python 2.7\n\n**Libraries:** OpenCV 2.4.8, Numpy\n\n**Hardware Requirements:** Camera/Webcam\n\n### USAGE\n\nRun HandRecognition.py to begin the program.\n\n**Note for Windows users:**\nRemove this line from all .py files: '#!/usr/bin/python' or else you might get some error.\n\nYou will find a window that shows your camera feed. Notice a rectangular frame on the right side of the window. That's the frame where all the detection and recognition works.\n\nTo begin, keep your hand and body outside the frame, so as to capture just the background environment, and press 'b'. This will capture the background and create a model of it. This model will be used to remove background from every frame captured once the program setup is complete. \n\nNow, you have to capture your hand histogram. Place your hand over the 9 small boxes in the frame so as to capture the maximum range of shades of your hand. Don't let any shadow or air gap show on the boxed areas for best results. Press 'c' to capture the hand and generate a histogram. \n\nThe setup is now complete. Now you will see, by keeping your hand inside the rectangular frame, it gets detected and you will notice a circle inside your palm area, with lines projecting out from it towards your fingers. Try moving your hands, hiding a few fingers or giving it one of the sample gestures implemented in the program.\n\nThe sample gestures implemented are described with screenshots in the documentation. \n\nThey are:\n\n1. \"V\" with your index and middle finger\n\n2. A flipped \"L\" with thumb and index finger\n\n3. Pointing with your index finger in vertical position\n\n**Note:** Press 'q' at any time to stop the program or 'r' to restart the program.\n\n### HOW DOES IT WORK?\n\nRead full documentation for detailed explanation about implementation in \"docs\" folder.\n\nDuring setup, first a background model is generated when the user presses 'b'. Then, a histogram is generated when the user provides his hand as a sample by pressing 'c'. When the setup is completed, the program goes into an infinite while loop which does as follows.\n\nCamera input frame is saved to a numpy array. A mask is generated based on background model and applied on the frame. This removes background from the captured frame. Now the frame containing only the foreground is converted to HSV color space, followed by histogram comparison (generating back projection). This leaves us with the detected hand. Morphology and smoothening is applied to get a proper hand shape out of the frame. A threshold converts this into a binary image.\n\nNext, we find contours of the binary image obtained, look for the largest contour and find its convex hull.\n\nUsing points from the largest contour we determine center of the palm by finding the largest circle inscribed inside the contour and then the dimension of palm. Using the center of palm as reference, we eliminate all points from the convex hull which do not seem to be part of hand. Also, nearby convex hull points are eliminated so that we are left with exactly only those many points as the number of fingers stretched out.\n\nUsing the positions of fingers and palm dimensions, we model our hand.\n\nThen we compare the model with a dictionary of Gestures defined in GestureAPI.py to determine presence of gestures.\n\n**Full explanation with screenshots is provided in /docs/Documentation.pdf**\n\nFor any queries, contact: mahaveer.verma1@gmail.com"
 },
 {
  "repo": "christianroman/ANPR",
  "language": "C++",
  "readme_contents": "#ANPR\nSource code for a License plate recognition ([ANPR]) demo for iOS using [OpenCV] and [Tesseract OCR] engine.\n\n#Example\n\n###Input\n![Alt text](https://raw.github.com/chroman/ANPR/master/input.png \"Input\")\n\n###Output\n![Alt text](https://raw.github.com/chroman/ANPR/master/output.png \"Output\")\n\n#Introduction\n  - Experimental project\n  - Require `<opencv2.framework>`\n  - Tesseract OCR lang file (`testdata/eng.traineddata`) was trained using this [Gist] script file and a License Plate font.\n\n#Version\n1.0\n\n##Improvements\n* Improve image processing.\n* Improve square (plate) detection.\n* Perspective transform (or 3D).\n* Improve contour detection algorithm.\n* Improve ROI license plate area.\n* Better image crop.\n\n##Supports\n* iOS 6.0 or later.\n* Xcode 4.6 (ARC enabled).\n* Required frameworks: opencv2, UIKit, CoreGraphics and ImageIO.\n\n#Contact\n<a href=\"https://twitter.com/chroman\">Follow @chroman</a>\n\nI'm open to any pull request that can improve this project.\n\n#License\n\nCopyright (c) 2013 Christian Roman, Licensed under the MIT license (http://www.opensource.org/licenses/mit-license.php)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u2018Software\u2019), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u2018AS IS\u2019, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n[ANPR]: http://en.wikipedia.org/wiki/Automatic_number_plate_recognition\n[Tesseract OCR]: https://code.google.com/p/tesseract-ocr/\n[OpenCV]: http://opencv.org/\n[Gist]: https://gist.github.com/chroman/5745206"
 },
 {
  "repo": "apoorvavinod/Real_time_Object_detection_and_tracking",
  "language": "Python",
  "readme_contents": "# Real-time Object Tracking and Detection for Video-streams\n\n\n\n## Pre-req:\n\n1. OpenCV 3.4\n2. imutils\n\n\n*Download weights here and place them in model_data/*\n- [yolov2.weights](https://www.dropbox.com/s/57zhd75mmmc5olf/yolov2.weights?dl=0)\n- [MobileNetSSD_deploy.caffemodel](https://www.dropbox.com/s/d7pxo7kw67zb0e1/MobileNetSSD_deploy.caffemodel?dl=0)\n\n\n## Arguments:\n```\n$python3 src/main.py -h\nusage: main.py [-h] [--input INPUT] [--output OUTPUT] --model MODEL\n               [--config CONFIG] [--classes CLASSES] [--thr THR]\n\nObject Detection and Tracking on Video Streams\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --input INPUT      Path to input image or video file. Skip this argument to\n                     capture frames from a camera.\n  --output OUTPUT    Path to save output as video file. Skip this argument if\n  \t\t\t\t\t you don't want the output to be saved. \n  --model MODEL      Path to a binary file of model that contains trained weights.\n                     It could be a file with extensions .caffemodel (Caffe) or\n                     .weights (Darknet)\n  --config CONFIG    Path to a text file of model that contains network\n                     configuration. It could be a file with extensions\n                     .prototxt (Caffe) or .cfg (Darknet)\n  --classes CLASSES  Optional path to a text file with names of classes to\n                     label detected objects.\n  --thr THR          Confidence threshold for detection. Default: 0.35\n```\n\n\nExecute code from root directory. Example: \n```\npython3 src/main.py --model model_data/yolov2.weights --config model_data/yolov2.cfg --classes model_data/coco_classes.txt --input media/sample_video.mp4 --output out/sample_output.avi\n```\n\n\nor \n\n\n```\npython3 src/main.py --model model_data/MobileNetSSD_deploy.caffemodel --config model_data/MobileNetSSD_deploy.prototxt --classes model_data/MobileNet_classes.txt --input media/sample_video.mp4 --output out/sample_output.avi\n```\n\n\n*Note: --input can be ommitted, which will activate stream from webcam. New objects are detected when all current objects being tracked are lost, or when 'q' is pressed*\n\n\n## MobileNet_SSD with KCF tracker\n\n[![MobileNet_SSD with KCF tracker](https://raw.githubusercontent.com/apoorvavinod/Real_time_Object_detection_and_tracking/master/misc/MobileNet_SSD_KCF.gif)](https://www.youtube.com/watch?v=levZEJKcPjM&feature=youtu.be \"MobileNet_SSD with KCF tracker\")\n\n\n## YOLOv2 with KCF tracker\n\n[![YOLOv2 with KCF tracker](https://raw.githubusercontent.com/apoorvavinod/Real_time_Object_detection_and_tracking/master/misc/YOLOv2_with_KCF.gif)](https://www.youtube.com/watch?v=KmyrSarmvhg&feature=youtu.be \"YOLOv2 with KCF tracker\")\n\n\n"
 },
 {
  "repo": "DataXujing/vehicle-license-plate-recognition",
  "language": "Python",
  "readme_contents": "# \u8f66\u724c\u68c0\u6d4b\u548c\u8bc6\u522b\u7684Python\u5e94\u7528\u8f6f\u4ef6\u5b9e\u73b0\n\n\u5f90\u9759 \n\n## 1.\u8f66\u724c\u68c0\u6d4b\u548c\u8bc6\u522b\u9879\u76ee\u4ecb\u7ecd\n\n![](./pic/p2.jpg)\n*\u56fe\u7247\u6765\u6e90\uff1ahttps://www.cnblogs.com/polly333/p/7367479.html*\n\n\u8f66\u724c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\u7684\u5e94\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u6bd4\u5982\u4ea4\u901a\u8fdd\u7ae0\u8f66\u724c\u8ffd\u8e2a\uff0c\u5c0f\u533a\u6216\u5730\u4e0b\u8f66\u5e93\u95e8\u7981\u3002\u5728\u5bf9\u8f66\u724c\u8bc6\u522b\u548c\u68c0\u6d4b\u7684\u8fc7\u7a0b\u4e2d\uff0c\u56e0\u4e3a\u8f66\u724c\u5f80\u5f80\u662f\u89c4\u6574\u7684\u77e9\u5f62\uff0c\u957f\u5bbd\u6bd4\u76f8\u5bf9\u56fa\u5b9a\uff0c\u8272\u8c03\u7eb9\u7406\u76f8\u5bf9\u56fa\u5b9a\uff0c\u5e38\u7528\u7684\u65b9\u6cd5\u6709\uff1a\u57fa\u4e8e\u5f62\u72b6\u3001\u57fa\u4e8e\u8272\u8c03\u3001\u57fa\u4e8e\u7eb9\u7406\u3001\u57fa\u4e8e\u6587\u5b57\u7279\u5f81\u7b49\u65b9\u6cd5\uff0c\u8fd1\u5e74\u6765\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\u4e5f\u4f1a\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u7684\u4e00\u4e9b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u8be5\u9879\u76ee\u4e3b\u8981\u7684\u6d41\u7a0b\u5982\u4e0b\u56fe\u6240\u793a\uff1a\n\n![](./pic/p1.png)\n\n1.\u8f93\u5165\u539f\u59cb\u56fe\u7247\uff0c\u901a\u8fc7\u4e8c\u503c\u5316\uff0c\u8fb9\u7f18\u68c0\u6d4b\uff0c\u548c\u57fa\u4e8e\u8272\u8c03\u7684\u989c\u8272\u5fae\u8c03\u7b49\u529e\u6cd5\u68c0\u6d4b\u51fa\u539f\u56fe\u4e2d\u7684\u8f66\u724c\u53f7\u7684\u4f4d\u7f6e\uff1b\n\n2.\u628a\u68c0\u6d4b\u5230\u7684\u8f66\u724c(ROI)\u88c1\u526a\uff0c\u4e3a\u8f66\u724c\u53f7\u7684\u8bc6\u522b\u505a\u51c6\u5907\uff1b\n\n3.\u57fa\u4e8e\u88c1\u526a\u7684\u8f66\u724c\u53f7\uff0c\u4f7f\u7528\u76f4\u65b9\u56fe\u7684\u6ce2\u5cf0\u6ce2\u8c37\u5206\u5272\u88c1\u526a\u7684\u8f66\u724c\u53f7\uff08\u5982\u4e0a\u56fe\u4e2d\u7684\u7b2c3\u6b65\uff09\n\n4.\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u505a\u8f66\u724c\u8bc6\u522b\uff0c\u8fd9\u91cc\u8bad\u7ec3\u4e862\u4e2aSVM,\u4e00\u4e2aSVM\u7528\u6765\u8bc6\u522b\u7701\u4efd\u7b80\u79f0(\u5982 \u9c81)\uff0c\u53e6\u4e00\u4e2aSVM\u7528\u6765\u8bc6\u522b\u5b57\u6bcd\u548c\u6570\u5b57\u3002\n\n5.\u901a\u8fc7PyQt5\u628a\u6574\u4e2a\u7b97\u6cd5\u5c01\u88c5\u6210GUI\u7a0b\u5e8f\uff0c\u5e76\u6253\u5305\u53d1\u5e03\u5b89\u88c5\u8f6f\u4ef6\u3002\n\n\n## 2.\u9879\u76ee\u4ee3\u7801\u89e3\u6790\n\u4e0b\u56fe\u63cf\u8ff0\u4e86\u6574\u4e2a\u9879\u76ee\u7684\u4ee3\u7801\u7ed3\u6784\uff0c\u53ef\u4ee5\u8bbf\u95eehttps://github.com/DataXujing/vehicle-license-plate-recognition \u67e5\u770b\uff0c\u5176\u7ed3\u6784\u5982\u4e0b\uff1a\n![](./pic/file_struct.png)\n\n\n\n## 3.\u9879\u76ee\u6f14\u793a\n\n\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee\u9879\u76ee\u5730\u5740 ( https://github.com/DataXujing/vehicle-license-plate-recognition \uff09\u67e5\u770b\u6574\u4e2a\u5e94\u7528,\u6216\u8005\u8bbf\u95ee\u5b89\u88c5\u7a0b\u5e8f\u4e0b\u8f7d\u5730\u5740 \uff08https://pan.baidu.com/s/1IazbGFLlQkb8BQmK_EAeRA  \u63d0\u53d6\u7801\uff1av103 \uff09\u5b89\u88c5\u5b89\u88c5\u7a0b\u5e8f\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8fd9\u91cc\u5c55\u793a\u4e00\u4e9b\u8bc6\u522b\u7ed3\u679c\u548c\u6d4b\u8bd5\u89c6\u9891\uff1a\n\n![](./pic/test1.png)\n\n![](./pic/test2.png)\n\n![](./pic/test3.png)\n\n\n<video src=\"./pic/demo.mp4\" controls=\"controls\" ></video>\n\n\n## 4.TODO\n\n\u76ee\u524d\u8bc6\u522b\u7684\u6548\u679c\u9488\u5bf9\u4e8e\u67d0\u4e9b\u573a\u666f\u4e0b\u4ecd\u7136\u5f88\u4e0d\u7406\u60f3\uff0c\u6280\u672f\u5c42\u9762\u4e0a\u7684\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662f\u8f66\u724c\u68c0\u6d4b\u7b97\u6cd5\u5e76\u6ca1\u6709\u68c0\u6d4b\u5230\u8f66\u724c\uff08\u8fd9\u4e3b\u8981\u662f\u68c0\u6d4b\u7b97\u6cd5\u7684\u95ee\u9898\uff09\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e9b\u76ee\u6807\u68c0\u6d4b\u7684\u7b97\u6cd5\uff0c\u6bd4\u5982Faster R-CNN(\u901f\u5ea6\u53ef\u80fd\u6162\u4e00\u4e9b)\uff0cYOLO\u7cfb\u5217, SSD\u7cfb\u5217\u7b49\u7684\u7ecf\u5178\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7136\u540e\u505a\u77eb\u6b63\u6216\u8fdb\u4e00\u6b65\u7684\u533a\u57df\u7b5b\u9009\uff1b\u53e6\u4e00\u4e2a\u539f\u56e0\u662f\u662f\u5728\u8bc6\u522b\u7b97\u6cd5\u4e0a\uff0c\u672c\u6b21\u6211\u4eec\u4ec5\u662f\u57fa\u4e8e\u5c11\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u4e86SVM\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u589e\u52a0\u8bad\u7ec3\u96c6\u5e76\u628a\u6a21\u578b\u66ff\u6362\u6210\u4e00\u4e9b\u66f4\u590d\u6742\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5982XGBoost,LightGBM,CatBoost\u7b49\u6a21\u578b\u6216\u4f7f\u7528CNN\u8bad\u7ec3\u4e00\u4e2a\u591a\u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c \u4ea6\u6216\u8005\u662f\u76f4\u63a5\u8003\u8651\u4e00\u4e9b\u57fa\u4e8eAttention\u7684CNN-RNN\u67b6\u6784\u7684OCR\u8bc6\u522b\u6a21\u578b\u3002\n\n\n## Reference\n\n1.[OpenCV\u56fe\u50cf\u8bc6\u522b\uff1a\u8f66\u724c\u5b9a\u4f4d\u7b97\u6cd5\u6e90\u7801\uff0cPython\u8bed\u8a00\u5b9e\u73b0](https://blog.csdn.net/sumkee911/article/details/79435983)\n\n2.[\u8f66\u724c\u53f7\u8bc6\u522b python + opencv](https://blog.csdn.net/wzh191920/article/details/79589506)\n\n3 [License-Plate-Recognition](https://github.com/wzh191920/License-Plate-Recognition)\n\n4.[\u8f66\u724c\u8bc6\u522b\uff08\u4e00\uff09-\u8f66\u724c\u5b9a\u4f4d](https://www.cnblogs.com/polly333/p/7367479.html)\n\n5.[\u5728PyQt5\u4e2d\u7f8e\u5316\u548c\u88c5\u626e\u56fe\u5f62\u754c\u9762](https://zmister.com/archives/477.html)\n"
 },
 {
  "repo": "PSMM/SLIC-Superpixels",
  "language": "C++",
  "readme_contents": "# SLIC Superpixel Implementation\nThis repository contains an implementation of the SLIC Superpixel algorithm by Achanta et al. (PAMI'12, vol. 34, num. 11, pp. 2274-2282). The C++ implementation is created to work with the strutures of OpenCV.\n\n## Exemplary result\nThe images below shows an example of an over-segmentation using 400 superpixels and a weight factor of 40.\n<p align=\"center\">\n  <img src=\"https://github.com/PSMM/SLIC-Superpixels/blob/master/dog.png?raw=true\" alt=\"Dog\"/>\n  <img src=\"https://github.com/PSMM/SLIC-Superpixels/blob/master/dog_segmentation.png?raw=true\" alt=\"Dog Segmentation\"/>\n</p>\n"
 },
 {
  "repo": "infusion/PHP-Facedetect",
  "language": "C++",
  "readme_contents": "# PHP-Facedetect\nA simple OpenCV wrapper for PHP to detect faces on images\n\nOpenCV 3 Support\n\nSee [Details](http://www.xarg.org/project/php-facedetect/)\n"
 },
 {
  "repo": "akshaybahadur21/Drowsiness_Detection",
  "language": "Jupyter Notebook",
  "readme_contents": "# Drowsiness Detection OpenCV [![](https://img.shields.io/github/license/sourcerer-io/hall-of-fame.svg?colorB=ff0000)](https://github.com/akshaybahadur21/Drowsiness_Detection/blob/master/LICENSE.txt)  [![](https://img.shields.io/badge/Akshay-Bahadur-brightgreen.svg?colorB=ff0000)](https://akshaybahadur.com)\nThis code can detect your eyes and alert when the user is drowsy.\n\n### Sourcerer\n[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/0)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/0)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/1)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/1)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/2)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/2)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/3)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/3)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/4)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/4)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/5)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/5)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/6)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/6)[![](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/images/7)](https://sourcerer.io/fame/akshaybahadur21/akshaybahadur21/Drowsiness_Detection/links/7)\n\n## Applications\nThis can be used by riders who tend to drive for a longer period of time that may lead to accidents\n\n\n### Code Requirements\nThe example code is in Python ([version 2.7](https://www.python.org/download/releases/2.7/) or higher will work). \n\n### Dependencies\n\n1) import cv2\n2) import imutils\n3) import dlib\n4) import scipy\n\n\n### Description\n\nA computer vision system that can automatically detect driver drowsiness in a real-time video stream and then play an alarm if the driver appears to be drowsy.\n\n### Algorithm\n\nEach eye is represented by 6 (x, y)-coordinates, starting at the left-corner of the eye (as if you were looking at the person), and then working clockwise around the eye:.\n\n<img src=\"https://github.com/akshaybahadur21/Drowsiness_Detection/blob/master/eye1.jpg\">\n\n### Condition\n\nIt checks 20 consecutive frames and if the Eye Aspect ratio is less than 0.25, Alert is generated.\n\n#### Relationship\n\n<img src=\"https://github.com/akshaybahadur21/Drowsiness_Detection/blob/master/eye2.png\">\n\n#### Summing up\n\n<img src=\"https://github.com/akshaybahadur21/Drowsiness_Detection/blob/master/eye3.jpg\">\n\n\nFor more information, [see](https://www.pyimagesearch.com/2017/05/08/drowsiness-detection-opencv/)\n\n### Working Example\n\n<img src=\"https://github.com/akshaybahadur21/Drowsiness_Detection/blob/master/drowsy.gif\">\n\n\n\n### Execution\nTo run the code, type `python Drowsiness_Detection.py`\n\n```\npython Drowsiness_Detection.py\n```\n"
 },
 {
  "repo": "amineHorseman/facial-expression-recognition-using-cnn",
  "language": "Python",
  "readme_contents": "\n# Facial expression recognition using CNN in Tensorflow\n\nUsing a Convolutional Neural Network (CNN) to recognize facial expressions from images or video/camera stream.\n\n## Table of contents\n\n[1. Motivation](#motivation)\n\n[2. Why is Fer2013 challenging?](#fer2013)\n\n[3. Classification results](#results)\n\n[4. How to use?](#how-to-use) \n- [Install the dependeciens](#install)\n- [Download and prepare the data](#data)\n- [Train the model](#train)\n- [Optimize the hyperparameters](#optimize)\n- [Evaluate a trained model](#evaluate)\n- [Recognizing facial expressions from an image file](#recognize-image)\n- [Recognizing facial expressions in real time from video/camera](#recognize-video)\n\n[5. Contributing](#contrib)\n\n<br />\n\n# <a name=\"motivation\">1. Motivation</a>\n\nThe goal is to get a quick baseline to compare if the CNN architecture performs better when it uses only the raw pixels of images for training, or if it's better to feed some extra information to the CNN (such as face landmarks or HOG features). The results show that the extra information helps the CNN to perform better.\n\nTo train the model, we used Fer2013 datset that contains 30,000 images of facial expressions grouped in seven categories: Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral.\n\nThe faces are first detected using opencv, then we extract the face landmarks using dlib. We also extracted the HOG features and we input the raw image data with the face landmarks+hog into a convolutional neural network.\n\nFor our experiments, we used 2 CNN models:\n\n![Model's architecture](img/CNN_models_architecture.png)\n\n\n# <a name=\"fer2013\">2. Why is Fer2013 challenging?</a>\n\nFer2013 is a challenging dataset. The images are not aligned and some of them are uncorrectly labeled as we can see from the following images. Moreover, some samples do not contain faces. \n\n![Fer2013 incorrect labeled images](img/fer2013_incorrect_labels.png)\n\n![Fer2013 strange samples](img/FER2013_strange_samples.png)\n\nThis makes the classification harder because the model have to generalize well and be robust to incorrect data. The best accuracy results obtained on this dataset, as far as I know, is 75.2% described in this paper: \n[[Facial Expression Recognition using Convolutional Neural Networks: State of the Art, Pramerdorfer & al. 2016]](https://arxiv.org/abs/1612.02903)\n\n\n# <a name=\"results\">3. Classification Results (training on 5 expressions)</a>\n\n|       Experiments                            |    SVM    | Model A  |  Model B  |  Difference |\n|----------------------------------------------|-----------|----------|-----------|-------------|\n| CNN (on raw pixels)                          |   -----   |   72.4%  |   73.5%   |    +1.1%    |\n| CNN + Face landmarks                         |   46.9%   |   **73.5%**  |   74.4%   |    +0.9%    |\n| CNN + Face landmarks + HOG                   |   55.0%   |   68.7%  |   73.2%   |    +4.5%    |\n| CNN + Face landmarks + HOG + sliding window  |   **59.4%**   |   71.4%  |   **75.1%**   |    +3.7%    |  \n\nAs expected:\n- The CNN models gives better results than the SVM (You can find the code for the SVM implmentation in the following repository: [Facial Expressions Recognition using SVM](https://github.com/amineHorseman/facial-expression-recognition-svm))\n- Combining more features such as Face Landmarks and HOG, improves *slightly* the accuray.\n- Since the CNN Model B uses deep convolutions, it gives better results on all experiments (up to 4.5%).\n\nIt's interesting to note that using HOG features in the CNN Model A decreased the results compared to using only the RAW data. This may be caused by an overfitting or a failure to extract the coorelation between the information.\n\nIn the following table, we can see the effects of the batch normalization on improving the results:\n\n|   Batch norm effects                         |  on Model A  |  on Model B  |\n|----------------------------------------------|--------------|--------------|\n| CNN (on raw pixels)                          |     +7.4%    |    +39.3%    |\n| CNN + Face landmarks                         |    +26.2%    |    +50.0%    |\n| CNN + Face landmarks + HOG                   |     +1.9%    |    +50.1%    |\n| CNN + Face landmarks + HOG + sliding window  |    +16.7%    |    +16.9%    |\n\nIn the previous experiments, I used only 5 expressions for the training: Angry, Happy, Sad, Surprise and Neutral.\n\nThe accuracy using the best model trained on the whole dataset (7 emotions) dropped to 61.4%. \nThe state of the art results obtained on this dataset, as far as I know, is 75.2% described in [this paper](https://arxiv.org/abs/1612.02903).\n\n\nNote: the code was tested in python 2.7 and 3.6.\n\n# <a name=\"how-to-use\">4. HOW TO USE?</a>\n\n## <a name=\"install\">4.1. Install dependencies</a>\n\n- Tensorflow\n- Tflearn\n- Numpy\n- Argparse\n- [optional] Hyperopt + pymongo + networkx\n- [optional] dlib, imutils, opencv 3\n- [optional] scipy, pandas, skimage\n\nBetter to use anaconda environemnt to easily install the dependencies (especially opencv and dlib)\n\n## <a name=\"data\">4.2. Download and prepare the data</a>\n\n1. Download Fer2013 dataset and the Face Landmarks model\n\n    - [Kaggle Fer2013 challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n    - [Dlib Shape Predictor model](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)\n\n2. Unzip the downloaded files\n\n    And put the files `fer2013.csv` and `shape_predictor_68_face_landmarks.dat` in the root folder of this package.\n\n3. Convert the dataset to extract Face Landmarks and HOG Features\n    ```\n    python convert_fer2013_to_images_and_landmarks.py\n    ```\n    \n    You can also use these optional arguments according to your needs:\n    - `-j`, `--jpg` (yes|no): **save images as .jpg files (default=no)**\n    - `-l`, `--landmarks` *(yes|no)*: **extract Dlib Face landmarks (default=yes)**\n    - `-ho`, `--hog` (yes|no): **extract HOG features (default=yes)**\n    - `-hw`, `--hog_windows` (yes|no): **extract HOG features using a sliding window (default=yes)**\n    - `-hi`, `--hog_images` (yes|no): **extract HOG images (default=no)**\n    - `-o`, `--onehot` (yes|no): **one hot encoding (default=yes)**\n    - `-e`, `--expressions` (list of numbers): **choose the faciale expression you want to use: *0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral* (default=0,1,2,3,4,5,6)**\n\n    Examples:\n    ```\n    python convert_fer2013_to_images_and_landmarks.py\n    python convert_fer2013_to_images_and_landmarks.py --landmarks=yes --hog=no --how_windows=no --jpg=no --expressions=1,4,6\n    ```\n    The script will create a folder with the data prepared and saved as numpy arrays.\n    Make sure the `--onehot` argument set to `yes` (default value)\n\n## <a name=\"train\">4.3. Train the model</a>\n1. Choose your parameters in 'parameters.py'\n\n2. Launch training:\n\n```\npython train.py --train=yes\n```\n\nThe variable `output_size` in parameters.py (line 20), should correspond to the number of facial expressions you want to train on. By default it is set to 7 expressions.\n\n3. Train and evaluate:\n\n```\npython train.py --train=yes --evaluate=yes\n```\n\nN.B: make sure the parameter \"save_model\" (in parameters.py) is set to True if you want to train and evaluate\n\n## <a name=\"optimize\">4.4. Optimize training hyperparameters</a>\n1. For this section, you'll need to install first these optional dependencies:\n```\npip install hyperopt, pymongo, networkx\n```\n\n2. Lunch the hyperparamets search:\n```\npython optimize_hyperparams.py --max_evals=20\n```\n\n3. You should then retrain your model with the best parameters\n\nN.B: the accuracies displayed are for validation_set only (not test_set)\n\n## <a name=\"evaluate\">4.5. Evaluate a trained model (calculating test accuracy)</a>\n\n1. Modify 'parameters.py':\n \nSet \"save_model_path\" parameter to the path of your pretrained file.\n\n2. Launch evaluation on test_set:\n\n```\npython train.py --evaluate=yes\n```\n\n## <a name=\"recognize-image\">4.6. Recognizing facial expressions from an image file</a>\n\n1. For this section you will need to install `dlib` and `opencv 3` dependencies\n\n2. Modify 'parameters.py':\n\nSet \"save_model_path\" parameter to the path of your pretrained file\n\n3. Predict emotions from a file\n\n```\npython predict.py --image path/to/image.jpg\n```\n\n## <a name=\"recognize-video\">4.7. Recognizing facial expressions in real time from video</a>\n\n1. For this section you will need to install `dlib`, `imutils` and `opencv 3` dependencies\n\n2. Modify 'parameters.py':\n\nSet \"save_model_path\" parameter to the path of your pretrained file\n\n3. Predict emotions from a file\n\n```\npython predict-from-video.py\n```\nA window will appear with a box around the face and the predicted expression.\nPress 'q' key to stop.\n\nN.B: If you changed the number of expressions while training the model (default 7 expressions), please update the emotions array in `parameters.py` line 51.\n\n\n# <a name=\"contrib\">5. Contributing</a>\n\nSome ideas for interessted contributors:\n- Automatically downloading the data\n- Adding data augmentation?\n- Adding other features extraction techniques?\n- Improving the models\n\nFeel free to add or suggest more ideas.\nPlease report any bug in the [issues section](https://github.com/amineHorseman/facial-expression-recognition-using-cnn/issues).\n"
 },
 {
  "repo": "MicrocontrollersAndMore/OpenCV_3_Windows_10_Installation_Tutorial",
  "language": "C++",
  "readme_contents": "Welcome to the OpenCV 3 Windows 10 Installation Tutorial !!\n\nIf you are unsure where to start I would recommend starting at the beginning of Installation Cheat Sheet 1, then working your way through each of the 4 Cheat Sheets.  It would be highly recommended to view the YouTube videos accompanying each Cheat Sheet (link available at the top of each Cheat Sheet).\n\nAll installation/configuration procedures and programs have been thoroughly tested on Windows 10.  To my knowledge they should all work on Windows 7/8/8.1 with only minor differences in menu choices, but this has not been tested by me personally so I cannot vouch for 7/8/8.1.  Testing was performed on a Dell desktop and HP ProBook, both Windows 10 64-bit computers.\n\nParts 1 - 4 cover the following:\n\n1) Visual Studio 2015 and precompiled binaries\n2) Python 2\n3) Emgu CV (Visual Basic.NET and C#)\n4) OpenCV compile from source, configuration with Visual Studio 2015 and Qt, and a Qt GUI program\n\nPart 1 video: https://www.youtube.com/watch?v=7SM5OD2pZKY\nPart 2 video: https://www.youtube.com/watch?v=hMXldo27L8c\nPart 3 video: https://www.youtube.com/watch?v=7iyfJ-YaKvw\nPart 4 video: http://youtu.be/akAAAvGyLn0\n\nEnjoy !!\n"
 },
 {
  "repo": "ww23/BlindWatermark",
  "language": "Java",
  "readme_contents": "\n# BlindWatermark \n\n\u5728\u56fe\u7247\u4e0a\u52a0\u9690\u85cf\u7684\u6c34\u5370\n\n* \u539f\u7406\n     * Encode:  \n     \u539f\u56fe --- \u53d8\u6362 ---> \u53d8\u6362\u57df + \u6c34\u5370 --- \u9006\u53d8\u6362 ---> \u5e26\u6c34\u5370\u56fe\n     * Decode:  \n     \u5e26\u6c34\u5370\u56fe --- \u53d8\u6362 ---> \u53d8\u6362\u57df\n\n### Usage\n    \n    Usage: java -jar BlindWatermark.jar <commands>\n        commands:\n            encode <option> <original image> <watermark> <embedded image>\n            decode <option> <original image> <embedded image>\n        encode options:\n            -c discrete cosine transform\n            -f discrete fourier transform (Deprecated)\n            -i image watermark\n            -t text  watermark\n        decode options:\n            -c discrete cosine transform\n            -f discrete fourier transform (Deprecated)\n        example:\n            encode -ct input.png watermark output.png\n            decode -c  input.png output.png\n\n### Build\n\n\tgradle build\n\n### Demo\n\n    \u539f\u56fe:\n![image](image/gakki-src.png)\n\n    \u52a0\u6587\u5b57\u6c34\u5370:\n    java -jar BlindWatermark.jar encode -ct gakki-src.png \u6d4b\u8bd5test gakki-dct-text-ec.jpg\n![image](image/gakki-dct-text-ec.jpg)\n\n    \u6587\u5b57\u6c34\u5370\u89e3\u7801:\n    java -jar BlindWatermark.jar decode -c gakki-dct-text-ec.jpg gakki-dct-text-dc.jpg\n![image](image/gakki-dct-text-dc.jpg)\n\n    \u52a0\u56fe\u7247\u6c34\u5370:\n    java -jar BlindWatermark.jar encode -ci gakki-src.png watermark.png gakki-dct-img-ec.jpg\n![image](image/gakki-dct-img-ec.jpg)\n\n    \u56fe\u7247\u6c34\u5370\u89e3\u7801:\n    java -jar BlindWatermark.jar decode -c gakki-dct-img-ec.jpg gakki-dct-img-dc.jpg\n![image](image/gakki-dct-img-dc.jpg)\n\n### License\n[Apache-2.0](https://github.com/ww23/BlindWatermark/blob/master/LICENSE)"
 },
 {
  "repo": "dctian/DeepPiCar",
  "language": "Jupyter Notebook",
  "readme_contents": "#### [DeepPiCar Series](https://towardsdatascience.com/tagged/deep-pi-car)\n\n# DeepPiCar \u2014 Part 1: How to Build a Deep Learning, Self Driving Robotic Car on a Shoestring Budget\n\n## An overview of how to build a Raspberry Pi and TensorFlow powered, self-driving robotic car\n\n### By [David Tian](https://towardsdatascience.com/@dctian)\n\nHacker, tinkerer, and engineer. I am passionate about machine learning, AI, and\nanything technology related.\n\n![](https://cdn-images-1.medium.com/max/800/1*4GhtKM-eyuYqEpZnnUJZ9w@2x.jpeg)\n\n### Introduction\n\nToday, Tesla, Google, Uber, and GM are all trying to create their own\nself-driving cars that can run on real-world roads. Many analysts predict that\nwithin the next 5 years, we will start to have fully autonomous cars running in\nour cities, and within 30 years, nearly ALL cars will be fully autonomous.\nWouldn\u2019t it be cool to build your very own self-driving car using some of the\nsame techniques the big guys use? In this and next few articles, I will guide\nyou through how to build your own physical, deep-learning, self-driving robotic\ncar from scratch. You will be able to make your car detect and follow lanes,\nrecognize and respond to traffic signs and people on the road in under a week.\nHere is a sneak peek at your final product.\n\n![](https://cdn-images-1.medium.com/max/600/1*3sMJxWJ34vQH0WobdFPVAA.jpeg)\n\nLane Following\n\n![](https://cdn-images-1.medium.com/max/600/1*bYqrTsiMnoaKu9CfjewlEg.jpeg)\n\nTraffic Sign and People Detection (right) from\nDeepPiCar\u2019s DashCam</span>\n\n### Our Road Map\n\n*Part 2*: I will list what hardware to buy and how to set them up. In short, you\nwill need a [Raspberry\nPi](https://www.amazon.com/CanaKit-Raspberry-Power-Supply-Listed/dp/B07BC6WH7V/)\nboard($50), [SunFounder PiCar\nkit](https://www.amazon.com/SunFounder-Raspberry-Graphical-Programming-Electronic/dp/B06XWSVLL8)\n($115), [Google\u2019s Edge TPU](https://coral.withgoogle.com/products/accelerator)\n($75) plus a few accessories, and how each part is important in later articles.\nThe total cost of the materials is around $250\u2013300. We will also install all the\nsoftware drivers needed by Raspberry Pi and PiCar.\n\n![](https://cdn-images-1.medium.com/max/400/1*H7mwt6TcJtZc28fsKh42xg.jpeg)\n\nRaspberry Pi 3 B+\n\n![](https://cdn-images-1.medium.com/max/400/1*LUD3NFk4hCz5wFpRWSGODQ.jpeg)\n\nSunFounder PiCar-V Robotic Car Kit\n\n<img src=\"https://cdn-images-1.medium.com/max/600/1*RIddRse2MoaJtSFes6VkgQ.jpeg\" width=\"300\" />\n\nGoogle Edge TPU Accelerator\n\n*Part 3*: We will set up all the Computer Vision and Deep Learning software\nneeded. The main software tools we use are [Python](https://www.python.org/)\n(the de-facto programming language for Machine Learning/AI tasks), [OpenCV\n](https://github.com/opencv/opencv)(a powerful computer vision package) and\n[Tensorflow ](https://www.tensorflow.org/)(Google\u2019s popular deep learning\nframework). Note all the software we use here are FREE and open source!\n\n![](https://cdn-images-1.medium.com/max/800/1*_jABdMfUVcyPdi5b3zlfVg.jpeg)\n\n*Part 4*: With the (tedious) hardware and software setup out of the way, we will\ndive right into the FUN parts! Our first project is to use python and OpenCV to\nteach DeepPiCar to navigate autonomously on a winding single lane road by\ndetecting lane lines and steer accordingly.\n\n![](https://cdn-images-1.medium.com/max/800/1*cVqpqZ129JiiQZxZwqMlMg.jpeg)\n<span class=\"figcaption_hack\">Step-by-Step Lane Detection</span>\n\n*Part 5*: we will train DeepPiCar to navigate the lane autonomously without having\nto explicitly write logic to control it, as was done in our first project. This\nis achieved by using \u201cbehavior cloning\u201d, where we use just the videos of the\nroad and the correct steering angles for each video frame to train DeepPiCar to\ndrive itself. The implementation is inspired by [NVIDIA\u2019s\nDAVE-2](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)\nfull-sized autonomous car, which uses a deep Convolutional Neural Network to\ndetect road features and make the correct steering decisions.\n\n<span class=\"figcaption_hack\">Lane Following in Action</span>\n\nLastly, in *Part 6*: We will use deep learning techniques such as [single shot\nmulti-box object detection](https://arxiv.org/abs/1512.02325) and [transfer\nlearning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\nto teach DeepPiCar to detect various (miniature) traffic signs and pedestrians\non the road. And then we will teach it to stop at red lights and stop signs, go\non green lights, stop to wait for a pedestrian to cross, and change its speed\nlimit according to the posted speed signs, etc.\n\n![](https://cdn-images-1.medium.com/max/1200/1*Hw7r95umdwnzK2EPTayvfg.jpeg)\n<span class=\"figcaption_hack\">Traffic Signs and People Detection Model Training in TensorFlow</span>\n\n### Prerequisite\n\nHere are the prerequisites of these articles:\n\n* First and foremost is the willingness to *tinker and break things*. Unlike in a\ncar simulator, where everything is deterministic and perfectly repeatable,\nreal-world model cars can be unpredictable and you must be willing to get your\nhands dirty and start to tinker with both the hardware and software.\n* Basic *Python programming* skills. I will assume you know how to read python\ncode and write functions, if statements and loops in python. Most of my code is\nwell documented, specifically the harder to understand parts.\n* Basic *Linux operating system* knowledge. I will assume you know how to run\ncommands in Bash shell in Linux, which is Raspberry Pi\u2019s operating system. My\narticles will tell you exactly which commands to run, why we run them, and what\nto expect as output.\n* Lastly, you will need about *$250-$300* to buy all the hardware and working PC\n(Windows/Mac or Linux). Again, all the software used will be free.\n\n\n### What\u2019s Next\n\nThat\u2019s all for the first article. I will see you in Part 2 where we will get our\nhands dirty and build a robotic car together!\n\nHere are the links to the whole guide:\n\nPart 1: [Overview](https://medium.com/@dctian/deeppicar-part-1-102e03c83f2c?source=your_stories_page---------------------------)\n(This article)\n\nPart 2: [Raspberry Pi Setup and PiCar\nAssembly](https://medium.com/@dctian/deeppicar-part-2-8512be2133f3?source=your_stories_page---------------------------)\n\nPart 3: [Make PiCar See and\nThink](https://medium.com/@dctian/deeppicar-part-3-d648b76fc0be?source=your_stories_page---------------------------)\n\nPart 4: [Autonomous Lane Navigation via\nOpenCV](https://medium.com/@dctian/deeppicar-part-4-lane-following-via-opencv-737dd9e47c96?source=your_stories_page---------------------------)\n\nPart 5:\n[Autonomous](https://medium.com/@dctian/deeppicar-part-4-lane-following-via-opencv-737dd9e47c96?source=your_stories_page---------------------------)[\nLane Navigation via Deep\nLearning](https://medium.com/@dctian/deeppicar-part-5-lane-following-via-deep-learning-d93acdce6110?source=your_stories_page---------------------------)\n\nPart 6: [Traffic Sign and Pedestrian Detection and\nHandling](https://medium.com/@dctian/deeppicar-part-6-963334b2abe0?source=your_stories_page---------------------------)\n\n\n\n\n"
 },
 {
  "repo": "RobinDavid/Motion-detection-OpenCV",
  "language": "Python",
  "readme_contents": "Motion-detection-OpenCV\n=======================\n\nPython/OpenCV script that detect motion on webcam and allow record it to a file.\n\n## The simple way ##\n\nThe trivial idea is to compute the difference between two frames apply a threshold the separate pixels that have changed from the others and then count all the black pixels. Then the average is calculated with this count and the total number of pixels and depending of the ceil the event is triggered or not.\n\n**Additional informations:**\n\n* initRecorder: initialise the recorder with an arbitrary codec it can be changed with problems\n* in the run method no motion can be detected in the first 5 second because it is almost the time needed for the webcam to adjust the focus and the luminosity which imply lot's of changes on the image\n* processImage: contains all the images operations applied to the image\n* somethingHasMoved: The image iteration to count black pixels is contained in this method\n\n\nThe result of applying it can be seen here: https://www.youtube.com/watch?v=-RUu3EcielI\n\n\n## The smart way ##\n\nIis way to operate is less trivial than the previous one, but the results are identical if not more accurate in the previous method. I inspired myself of the [Motion-tracker](\"https://github.com/mattwilliamson/Motion-Tracker/\") by Matt Williamson for the operations and filters to apply on the image but all the rest is different. The idea in this method is to find the contours of the moving objects and calculate the area of all of them. Then the average of the surface changing is compared with the total surface of the image and the alarm is triggered if it exceed the given threshold. Note the code shown below does not implement the recording system as it is the case on the previous example, but it can be made easily.\n\nThe result of applying it can be seen here: https://www.youtube.com/watch?v=sRIdyfh3054\n"
 },
 {
  "repo": "reuterbal/photobooth",
  "language": "Python",
  "readme_contents": "# photobooth\n\n[![Buy me a coffee](https://www.buymeacoffee.com/assets/img/custom_images/black_img.png)](https://www.buymeacoffee.com/reuterbal)\n\nA flexible Photobooth software.\n\nIt supports many different camera models, the appearance can be adapted to your likings, and it runs on many different hardware setups.\n\n## Description\nThis is a Python application to build your own photobooth.\n\n### Features\n* Capture a single or multiple pictures and assemble them in an m-by-n grid layout\n* Live preview during countdown\n* Store assembled pictures (and optionally the individual shots)\n* Printing of captured pictures (via Qt printing module or pycups)\n* Highly customizable via settings menu inside the graphical user interface\n* Custom background for assembled pictures\n* Ability to skip single pictures in the m-by-n grid (e.g., to include a logo in the background image)\n* Support for external buttons and lamps via GPIO interface\n* Rudimentary WebDAV upload functionality (saves pictures to WebDAV storage) and mailer feature (mails pictures to a fixed email address)\n* Theming support using [Qt stylesheets](https://doc.qt.io/qt-5/stylesheet-syntax.html)\n\n### Screenshots\nScreenshots produced using `CameraDummy` that produces unicolor images.\n\n#### Theme _pastel_\n<img alt=\"Idle screen\" title=\"Idle screen\" src=\"screenshots/pastel_1.png\" width=\"280\" /> <img alt=\"Greeter screen\" title=\"Greeter screen\" src=\"screenshots/pastel_2.png\" width=\"280\" /> <img alt=\"Countdown screen\" title=\"Countdown screen\" src=\"screenshots/pastel_3.png\" width=\"280\" /> <img alt=\"Postprocessing screen\" title=\"Postprocessing screen\" src=\"screenshots/pastel_4.png\" width=\"280\" /> <img alt=\"Settings screen\" title=\"Settings screen\" src=\"screenshots/pastel_settings.png\" width=\"280\" />\n\n#### Theme _dark_\n<img alt=\"Idle screen\" title=\"Idle screen\" src=\"screenshots/dark_1.png\" width=\"280\" /> <img alt=\"Greeter screen\" title=\"Greeter screen\" src=\"screenshots/dark_2.png\" width=\"280\" /> <img alt=\"Countdown screen\" title=\"Countdown screen\" src=\"screenshots/dark_3.png\" width=\"280\" /> <img alt=\"Postprocessing screen\" title=\"Postprocessing screen\" src=\"screenshots/dark_4.png\" width=\"280\" />\n\n### Technical specifications\n* Many camera models supported, thanks to interfaces to [gPhoto2](http://www.gphoto.org/), [OpenCV](https://opencv.org/),  [Raspberry Pi camera module](https://projects.raspberrypi.org/en/projects/getting-started-with-picamera)\n* Tested on Standard x86 hardware and [Raspberry Pi](https://raspberrypi.org/) models 1B+, 2B, 3B, and 3B+\n* Flexible, modular design: Easy to add features or customize the appearance\n* Multi-threaded for responsive GUI and fast processing\n* Based on [Python 3](https://www.python.org/), [Pillow](https://pillow.readthedocs.io), and [Qt5](https://www.qt.io/developers/)\n\n### History\nI started this project for my own wedding in 2015. \nSee [Version 0.1](https://github.com/reuterbal/photobooth/tree/v0.1) for the original version.\nGithub user [hackerb9](https://github.com/hackerb9/photobooth) forked this version and added a print functionality.\nHowever, I was not happy with the original software design and the limited options provided by the previously used [pygame](https://www.pygame.org) GUI library and thus abandoned the original version.\nSince then it underwent a complete rewrite, with vastly improved performance and a much more modular and mature software design.\n\n## Installation and usage\n\n### Hardware requirements\n* Some computer/SoC that is able to run Python 3.5+ as well as any of the supported camera libraries\n* Camera supported by gPhoto 2 (see [compatibility list](http://gphoto.org/proj/libgphoto2/support.php)), OpenCV (e.g., most standard webcams), or a Raspberry Pi Camera Module.\n* Optional: External buttons and lamps (in combination with gpiozero-compatible hardware)\n\n### Installing and running the photobooth\n\nSee [installation instructions](INSTALL.md).\n\n## Configuration and modifications\nDefault settings are stored in [`defaults.cfg`](photobooth/defaults.cfg) and can either be changed in the graphical user interface or by creating a file `photobooth.cfg` in the top folder and overwriting your settings there.\n\nThe software design is very modular.\nFeel free to add new postprocessing components, a GUI based on some other library, etc.\n\n## Feedback and bugs\nI appreciate any feedback or bug reports.\nPlease submit them via the [Issue tracker](https://github.com/reuterbal/photobooth/issues/new?template=bug_report.md) and always include your `photobooth.log` file (is created automatically in the top folder) and a description of your hardware and software setup.\n\nI am also happy to hear any success stories! Feel free to [submit them here](https://github.com/reuterbal/photobooth/issues/new?template=success-story.md).\n\nIf you find this application useful, please consider [buying me a coffee](https://www.buymeacoffee.com/reuterbal).\n\n\n## License\nI provide this code under AGPL v3. See [LICENSE](https://github.com/reuterbal/photobooth/blob/master/LICENSE.txt).\n"
 },
 {
  "repo": "jerry-git/thug-memes",
  "language": "Python",
  "readme_contents": "Thug Memes\n==========\n|build|\n|pypi|\n|python_versions|\n|codecov|\n\nCommand line Thug Meme generator written in Python.\n\nInstallation\n------------\n\nRequirements\n^^^^^^^^^^^^\n - Python 3.4+\n\n.. code:: bash\n\n    pip3 install thug-memes\n\nThis installs the core version which supports `opencv <https://pypi.python.org/pypi/opencv-python>`__ haarcascade based detector. If you are serious with your thug memes (like you definitely should), there is also support for \n`dlib <http://dlib.net/>`__ based detector which, in general, provides better results. If you want to enjoy dlib's accuracy, please follow `dlib's own installation instructions <https://pypi.python.org/pypi/dlib>`__. Some additional dlib installation guides for macOS and Ubuntu can be found in: `[1] <https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf>`__  `[2] <https://www.pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/>`__.\nThug memes can be installed with dlib's Python bindings included:\n\n.. code:: bash\n\n    pip3 install \"thug-memes[dlib]\"\n\nUsage\n-----\n.. code:: bash\n\n    thug path/to/the/original/image 'JUST CASUALLY LIFTING' '20KGs HERE'\n\nWill store something like this in your current directory:\n\n|img1|\n\nIf you have installed dlib and want to use it instead of opencv detector, add option:\n\n.. code:: bash\n\n    --detector dlib\n\nIf you want to see the results of the detection, add *--debug* option:\n \n\nOpencv (*--debug*):\n\n|img2|\n\nDlib (*--detector dlib --debug*):\n\n|img3|\n\nAlmost everything is configurable. You can see the used configuration with: \n\n.. code:: bash\n\n    --show-config\n\nThe default configuration is in `src/thug/defaul.conf`. You can override this by defining environment variable `THUG_CONF` and pointing it to a file which contains overrides. In addition, you can override any of the configuration variables from command line with *--override* or *-o* options. For example:\n\n.. code:: bash\n\n    thug path/to/original/img.jpg 'WE HAVE HUGE CIGARS' 'BUT ALSO PINK TEXT' --detector dlib -o cigar_length 2 -o glasses_width 5 -o font_bgr '[180,105,255]'\n\n|img4|\n\nFor all available options, see:\n\n.. code:: bash\n\n    thug --help\n\n\n\nIf you don't want to have awesome Thug elements in your meme, there is also 'a plain meme' alternative:\n\n.. code:: bash\n\n    meme path/to/the/original/image 'THIS IS A NORMAL MEME' 'WITHOUT THUG STUFF :(' -o font_bgr '[255,255,255]'\n\n|img5|\n\nExample images are from `pexels <https://www.pexels.com/photo-license/>`__.\n\nContributing\n------------\nSee `contributing <https://github.com/jerry-git/thug-memes/blob/master/CONTRIBUTING.md>`__ guide.\n\n\n.. |pypi| image:: https://img.shields.io/pypi/v/thug-memes.svg\n   :target: https://pypi.python.org/pypi/thug-memes\n\n\n.. |build| image:: https://travis-ci.org/jerry-git/thug-memes.svg?branch=master\n   :target: https://travis-ci.org/jerry-git/thug-memes\n\n.. |python_versions| image:: https://img.shields.io/pypi/pyversions/thug-memes.svg\n   :target: https://pypi.python.org/pypi/thug-memes\n\n.. |codecov| image:: https://codecov.io/gh/jerry-git/thug-memes/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/jerry-git/thug-memes\n\n\n.. |img1| image:: https://raw.githubusercontent.com/jerry-git/thug-memes/master/doc/examples/1_face_out_thug.jpg\n\t:height: 600pt\n\n.. |img2| image:: https://raw.githubusercontent.com/jerry-git/thug-memes/master/doc/examples/1_face_debug_opencv.jpg\n\t:height: 600pt\n\n.. |img3| image:: https://raw.githubusercontent.com/jerry-git/thug-memes/master/doc/examples/1_face_debug_dlib.jpg\n\t:height: 600pt\n\n.. |img4| image:: https://raw.githubusercontent.com/jerry-git/thug-memes/master/doc/examples/3_faces_thug_custom.jpeg\n\t:width: 600pt\n\n.. |img5| image:: https://raw.githubusercontent.com/jerry-git/thug-memes/master/doc/examples/normal_meme_out.jpg\n\t:width: 600pt"
 },
 {
  "repo": "mesutpiskin/computer-vision-guide",
  "language": "Python",
  "readme_contents": "English | [T\u00fcrk\u00e7e](./README.md)\n\n\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/opencv-turkish-tutorial) [![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](#)\n[![Video](https://img.shields.io/badge/Video-@mesutpiskin-red.svg?logo=youtube&longCache=true&style=flat)](https://www.youtube.com/channel/UC_ko-bnDYXCVL1XJG0doRDg)\n\n- [Image Processing and Computer Vision Guide](#image-processing-and-computer-vision-guide)\n  - [Documentation](#documentation)\n  - [Example Projects](#example-projects)\n  - [Contribute](#contribute)\n  - [Question & Answer](#question--answer)\n  - [License](#license)\n\nThis guide to help you understand the basics of computerized view and to develop computer vision vision with OpenCV. Python, Java, JavaScript, C # and C ++ are supported with examples. If you want to contribute the project; You can look at the contribute section.Feel free to contact us if you have a request or suggestion. To contact <a href=\"http://mesutpiskin.com\">websitee</a> or <a href=\"mailto:mesutpiskin@outlook.com\">email</a>. Much obliged!\n\n# Image Processing and Computer Vision Guide\n\n You will learn the image processing and machine learning algorithms.  We will develop example applications using **Python** , **C++**,  **JavaScript (OpenCV.JS)**, **MATLAB** and **C# (EmguCV)**  programming languages. This guide is intended to help you understand the basics of computerized view and develop OpenCV and computer vision applications. This document has been prepared to address those who have not previously been engaged in image processing, who are not familiar with the subject matter, and who, on the contrary, are knowledgeable about this subject and who want to develop themselves on different topics. Sample projects can be found in  <a href=\"/code/\">code</a> and documents in <a href=\"/docs/\">docs</a> directory.\n\n<p align=\"center\">\n\n<img src=\"/other/banner-en.png\"/>\n</p>\n\n\n\n## Documentation\n\n| Lecture |Abstract|\n|----------|--------|\n| [What is OpenCV?](/docs/1-introduction.en-US.md)|OpenCV history and components, alternative image processing libraries,  OpenCV 2 vs OpenCV 3 vs OpenCV 4 and OpenCV 4 future.|\n| [Development Environment and Platforms](/docs/2-development-environments.en-US.md) |Which platform and development environment should be selected, why Java, C ++ and Python are used for image processing.|\n| [OpenCV Wrappers](/docs/3-opencv-wrappers.en-US.md) |What is Wrapper? What is EmguCV, JavaCV, LiveCV, Ruby CV, and what are the differences between wrappers?|\n| [Installation and Compilation](/docs/4-opencv-installation.en-US.md) |OpenCV installation for Windows, Linux, macOS and Raspberry Pi.|\n| [IDE Configuration](/docs/5-ide-Configuration.en-US.md) |Configuration settings for Eclipse, Netbeans, Android Studio and Intellij IDEA.|\n| [Introduction and Basic Concepts](/docs/6-image-processing-concepts.en-US.md) |Image processing concepts. From the file, from the camera, reading the image from the IP camera. Changing camera parameters.|\n| [Video Recorders and Decoders](/docs/7-video-recorder-codec.en-US.md) |Codec, FourCC and video recorder.|\n| [Image Manipulation](/docs/8-pixel-manipulation.en-US.md) |Pixel manipulation, geometric objects and geometric transformations.|\n| [Color Spaces and Histogram](/docs/9-color-spaces.en-US.md) |Color spaces, color spaces conversion. Histogram concept and histogram matching.|\n| [Morphological Image Processing](/docs/10-morphological-operators.en-US.en-US.md) |Morphological operators and thresholding: Erosion, dilation, closing, gradyan, thresholding etc.\n| [Filters and Edge Detection](/docs/11-filtering-and-edge-detection.en-US.md) |Filtering and edge detection algorithms: Blur, Sobel, Laplace, Canny etc.|\n| [Background Subtraction](/docs/12-background-subtraction.en-US.md) | OpenCV background subtractor: Absdiff, MOG, MOG2 and GMG.|\n| [Object Detection and Object Recognition](/docs/14-object-detection.en-US.md) |Object detection processes and algorithms. HaarCascade, TemplateMatching, DNN, CNN, SVM Machine Learning and Deep Learning Algorithms etc.\n [Feature and Feature Extraction](/docs/19-feature-extraction.en-US.md) |Feature , feature extraction for object detection, and feature matching. Brute-Force, FLANN, SURF, SIFT, BRIEF, ORB, FAST algorithms etc. \n| [Video Analysis and Object Tracking](/docs/13-object-tracking.en-US.md) |Mean Shift, Camshift, Optical flow, GOTURN, BOOSTING, MIL, CNN etc. Object or area tracking on video. |\n| [Image Distortion and Stereo Vision](/docs/15-image-distortion-and-camera-calibration.en-US.md) | Image distortion and camera calibration, 3D images, depth estimation, stereoscopic vision and stereo image processing.|\n| [Face Recognition](/docs/17-face-recognition.en-US.md) | What is face recognition? Face recognition with Eigenfaces, Fisherfaces, LBPH and machine learning algorithms. Different library integrations (dlib, tensorflow and face_recognition). |\n| [Optical Character Recognition OCR ](/docs/18-optic-character-recognition.en-US.md)| Detecting text on the image. OCR processes, algorithms and libraries. Tesseract, textocr etc.\n| GPU and Parallel Computing | Development of parallelized computerized vision applications on Nvidia GPU with Cuda module.|\n| OpenCV Mobile  |Computerized view and image processing on mobile devices with Android and iOS operating system.|\n| Augmented Reality | 3D models have been acquired through the camera, dressed on real-world image. OpenCV and OpenGL integration. Interactive computer vision application development.|\n\n\n\n\n\n## Example Projects\n\n<table style=\"width: 100%;\">\n<tbody>\n<tr>\n<td><strong>&nbsp;Topic</strong></td>\n<td><strong>&nbsp;Python</strong></td>\n<td><strong>&nbsp;Java</strong></td>\n<td><strong>&nbsp;C++</strong></td>\n<td><strong>C#</strong></td>\n<td><strong>JavaScript</strong></td>\n \n</tr>\n<tr>\n<td>Video I/O</td>\n<td>\n<ul>\n<li><a href=\"/code/kamera-io/python/video_io.py\">Video and Camera I/O</a></li>\n<li><a href=\"/code/kamera-io/python/video-kaydet.py\">Video Recorder</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Image Manipulation</td>\n<td>\n<ul>\n<li><a href=\"/code/goruntu-manipulasyonu/python/geometrik-sekiller.py\">Geometric Shape Drawing</a></li>\n<li><a href=\"/code/goruntu-manipulasyonu/python/goruntu-kirpma.py\">Image Crop</a></li>\n<li><a href=\"/code/goruntu-manipulasyonu/python/piksel-manupulasyonu.py\">Pixel Manipulation</a></li>\n<li><a href=\"/code/goruntu-manipulasyonu/python/yeniden-boyutlandirma.py\">Image Resize</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Color Spaces</td>\n<td>\n<ul>\n<li><a href=\"/code/renk-uzaylari/python/renk-uzayi-donusumu.py\">Color Space Conversion</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Morphological Image Processing</td>\n<td>\n<ul>\n<li><a href=\"/code/morfolojik-goruntu-isleme/python/acinim-opening.py\">Opening</a></li>\n<li><a href=\"/code/morfolojik-goruntu-isleme/python/kapanim-closing.py\">Closing</a></li>\n<li><a href=\"/code/morfolojik-goruntu-isleme/python/genisletme-dilation.py\">Dilation</a></li>\n<li><a href=\"/code/morfolojik-goruntu-isleme/python/gradient.py\">Gradient</a></li>\n<li><a href=\"/code/morfolojik-goruntu-isleme/python/asindirme-erozyon.py\">Erosion</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>&nbsp;Filters</td>\n<td>\n<ul>\n<li><a href=\"/code/filtreleme/python/guasian.py\">Guassian</a></li>\n<li><a href=\"/code/filtreleme/python/sobel.py\">Sobel</a></li>\n<li><a href=\"/code/filtreleme/python/laplacian-sobel.py\">Laplacian &amp; Sobel</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>&nbsp;Edge Detection</td>\n<td>\n<ul>\n<li><a href=\"/code/kenar-belirleme/python/canny.py\">Canny</a></li>\n<li><a href=\"code/kenar-belirleme/python/kenar-belirleme-toplu.py\">GaussianBlur &amp; Canny &amp; Sobel &amp;&nbsp;Prewitt</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n  <td><ul>\n<li><a href=\"/code/kenar-belirleme/javascript/edge_detector.html\">Canny</a></li>\n</ul></td>\n</tr>\n<tr>\n<td>Background Subtraction</td>\n<td>\n<ul>\n<li><a href=\"/code/arkaplan-cikarma/python/absdif.py\">Absdiff</a></li>\n<li><a href=\"/code/arkaplan-cikarma/python/background-subtractor-gmg.py\">BackgroundSubtractorMOG - MOG2 - GMG</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Video Analysis and Object Tracking</td>\n<td>\n<ul>\n<li><a href=\"/code/video-analiz/python/meanshift.py\">MeanShift</a></li>\n <li><a href=\"/code/video-analiz/python/camshift.py\">CamShift</a></li>\n</ul>\n</td>\n<td>\n<ul>\n<li><a href=\"https://github.com/mesutpiskin/opencv-object-detection/tree/master/src/ColorBasedObjectTracker\">Color-Based Object Tracking</a></li>\n</ul>\n\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n\n<tr>\n<td>Object Detection and Object Recognition</td>\n<td>\n<ul>\n<li><a href=\"/code/nesne-tespit-ve-tanima/python/template-matching.py\">Template Matching</a></li>\n<li><a href=\"/code/nesne-tespit-ve-tanima/python/derin_sinir_agi.py\">Object Recognition with Deep Neural Network DNN</a></li>\n<li><a href=\"/code/nesne-tespit-ve-tanima/python/haar-cascade.py\">Face Detection with Haar Cascade Classifier</a></li>\n</ul>\n</td>\n<td>\n\n<ul>\n<li><a href=\"/code/nesne-tespit-ve-tanima/java/TemplateMatching.java\">Object Recognition with Template Matching</a></li>\n<li><a href=\"https://github.com/mesutpiskin/opencv-object-detection/tree/master/src/DeepNeuralNetwork\">Object Recognition with Deep Neural Network DNN</a></li>\n<li><a href=\"/code/nesne-tespit-ve-tanima/java/DetectFace.java\">Face Detection with Haar Cascade Classifier</a></li>\n\n</ul>\n\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>Feature Extraction</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Camera Calibration and 3D Vision</td>\n<td>\n<ul>\n<li><a href=\"/code/kamera-kalibrasyon-3d-goru/python/fisheye-lens-duzeltme.py\">Fisheye Image Correction</a></li>\n<li><a href=\"/code/kamera-kalibrasyon-3d-goru/python/kamera-kalibrasyonu.py\">Camera Calibration</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n\n<tr>\n<td>Machine Learning and Deep Learning</td>\n<td>\n<ul>\n<li><a href=\"/code/yuz-tanima/python/dnn_yuz_tespiti/\">Face Detection with Deep Neural Network (DNN)</a></li>\n</ul>\n\n\n</td>\n<td>\n<ul>\n<li><a href=\"https://github.com/mesutpiskin/GenderClassification\">Gender Detection with Deep Neural Network (DNN)</a></li>\n</ul>\n\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>Face recognition</td>\n<td>\n<ul>\n<li><a href=\"/code/yuz-tanima/python/facerecognition_kutuphanesi/\">Face Recognition with \"FaceRecognition Lib\"</a></li>\n<li><a href=\"/code/yuz-tanima/python/facenet/\">Face Recognition with FaceNet</a></li>\n</ul>\n\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>\n<ul>\n<li><a href=\"https://github.com/mesutpiskin/face-detection-and-recognition\">Face Recognition with Eigenfaces and Fisherfaces</a></li>\n</ul>\n\n\n\n</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>Optical Character Recognition OCR</td>\n<td>\n<ul>\n<li><a href=\"/code/optik-karakter-tanima-ocr/python/tesseract-python/\">OCR with Tesseract</a></li>\n</ul>\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>GPU and Parallel Computing</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>OpenCV MobilE</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n <td>&nbsp;</td>\n</tr>\n<tr>\n<td>Augmented Reality and Image Blending</td>\n<td>\n<ul>\n<li><a href=\"/code/arttirilmis-gerceklik/python/sapka-filtresi/\">Hat Filter - Face Detection and Hat Adding</a></li>\n</ul>\n\n\n</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n</tbody>\n</table>\n\n\n## Contribute\n\nIt's really nice to think about contributing, so you can look at the [Contribute](/CONTRIBUTING.en-US.md) link.\n\n\n## Question & Answer\nYou can ask questions, source codes or any other questions by using the **issues** section (new issues). You can also use this section if you want to answer a question or check out the previous ones.\n\n## License\nDocuments and source codes contained in this project are licensed  [MIT License](/LICENSE.en-US.md).\n"
 },
 {
  "repo": "atilimcetin/guided-filter",
  "language": "C++",
  "readme_contents": "# Guided filter for OpenCV\n\nGuided filter is an edge-preserving smoothing filter like the bilateral filter. It is straightforward to implement and has linear complexity independent of the kernel size. For more details about this filter see [[Kaiming10]](http://research.microsoft.com/en-us/um/people/kahe/eccv10/).\n\n\n## Usage\n\nThe interface consists of one simple function `guidedFilter` and a class `GuidedFilter`. If you have multiple images to filter with the same guidance image then use `GuidedFilter` class to avoid extra computations on initialization stage. The code supports single-channel and 3-channel (color) guidance images and `CV_8U`, `CV_8S`, `CV_16U`, `CV_16S`, `CV_32S`, `CV_32F` and `CV_64F` data types.\n\n\n## Examples\n\nThese examples are adapted from the [original MATLAB implementation](http://research.microsoft.com/en-us/um/people/kahe/eccv10/guided-filter-code-v1.rar).\n\n### Smoothing\n\n```c++\ncv::Mat I = cv::imread(\"./img_smoothing/cat.bmp\", CV_LOAD_IMAGE_GRAYSCALE);\ncv::Mat p = I;\n\nint r = 4; // try r=2, 4, or 8\ndouble eps = 0.2 * 0.2; // try eps=0.1^2, 0.2^2, 0.4^2\n\neps *= 255 * 255;   // Because the intensity range of our images is [0, 255]\n\ncv::Mat q = guidedFilter(I, p, r, eps);\n```\n\n[![Cat](http://atilimcetin.com/guided-filter/img_smoothing/cat-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat.png)\n\n[![r=2, eps=0.1^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.1-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.1.png)\n[![r=2, eps=0.2^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.2-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.2.png)\n[![r=2, eps=0.4^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.4-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-2-0.4.png)\n\n[![r=4, eps=0.1^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.1-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.1.png)\n[![r=4, eps=0.2^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.2-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.2.png)\n[![r=4, eps=0.4^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.4-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-4-0.4.png)\n\n[![r=8, eps=0.1^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.1-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.1.png)\n[![r=8, eps=0.2^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.2-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.2.png)\n[![r=8, eps=0.4^2](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.4-small.png)](http://atilimcetin.com/guided-filter/img_smoothing/cat-8-0.4.png)\n\n\n### Flash/no-flash denoising\n\n```c++\ncv::Mat I = cv::imread(\"./img_flash/cave-flash.bmp\", CV_LOAD_IMAGE_COLOR);\ncv::Mat p = cv::imread(\"./img_flash/cave-noflash.bmp\", CV_LOAD_IMAGE_COLOR);\n\nint r = 8;\ndouble eps = 0.02 * 0.02;\n\neps *= 255 * 255;   // Because the intensity range of our images is [0, 255]\n\ncv::Mat q = guidedFilter(I, p, r, eps);\n```\n\n[![Cave Flash](http://atilimcetin.com/guided-filter/img_flash/cave-flash-small.png)](http://atilimcetin.com/guided-filter/img_flash/cave-flash.png)\n[![Cave No Flash](http://atilimcetin.com/guided-filter/img_flash/cave-noflash-small.png)](http://atilimcetin.com/guided-filter/img_flash/cave-noflash.png)\n[![Cave Denoised](http://atilimcetin.com/guided-filter/img_flash/cave-denoised-small.png)](http://atilimcetin.com/guided-filter/img_flash/cave-denoised.png)\n\n\n### Feathering\n\n```c++\ncv::Mat I = cv::imread(\"./img_feathering/toy.bmp\", CV_LOAD_IMAGE_COLOR);\ncv::Mat p = cv::imread(\"./img_feathering/toy-mask.bmp\", CV_LOAD_IMAGE_GRAYSCALE);\n\nint r = 60;\ndouble eps = 1e-6;\n\neps *= 255 * 255;   // Because the intensity range of our images is [0, 255]\n\ncv::Mat q = guidedFilter(I, p, r, eps);\n```\n\n[![Mask](http://atilimcetin.com/guided-filter/img_feathering/toy-mask-small.png)](http://atilimcetin.com/guided-filter/img_feathering/toy-mask.png)\n[![Guidance](http://atilimcetin.com/guided-filter/img_feathering/toy-small.png)](http://atilimcetin.com/guided-filter/img_feathering/toy.png)\n[![Feathering](http://atilimcetin.com/guided-filter/img_feathering/toy-feather-small.png)](http://atilimcetin.com/guided-filter/img_feathering/toy-feather.png)\n\n\n### Enhancement\n\n```c++\ncv::Mat I = cv::imread(\"./img_enhancement/tulips.bmp\", CV_LOAD_IMAGE_COLOR);\nI.convertTo(I, CV_32F, 1.0 / 255.0);\n\ncv::Mat p = I;\n\nint r = 16;\ndouble eps = 0.1 * 0.1;\n\ncv::Mat q = guidedFilter(I, p, r, eps);\n\ncv::Mat I_enhanced = (I - q) * 5 + q;\n```\n\n[![Tulip](http://atilimcetin.com/guided-filter/img_enhancement/tulips-small.png)](http://atilimcetin.com/guided-filter/img_enhancement/tulips.png)\n[![Smoothed](http://atilimcetin.com/guided-filter/img_enhancement/tulips-smoothed-small.png)](http://atilimcetin.com/guided-filter/img_enhancement/tulips-smoothed.png)\n[![Enhanced](http://atilimcetin.com/guided-filter/img_enhancement/tulips-enhanced-small.png)](http://atilimcetin.com/guided-filter/img_enhancement/tulips-enhanced.png)\n\n\n## License\n\nMIT license.\n\n"
 },
 {
  "repo": "pablosproject/iPhone-OCR-Tesseract-and-OpenCV",
  "language": "Objective-C",
  "readme_contents": "iPhone-OCR-Tesseract-and-OpenCV\n===============================\n\n[![Join the chat at https://gitter.im/pablosproject/iPhone-OCR-Tesseract-and-OpenCV](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pablosproject/iPhone-OCR-Tesseract-and-OpenCV?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\nThis is a sample project created by me (@PablosPoject) and @_AJ_R for academic purpose.\nIt use the OpenCV framework and tutorial made by BloodAxe(https://github.com/BloodAxe) and some other utilities class made by Aptogo (https://github.com/aptogo).\nIt also uses the Tesseract OCR engine to read the text processed with openCV.\n\nI also build a simple user interface that permit to take a photo or choose one from library, and also permit to apply to the image every single step in the image processing, or to apply directly all the processing.\n\nI hope that these can be useful for someone!\nEnjoy!\n\n#Usage\nDownload the project and use [CocoaPods](http://cocoapods.org/) to install required libraries, by running `pod install`.\n\nContact Me\n===============================\npablosproject@gmail.com\n@PablosProject\n\nRelease under GPL\n\n[![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/pablosproject/iphone-ocr-tesseract-and-opencv/trend.png)](https://bitdeli.com/free \"Bitdeli Badge\")\n\n"
 },
 {
  "repo": "ros-perception/vision_opencv",
  "language": "C++",
  "readme_contents": "vision_opencv\n=============\n\n.. image:: https://travis-ci.org/ros-perception/vision_opencv.svg?branch=indigo\n    :target: https://travis-ci.org/ros-perception/vision_opencv\n\nPackages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision.\n"
 }
]