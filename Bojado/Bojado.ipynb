{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prepare'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-54eea3f06007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prepare'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "from requests import get\n",
    "from os import path\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "from env import github_token, github_username\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project, you will have to build a dataset yourself. Decide on a list of GitHub repositories to scrape, and write the python code necessary to extract the text of the README file for each page, and the primary language of the repository.\n",
    "\n",
    "#### Which repositories you use are up to you, but you should include at least 100 repositories in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/search?o=desc&p={num}&q=OpenCV&s=stars&type=Repositories'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ac.get_all_urls() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 repos\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_count = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "language_count.columns = ['n', 'percent']\n",
    "language_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df.readme_contents.apply(prepare.basic_clean)\n",
    "df['text_tokenized'] = df.text_cleaned.apply(prepare.tokenize)\n",
    "df['text_lemmatized'] = df.text_tokenized.apply(prepare.lemmatize)\n",
    "df['text_filtered'] = df.text_lemmatized.apply(prepare.remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a|b)\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.text_filtered]\n",
    "# words \n",
    "\n",
    "# column name will be words, and the column will contain lists of the words in each doc\n",
    "df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common words in READMEs?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_words = ' '.join(df[df.language=='Python'].text_filtered)\n",
    "c_plus_plus_words = ' '.join(df[df.language=='C++'].text_filtered)\n",
    "all_words = ' '.join(df.text_filtered)\n",
    "\n",
    "python_words = re.sub(r'\\s.\\s', '', python_words)\n",
    "c_plus_plus_words = re.sub(r'\\s.\\s', '', c_plus_plus_words)\n",
    "all_words = re.sub(r'\\s.\\s', '', all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_freq = pd.Series(python_words.split()).value_counts()\n",
    "c_plus_plus_freq = pd.Series(c_plus_plus_words.split()).value_counts()\n",
    "all_freq = pd.Series(all_words.split()).value_counts()\n",
    "\n",
    "all_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([all_freq, python_freq, c_plus_plus_freq], axis=1, sort=True)\n",
    "               .set_axis(['All', 'Python', 'C++'], axis=1, inplace=False)\n",
    "               .fillna(0)\n",
    "               .apply(lambda s: s.astype(int))\n",
    "              )\n",
    "\n",
    "word_counts.sort_values(by='All', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_cloud = WordCloud(background_color='white',\n",
    "                      height=800, width=800).generate(python_words)\n",
    "\n",
    "c_plus_plus_cloud = WordCloud(background_color='white', \n",
    "                      height=800, width=800).generate(c_plus_plus_words)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "axs = [plt.axes([.25, 1, .5, .5]), plt.axes([.8, 1, .5, .5])]\n",
    "\n",
    "# imshow => display data as an image\n",
    "axs[0].imshow(python_cloud)\n",
    "axs[1].imshow(c_plus_plus_cloud)\n",
    "\n",
    "axs[0].set_title('Python')\n",
    "axs[1].set_title('C++')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df.words]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_bigrams = pd.Series(list(nltk.ngrams(python_words.split(), 2))).value_counts().head(20) \n",
    "c_plus_plus_bigrams = pd.Series(list(nltk.ngrams(c_plus_plus_words.split(), 2))).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_data = {k[0] + ' ' + k[1]: v for k, v in python_bigrams.to_dict().items()}\n",
    "c_plus_plus_data = {k[0] + ' ' + k[1]: v for k, v in c_plus_plus_bigrams.to_dict().items()}\n",
    "\n",
    "python_bigram_cloud = WordCloud(background_color='white', \n",
    "                width=800, height=400).generate_from_frequencies(python_data)\n",
    "\n",
    "c_plus_plus_bigram_cloud = WordCloud(background_color='white', \n",
    "                width=800, height=400).generate_from_frequencies(c_plus_plus_data)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "axs = [plt.axes([.25, 1, .5, .5]), plt.axes([.8, 1, .5, .5])]\n",
    "\n",
    "axs[0].imshow(python_bigram_cloud)\n",
    "axs[1].imshow(c_plus_plus_bigram_cloud)\n",
    "\n",
    "axs[0].set_title('Python Bigram')\n",
    "axs[1].set_title('C++ Bigram')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trigrams'] = [list(nltk.ngrams(wordlist, 3)) for wordlist in df.words]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_trigrams = pd.Series(list(nltk.ngrams(python_words.split(), 3))).value_counts().head(20)\n",
    "c_plus_plus_trigrams = pd.Series(list(nltk.ngrams(c_plus_plus_words.split(), 3))).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_tridata = {k[0] + ' ' + k[1]+ ' ' + k[2]: v for k, v in python_trigrams.to_dict().items()}\n",
    "c_plus_plus_tridata = {k[0] + ' ' + k[1]+ ' ' + k[2]: v for k, v in c_plus_plus_trigrams.to_dict().items()}\n",
    "\n",
    "python_trigram_cloud = WordCloud(background_color='white', \n",
    "                width=800, height=400).generate_from_frequencies(python_tridata)\n",
    "\n",
    "c_plus_plus_trigram_cloud = WordCloud(background_color='white', \n",
    "                width=800, height=400).generate_from_frequencies(c_plus_plus_tridata)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "axs = [plt.axes([.25, 1, .5, .5]), plt.axes([.8, 1, .5, .5])]\n",
    "\n",
    "axs[0].imshow(python_trigram_cloud)\n",
    "axs[1].imshow(c_plus_plus_trigram_cloud)\n",
    "\n",
    "axs[0].set_title('Python Trigram')\n",
    "axs[1].set_title('C++ Trigram')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the distribution of IDFs look like for the most common words?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.plot.barh(color='pink', width=.9, figsize=(10, 10), alpha=.8)\n",
    "plt.title('20 most frequently occurring words')\n",
    "plt.ylabel('Trigram')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the length of the README vary by programming language?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do different programming languages use a different number of unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform your documents into a form that can be used in a machine learning model. You should use the programming language of the repository as the label to predict.  \n",
    "\n",
    "\n",
    "- Try fitting several different models and using several different representations of the text (e.g. a simple bag of words, then also the TF-IDF values for each).   \n",
    "\n",
    "\n",
    "- Build a function that will take in the text of a README file, and tries to predict the programming language.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion/Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though OpenCV is primarly written in C++, words repos are written in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
